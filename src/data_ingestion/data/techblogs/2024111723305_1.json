[{"id": 91656, "date": "2024-11-11T11:50:19", "date_gmt": "2024-11-11T19:50:19", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91656"}, "modified": "2024-11-11T11:50:23", "modified_gmt": "2024-11-11T19:50:23", "slug": "developing-a-172b-llm-with-strong-japanese-capabilities-using-nvidia-megatron-lm", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/developing-a-172b-llm-with-strong-japanese-capabilities-using-nvidia-megatron-lm/", "title": {"rendered": "Developing a 172B LLM with Strong Japanese Capabilities Using NVIDIA Megatron-LM"}, "content": {"rendered": "\n<p>Generative AI has the ability to create entirely new content that traditional machine learning (ML) methods struggle to produce. In the field of natural language processing (NLP), the advent of <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\" target=\"_blank\" rel=\"noreferrer noopener\">large language models (LLMs)</a> specifically has led to many innovative and creative AI use cases. These include customer support chatbots, voice assistants, text summarization and translation, and more\u2014tasks previously handled by humans.</p>\n\n\n\n<p>LLMs continue to evolve through various approaches, including increasing the number of parameters and the adoption of new algorithms like Mixture of Experts (MoE). The application and adaptation of LLMs are anticipated across many industries, including retail, manufacturing, and finance.</p>\n\n\n\n<p>However, many models that currently top the LLM leaderboard show insufficient understanding and performance in non-English languages, including Japanese. One of the reasons for this is that the training corpus contains a high proportion of English data. For example, <a href=\"https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv\">only 0.11% of the GPT-3 corpus is Japanese data</a>. Creating LLM models that perform well in Japanese, which has less training data than English, has been immensely challenging.</p>\n\n\n\n<p>This post presents insights gained from training an AI model with 172 billion parameters as part of the <a href=\"https://www.meti.go.jp/english/policy/mono_info_service/geniac/index.html\">Generative AI Accelerator Challenge (GENIAC)</a> project, using <a href=\"https://github.com/NVIDIA/Megatron-LM\">NVIDIA Megatron-LM</a> to help address the shortage of high-performance models for Japanese language understanding.</p>\n\n\n\n<h2 id=\"llm-jp_initiatives_at_geniac\"  class=\"wp-block-heading\">LLM-jp initiatives at GENIAC<a href=\"#llm-jp_initiatives_at_geniac\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The <a href=\"https://www.meti.go.jp/english/\">Ministry of Economy, Trade and Industry (METI)</a> launched GENIAC to raise the level of platform model development capability in Japan and to encourage companies and others to be creative. GENIAC has provided computational resources, supported matching with companies and data holders, fostered collaboration with global technology companies, held community events, and evaluated the performance of the developed platform models.</p>\n\n\n\n<p>The <a href=\"https://llm-jp.nii.ac.jp/en/\">LLM-jp</a> project to develop a completely <a href=\"https://huggingface.co/llm-jp/llm-jp-3-172b-beta1\">open model with 172 billion parameters</a> (available on Hugging Face) with strong Japanese language capabilities was selected for the GENIAC initiative. LLM-jp 172B was the largest model development in Japan at that time (February to August 2024), and it was meaningful to share the knowledge of its development widely.</p>\n\n\n\n<p>LLM-jp is an initiative launched by researchers in the field of natural language processing and computer systems, mainly at NII, to accumulate know-how on the mathematical elucidation of training principles, such as how large-scale models acquire generalization performance and the efficiency of learning, through the continuous development of models that are completely open and commercially available. The objective is to accumulate know-how on the efficiency of training.</p>\n\n\n\n<h2 id=\"training_the_model_using_nvidia_megatron-lm\"  class=\"wp-block-heading\">Training the model using NVIDIA Megatron-LM<a href=\"#training_the_model_using_nvidia_megatron-lm\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://github.com/NVIDIA/Megatron-LM\">Megatron-LM</a> serves as a lightweight research-oriented framework leveraging <a href=\"https://developer.nvidia.com/megatron-core\">Megatron-Core</a> for training LLMs at unparalleled speed. Megatron-Core, the main component, is an open-source library that contains GPU-optimized techniques and cutting-edge system-level optimizations essential for large-scale training.</p>\n\n\n\n<p>Megatron-Core supports various advanced model parallelism techniques, including tensor, sequence, pipeline, context, and MoE expert parallelism. This library offers <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/nemo_megatron/mcore_customization.html\">customizable building blocks</a>, training resiliency features such as <a href=\"https://developer.nvidia.com/blog/train-generative-ai-models-more-efficiently-with-new-nvidia-megatron-core-functionalities/\">fast distributed checkpointing</a>, and many other innovations such as <a href=\"https://github.com/NVIDIA/Megatron-LM/tree/ssm/examples/mamba\">Mamba-based hybrid model training</a>. It\u2019s compatible with all NVIDIA Tensor Core GPUs, and includes support for <a href=\"https://github.com/NVIDIA/TransformerEngine\">Transformer Engine (TE)</a> with FP8 precision introduced with <a href=\"https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/\">NVIDIA Hopper architecture</a>.</p>\n\n\n\n<h3 id=\"model_architecture_and_training_settings\"  class=\"wp-block-heading\">Model architecture and training settings<a href=\"#model_architecture_and_training_settings\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Table 1 provides an overview of the model architecture for this project, which follows <a href=\"https://llama.meta.com/llama2/\">Llama 2 architecture</a>.</p>\n\n\n\n<figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Parameter</strong></td><td><strong>Value</strong></td></tr><tr><td>Hidden size</td><td>12288</td></tr><tr><td>FFN intermediate size</td><td>38464</td></tr><tr><td>Number of layers</td><td>96</td></tr><tr><td>Number of attention heads</td><td>96</td></tr><tr><td>Number of query groups</td><td>16</td></tr><tr><td>Activation function</td><td>SwiGLU</td></tr><tr><td>Position embedding</td><td>RoPE</td></tr><tr><td>Normalization</td><td>RMSNorm</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Overview of LLM-jp 172B model architecture</em></figcaption></figure>\n\n\n\n<p>The LLM-jp 172B model is being trained from scratch using 2.1 trillion tokens of a multilingual corpus developed for the project consisting mainly of Japanese and English. The training is performed using NVIDIA H100 Tensor Core GPUs on Google Cloud A3 Instance with FP8 hybrid training using the Transformer Engine. Megatron-Core v0.6 and Transformer Engine v1.4 are used in the experiment.</p>\n\n\n\n<p>Table 2 shows hyperparameter settings for training.</p>\n\n\n\n<figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Parameter</strong></td><td><strong>Value</strong></td></tr><tr><td>LR</td><td>1E-4</td></tr><tr><td>min LR</td><td>1E-5</td></tr><tr><td>LR WARMUP iters</td><td>2000</td></tr><tr><td>Weight decay</td><td>0.1</td></tr><tr><td>Grad clip</td><td>1.0</td></tr><tr><td>Global batch size</td><td>1728</td></tr><tr><td>Context length</td><td>4096</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Hyperparameters used for the model training</em></figcaption></figure>\n\n\n\n<p>In addition, z-loss and batch-skipping techniques, which are used in <a href=\"https://arxiv.org/abs/2204.02311\">PaLM</a>, are incorporated to stabilize the training process, and flash attention is used to further speed up the training process.</p>\n\n\n\n<p>To view other training configurations, please see <a href=\"https://github.com/llm-jp/Megatron-LM/blob/nii-geniac/scripts/gcp/Llama-2-175B/llama-2-175b.sh\">llm-jp/Megatron-LM</a>.</p>\n\n\n\n<h2 id=\"training_throughput_and_results&nbsp;\"  class=\"wp-block-heading\"><strong>Training throughput and results&nbsp;</strong><a href=\"#training_throughput_and_results&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Pretraining for the latest LLM-jp 172B model is currently underway, with periodic evaluations every few thousand iterations to monitor training progress and ensure successful accuracy results on Japanese and English downstream tasks (Figure 1). So far, over 80% is complete, of the targeted 2.1 trillion tokens.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1000\" height=\"600\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-Training-loss-curves-for-pretraining-with-1.7-trillion-tokens-using-Megatron-FP8-hybrid-training-.png\" alt=\"Graph showing training tokens (x-axis) and training loss (y-axis), with the curve showing a steady decrease in loss up to 240,000 steps.\" class=\"wp-image-91657\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-Training-loss-curves-for-pretraining-with-1.7-trillion-tokens-using-Megatron-FP8-hybrid-training-.png 1000w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-Training-loss-curves-for-pretraining-with-1.7-trillion-tokens-using-Megatron-FP8-hybrid-training--300x180.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-Training-loss-curves-for-pretraining-with-1.7-trillion-tokens-using-Megatron-FP8-hybrid-training--625x375.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-Training-loss-curves-for-pretraining-with-1.7-trillion-tokens-using-Megatron-FP8-hybrid-training--179x107.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-Training-loss-curves-for-pretraining-with-1.7-trillion-tokens-using-Megatron-FP8-hybrid-training--768x461.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-Training-loss-curves-for-pretraining-with-1.7-trillion-tokens-using-Megatron-FP8-hybrid-training--645x387.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-Training-loss-curves-for-pretraining-with-1.7-trillion-tokens-using-Megatron-FP8-hybrid-training--500x300.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-Training-loss-curves-for-pretraining-with-1.7-trillion-tokens-using-Megatron-FP8-hybrid-training--150x90.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-Training-loss-curves-for-pretraining-with-1.7-trillion-tokens-using-Megatron-FP8-hybrid-training--362x217.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-Training-loss-curves-for-pretraining-with-1.7-trillion-tokens-using-Megatron-FP8-hybrid-training--183x110.png 183w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Training loss curves for pretraining with 1.7 trillion tokens using Megatron FP8 hybrid training&nbsp;</em></figcaption></figure></div>\n\n\n<p>Notably, there is a sharp increase in TFLOP/s after approximately 7,000 iterations, corresponding to the transition from BF16 to FP8-hybrid precision. In this experiment, BF16 plus TE was used for training before 7,000 iterations, and FP8 hybrid plus TE was used after 7,000 iterations. In Megatron-LM, it is possible to enable hybrid FP8 training with the simple option <code>--fp8-format</code> &#8216;<code>hybrid</code>&#8216;. Note that this feature is experimental, with further optimizations coming soon.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXciy0pbqXP0TyttGb8Ln0GZJz5Zxalrk9xBupsl1Ju-C1PfhW3PVkBmP208HS7Y_20yCEgRJDaS5ouVkVG3ujeA4gNCzB9ViLfPyKv9C83EBoPopoqwjr1K8qO6WN8gQsa8NFKTNKNaa8BAwUmUcMUlZkfA?key=V-y0PkGDqc6FmuKJY_JOXQ\" alt=\"Graph showing TFLOP/s on the y-axis and the number of iterations on the x-axis.\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Training throughput (TFLOP/s) when TE is used with BF16 and FP8 hybrid</em></figcaption></figure></div>\n\n\n<p>The reason we started the training with BF16 plus TE and then switched to FP8 hybrid was not only to see the tokens/sec performance difference between BF16 and FP8, but also to make the initial training more stable. In the early stages of training, the learning rate (LR) increases due to the warm-up, leading to unstable training.</p>\n\n\n\n<p>We chose to perform the initial training with BF16, and after confirming that there were no problems with the values of training loss, optimizer states, gradient norm, and so on, we switched to FP8 to speed up the training process. FP8 hybrid has improved the training speed. We observed a training speed of 545-553 TFLOP/s with Megatron-LM.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1000\" height=\"600\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-4.-Weak-scaling-performance-based-on-the-results-of-the-main-and-preliminary-experiments-of-the-LLM-jp-172B-model-training.png\" alt=\"Graph with y-axis representing Aggregate Throughput, and x-axis representing the number of GPUs used in the training. The training results of Llama 2 7B, Llama 2 13B, and LLM-jp 172B exhibit linear scaling.\" class=\"wp-image-91659\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-4.-Weak-scaling-performance-based-on-the-results-of-the-main-and-preliminary-experiments-of-the-LLM-jp-172B-model-training.png 1000w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-4.-Weak-scaling-performance-based-on-the-results-of-the-main-and-preliminary-experiments-of-the-LLM-jp-172B-model-training-300x180.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-4.-Weak-scaling-performance-based-on-the-results-of-the-main-and-preliminary-experiments-of-the-LLM-jp-172B-model-training-625x375.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-4.-Weak-scaling-performance-based-on-the-results-of-the-main-and-preliminary-experiments-of-the-LLM-jp-172B-model-training-179x107.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-4.-Weak-scaling-performance-based-on-the-results-of-the-main-and-preliminary-experiments-of-the-LLM-jp-172B-model-training-768x461.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-4.-Weak-scaling-performance-based-on-the-results-of-the-main-and-preliminary-experiments-of-the-LLM-jp-172B-model-training-645x387.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-4.-Weak-scaling-performance-based-on-the-results-of-the-main-and-preliminary-experiments-of-the-LLM-jp-172B-model-training-500x300.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-4.-Weak-scaling-performance-based-on-the-results-of-the-main-and-preliminary-experiments-of-the-LLM-jp-172B-model-training-150x90.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-4.-Weak-scaling-performance-based-on-the-results-of-the-main-and-preliminary-experiments-of-the-LLM-jp-172B-model-training-362x217.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-4.-Weak-scaling-performance-based-on-the-results-of-the-main-and-preliminary-experiments-of-the-LLM-jp-172B-model-training-183x110.png 183w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Weak scaling performance based on the results of the main and preliminary experiments of the LLM-jp 172B model training</em></figcaption></figure></div>\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\"><strong>Conclusion</strong><a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>As mentioned above, the training of LLM-jp 172B&nbsp; is still ongoing using Megatron-LM. Based on the evaluation results of downstream tasks using the current checkpoint data, we suppose that the model has already acquired excellent Japanese language capabilities, but the complete model is expected to be ready early next year.Training time is often a significant challenge in pretraining LLMs, where vast datasets are required.Therefore, efficient training frameworks like Megatron-LM are crucial for accelerating Generative AI research and development.\u3000For the 172B model trained with <a href=\"https://github.com/NVIDIA/Megatron-LM\">Megatron-LM</a>, we explored FP8-hybrid training as a potential method for improving training speed, achieving a 1.4x training speed acceleration from 400 TFLOP/s to 550 TFLOP/s. We observed a performance acceleration from 400 TFLOP/s to 550 TFLOP/s, suggesting that FP8-hybrid could be a valuable approach for enhancing the efficiency of large-scale model pretraining.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Generative AI has the ability to create entirely new content that traditional machine learning (ML) methods struggle to produce. In the field of natural language processing (NLP), the advent of large language models (LLMs) specifically has led to many innovative and creative AI use cases. These include customer support chatbots, voice assistants, text summarization and &hellip; <a href=\"https://developer.nvidia.com/blog/developing-a-172b-llm-with-strong-japanese-capabilities-using-nvidia-megatron-lm/\">Continued</a></p>\n", "protected": false}, "author": 2417, "featured_media": 91661, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1518096", "discourse_permalink": "https://forums.developer.nvidia.com/t/developing-a-172b-llm-with-strong-japanese-capabilities-using-nvidia-megatron-lm/312888", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [3266, 2932, 3270], "coauthors": [4165, 2254], "class_list": ["post-91656", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "tag-chatbot", "tag-large-language-models", "tag-generative-ai-text"], "acf": {"post_industry": ["General"], "post_products": ["Hopper"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-megatron-mini-beat-promo-li-tw-2048x1024-copy.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nQk", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91656"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2417"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91656"}], "version-history": [{"count": 3, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91656/revisions"}], "predecessor-version": [{"id": 91668, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91656/revisions/91668"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91661"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91656"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91656"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91656"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91656"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91625, "date": "2024-11-08T15:55:43", "date_gmt": "2024-11-08T23:55:43", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91625"}, "modified": "2024-11-08T16:35:43", "modified_gmt": "2024-11-09T00:35:43", "slug": "5x-faster-time-to-first-token-with-nvidia-tensorrt-llm-kv-cache-early-reuse", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/5x-faster-time-to-first-token-with-nvidia-tensorrt-llm-kv-cache-early-reuse/", "title": {"rendered": "5x Faster Time to First Token with NVIDIA TensorRT-LLM KV Cache Early Reuse"}, "content": {"rendered": "\n<p>In our previous <a href=\"https://developer.nvidia.com/blog/nvidia-gh200-superchip-accelerates-inference-by-2x-in-multiturn-interactions-with-llama-models/\">blog post</a>, we demonstrated how reusing the key-value (KV) cache by offloading it to CPU memory can accelerate time to first token (TTFT) by up to 14x on x86-based NVIDIA H100 Tensor Core GPUs and 28x on the NVIDIA GH200 Superchip. In this post, we shed light on KV cache reuse techniques and best practices that can drive even further TTFT speedups.</p>\n\n\n\n<h2 id=\"introduction_to_kv_cache&nbsp;\"  class=\"wp-block-heading\">Introduction to KV cache&nbsp;<a href=\"#introduction_to_kv_cache&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>LLM models are rapidly being adopted for many tasks, including question-answering, and code generation. To generate a response, these models begin by converting the user\u2019s prompt into tokens, which are then transformed into dense vectors. Extensive dot-product operations follow to mathematically model the relationships between the tokens and build a contextual understanding of the user input. The computational cost of generating this contextual understanding increases quadratically with the length of the input sequence.</p>\n\n\n\n<p>This resource-intensive process generates keys and values, which are cached to avoid recomputation when generating subsequent tokens. Reusing the KV cache reduces the computational load and time needed to generate additional tokens\u2014leading to a faster and more efficient user experience.</p>\n\n\n\n<p>When reusing the KV cache, careful attention must be given to how long it remains in memory, which components to evict first when memory is full, and when it can be reused for new incoming prompts. Optimizing these factors can lead to incremental performance improvements in KV cache reuse. NVIDIA TensorRT-LLM offers three key features that specifically address these areas.</p>\n\n\n\n<h2 id=\"early_kv_cache_reuse\"  class=\"wp-block-heading\">Early KV cache reuse<a href=\"#early_kv_cache_reuse\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Traditional reuse algorithms require the entire KV cache computation to be completed before any portions of it can be reused with new user prompts. In scenarios such as enterprise chatbots, where system prompts\u2014predefined instructions added to user queries\u2014are essential to direct the LLM\u2019s responses in line with enterprise guidelines, this method can be inefficient.&nbsp;</p>\n\n\n\n<p>When a surge of users interacts with the chatbot simultaneously, each user would require a separate computation of the system prompt KV cache. With TensorRT-LLM, we can instead reuse the system prompt as it is being generated in real time, enabling it to be shared across all users during the burst, rather than recalculating it for each user. This can significantly accelerate inference for use cases requiring system prompts by up to 5x.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1271\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache.png\" alt=\"A bar Chart showing time to first token speedup with and without TensorRT-LLM KV cache reuse.\" class=\"wp-image-91645\" style=\"width:840px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache-300x191.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache-625x397.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache-768x488.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache-1536x977.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache-645x410.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache-472x300.png 472w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache-142x90.png 142w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache-362x230.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache-173x110.png 173w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/TRT-KV-cache-1024x651.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. TensorRT-LLM KV cache reuse can speed up TTFT by up to 5x</em><br><br></figcaption></figure></div>\n\n\n<h2 id=\"flexible_kv_cache_block_sizing&nbsp;\"  class=\"wp-block-heading\">Flexible KV cache block sizing&nbsp;<a href=\"#flexible_kv_cache_block_sizing&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In reuse implementations, only entire cache memory blocks can be allocated for reuse. For example, if the cache memory block size is 64 tokens and KV cache is 80 tokens, only 64 tokens will be stored for reuse, while the remaining 16 tokens will need to be recomputed. However, if the memory block size is reduced to 16 tokens, all 64 tokens can be stored across five memory blocks, eliminating the need for re-computation.&nbsp;</p>\n\n\n\n<p>This effect is most pronounced when the input sequences are short. For long input sequences, larger blocks can be more beneficial.&nbsp; As is clear, the more granular the control you have over the KV cache, the better you can optimize it for your specific use case.&nbsp;</p>\n\n\n\n<p>TensorRT-LLM provides fine-grained control over KV cache memory blocks, giving developers the ability to chop them into smaller blocks between 64 to 2 tokens. This optimizes the usage of allocated memory, increases reuse rates, and improves TTFT. When running LLAMA70B on NVIDIA H100 Tensor Core GPUs, we can speed up TTFT up to 7% in multi-user environments by reducing KV cache block size from 64 tokens to 8 tokens.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1971\" height=\"1351\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size.png\" alt=\"A bar chart showing time to first token speedup with varying KV cache block sizes.\" class=\"wp-image-91626\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size.png 1971w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size-300x206.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size-625x428.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size-168x115.png 168w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size-768x526.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size-1536x1053.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size-645x442.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size-438x300.png 438w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size-131x90.png 131w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size-362x248.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size-160x110.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/KV-cache-block-size-1024x702.png 1024w\" sizes=\"(max-width: 1971px) 100vw, 1971px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Impact of changing KV cache block size on inference speedup</em></figcaption></figure></div>\n\n\n<h2 id=\"efficient_kv_cache_eviction_protocols\"  class=\"wp-block-heading\">Efficient KV cache eviction protocols<a href=\"#efficient_kv_cache_eviction_protocols\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Partitioning the KV cache into smaller blocks and evicting unused ones can be effective for memory optimization, but it introduces dependency complexities. When a specific block is used to generate a response, and the result is stored as a new block, it can form a tree-like structure of dependencies.&nbsp;</p>\n\n\n\n<p>Over time, the counters tracking the usage of the source blocks (the branches) may become stale as the dependent nodes (the leaves) are reused. Evicting the source block then requires the eviction of all dependent blocks, which would require recalculation of the KV cache for new user prompts, increasing TTFT.&nbsp;</p>\n\n\n\n<p>To address this challenge, TensorRT-LLM includes intelligent eviction algorithms that can trace the dependent nodes from their source nodes and evict dependent nodes first, even if they have more recent reuse counters. This ensures more efficient memory management while preventing unnecessary evictions of dependent blocks.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"882\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache-.png\" alt=\"An image of a KV cache block represented by a table with seven rows and one column. Two similar tables appear on either side. On the left, two rows have been evicted due to dependencies. On the right, only one row has been evicted, showing the efficiency of TensorRT-LLM eviction protocols.\u00a0\" class=\"wp-image-91628\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache-.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache--300x132.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache--625x276.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache--179x79.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache--768x339.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache--1536x678.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache--645x285.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache--500x221.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache--160x71.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache--362x160.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache--249x110.png 249w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Logical-representation-KV-cache--1024x452.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. A logical representation of KV cache eviction algorithm show how it can reduce the number of evicted blocks, increasing the likelihood of reuse</em></figcaption></figure></div>\n\n\n<h2 id=\"getting_started_with_tensorrt-llm_kv_cache_reuse\"  class=\"wp-block-heading\">Getting started with TensorRT-LLM KV cache reuse<a href=\"#getting_started_with_tensorrt-llm_kv_cache_reuse\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Generating KV cache during inference requires a lot of compute and memory resources. Using it efficiently is critical to improving model response, accelerating inference, and increasing system throughput. TensorRT-LLM provides advanced reuse features for developers looking to further optimize TTFT response times for peak performance.&nbsp;</p>\n\n\n\n<p>To start using TensorRT-LLM KV cache reuse check out our <a href=\"https://nvidia.github.io/TensorRT-LLM/advanced/kv-cache-reuse.html\">GitHub documentation</a>.</p>\n\n\n\n<p></p>\n", "protected": false}, "excerpt": {"rendered": "<p>In our previous blog post, we demonstrated how reusing the key-value (KV) cache by offloading it to CPU memory can accelerate time to first token (TTFT) by up to 14x on x86-based NVIDIA H100 Tensor Core GPUs and 28x on the NVIDIA GH200 Superchip. In this post, we shed light on KV cache reuse techniques &hellip; <a href=\"https://developer.nvidia.com/blog/5x-faster-time-to-first-token-with-nvidia-tensorrt-llm-kv-cache-early-reuse/\">Continued</a></p>\n", "protected": false}, "author": 2008, "featured_media": 91630, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1517179", "discourse_permalink": "https://forums.developer.nvidia.com/t/optimizing-time-to-first-token-with-fine-grained-kv-cache-blocks-real-time-reuse-and-efficient-eviction-algorithms/312692", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [4150, 696, 3110], "tags": [296, 4159, 3933], "coauthors": [3708, 2940], "class_list": ["post-91625", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-deployment", "category-data-science", "category-generative-ai", "tag-ai-inference-microservices", "tag-inference-performance", "tag-llama"], "acf": {"post_industry": ["General"], "post_products": ["H100", "TensorRT", "TensorRT-LLM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Benchmark"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/h100.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nPP", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91625"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2008"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91625"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91625/revisions"}], "predecessor-version": [{"id": 91653, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91625/revisions/91653"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91630"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91625"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91625"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91625"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91625"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91542, "date": "2024-11-08T08:00:00", "date_gmt": "2024-11-08T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91542"}, "modified": "2024-11-11T20:31:39", "modified_gmt": "2024-11-12T04:31:39", "slug": "transforming-telecom-networks-to-manage-and-optimize-ai-workloads", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/transforming-telecom-networks-to-manage-and-optimize-ai-workloads/", "title": {"rendered": "Transforming Telecom Networks to Manage and Optimize AI Workloads"}, "content": {"rendered": "\n<p>5G global connections numbered nearly 2 billion earlier this year, and are projected to reach 7.7 billion by 2028. While 5G has delivered faster speeds, higher capacity, and improved latency, particularly for video and data traffic, the initial promise of creating new revenues for network operators has remained elusive.&nbsp;</p>\n\n\n\n<p>Most mobile applications are now routed to the cloud. At the same time, radio access network (RAN) and packet core solutions based on traditional designs continue to improve incrementally with software enhancements and more efficient hardware. These single-purpose systems running traditional voice, data, and video workloads aren\u2019t significantly increasing average revenue per user for telecom companies. Instead, these systems are primarily enabling connectivity and viewed as operating expenditures, which reduces the return on investment. However, this approach is about to change.</p>\n\n\n\n<p>The emergence of ChatGPT and rapid <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language model (LLM)</a> innovation provide a first glimpse at a new type of application that requires accelerated computing. This demands a different kind of multipurpose network to optimize AI and generative AI workloads. Initially, AI network deployments were focused on heavy training workloads that were centralized and required large data centers. Early LLM inference has also been mostly centralized and bound tightly to the training efforts, resulting in more of the same traffic motion from edge to cloud.</p>\n\n\n\n<p>Vision language models (VLMs) and small language models (SLMs), as well as enhanced efficiency of LLM inference, lend well to a distributed architecture spread across networks that bring generative AI models closer to data. The evolution of generative AI towards agentic AI and multimodal AI will demand closer alignment of inferencing to network endpoints, as enterprises need data localization, security, and guaranteed quality of service (QoS). These requirements are already delivered by today\u2019s telecom networks.</p>\n\n\n\n<p>This post explains the need for AI-native network infrastructure and presents key implications of and opportunities for meeting the demands of AI workloads in a telco network.&nbsp;</p>\n\n\n\n<h2 id=\"balancing_ai_inference_traffic_with_legacy_workloads&nbsp;\"  class=\"wp-block-heading\">Balancing AI inference traffic with legacy workloads&nbsp;<a href=\"#balancing_ai_inference_traffic_with_legacy_workloads&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Moving from a centralized compute architecture targeted at LLM training to a highly distributed inference approach for generative AI will have profound effects on future networks. With an increase in SLM, VLM, and LLM inference traffic, more requests with data flow in the network. End devices will evolve to intercept some requests, but are limited by on-device compute, memory, and power.</p>\n\n\n\n<p>Sending all network traffic to the cloud, as traditional apps do, is problematic, as generative AI models carry data to generate unique and real-time responses. Adding up the volume of expected consumer and enterprise inference requests with internal models on the network results in a busy data pipeline.</p>\n\n\n\n<p>Cases are emerging for multimodal requests that need adaptive routing to improve throughput and latency. Other requirements that will affect data movement are user privacy, sovereignty, and security, including the data allowed through packet core and UPF.</p>\n\n\n\n<p>This is a perfect opportunity for telecom companies, as their wireless networks and compute clusters are highly distributed, and available in many geographic locations. If telecom companies can balance critical legacy workloads and new AI inference traffic, then the application generates revenue. This approach has started working in LLM training, and generative AI inference is logically the next area telcos should focus on for monetization opportunities.&nbsp;&nbsp;</p>\n\n\n\n<h2 id=\"supporting_agile_inference_services\"  class=\"wp-block-heading\">Supporting agile inference services<a href=\"#supporting_agile_inference_services\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA and the AI ecosystem are fully invested in creating lighter and more agile inference services. These are built as containers that can run anywhere, and they can be load balanced, scheduled, and combined in diverse ways. Deployable containers enable generative AI models to be closer to data and leverage <a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">retrieval-augmented generation (RAG)</a>. This will be increasingly important as SLMs, VLMs, and multimodel models are created and agentic workflows become prevalent.</p>\n\n\n\n<p>A network can support AI containers almost anywhere. The AI agents and smaller models will lead to a new type of LLM, or model-based routing, where model weights and network insights are used to determine which model is needed where and how to load balance the network traffic to these models to prevent blocks and oversubscription.</p>\n\n\n\n<p>Current networks, especially at the edge, are not built for this adaptive routing of AI traffic.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"1080\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks.png\" alt=\"Diagram showing the effect of AI training and inference on telecom networks from private or public clouds through data centers to the network and far edge.\n\" class=\"wp-image-91548\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks.png 1920w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/ai-training-inference-impact-telecom-networks-1024x576.png 1024w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The effect of AI training and inference on telecom networks extends from private or public clouds through data centers to the network and far edge</em></figcaption></figure>\n\n\n\n<h2 id=\"ai-native_network_infrastructure\"  class=\"wp-block-heading\">AI-native network infrastructure<a href=\"#ai-native_network_infrastructure\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Telecom networks today often use single-purpose and hardware-optimized solutions for each part of the network. The Distributed Unit (DU) and Centralized Unit (CU) have a box, the firewall has a box, the User Plane Function (UPF) has a box, and so on. Connecting these units are network switches and a fabric that\u2019s not optimized in most cases.</p>\n\n\n\n<p>Using software-defined workloads is the first step to solve this problem. If every application can be software-defined, then applications can run in a container that has been optimized and can be loaded anywhere.</p>\n\n\n\n<p>The generative AI landscape and the emergence of LLMs require a full-stack accelerated compute platform as the foundation. This AI-native network infrastructure can scale and be well utilized by many industry standard applications. In theory, the same infrastructure can host any software-defined application and be shared dynamically, based on demand to create future networks that are AI-native, programmable, and multipurpose, including:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Low power, high-performance energy efficient CPU (usually ARM-based) for serial, virtualized, and tenant-based applications.</li>\n\n\n\n<li>High-power accelerator for parallel, vector, and matrix-based applications (usually a GPU).</li>\n\n\n\n<li>Low-power accelerator of traffic to handle interrupt-driven and packet shaping use cases (usually a DPU).</li>\n</ol>\n\n\n\n<p>Nearly any software-defined application can be optimized with a capable software framework across CPU, GPU, and DPU.&nbsp;</p>\n\n\n\n<p>The second step is to ensure the optimization of the network routing and fabric. Additional network security and optimization are needed inside the data pipeline, not on traditional compute architecture. Network fabric must be programmable to adapt to AI or non-AI workloads in real time, whether to support east-west traffic in an AI cluster or traffic to a firewall or storage device. This fabric must be capable of adaptively routing and load balancing LLM inference requests.</p>\n\n\n\n<p>Much of this work is already happening with key independent software vendors (ISVs), firewall vendors, packet core providers, and other ecosystem members taking a software-defined approach to meet the demands of their telecom customers.</p>\n\n\n\n<h2 id=\"higher_performance_and_new_revenue_opportunities\"  class=\"wp-block-heading\">Higher performance and new revenue opportunities<a href=\"#higher_performance_and_new_revenue_opportunities\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>AI inference is moving at an extremely fast pace. Telecom companies must address the new demands of AI traffic, coupled with demands to maximize compute infrastructure utilization and increase revenues.</p>\n\n\n\n<p>By using the best tools for each task, telecom companies can achieve significantly higher performance per watt, while not compromising on programmability and multi-use case support. Running software-defined legacy workloads with new AI workloads on the same RAN infrastructure unlocks new revenue opportunities.</p>\n\n\n\n<p>Telecom companies looking to monetize their infrastructure can take any of the following steps:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Enhance sections of or all network fabric with lower power CPUs and capable DPUs. This is regardless of AI, and follows a reference architecture deployed successfully by cloud service providers and large telecom companies in the APAC region.</li>\n\n\n\n<li>Select edge locations and seed them with AI capable accelerated compute infrastructure. This is not a large investment, and enables telecom companies to target clients and use cases ready to get AI-powered solutions. These will benefit from being 5G/6G-ready, and could be used for AI today.</li>\n\n\n\n<li>Build an accelerated compute-based cluster in the data center for both internal and external use cases, using data from both internal teams and client use cases.</li>\n</ol>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\">Summary<a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>As LLM-powered applications and AI workloads accelerate at unprecedented speed, telecom companies need to rethink networks to manage AI traffic. NVIDIA is working across the telecom ecosystem with software providers and partners to map AI workloads, migrate to a software-defined model, and optimize on accelerated compute architecture. NVIDIA is also working closely with telecom companies to share global progress, collaborate on innovation projects, and welcome new partners to join the journey.</p>\n\n\n\n<p>Learn about <a href=\"https://blogs.nvidia.com/blog/enterprise-reference-architectures/\">NVIDIA Enterprise Reference Architectures</a>, including AI-optimized networking.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>5G global connections numbered nearly 2 billion earlier this year, and are projected to reach 7.7 billion by 2028. While 5G has delivered faster speeds, higher capacity, and improved latency, particularly for video and data traffic, the initial promise of creating new revenues for network operators has remained elusive.&nbsp; Most mobile applications are now routed &hellip; <a href=\"https://developer.nvidia.com/blog/transforming-telecom-networks-to-manage-and-optimize-ai-workloads/\">Continued</a></p>\n", "protected": false}, "author": 1680, "featured_media": 91564, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1517055", "discourse_permalink": "https://forums.developer.nvidia.com/t/transforming-telecom-networks-to-manage-and-optimize-ai-workloads/312668", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 852, 1205], "tags": [817, 2932, 3613, 549], "coauthors": [3238], "class_list": ["post-91542", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-cybersecurity", "category-data-center-cloud", "category-networking-communications", "tag-5g", "tag-large-language-models", "tag-retrieval-augmented-generation-rag", "tag-telecommunications"], "acf": {"post_industry": ["Telecommunications"], "post_products": ["General"], "post_learning_levels": ["General Interest", "Business / Executive", "Beginner Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/network-structure-2.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nOu", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Networking / Communications", "link": "https://developer.nvidia.com/blog/category/networking-communications/", "id": 1205}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91542"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1680"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91542"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91542/revisions"}], "predecessor-version": [{"id": 91551, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91542/revisions/91551"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91564"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91542"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91542"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91542"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91542"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91468, "date": "2024-11-07T00:00:00", "date_gmt": "2024-11-07T08:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91468"}, "modified": "2024-11-07T10:38:02", "modified_gmt": "2024-11-07T18:38:02", "slug": "building-custom-robot-simulations-with-wandelbots-nova-and-nvidia-isaac-sim", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/building-custom-robot-simulations-with-wandelbots-nova-and-nvidia-isaac-sim/", "title": {"rendered": "Building Custom Robot Simulations with Wandelbots NOVA and NVIDIA Isaac Sim"}, "content": {"rendered": "\n<p>Programming robots for real-world success requires a training process that accounts for unpredictable conditions, different surfaces, variations in object size, shape, texture, and more. Consequently, physically accurate simulations are vital for training AI-enabled robots before deployment.&nbsp;</p>\n\n\n\n<p>Crafting physically accurate simulation requires advanced programming skills to fine-tune algorithms, enabling robots to be trained in lifelike digital twins and tested across a wide variety of what-if scenarios that they may encounter. But even when organizations are able to build physics-based simulations, they are often so complex that only highly skilled robotics developers can use them.&nbsp;</p>\n\n\n\n<p>NVIDIA Inception partner and deep tech startup <a href=\"https://www.wandelbots.com/\">Wandelbots</a> is making it easier for any roboticist to simulate robots in physics-based digital training environments, delivered through intuitive human-machine interfaces (HMI). Developers, system integrators, and automation engineers use Wandelbots to build their own application interface, connecting end users to the simulated environment. Factory planners can then interact with a robot cell on the shop floor and use the digital world to train the robot.</p>\n\n\n\n<p>Wandelbots\u2019 hardware-agnostic operating system, <a href=\"https://www.wandelbots.com/de/_files/ugd/dc8e24_f1f309ccfa704f87947aab09d0a4e9ab.pdf\">NOVA</a>, seamlessly integrates with <a href=\"https://developer.nvidia.com/isaac/sim\">NVIDIA Isaac Sim</a>, a robotics reference application built on top of <a href=\"https://developer.nvidia.com/omniverse\">NVIDIA Omniverse</a>. Omniverse is a platform of APIs and SDKs for building workflows in <a href=\"https://developer.nvidia.com/usd\">Universal Scene Description (OpenUSD)</a>.\u00a0</p>\n\n\n\n<p>By enabling robot interaction and training in virtual worlds that mirror shop floor conditions, Wandelbots streamlines the transition from simulation to deployment, and enables AI-driven robotic use cases. Wandelbots provides out-of-the-box solutions, a downloadable software stack, APIs, and SDKs that enable you to run precise simulations through a low-code, intuitive interface and seamlessly deploy the final program onto physical robots.</p>\n\n\n\n<h2 id=\"simplifying_robot_interactions_with_omniverse&nbsp;\"  class=\"wp-block-heading\">Simplifying robot interactions with Omniverse&nbsp;<a href=\"#simplifying_robot_interactions_with_omniverse&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Wandelbots\u2019 advanced robotics platform integrates with NVIDIA Isaac Sim through a custom extension. The Robotics Connector simplifies robot interaction through an intuitive low-code, browser-based user interface, enabling complex training and operational tasks for users with varying degrees of robotics expertise.&nbsp;</p>\n\n\n\n<p>Wandelbots NOVA makes teaching robots new movements straightforward. You can streamline production processes by simulating tasks in virtual environments, optimizing workflows before commissioning tasks to the physical robots.</p>\n\n\n\n<h3 id=\"precision_simulation_training&nbsp;\"  class=\"wp-block-heading\">Precision simulation training&nbsp;<a href=\"#precision_simulation_training&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Wandelbots NOVA created an extensive library of annotated OpenUSD robot models from manufacturers such as FANUC, Yaskawa, Universal Robots, ABB, and KUKA that reflect the kinematics of real-world models to expedite robotic simulation workflows.&nbsp;&nbsp;</p>\n\n\n\n<p>Wandelbots NOVA enables you to create a universal robot program that can include different robot manufacturers. This provides you with the opportunity to create a single robot program for complex automation tasks.</p>\n\n\n\n<p>Using the \u201cghost teaching\u201d method (Video 1), developers use a visual tool to manipulate a robot\u2019s end effector or workpieces in simulation, teaching it how to move and pick up objects. This intuitive approach simplifies the programming of robot cells by providing a visual interface for positioning and movement, making it accessible even for those with minimal programming experience. With pose data from the simulation automatically transferred to the robot&#8217;s program, the need for complex coding is eliminated.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/pCn8SsHW9KQ?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Wandelbots NOVA integration in NVIDIA Isaac Sim &#8211; Showing Ghost Teaching</em></figcaption></figure>\n\n\n\n<p>Immediate feedback enables users to see the real-time effects of their actions, facilitating quick adjustments and fine-tuning. Wandelbots also supports ongoing training with both synthetic and ground truth data, allowing for continuous training in simulated environments and reducing the need for on-site expert training for every new robot task.</p>\n\n\n\n<h3 id=\"robot-agnostic_unified_programming_interface&nbsp;\"  class=\"wp-block-heading\">Robot-agnostic unified programming interface&nbsp;<a href=\"#robot-agnostic_unified_programming_interface&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Wandelbots NOVA offers a universal programming interface consistent across all robot models. This enables users to program their robots the same way, regardless of the specific robot model they are using. It enables the users to switch robot models during the simulation phase without the need of making major adjustments to the robot program as a whole. You are also enabled to build your own robotic apps and HMI on top of Wandelbots NOVA. For example, you can build your own standardized sanding app.&nbsp;</p>\n\n\n\n<p>The simple HMI also facilitates the creation of standardized cell setups, ensuring consistency and efficiency for users. With NOVA, you can teach robots various sanding paths, streamlining the process of programming complex tasks.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"975\" height=\"495\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Universal-programming-Interface-Wandelbots-.png\" alt=\"Two pictures, with one showing a robotic app built on Wandelbots NOVA human machine interface and the robot cell simulation setup pictured on the left.\n\" class=\"wp-image-91535\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Universal-programming-Interface-Wandelbots-.png 975w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Universal-programming-Interface-Wandelbots--300x152.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Universal-programming-Interface-Wandelbots--625x317.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Universal-programming-Interface-Wandelbots--179x91.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Universal-programming-Interface-Wandelbots--768x390.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Universal-programming-Interface-Wandelbots--645x327.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Universal-programming-Interface-Wandelbots--500x254.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Universal-programming-Interface-Wandelbots--160x81.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Universal-programming-Interface-Wandelbots--362x184.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Universal-programming-Interface-Wandelbots--217x110.png 217w\" sizes=\"(max-width: 975px) 100vw, 975px\" /></figure>\n\n\n\n<p class=\"has-text-align-center\"><em>Figure 1. Universal programming interface from Wandelbots NOVA</em></p>\n\n\n\n<h3 id=\"seamless_transition_between_virtual_and_physical_worlds&nbsp;\"  class=\"wp-block-heading\">Seamless transition between virtual and physical worlds&nbsp;<a href=\"#seamless_transition_between_virtual_and_physical_worlds&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Wandelbots NOVA enables users to seamlessly transition between physical and virtual cell setups by connecting to the robot&#8217;s controller and streaming virtually planned motions to the robot.</p>\n\n\n\n<p>This supports both greenfield and brownfield robotics system development:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>In <em>greenfield systems</em>, you can test new robot setups in an entirely virtual environment before any physical infrastructure is built.&nbsp;</li>\n\n\n\n<li>For <em>brownfield systems</em>, accurate simulations with ground truth data enables you to understand how new robots will interact with the existing infrastructure and make sure that they are compatible before deployment.&nbsp;</li>\n</ul>\n\n\n\n<p>In either case, when the robot program is validated in simulation, the program can be transferred to physical robots using direct motion streaming, ensuring smooth operation and maximized productivity.</p>\n\n\n\n<p>Wandelbots\u2019 robotics connector enables direct communication between the physical robot and Isaac Sim, ensuring that robots in simulation operate synchronously with their physical counterparts. This facilitates real-time testing, validation, and troubleshooting, reducing development time and improving reliability.&nbsp;&nbsp;</p>\n\n\n\n<p>Whether you are programming a robot in the real world or in a simulated environment, the process and interface remain the same. This consistency simplifies the programming process and also ensures that the skills and knowledge gained in one environment are directly transferable to the other.&nbsp;</p>\n\n\n\n<h2 id=\"developing_robotics_systems_with_isaac_sim\"  class=\"wp-block-heading\">Developing robotics systems with Isaac Sim<a href=\"#developing_robotics_systems_with_isaac_sim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"975\" height=\"548\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-architecture.png\" alt=\"Diagram shows the components, such as custom apps (palletizing, sanding), core apps and functions (home screen, settings, robot pad, Wandelscript, AI Assistant, and virtual robot), the NOVA API (data storage, diagnosis collector, robot movement calculation and control, and program engine), NVIDIA Omniverse as integrated software, and the robot cell.\" class=\"wp-image-91537\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-architecture.png 975w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-architecture-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-architecture-625x351.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-architecture-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-architecture-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-architecture-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-architecture-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-architecture-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-architecture-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-architecture-362x203.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-architecture-196x110.png 196w\" sizes=\"(max-width: 975px) 100vw, 975px\" /></figure></div>\n\n\n<p class=\"has-text-align-center\"><em>Figure 2. Wandelbots NOVA architecture</em></p>\n\n\n\n<p>Wandelbots uses Isaac Sim to connect seamlessly to articulations through Web Sockets for real-time motion data exchange. Operators can control tools like surface grippers through robot controller IOs, with future support for the <a href=\"https://opcfoundation.org/about/opc-technologies/opc-ua/\">OPC Unified Architecture</a>.</p>\n\n\n\n<p>A German manufacturer of bouldering equipment, <a href=\"https://www.blocz.de/en/produkte-volumen-blocz/volumen/\">blocz</a>, built an app based on Wandelbots NOVA that accompanies an abrasive cell using camera input to optimize motion for abrasive processes. To meet the demand of flexible and adaptive production, the app with Wandelbots NOVA enabled experts to remotely teach new models of workpieces, reducing the need for experts to appear onsite for each new workpiece.</p>\n\n\n\n<p>Another example comes from a collaboration with an automotive parts manufacturer, where Wandelbots helped rearrange the setup and handover mechanisms between two robots. A test case showed that original robot cell cycle times could be improved by 50%.&nbsp;</p>\n\n\n\n<p>To achieve this improved cycle time, a new cell setup was tested with Wandelbots NOVA. After the physical cell was set up according to the parameters of the simulated cell, only minor fine-tuning was needed for the final setup, resulting in a 90% accuracy of the simulated setup and faster time to installation for the customer.</p>\n\n\n\n<p>The Wandelbots SaaS application is accessible through a web browser and provides immediate, cloud-hosted services. Downloadable software stacks enable on-premises deployment, ensuring data privacy and control.\u00a0</p>\n\n\n\n<p>With Wandelbots\u2019 physics accurate simulations, your robotics programs can reduce development time and cost, and confidently go live with <a href=\"https://www.nvidia.com/en-us/glossary/generative-physical-ai/\">physical AI</a> faster.&nbsp;</p>\n\n\n\n<h2 id=\"getting_started_with_isaac_sim\"  class=\"wp-block-heading\">Getting started with Isaac Sim<a href=\"#getting_started_with_isaac_sim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>You can build custom simulators or integrate Isaac Sim components into your existing workflows for more user-friendly robot training and interaction. For more information, see the following resources:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://developer.nvidia.com/isaac/sim\">NVIDIA Isaac Sim</a> developer page</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/use-cases/robotics-simulation/\">Robotic Simulation Use Case</a></li>\n\n\n\n<li><a href=\"https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-OV-03+V1\">Self-Paced Course: Introduction to Robotic Simulation in Isaac Sim</a></li>\n\n\n\n<li><a href=\"https://www.addevent.com/event/Pr23707825\">Building Custom Robot Simulations feat Wandelbots</a> livestream on Dec. 11, 2024</li>\n</ul>\n\n\n\n<p>Discover how <a href=\"https://www.wandelbots.com/\">Wandelbots</a> can help you plan your robotic application in Isaac Sim and bring simulations to life on the shop floor, by scheduling a meeting today.</p>\n\n\n\n<p><em>Stay up to date by subscribing to the </em><a href=\"https://www.nvidia.com/en-us/industries/robotics/robotics-stay-informed/\"><em>NVIDIA Robotics newsletter</em></a><em> and following NVIDIA Robotics on </em><a href=\"https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA\"><em>YouTube</em></a><em>, </em><a href=\"https://discord.gg/w9VvuYdq\"><em>Discord</em></a><em>, and the </em><a href=\"https://forums.developer.nvidia.com/c/omniverse/simulation/69\"><em>Isaac Sim developer forum</em></a><em>.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Programming robots for real-world success requires a training process that accounts for unpredictable conditions, different surfaces, variations in object size, shape, texture, and more. Consequently, physically accurate simulations are vital for training AI-enabled robots before deployment.&nbsp; Crafting physically accurate simulation requires advanced programming skills to fine-tune algorithms, enabling robots to be trained in lifelike digital &hellip; <a href=\"https://developer.nvidia.com/blog/building-custom-robot-simulations-with-wandelbots-nova-and-nvidia-isaac-sim/\">Continued</a></p>\n", "protected": false}, "author": 689, "featured_media": 91555, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1515843", "discourse_permalink": "https://forums.developer.nvidia.com/t/building-custom-robot-simulations-with-wandelbots-nova-and-nvidia-isaac-sim/312473", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [503], "tags": [3700, 3584], "coauthors": [1072, 4158], "class_list": ["post-91468", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-simulation-modeling-design", "tag-openusd", "tag-robotics-simulation"], "acf": {"post_industry": "", "post_products": ["Isaac Sim", "Omniverse"], "post_learning_levels": ["General Interest"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/wandelbots-nova-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nNi", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91468"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/689"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91468"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91468/revisions"}], "predecessor-version": [{"id": 91554, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91468/revisions/91554"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91555"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91468"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91468"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91468"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91468"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91494, "date": "2024-11-06T08:00:00", "date_gmt": "2024-11-06T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91494"}, "modified": "2024-11-07T10:26:58", "modified_gmt": "2024-11-07T18:26:58", "slug": "spotlight-fourier-trains-humanoid-robots-for-real-world-roles-using-nvidia-isaac-gym", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/spotlight-fourier-trains-humanoid-robots-for-real-world-roles-using-nvidia-isaac-gym/", "title": {"rendered": "Spotlight: Fourier Trains Humanoid Robots for Real-World Roles Using NVIDIA Isaac Gym"}, "content": {"rendered": "\n<p><em>This post was written in partnership with the </em><a href=\"https://www.fftai.com/\"><em>Fourier</em></a><em> research team.</em></p>\n\n\n\n<p>Training humanoid robots to operate in fields that demand high levels of interaction and adaptability\u2014such as scientific research, healthcare and manufacturing\u2014can be a challenging and resource-intensive feat.</p>\n\n\n\n<p><a href=\"http://www.fftai.com/\">Fourier</a>, a Shanghai-based robotics company, is doing the heavy lifting by developing advanced humanoid robots that can be integrated into real-world applications where precision and agility are critical.</p>\n\n\n\n<p>The company announced the expansion of its GRx humanoid robot series with the launch of GR-2 in late September. Building on the previous-generation GR-1, the world\u2019s first mass-produced humanoid robot, GR-2 features an upgraded hardware design, greater adaptability, advanced dexterity and a humanlike range of motion.</p>\n\n\n\n<h2 id=\"developing_humanoid_robot_gr-2_with_nvidia_isaac_gym\"  class=\"wp-block-heading\">Developing humanoid robot GR-2 with NVIDIA Isaac Gym<a href=\"#developing_humanoid_robot_gr-2_with_nvidia_isaac_gym\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To develop and test GR-2, the Fourier team turned to NVIDIA Isaac Gym (now deprecated) for reinforcement learning. They are currently porting their workflows to the recently launched <a href=\"https://developer.nvidia.com/isaac/lab\">NVIDIA Isaac Lab</a>, an open-source modular framework for robot learning designed to simplify how robots adapt to new skills.</p>\n\n\n\n<p>Sim-to-real learning has become essential for robotics, especially for complex movements like sitting down, getting up, or even dancing. With Isaac Gym, Fourier was able to simulate real-world conditions, minimizing the time and cost of testing and maintenance.</p>\n\n\n\n<p>The team simulated complex multi-robot scenarios and real-world environments, leading to more robust AI decision-making and enhanced real-world performance\u2014even in unpredictable settings. Fourier also used Isaac Gym to pretrain grasping algorithms, simulating success rates before deployment. This approach significantly reduces the real-world trial and error, saving time and resources.</p>\n\n\n\n<h2 id=\"optimizing_ai_for_real-world_robotics\"  class=\"wp-block-heading\"><strong>Optimizing AI for real-world robotics</strong><a href=\"#optimizing_ai_for_real-world_robotics\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>While training GR-2 for the floor-to-stand maneuver, Fourier simulated the physical demands required for completing tasks at different levels of elevation. By replicating the GR-2 model, they tested how it performs under various settings and completed 3,000 iterations in around 15 hours, a notable reduction compared to traditional training methods. When transferred directly to GR-2\u2019s physical controls, the model\u2019s action tensors achieved an 89% success rate.</p>\n\n\n\n<figure class=\"wp-block-gallery aligncenter has-nested-images columns-2 is-cropped wp-block-gallery-2 is-layout-flex wp-block-gallery-is-layout-flex\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"426\" height=\"240\" data-id=\"91503\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Checkpoint100-1.gif\" alt=\"Robots perform the floor-to-stand maneuver in an animated image at a checkpoint after 100 test iterations.\" class=\"wp-image-91503\" /><figcaption class=\"wp-element-caption\"><em>Checkpoint after 100 test iterations</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"426\" height=\"240\" data-id=\"91502\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Checkpoint500-2.gif\" alt=\"Robots perform the floor-to-stand maneuver in an animated image at a checkpoint after 500 test iterations.\" class=\"wp-image-91502\" /><figcaption class=\"wp-element-caption\"><em>Checkpoint after 500 test iterations</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"426\" height=\"240\" data-id=\"91504\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Checkpoint1600-1.gif\" alt=\"Robots perform the floor-to-stand maneuver in an animated image at a checkpoint after 1,600 test iterations.\" class=\"wp-image-91504\" /><figcaption class=\"wp-element-caption\"><em>Checkpoint after 1,600 test iterations</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"426\" height=\"240\" data-id=\"91505\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Checkpoint3000-1.gif\" alt=\"Robots perform the floor-to-stand maneuver in an animated image at a checkpoint after 3,000 test iterations.\" class=\"wp-image-91505\" /><figcaption class=\"wp-element-caption\"><em>Checkpoint after 3,000 test iterations</em></figcaption></figure>\n<figcaption class=\"blocks-gallery-caption wp-element-caption\"><em>Figure 1. The Fourier team observed a significant increase in performance success rates of the floor-to-stand maneuver after conducting 1,600 test iterations</em></figcaption></figure>\n\n\n\n<p>To enhance the development process, the team tapped the<a href=\"https://developer.nvidia.com/tensorrt\"> NVIDIA TensorRT</a> software development kit for real-time inference optimization, CUDA libraries for parallel processing, and the <a href=\"https://developer.nvidia.com/cudnn\">NVIDIA cuDNN</a> library for accelerating deep learning frameworks like PyTorch.</p>\n\n\n\n<p>Moving to <a href=\"https://developer.nvidia.com/isaac/lab\">NVIDIA Isaac Lab</a> will enable Fourier to train more complex algorithms and simulations in multiphysics virtual environments powered by NVIDIA RTX tiled rendering.</p>\n\n\n\n<h2 id=\"exploring_next-generation_robotic_capabilities\"  class=\"wp-block-heading\">Exploring next-generation robotic capabilities<a href=\"#exploring_next-generation_robotic_capabilities\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>By adopting NVIDIA technologies, Fourier significantly reduced model training times and improved the accuracy of simulations, which resulted in enhanced collaboration across its engineering and R&amp;D teams.</p>\n\n\n\n<p>NVIDIA tools also opened the door to complex AI functions like language models and predictive analytics, previously too resource-heavy to implement.</p>\n\n\n\n<p>\u201cThe advancements we\u2019ve achieved are pushing the boundaries of what\u2019s possible in humanoid robotics,\u201d said Fourier CEO Alex Gu.</p>\n\n\n\n<p>\u201cBy improving the robot\u2019s real-time motion control and AI-driven decision-making, we are setting new standards for human-robot interaction across industries such as the service sector, academic research, and medical rehabilitation.\u201d</p>\n\n\n\n<p><a href=\"http://www.fftai.com/\">Learn more about Fourier GR-2 humanoid robots</a>.</p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Need to migrate from NVIDIA Isaac Gym to <a href=\"https://developer.nvidia.com/isaac/lab\">NVIDIA Isaac Lab</a>? Check out the <a href=\"https://isaac-sim.github.io/IsaacLab/main/source/migration/migrating_from_isaacgymenvs.html\">Isaac Lab Migration Guide</a>. If you\u2019re a first-time user of Isaac Lab, see the <a href=\"https://isaac-sim.github.io/IsaacLab/main/source/tutorials/index.html#\">Getting Started Developer\u2019s Guide</a>. Discover the latest in robot learning and simulation in the <a href=\"https://www.addevent.com/event/GA23422424\">November 13 livestream, OpenUSD Insider Livestream &#8211; Robot Sim and Learning</a>. And don\u2019t miss the <a href=\"https://www.addevent.com/event/Uz23738360\">NVIDIA Isaac Lab Office Hours</a> for hands-on support and insights.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>This post was written in partnership with the Fourier research team. Training humanoid robots to operate in fields that demand high levels of interaction and adaptability\u2014such as scientific research, healthcare and manufacturing\u2014can be a challenging and resource-intensive feat. Fourier, a Shanghai-based robotics company, is doing the heavy lifting by developing advanced humanoid robots that can &hellip; <a href=\"https://developer.nvidia.com/blog/spotlight-fourier-trains-humanoid-robots-for-real-world-roles-using-nvidia-isaac-gym/\">Continued</a></p>\n", "protected": false}, "author": 2409, "featured_media": 91507, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1515721", "discourse_permalink": "https://forums.developer.nvidia.com/t/spotlight-fourier-trains-humanoid-robots-for-real-world-roles-using-nvidia-isaac-gym/312437", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [4145], "coauthors": [4158], "class_list": ["post-91494", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-humanoid-robots"], "acf": {"post_industry": ["General"], "post_products": ["cuDNN", "TensorRT"], "post_learning_levels": ["Beginner Technical", "Intermediate Technical"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/fourier-humanoid-robots-featured-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nNI", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91494"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2409"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91494"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91494/revisions"}], "predecessor-version": [{"id": 91516, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91494/revisions/91516"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91507"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91494"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91494"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91494"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91494"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91458, "date": "2024-11-06T08:00:00", "date_gmt": "2024-11-06T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91458"}, "modified": "2024-11-06T14:08:29", "modified_gmt": "2024-11-06T22:08:29", "slug": "spotlight-galbot-builds-a-large-scale-dexterous-hand-dataset-for-humanoid-robots-using-nvidia-isaac-sim", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/spotlight-galbot-builds-a-large-scale-dexterous-hand-dataset-for-humanoid-robots-using-nvidia-isaac-sim/", "title": {"rendered": "Spotlight: Galbot Builds a Large-Scale Dexterous Hand Dataset for Humanoid Robots Using NVIDIA Isaac Sim"}, "content": {"rendered": "\n<p>Robotic dexterous grasping is a critical area of research and development, aimed at enabling robots to interact with and manipulate objects as flexibly as humans can. By enabling robots to handle complex tasks that require fine motor skills, dexterous grasping can significantly enhance productivity and efficiency.</p>\n\n\n\n<p>The first step to enabling human-like dexterous object manipulation for robots is through robotic dexterous grasping. However, validating dexterous grasping data has been a major challenge, as human annotations are impractical on a scale of millions of grasps. Without large-scale datasets, robotic dexterous grasping has remained under-explored compared to object grasping with parallel grippers.</p>\n\n\n\n<p>Using <a href=\"https://developer.nvidia.com/isaac/sim\">NVIDIA Isaac Sim</a>, a reference application for robotics simulation, robotics company <a href=\"https://www.galbot.com/\">Galbot</a> successfully addressed this challenge. They validated a vast number of grasps to develop <a href=\"https://pku-epic.github.io/DexGraspNet/\">DexGraspNet</a>, a comprehensive simulated dataset for dexterous robotic grasps that can be applied to any dexterous robotic hand.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"934\" height=\"490\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/diverse-grasps-objects.png\" alt=\"DexGraspNet demonstrates diverse object grasps using Isaac Sim for verification.\" class=\"wp-image-91459\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/diverse-grasps-objects.png 934w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/diverse-grasps-objects-300x157.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/diverse-grasps-objects-625x328.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/diverse-grasps-objects-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/diverse-grasps-objects-768x403.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/diverse-grasps-objects-645x338.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/diverse-grasps-objects-500x262.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/diverse-grasps-objects-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/diverse-grasps-objects-362x190.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/diverse-grasps-objects-210x110.png 210w\" sizes=\"(max-width: 934px) 100vw, 934px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Some diverse grasps on the objects from DexGraspNet, a dataset built by robotics company Galbot</em></figcaption></figure></div>\n\n\n<p>DexGraspNet contains 1.32 million ShadowHand grasps on 5,355 objects\u2014two orders of magnitude larger than the previous <a href=\"https://arxiv.org/pdf/2210.02697\">Deep Differentiable Grasp</a> dataset. DexGraspNet covers more than 133 object categories and contains more than 200 diverse grasps for each object instance, making it a more complete sample for research.</p>\n\n\n\n<figure class=\"wp-block-video aligncenter\"><video controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/dexgraspnet-galbot.mp4\"></video><figcaption class=\"wp-element-caption\"><em><em>Video 1. Learn more about DexGraspNet, a large-scale robotic dexterous grasp dataset for general objects based on simulation</em></em></figcaption></figure>\n\n\n\n<h2 id=\"balancing_data_variety_and_quantity_on_robotic_dexterous_hand_grasping\"  class=\"wp-block-heading\">Balancing data variety and quantity on robotic dexterous hand grasping<a href=\"#balancing_data_variety_and_quantity_on_robotic_dexterous_hand_grasping\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Galbot leveraged a deeply accelerated optimizer that can efficiently and robustly synthesize stable and diverse grasps on a large scale to find the grasping poses that meet force-closure conditions and have high graspness scores. The dataset includes many types of grasps that are not possible with other popular tools like GraspIt.</p>\n\n\n\n<p>Through cross-dataset experiments, the Galbot team demonstrated that training several algorithms for dexterous grasp synthesis on DexGraspNet significantly outperformed training on the previous dataset.</p>\n\n\n\n<h2 id=\"building_generalized_dexterous_hand_grasping_skills\"  class=\"wp-block-heading\">Building generalized dexterous hand grasping skills<a href=\"#building_generalized_dexterous_hand_grasping_skills\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The Galbot research team proposed <a href=\"https://pku-epic.github.io/UniDexGrasp++/\">UniDexGrasp++</a>, a novel, object-independent approach for learning generalized strategies for dexterous object grasping from real point cloud observations and proprioceptive information in a tabletop environment.</p>\n\n\n\n<p>To address the challenge of learning vision-based strategies across thousands of object instances, the team used GeoCurriculum Learning and Geometry-Aware Iterative Generalist-Specialist Learning (GiGSL), which leverages the geometric features of the task to significantly improve generalizability.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"932\" height=\"384\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/robots-grasping-objects.png\" alt=\"Image displaying a variety of robot hands grasping objects in different ways.\" class=\"wp-image-91461\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/robots-grasping-objects.png 932w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/robots-grasping-objects-300x124.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/robots-grasping-objects-625x258.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/robots-grasping-objects-179x74.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/robots-grasping-objects-768x316.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/robots-grasping-objects-645x266.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/robots-grasping-objects-500x206.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/robots-grasping-objects-160x66.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/robots-grasping-objects-362x149.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/robots-grasping-objects-267x110.png 267w\" sizes=\"(max-width: 932px) 100vw, 932px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Galbot uses Isaac Sim to verify the dexterous grasping policy learned through Geometry-Aware Curriculum Learning at scale</em></figcaption></figure></div>\n\n\n<p>Using these techniques in Isaac Sim, the Galbot team&#8217;s final iteration successfully demonstrated generalized dexterous grasping of more than 3,000 object instances with random object poses on a table-top setting. The success rates were 85.4% on the training set and 78.2% on the test sets, outperforming the state-of-the-art baseline UniDexGrasp by 11.7% and 11.3%, respectively.</p>\n\n\n\n<p>The research paper, <a href=\"https://pku-epic.github.io/UniDexGrasp++/\">UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning</a>, was awarded Best Paper Finalist at the 2023 International Conference on Computer Vision.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"600\" height=\"338\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/galbot-isaac-sim-robot-training.gif\" alt=\"A GIF showing a large grid of rectangular cubes arranged in rows and columns, each with a robot hand grasping a different object. \n\" class=\"wp-image-91480\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. Galbot used Isaac Sim to train policies that contain more than 3,000 training objects with random poses</em></em></figcaption></figure>\n\n\n\n<h2 id=\"scaling_dexterous_hand_grasping_models\"  class=\"wp-block-heading\">Scaling dexterous hand grasping models<a href=\"#scaling_dexterous_hand_grasping_models\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Galbot\u2019s most recent work is DexGraspNet 2.0. It features dexterous grasping in cluttered scenes and the team has demonstrated zero-shot sim-to-real transfer with 90.70% real-world dexterous grasping success rate. DexGraspNet 2.0 has been evaluated on real robots, featuring a LEAP hand mounted on a UR-5 robot arm for dexterous grasp experiments, and a Franka Panda arm for gripper tasks. The <a href=\"https://openreview.net/pdf?id=5W0iZR9J7h\">DexGraspNet 2.0 project</a> will be showcased at the 2024 <a href=\"https://www.corl.org/\">Conference on Robot Learning (CoRL)</a>.</p>\n\n\n\n<p>The Galbot team also built a simulation test environment for dexterous hand grasping models with Isaac Sim and <a href=\"https://developer.nvidia.com/isaac/lab\">NVIDIA Isaac Lab</a>, an open-source modular framework for robot learning designed to simplify how robots adapt to new skills. With the simulation environment, developers can significantly accelerate the exploration and scaling of dexterous hand-grasping models, and more quickly implement generalized dexterous hand-grasping skills in real-world use cases.</p>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\">Summary<a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Using <a href=\"https://developer.nvidia.com/isaac/sim\">NVIDIA Isaac Sim</a>, Galbot developed DexGraspNet, a comprehensive dataset for humanoid robots, which includes 1.32 million ShadowHand grasps on 5,355 objects across more than 133 categories, providing a vast and diverse range of grasps. The dataset has proven effective for training algorithms in dexterous grasp synthesis, significantly outperforming previous datasets in cross-dataset experiments. This work enables robots to better handle complex tasks that require fine motor skills, enhancing productivity and efficiency.</p>\n\n\n\n<p>To see the DexGraspNet code, visit the <a href=\"https://github.com/PKU-EPIC/DexGraspNet\">PKU-EPIC/DexGraspNet</a> GitHub repo. You can also reference the <a href=\"https://mirrors.pku.edu.cn/dl-release/DexGraspNet-ICRA2023/\">dataset</a> the team used. To learn more, see <a href=\"https://arxiv.org/abs/2210.02697\">DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation</a>.</p>\n\n\n\n<p>Discover the latest in robot learning and simulation in the <a href=\"https://www.addevent.com/event/GA23422424\">livestream on November 13</a>, and don\u2019t miss the <a href=\"https://www.addevent.com/event/Uz23738360\">NVIDIA Isaac Lab Office Hours</a> for hands-on support and insights.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Robotic dexterous grasping is a critical area of research and development, aimed at enabling robots to interact with and manipulate objects as flexibly as humans can. By enabling robots to handle complex tasks that require fine motor skills, dexterous grasping can significantly enhance productivity and efficiency. The first step to enabling human-like dexterous object manipulation &hellip; <a href=\"https://developer.nvidia.com/blog/spotlight-galbot-builds-a-large-scale-dexterous-hand-dataset-for-humanoid-robots-using-nvidia-isaac-sim/\">Continued</a></p>\n", "protected": false}, "author": 2409, "featured_media": 91462, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1515719", "discourse_permalink": "https://forums.developer.nvidia.com/t/spotlight-galbot-builds-a-large-scale-dexterous-hand-dataset-for-humanoid-robots-using-nvidia-isaac-sim/312436", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 63], "tags": [453, 4145, 1305], "coauthors": [4158], "class_list": ["post-91458", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "category-robotics", "tag-featured", "tag-humanoid-robots", "tag-isaac-sim"], "acf": {"post_industry": ["General"], "post_products": ["Isaac Sim"], "post_learning_levels": ["Beginner Technical", "Intermediate Technical"], "post_content_types": ["News", "Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/galbot-humanoid-robotcs-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nN8", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Robotics", "link": "https://developer.nvidia.com/blog/category/robotics/", "id": 63}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91458"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2409"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91458"}], "version-history": [{"count": 14, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91458/revisions"}], "predecessor-version": [{"id": 91529, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91458/revisions/91529"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91462"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91458"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91458"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91458"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91458"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91333, "date": "2024-11-06T08:00:00", "date_gmt": "2024-11-06T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91333"}, "modified": "2024-11-06T12:29:43", "modified_gmt": "2024-11-06T20:29:43", "slug": "advancing-humanoid-robot-sight-and-skill-development-with-nvidia-project-gr00t", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/advancing-humanoid-robot-sight-and-skill-development-with-nvidia-project-gr00t/", "title": {"rendered": "Advancing Humanoid Robot Sight and Skill Development with NVIDIA Project GR00T"}, "content": {"rendered": "\n<p><a href=\"https://www.nvidia.com/en-us/glossary/humanoid-robot/\">Humanoid robots</a> present a multifaceted challenge at the intersection of mechatronics, control theory, and AI. The dynamics and control of humanoid robots are complex, requiring advanced tools, techniques, and algorithms to maintain balance during locomotion and manipulation tasks. Collecting robot data and integrating sensors also pose significant challenges, as humanoid robots require a fusion of sophisticated sensors and high-resolution cameras to perceive the environment effectively and reason how to interact with surroundings in real-time. The computational demands for real-time processing of sensory data and decision-making also necessitate powerful onboard computers.&nbsp;</p>\n\n\n\n<p>Developing technologies, tools, and robot foundation models that can enable adaptive robot behavior, and facilitate natural human-robot interaction, remains an ongoing research focus. <a href=\"https://developer.nvidia.com/project-gr00t\">NVIDIA Project GR00T</a> is an active research initiative to enable the humanoid robot ecosystem of builders to accelerate these next-generation advanced robot development efforts. In this post, we will discuss the new GR00T workflows for humanoid development, including:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>GR00T-Gen for diverse environment generation</li>\n\n\n\n<li>GR00T-Mimic for robot motion and trajectory generation</li>\n\n\n\n<li>GR00T-Dexterity for fine-grained and dexterous manipulation</li>\n\n\n\n<li>GR00T-Mobility for locomotion and navigation</li>\n\n\n\n<li>GR00T-Control for whole-body control (WBC)</li>\n\n\n\n<li>GR00T-Perception for multimodal sensing&nbsp;</li>\n</ul>\n\n\n\n<h2 id=\"gr00t-gen_for_diverse_environment_generation\"  class=\"wp-block-heading\">GR00T-Gen for diverse environment generation<a href=\"#gr00t-gen_for_diverse_environment_generation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>GR00T-Gen is a workflow to generate robot tasks and simulation-ready environments in <a href=\"https://www.nvidia.com/en-us/omniverse/usd/\">OpenUSD</a> for training generalist robots to perform manipulation, locomotion, and navigation.</p>\n\n\n\n<p>For robust <a href=\"https://www.nvidia.com/en-us/glossary/robot-learning/\">robot learning</a>, it is important to train in diverse environments with a variety of objects and scenes. Generating a large variety of environments in the real world is often expensive, time-consuming, and not accessible to most developers, making simulation a compelling alternative.&nbsp;&nbsp;</p>\n\n\n\n<p>GR00T-Gen provides realistic and diverse human-centric environments, created using <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models (LLMs)</a> and 3D generative AI models. It features over 2,500 3D assets spanning more than 150 object categories. To create visually diverse scenes, multiple textures are included for domain randomization in simulation. Domain randomization enables trained models and policies to generalize effectively when deployed in the real world.&nbsp;</p>\n\n\n\n<p>GR00T-Gen provides cross-embodiment support for mobile manipulators and humanoid robots and includes over 100 tasks like opening doors, pressing buttons, and navigation.</p>\n\n\n\n<h2 id=\"gr00t-mimic_for_robot_motion_and_trajectory_generation\"  class=\"wp-block-heading\">GR00T-Mimic for robot motion and trajectory generation<a href=\"#gr00t-mimic_for_robot_motion_and_trajectory_generation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>GR00T-Mimic is a robust workflow to generate motion data from teleoperated demonstrations for imitation learning. Imitation learning is an approach to training robots where robots acquire skills through observation and replication of actions demonstrated by a teacher. A critical component of this training process is the volume and quality of demonstration data available.&nbsp;</p>\n\n\n\n<p>For humanoid robots to effectively and safely navigate human-centric environments, it is important that their &#8220;teachers&#8221; are human demonstrators, allowing robots to learn by mimicking human behavior. However, a significant challenge arises due to the scarcity of existing high-quality training data.&nbsp;</p>\n\n\n\n<p>To address this, there is a need to develop extensive datasets that capture human actions. One promising method for generating this data is through teleoperation, where a human operator remotely controls a robot to demonstrate specific tasks. While teleoperation can produce high-fidelity demonstration data, it is constrained by the number of individuals who can access these systems at a given time.</p>\n\n\n\n<p>GR00T-Mimic aims to scale up the data collection pipeline.&nbsp; The approach involves gathering a limited number of human demonstrations in the physical world using extended reality (XR) and spatial computing devices like the Apple Vision Pro. These initial demonstrations are then used to generate synthetic motion data, effectively scaling up the demonstration dataset. The goal is to create a comprehensive repository of human actions for robots to learn from, thus enhancing their ability to perform tasks in real-world settings.</p>\n\n\n\n<p>To further support GR00T-Mimic, NVIDIA Research also released <a href=\"https://skillgen.github.io/\">SkillMimicGen</a>, a fundamental first step to solve real-world manipulation tasks with minimal human demonstrations.&nbsp;</p>\n\n\n\n<h2 id=\"gr00t-dexterity_for_fine-grained_and_dexterous_manipulation&nbsp;\"  class=\"wp-block-heading\">GR00T-Dexterity for fine-grained and dexterous manipulation&nbsp;<a href=\"#gr00t-dexterity_for_fine-grained_and_dexterous_manipulation&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>GR00T-Dexterity is a suite of models and policies for fine-grained dexterous manipulation and reference workflows to develop them.&nbsp;</p>\n\n\n\n<p>Traditional robotics grasping requires the integration of multiple complex components, from identifying grasp points to planning motions and controlling fingers. For robots with many actuators, managing these systems\u2014especially using state machines to handle failures like missed grasps\u2014makes end-to-end grasping a significant challenge.</p>\n\n\n\n<p>GR00T-Dexterity introduces a workflow that leverages the research paper <a href=\"https://sites.google.com/view/dextrah-g\">DextrAH-G</a>. It is a <a href=\"https://www.nvidia.com/en-us/glossary/reinforcement-learning/\">reinforcement learning (RL)</a>-based approach to policy development for robot dexterity. This workflow enables the creation of an end-to-end, pixels-to-action grasping system, trained in simulation and deployable to a physical robot. The workflow is designed to produce policies capable of fast, reactive grasping with depth stream input, and is generalizable to new objects.&nbsp;</p>\n\n\n\n<p>The process involves creating a <a href=\"https://research.nvidia.com/labs/srl/project/geometric-fabrics/\">geometric fabric</a> to define the robot\u2019s motion space and simplify grasping actions, optimized for parallelized training. Using <a href=\"https://developer.nvidia.com/isaac/lab\">NVIDIA Isaac Lab</a>, a fabric-guided policy is trained by applying reinforcement learning across multiple GPUs to generalize grasping behaviors. Finally, the learned policy is distilled into a real-world-ready version using depth input through imitation learning, producing a robust policy within a few hours.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/u9raCniU0nI?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 1. Training in simulation using NVIDIA Isaac Lab</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/cnYCq5NmiII?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 2. Erratic robot motion while training a grasping task without geometric fabrics</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Zp6zmfSlRI4?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 3. Avoiding hardware damage while transferring policy to the real world with bin packing demonstration on a physical robot arm using GR00T-Dexterity workflow</em></em></figcaption></figure>\n\n\n\n<p>Note that the GR00T-Dexterity workflow preview is based on the research paper, <a href=\"https://arxiv.org/pdf/2407.02274\">DextrAH-G: Pixels-to-Action Dexterous Arm-Hand Grasping with Geometric Fabrics</a> and has been migrated from NVIDIA Isaac Gym (deprecated) to Isaac Lab. If you\u2019re a current Isaac Gym user, follow the <a href=\"https://isaac-sim.github.io/IsaacLab/main/source/tutorials/index.html\">tutorials</a> and <a href=\"https://isaac-sim.github.io/IsaacLab/main/source/migration/migrating_from_isaacgymenvs.html\">migration guide</a> to get started with Isaac Lab.\u00a0</p>\n\n\n\n<h2 id=\"gr00t-mobility_for_locomotion_and_navigation&nbsp;\"  class=\"wp-block-heading\">GR00T-Mobility for locomotion and navigation&nbsp;<a href=\"#gr00t-mobility_for_locomotion_and_navigation&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>GR00T-Mobility is a suite of models and policies for locomotion and navigation and reference workflows to develop them.&nbsp;</p>\n\n\n\n<p>Classical navigation methods struggle in cluttered environments and demand extensive tuning, while learning-based approaches face challenges in generalizing to new environments.&nbsp;</p>\n\n\n\n<p>GR00T-Mobility introduces a novel workflow built on RL and imitation learning (IL) supported in Isaac Lab, designed to create a mobility generalist for navigation across varied settings and embodiments.&nbsp;</p>\n\n\n\n<p>By leveraging world modeling using <a href=\"https://developer.nvidia.com/isaac/sim\">NVIDIA Isaac Sim</a>, this workflow generates a rich latent representation of environment dynamics, enabling more adaptable training. The workflow decouples world modeling from action policy learning and RL fine-tuning, thereby enhancing flexibility, supporting diverse data sources for greater generalization.</p>\n\n\n\n<p>A model trained (using this workflow) solely on photorealistic synthetic datasets from Isaac Sim enables zero-shot sim-to-real transfer and can be applied to a range of embodiments, including differential drive, Ackermann, quadrupeds, and humanoids.</p>\n\n\n\n<p>This workflow builds on NVIDIA Applied Research efforts presented in <a href=\"https://arxiv.org/abs/2410.17491\">X-MOBILITY: End-To-End Generalizable Navigation via World Modeling</a>.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"961\" height=\"460\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/groot-mobility-workflow.jpg\" alt=\"Block diagram showing workflow. GR00T-mobility is trained on synthetic data from Isaac Sim. The GR00T-mobility world model and action policy are trained on random actions and expert policy from simulation. This leads to generalizable navigation and zero-shot transfer of the policy from simulation to the real world. \n\" class=\"wp-image-91396\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/groot-mobility-workflow.jpg 961w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/groot-mobility-workflow-300x144.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/groot-mobility-workflow-625x299.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/groot-mobility-workflow-179x86.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/groot-mobility-workflow-768x368.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/groot-mobility-workflow-645x309.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/groot-mobility-workflow-500x239.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/groot-mobility-workflow-160x77.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/groot-mobility-workflow-362x173.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/groot-mobility-workflow-230x110.jpg 230w\" sizes=\"(max-width: 961px) 100vw, 961px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Using&nbsp; synthetic datasets from Isaac Sim, GR00T-Mobility workflow combines world modeling and action policy training to unlock generalizable navigation and zero-shot sim-to-real transfer</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/SgM0IM-J3hA?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em><em>Video 4. When testing GR00T-Mobility on a simulated robot, the robot successfully navigates cluttered surroundings while avoiding obstacles</em></em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/oQqDr7WXVlg?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 5. When testing GR00T-Mobility on a real robot, the robot navigates around a lab environment successfully while avoiding boxes and obstacles</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/-a4arsvFUzQ?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 6. Humanoid, quadruped, and forklift robots successfully navigating around a simulated warehouse environment in Isaac Sim</em></em></figcaption></figure>\n\n\n\n<h2 id=\"gr00t-control_for_whole-body_control\"  class=\"wp-block-heading\">GR00T-Control for whole-body control<a href=\"#gr00t-control_for_whole-body_control\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>GR00T-Control is a suite of advanced motion planning and control libraries, models, policies and reference workflows to develop WBC. Reference workflows exercise various platforms, pretrained models and accelerated libraries.&nbsp;</p>\n\n\n\n<p>To achieve precise and responsive humanoid robot control, especially in tasks requiring dexterity and locomotion, WBC is essential. GR00T-Control introduces a learning-based alternative to traditional model predictive control (MPC) with workflow integrated with Isaac Lab developed by the NVIDIA Applied Research team. This work is based on the original research work presented in <a href=\"https://omni.human2humanoid.com/resources/OmniH2O_paper.pdf\">OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning</a>, as well as the newly released <a href=\"https://hover-versatile-humanoid.github.io/\">HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots</a>.</p>\n\n\n\n<p>This reference workflow enables the development of whole-body humanoid control policies (WBC Policy) for both teleoperation and autonomous control. For OmniH2O\u2019s teleoperation, input methods such as VR headsets, RGB cameras, and verbal commands allow for high-precision human control. Meanwhile, HOVER\u2019s multimode policy distillation framework facilitates seamless transitions between autonomous task modes, making it adaptable for complex tasks</p>\n\n\n\n<p>The WBC Policy workflow uses a sim-to-real learning pipeline. It starts by training a privileged control policy in simulation with reinforcement learning using Isaac Lab, which serves as the &#8220;teacher&#8221; model with access to detailed motion data. This model is then distilled into a deployable real-world version capable of operating with limited sensory input, addressing challenges like teleoperation delays, restricted inputs from VR or visual tracking (for OmniH2O) and adaptability across multiple autonomous task modes (for HOVER).</p>\n\n\n\n<p>A whole-body control policy (developed using the OmniH2O workflow) provides 19 degrees of freedom for precise humanoid robot control.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/zoOXXgraSAw?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 7. GR00T-Control workflow based policy trained in NVIDIA Isaac Lab. The red boxes are the reference robot body positions from the data set. The robot is tracking the reference motion</em></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/KSKkTF9i0pA?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 8. Training thousands of humanoid robots together in Isaac Lab</em></em></figcaption></figure>\n\n\n\n<p>GR00T-Control offers roboticists tools for further exploration of learning-based WBC for humanoids.&nbsp;</p>\n\n\n\n<h2 id=\"gr00t-perception_for_multimodal_sensing\"  class=\"wp-block-heading\">GR00T-Perception for multimodal sensing<a href=\"#gr00t-perception_for_multimodal_sensing\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>GR00T-Perception is a suite of advanced perception libraries (such as <a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_nvblox/index.html#repo-name\">nvblox</a> and <a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_visual_slam/index.html#repo-name\">cuVSLAM</a>), foundation models (such as <a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_pose_estimation/index.html#repo-name\">FoundationPose</a> and <a href=\"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_object_detection/index.html#repo-name\">RT-DETR</a>), and reference workflows built on Isaac Sim and <a href=\"https://developer.nvidia.com/isaac/ros\">NVIDIA Isaac ROS</a>. The reference workflows showcase how to use these platforms, pretrained models and accelerated libraries together in robotics solutions.&nbsp;</p>\n\n\n\n<p>A key new addition to GR00T-Perception is <a href=\"https://nvidia-ai-iot.github.io/remembr/\">ReMEmbR</a>, an applied research reference workflow that enhances human-robot interaction by enabling robots to &#8220;remember&#8221; long histories of events, significantly improving personalized and context-aware responses. This workflow integrates vision language models, LLMs, and retrieval-augmented memory to boost perception, cognition, and adaptability in humanoid robots.&nbsp;</p>\n\n\n\n<p>ReMEmbR enables robots to retain contextual information over time, improving spatial awareness, navigation, and interaction efficiency by integrating sensory data like images and sounds. It follows a structured memory-building and querying process and can be deployed on NVIDIA Jetson AGX Orin on real robots.</p>\n\n\n\n<p>To learn more about ReMEmbR, see <a href=\"https://developer.nvidia.com/blog/using-generative-ai-to-enable-robots-to-reason-and-act-with-remembr/\">Using Generative AI to Enable Robots to Reason and Act with ReMEmbR</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1265\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow.png\" alt=\"Diagram showing the two phases involved in the ReMEmbR workflow. The first phase is the memory building phase, in which information about the environment through images and prompts is captured in a vector database. The second phase is the querying phase, in which a user can ask the system a question like \u201cWhere is the nearest elevator?\u201d.  The database built in the first phase is queried for the answer, which the robot can speak out to the user. It can additionally generate information to navigate to the desired location in the environment.  \" class=\"wp-image-91397\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow-300x190.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow-625x396.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow-768x486.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow-1536x972.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow-645x408.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow-474x300.png 474w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow-142x90.png 142w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow-362x229.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow-174x110.png 174w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/remembr-workflow-1024x648.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. The high-level ReMEmbR workflow includes two different phases, one for memory building and the other for querying information</em></figcaption></figure></div>\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/JlYVBAQC0tQ?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 9. Discover how robots use generative AI to reason and act with ReMEmbR</em></figcaption></figure>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With <a href=\"https://developer.nvidia.com/project-gr00t\">NVIDIA Project GR00T</a> we are building advanced technology, tools, and GR00T workflows that can be used standalone or together based on humanoid robot developer needs. These enhancements contribute to the development of more intelligent, adaptive, and capable humanoid robots, pushing the boundaries of what these robots can achieve in real-world applications.</p>\n\n\n\n<p>Learn more about how <a href=\"https://blogs.nvidia.com/blog/robot-learning-humanoid-development\">leading robotics companies</a> are using NVIDIA platforms, including 1X, <a href=\"https://agilityrobotics.com/content/crossing-sim2real-gap-with-isaaclab\">Agility Robotics</a>, The AI Institute, <a href=\"https://berkeley-humanoid.com\">Berkeley Humanoid</a>, <a href=\"https://support.bostondynamics.com/s/article/Get-Started-with-Reinforcement-Learning-for-Spot-49966\">Boston Dynamics</a>, <a href=\"https://fieldai.com/news/field-ai-nvidia-partnership\">Field AI</a>, <a href=\"https://developer.nvidia.com/blog/spotlight-fourier-trains-humanoid-robots-for-real-world-roles-using-nvidia-isaac-gym/\">Fourier</a>, <a href=\"https://developer.nvidia.com/blog/spotlight-galbot-builds-a-large-scale-dexterous-hand-dataset-for-humanoid-robots-using-nvidia-isaac-sim/\">Galbot</a>, <a href=\"https://menteebot.com/blog/#shopping-companion-2024\">Mentee Robotics</a>, Skild AI, Swiss-Mile, Unitree Robotics, and XPENG Robotics.</p>\n\n\n\n<p>Learn more about NVIDIA at CoRL 2024, including <a href=\"https://www.nvidia.com/en-us/events/corl/#papers\">21 papers and nine workshops </a>related to robot learning, released training and workflow guides for developers.</p>\n\n\n\n<h2 id=\"get_started_now\"  class=\"wp-block-heading\">Get started now<a href=\"#get_started_now\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Get started with <a href=\"https://isaac-sim.github.io/IsaacLab/main/index.html\">Isaac Lab,</a> or <a href=\"https://isaac-sim.github.io/IsaacLab/main/source/migration/migrating_from_isaacgymenvs.html\">migrate from Isaac Gym to Isaac Lab</a> with new <a href=\"https://isaac-sim.github.io/IsaacLab/main/source/tutorials/index.html#\">developer onboarding guides and tutorials</a>.&nbsp;Check out the <a href=\"https://isaac-sim.github.io/IsaacLab/main/source/refs/reference_architecture/index.html\">Isaac Lab Reference Architecture</a> to understand the end-to-end robot learning process with Isaac Lab and Isaac Sim.</p>\n\n\n\n<p>Discover the latest in robot learning and simulation in the <a href=\"https://www.addevent.com/event/GA23422424\">livestream on November 13</a>, and don\u2019t miss the <a href=\"https://www.addevent.com/event/Uz23738360\">NVIDIA Isaac Lab Office Hours</a> for hands-on support and insights.&nbsp;</p>\n\n\n\n<p>If you\u2019re a humanoid robot company building software or hardware for the humanoid ecosystem, apply to join the <a href=\"https://developer.nvidia.com/humanoid-robot-program\">NVIDIA Humanoid Robot Developer Program</a>.&nbsp;</p>\n\n\n\n<h3 id=\"acknowledgments\"  class=\"wp-block-heading\">Acknowledgments<a href=\"#acknowledgments\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><em>This work would not have been possible without the contributions of Chenran Li, Huihua Zhao, Jim Fan, Karl Van Wyk, Oyindamola Omotuyi, Shri Sundaram, Soha Pouya, Spencer Huang, Wei Liu, Yuke Zhu, and many others.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Humanoid robots present a multifaceted challenge at the intersection of mechatronics, control theory, and AI. The dynamics and control of humanoid robots are complex, requiring advanced tools, techniques, and algorithms to maintain balance during locomotion and manipulation tasks. Collecting robot data and integrating sensors also pose significant challenges, as humanoid robots require a fusion of &hellip; <a href=\"https://developer.nvidia.com/blog/advancing-humanoid-robot-sight-and-skill-development-with-nvidia-project-gr00t/\">Continued</a></p>\n", "protected": false}, "author": 890, "featured_media": 91389, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1515723", "discourse_permalink": "https://forums.developer.nvidia.com/t/advancing-humanoid-robot-sight-and-skill-development-with-nvidia-project-gr00t/312439", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [63, 503], "tags": [4145, 2932, 3700], "coauthors": [1527, 4028], "class_list": ["post-91333", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-robotics", "category-simulation-modeling-design", "tag-humanoid-robots", "tag-large-language-models", "tag-openusd"], "acf": {"post_industry": ["Manufacturing"], "post_products": ["Isaac Sim"], "post_learning_levels": ["General Interest"], "post_content_types": ["Deep dive", "News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/humanoid-robot-gif.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-nL7", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Robotics", "link": "https://developer.nvidia.com/blog/category/robotics/", "id": 63}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91333"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/890"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91333"}], "version-history": [{"count": 23, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91333/revisions"}], "predecessor-version": [{"id": 91541, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91333/revisions/91541"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91389"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91333"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91333"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91333"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91333"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91184, "date": "2024-11-06T08:00:00", "date_gmt": "2024-11-06T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91184"}, "modified": "2024-11-06T09:57:02", "modified_gmt": "2024-11-06T17:57:02", "slug": "state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/", "title": {"rendered": "State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo"}, "content": {"rendered": "\n<p><a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">Generative AI</a> has rapidly evolved from text-based models to multimodal capabilities. These models perform tasks like image captioning and visual question answering, reflecting a shift toward more human-like AI. The community is now expanding from text and images to video, opening new possibilities across industries.</p>\n\n\n\n<p>Video AI models are poised to revolutionize industries such as robotics, automotive, and retail. In <a href=\"https://www.nvidia.com/en-us/industries/robotics/\">robotics</a>, they enhance autonomous navigation in complex, ever-changing environments, which is vital for sectors like manufacturing and warehouse management. In the automotive industry, video AI is propelling autonomous driving, boosting vehicle perception, safety, and predictive maintenance to improve efficiency.&nbsp;</p>\n\n\n\n<p>To build image and video foundation models, developers must curate and preprocess a large amount of training data, tokenize the resulting high-quality data at high fidelity, train or customize pretrained models efficiently and at scale, and then generate high-quality images and videos during inference.&nbsp;</p>\n\n\n\n<h2 id=\"announcing_nvidia_nemo_for_multimodal_generative_ai\"  class=\"wp-block-heading\">Announcing NVIDIA NeMo for multimodal generative AI<a href=\"#announcing_nvidia_nemo_for_multimodal_generative_ai\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo</a> is an end-to-end platform for developing, customizing, and deploying generative AI models.&nbsp;</p>\n\n\n\n<p>NVIDIA just announced the expansion of NeMo to support the end-to-end pipeline for developing multimodal models. NeMo enables you to easily curate high-quality visual data, accelerate <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-training/\">training</a> and <a href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\">customization</a> with highly efficient tokenizers and parallelism techniques, and reconstruct high-quality visuals during inference.&nbsp;</p>\n\n\n\n<h2 id=\"accelerated_video_and_image_data_curation\"  class=\"wp-block-heading\">Accelerated video and image data curation<a href=\"#accelerated_video_and_image_data_curation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>High-quality training data ensures high-accuracy results from an AI model. However, developers face various challenges in building data processing pipelines, ranging from scaling to data orchestration.&nbsp;</p>\n\n\n\n<p><a href=\"https://github.com/NVIDIA/NeMo-Curator\">NeMo Curator</a> streamlines the data curation process, making it easier and faster for you to build multimodal generative AI models. Its out-of-the-box experience minimizes the total cost of ownership (TCO) and accelerates time-to-market.&nbsp;</p>\n\n\n\n<p>While working with visuals, organizations can easily reach petabyte-scale data processing. NeMo Curator provides an orchestration pipeline that can load balance on multiple GPUs at each stage of the data curation. As a result, you can reduce video processing time by 7x compared to a naive GPU-based implementation. The scalable pipelines can efficiently process over 100 PB of data, ensuring the seamless handling of large datasets.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"882\" height=\"572\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-curator-video-processing-speed.png\" alt=\"The bar chart compares an unoptimized data curation pipeline to an NVIDIA NeMo Curator pipeline. NVIDIA NeMo Curator delivers up to 7x faster processing of video to generate high-quality training data. For this data, 1M hours of video were processed.\" class=\"wp-image-91450\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-curator-video-processing-speed.png 882w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-curator-video-processing-speed-300x195.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-curator-video-processing-speed-625x405.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-curator-video-processing-speed-177x115.png 177w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-curator-video-processing-speed-768x498.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-curator-video-processing-speed-645x418.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-curator-video-processing-speed-463x300.png 463w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-curator-video-processing-speed-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-curator-video-processing-speed-139x90.png 139w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-curator-video-processing-speed-362x235.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/nemo-curator-video-processing-speed-170x110.png 170w\" sizes=\"(max-width: 882px) 100vw, 882px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA NeMo Curator video processing speed</em></figcaption></figure></div>\n\n\n<p>NeMo Curator provides reference video curation models optimized for high-throughput filtering, captioning, and embedding stages to enhance dataset quality, empowering you to create more accurate AI models.&nbsp;</p>\n\n\n\n<p>For instance, NeMo Curator uses an optimized captioning model that delivers an order of magnitude throughput improvement compared to unoptimized inference model implementations.</p>\n\n\n\n<h2 id=\"nvidia_cosmos_tokenizers\"  class=\"wp-block-heading\">NVIDIA Cosmos tokenizers<a href=\"#nvidia_cosmos_tokenizers\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Tokenizers map redundant and implicit visual data into compact and semantic tokens, enabling efficient training of large-scale generative models and democratizing their inference on limited computational resources.</p>\n\n\n\n<p>Today&#8217;s open video and image tokenizers often generate poor data representations, leading to lossy reconstructions, distorted images, and temporally unstable videos and placing a cap on the capability of generative models built on top of the tokenizers.. Inefficient tokenization processes also result in slow encoding and decoding and longer training and inference times, negatively impacting both developer productivity and the user experience.</p>\n\n\n\n<p>NVIDIA Cosmos tokenizers are open models that offer superior visual tokenization with exceptionally large compression rates and cutting-edge reconstruction quality across diverse image and video categories.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Soy_myOfWIU?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Efficient Generative AI Tokenizers for Image and Video</em></figcaption></figure>\n\n\n\n<p>These tokenizers provide ease of use through a suite of tokenizer standardized models that support vision-language models (VLMs) with discrete latent codes, diffusion models with continuous latent embeddings, and various aspect ratios and resolutions, enabling the efficient management of large-resolution images and videos. This provides you with tools for tokenizing a wide variety of visual input data to build image and video AI models.</p>\n\n\n\n<h3 id=\"cosmos_tokenizer_architecture\"  class=\"wp-block-heading\">Cosmos tokenizer architecture<a href=\"#cosmos_tokenizer_architecture\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>A Cosmos tokenizer uses a sophisticated encoder-decoder structure designed for high efficiency and effective learning. At its core, it employs 3D <em>causal convolution blocks</em>, which are specialized layers that jointly process spatiotemporal information, and uses causal temporal attention that captures long-range dependencies in data.&nbsp;</p>\n\n\n\n<p>The causal structure ensures that the model uses only past and present frames when performing tokenization, avoiding future frames. This is crucial for aligning with the causal nature of many real-world systems, such as those in physical AI or multimodal LLMs. </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"374\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cosmos-tokenizer-architecture-1024x374.png\" alt=\"The diagram shows various components, from processing the data with a 3D wavelet and encoding with casual convolution to generating tokens in latent space. Then it shows the reverse process to reconstruct visuals from the generated tokens.\" class=\"wp-image-91193\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cosmos-tokenizer-architecture-1024x374.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cosmos-tokenizer-architecture-300x109.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cosmos-tokenizer-architecture-625x228.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cosmos-tokenizer-architecture-179x65.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cosmos-tokenizer-architecture-768x280.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cosmos-tokenizer-architecture-645x235.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cosmos-tokenizer-architecture-500x182.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cosmos-tokenizer-architecture-160x58.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cosmos-tokenizer-architecture-362x132.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cosmos-tokenizer-architecture-302x110.png 302w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cosmos-tokenizer-architecture.png 1524w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA Cosmos tokenizer architecture</em></figcaption></figure></div>\n\n\n<p>The input is downsampled using 3D wavelets, a signal processing technique that represents pixel information more efficiently. After the data is processed, an inverse wavelet transform reconstructs the original input.&nbsp;</p>\n\n\n\n<p>This approach improves learning efficiency, enabling the tokenizer encoder-decoder learnable modules to focus on meaningful features rather than redundant pixel details. The combination of such techniques and its unique training recipe makes the Cosmos tokenizers a cutting-edge architecture for efficient and powerful tokenization.</p>\n\n\n\n<p>During inference, the Cosmos tokenizers significantly reduce the cost of running the model by delivering up to 12x faster reconstruction compared to leading open-weight tokenizers (Figure 3).&nbsp;&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"820\" height=\"456\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/quantitative-comparison-quality-vs-performance.png\" alt=\"The bar graph compares the relative speedup of Cosmos tokenizer reconstruction time over open tokenizer models CogX and Omni. The graph shows 12x faster processing for 4x8x8, 8x8x8, and 8x16x16 compression rates compared to 4x8x8 for CogX and Omni.\" class=\"wp-image-91192\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/quantitative-comparison-quality-vs-performance.png 820w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/quantitative-comparison-quality-vs-performance-300x167.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/quantitative-comparison-quality-vs-performance-625x348.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/quantitative-comparison-quality-vs-performance-179x100.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/quantitative-comparison-quality-vs-performance-768x427.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/quantitative-comparison-quality-vs-performance-645x359.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/quantitative-comparison-quality-vs-performance-500x278.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/quantitative-comparison-quality-vs-performance-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/quantitative-comparison-quality-vs-performance-362x201.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/quantitative-comparison-quality-vs-performance-198x110.png 198w\" sizes=\"(max-width: 820px) 100vw, 820px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Quantitative comparison of reconstruction quality (left) and runtime performance (right) for video tokenizers</em></figcaption></figure></div>\n\n\n<p>The Cosmos tokenizers also produce high-fidelity images and videos while compressing more than other tokenizers, demonstrating an unprecedented quality-compression trade-off.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"469\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality-625x469.png\" alt=\"The dot plot shows reconstruction image quality generated by various continuous image and video tokenizers based on the different compression rates. Cosmos delivers the highest quality across different compression rates.\" class=\"wp-image-91299\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality-625x469.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality-300x225.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality-153x115.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality-768x576.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality-1536x1152.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality-645x484.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality-400x300.png 400w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality-120x90.png 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality-362x271.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality-147x110.png 147w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality-1024x768.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/continuous-tokenizers-quality.png 1574w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Continuous tokenizer compression rate compared to reconstruction quality</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"720\" height=\"540\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/discrete-tokenizer-compression-vs-quality.png\" alt=\"The dot plot shows reconstruction image quality generated by various discrete image and video tokenizers based on the different compression rates. Cosmos delivers the highest quality across different compression rates.\" class=\"wp-image-91191\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/discrete-tokenizer-compression-vs-quality.png 720w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/discrete-tokenizer-compression-vs-quality-300x225.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/discrete-tokenizer-compression-vs-quality-625x469.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/discrete-tokenizer-compression-vs-quality-153x115.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/discrete-tokenizer-compression-vs-quality-645x484.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/discrete-tokenizer-compression-vs-quality-400x300.png 400w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/discrete-tokenizer-compression-vs-quality-120x90.png 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/discrete-tokenizer-compression-vs-quality-362x272.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/discrete-tokenizer-compression-vs-quality-147x110.png 147w\" sizes=\"(max-width: 720px) 100vw, 720px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Discrete tokenizer compression rate compared to reconstruction quality</em></figcaption></figure></div>\n\n\n<p>Although the Cosmos tokenizer regenerates from highly compressed tokens, it is capable of creating high-quality images and videos due to an innovative neural network training technique and architecture.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"755\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/reconstructed-video-frames-2-1024x755.png\" alt=\"Three images of reconstructed images generated by different tokenizers, including Omni, CogX, and Cosmos. The Cosmos tokenizer provides the highest fidelity when compared to the ground truth.\" class=\"wp-image-91301\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/reconstructed-video-frames-2-1024x755.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/reconstructed-video-frames-2-300x221.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/reconstructed-video-frames-2-625x461.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/reconstructed-video-frames-2-156x115.png 156w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/reconstructed-video-frames-2-768x566.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/reconstructed-video-frames-2-645x475.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/reconstructed-video-frames-2-407x300.png 407w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/reconstructed-video-frames-2-122x90.png 122w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/reconstructed-video-frames-2-362x267.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/reconstructed-video-frames-2-149x110.png 149w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/reconstructed-video-frames-2.png 1198w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Reconstructed video frame for continuous video tokenizers</em></figcaption></figure></div>\n\n\n<h2 id=\"build_your_own_multimodal_models_with_nemo\"  class=\"wp-block-heading\">Build Your Own Multimodal Models with NeMo<a href=\"#build_your_own_multimodal_models_with_nemo\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The expansion of the NVIDIA NeMo platform with at-scale data processing using <a href=\"https://github.com/NVIDIA/NeMo-Curator\">NeMo Curator</a> and high-quality tokenization and visual reconstruction using the Cosmos tokenizer empowers you to build state-of-the-art multimodal, generative AI models.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/news/\">Join the waitlist</a> and be notified when NeMo Curator is available. The tokenizer is available now on the <a href=\"http://github.com/NVIDIA/cosmos-tokenizer\">/NVIDIA/cosmos-tokenizer</a> GitHub repo and <a href=\"https://huggingface.co/nvidia/Cosmos-Tokenizer-CV8x8x8\">Hugging Face</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Generative AI has rapidly evolved from text-based models to multimodal capabilities. These models perform tasks like image captioning and visual question answering, reflecting a shift toward more human-like AI. The community is now expanding from text and images to video, opening new possibilities across industries. Video AI models are poised to revolutionize industries such as &hellip; <a href=\"https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/\">Continued</a></p>\n", "protected": false}, "author": 2405, "featured_media": 91187, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1515722", "discourse_permalink": "https://forums.developer.nvidia.com/t/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/312438", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 63], "tags": [38, 561], "coauthors": [4154, 610, 3612, 3081], "class_list": ["post-91184", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "category-robotics", "tag-image-processing", "tag-video-processing"], "acf": {"post_industry": ["General"], "post_products": ["NeMo", "NeMo Curator"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Benchmark"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nemo-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nII", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91184"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2405"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91184"}], "version-history": [{"count": 13, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91184/revisions"}], "predecessor-version": [{"id": 91531, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91184/revisions/91531"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91187"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91184"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91184"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91184"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91184"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91111, "date": "2024-11-05T10:00:00", "date_gmt": "2024-11-05T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91111"}, "modified": "2024-11-07T10:38:21", "modified_gmt": "2024-11-07T18:38:21", "slug": "leverage-ai-coding-assistants-to-develop-quantum-applications-at-scale-with-nvidia-cuda-q", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/leverage-ai-coding-assistants-to-develop-quantum-applications-at-scale-with-nvidia-cuda-q/", "title": {"rendered": "Leverage AI Coding Assistants to Develop Quantum Applications at Scale with NVIDIA CUDA-Q"}, "content": {"rendered": "\n<p>AI coding assistants have become ubiquitous across the software development landscape. Developers are increasingly using tools like GitHub Copilot, Amazon CodeWhisperer, and Cursor to boost productivity in computing tasks. These tools enable you to quickly generate, debug, and understand code; streamline workflows; and enhance collaboration across projects.</p>\n\n\n\n<p>While AI coding assistants are well-established in classical computing, their application in <a href=\"https://www.nvidia.com/en-us/glossary/quantum-computing/\">quantum computing</a> is just beginning to gain traction. This application is one of many ways <a href=\"https://developer.nvidia.com/blog/enabling-quantum-computing-with-ai/\">AI can enable quantum computing</a>. This post explores how <a href=\"https://www.cursor.com/\">Cursor</a>, one of the leading AI-assisted integrated development environments (IDE), can be leveraged to develop code for <a href=\"https://developer.nvidia.com/cuda-q\">NVIDIA CUDA-Q</a>, a platform for high-performance, hybrid quantum applications.&nbsp;</p>\n\n\n\n<p>CUDA-Q is an <a href=\"https://github.com/NVIDIA/cuda-quantum\">open-source</a> platform that integrates GPUs, CPUs, and QPUs to enable scalable, hybrid quantum computing. It&#8217;s currently used by a diverse range of users, from students to quantum researchers to <a href=\"https://blogs.nvidia.com/blog/denmark-sovereign-ai-supercomputer/\">life scientists</a> to HPC scientists leveraging today\u2019s <a href=\"https://nvidianews.nvidia.com/news/nvidia-accelerates-quantum-computing-centers-worldwide-with-cuda-q-platform\">most advanced GPU-supercomputers</a>.</p>\n\n\n\n<p>With AI coding assistants, getting started with CUDA-Q and developing quantum applications at scale has never been easier. The following sections demonstrate how to use Cursor to:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Generate CUDA-Q code</li>\n\n\n\n<li>Query the codebase and docs&nbsp;</li>\n\n\n\n<li>Port existing code to CUDA-Q</li>\n</ul>\n\n\n\n<p>These examples will showcase how AI tools can streamline the process of building and accelerating CUDA-Q applications at scale.&nbsp;</p>\n\n\n\n<h2 id=\"getting_started\"  class=\"wp-block-heading\">Getting started<a href=\"#getting_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To begin, download and install <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/quick_start.html\">CUDA-Q</a> and <a href=\"https://www.cursor.com/\">Cursor</a>. An easy way to install CUDA-Q is to pull the CUDA-Q <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/install/local_installation.html#install-docker-image\">Docker image</a> and run the container. The PyPI Python installation instructions, as well as the C++ installation guide can be found in <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/quick_start.html\">CUDA-Q Quick Start</a>.&nbsp;</p>\n\n\n\n<p>Once the CUDA-Q container is running, download and install Cursor. Cursor\u2019s look and feel will be very familiar to users of Microsoft Visual Studio Code (Cursor was forked from VSCode). Then attach Cursor to the CUDA-Q container and open the environment, which will show a variety of CUDA-Q examples and tutorials included in the container. Cursor may also prompt you to install additional extensions that it detects are necessary for the CUDA-Q environment.</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"601\" height=\"256\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/dropdown-menu-cursor-ide.png\" alt=\"Screenshot of a dropdown menu in the Cursor IDE.\" class=\"wp-image-91118\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/dropdown-menu-cursor-ide.png 601w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/dropdown-menu-cursor-ide-300x128.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/dropdown-menu-cursor-ide-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/dropdown-menu-cursor-ide-500x213.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/dropdown-menu-cursor-ide-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/dropdown-menu-cursor-ide-362x154.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/dropdown-menu-cursor-ide-258x110.png 258w\" sizes=\"(max-width: 601px) 100vw, 601px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Connecting Cursor to a running CUDA-Q container with the Attach to Running Container option</em></figcaption></figure></div>\n\n\n\n<p>With a fresh Python notebook, you can start using the Cursor AI features to generate CUDA-Q code. Cursor offers a range of base models, including claude-3.5-sonnet, gpt-4o, cursor-small, and several others. It can index your entire codebase, enabling you to query it directly. Additionally, Cursor enables you to add context to queries by specifying files, documentation, and websites to the chat context before it provides answers. Its knowledge base was trained on major quantum computing frameworks, so it knows CUDA-Q syntax out of the box. Note that the examples in this post use the claude-3.5-sonnet model from the free version of Cursor.</p>\n\n\n\n<h2 id=\"generating_code\"  class=\"wp-block-heading\">Generating code<a href=\"#generating_code\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Use the Cursor built-in chat window (\u2318-L) to start generating a few CUDA-Q examples. To improve Cursor\u2019s responses, explicitly link the CUDA-Q docs to Cursor\u2019s chat context. In the chat window, type @docs, select +New doc, and link the URL of the <a href=\"https://nvidia.github.io/cuda-quantum/latest/index.html\">CUDA-Q documentation</a>.</p>\n\n\n\n<p>For example, ask the chat, \u201cHow do I initialize a CUDA-Q kernel?\u201d</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"827\" height=\"403\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-window-generating-cuda-q-kernel.png\" alt=\"Screenshot of Cursor\u2019s chat window generating a CUDA-Q kernel\n\" class=\"wp-image-91119\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-window-generating-cuda-q-kernel.png 827w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-window-generating-cuda-q-kernel-300x146.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-window-generating-cuda-q-kernel-625x305.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-window-generating-cuda-q-kernel-179x87.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-window-generating-cuda-q-kernel-768x374.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-window-generating-cuda-q-kernel-645x314.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-window-generating-cuda-q-kernel-500x244.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-window-generating-cuda-q-kernel-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-window-generating-cuda-q-kernel-362x176.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-window-generating-cuda-q-kernel-226x110.png 226w\" sizes=\"(max-width: 827px) 100vw, 827px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Asking Cursor\u2019s chat how to initialize a CUDA-Q kernel</em></figcaption></figure>\n\n\n\n<p>Cursor correctly outputs a simple hello world example to define a quantum kernel, giving a superposition state.&nbsp;</p>\n\n\n\n<p><a href=\"https://nvidia.github.io/cuda-quantum/latest/using/basics/kernel_intro.html\">CUDA-Q kernels</a> are functions that can be executed on a quantum resource. The quantum resource in these examples will be the CPU simulation backend; however, CUDA-Q makes it easy to accelerate your simulations on <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/backends/simulators.html\">GPU systems of all scales</a> and a variety of <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/backends/hardware.html\">physical QPUs</a>.</p>\n\n\n\n<p>You can ask for an explanation of certain aspects of the code, or ask follow-up questions. In response to the question, \u201cShow me how to execute the kernel and print the results\u201d (Figure 3). The chat generates a Bell state example and an explanation (Figure 4).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"834\" height=\"739\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-generating-code-executing-cuda-q-kernels.png\" alt=\"Screenshot  of Cursor\u2019s chat generating follow-up code on executing CUDA-Q kernels\n\" class=\"wp-image-91120\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-generating-code-executing-cuda-q-kernels.png 834w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-generating-code-executing-cuda-q-kernels-300x266.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-generating-code-executing-cuda-q-kernels-625x554.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-generating-code-executing-cuda-q-kernels-130x115.png 130w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-generating-code-executing-cuda-q-kernels-768x681.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-generating-code-executing-cuda-q-kernels-645x572.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-generating-code-executing-cuda-q-kernels-339x300.png 339w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-generating-code-executing-cuda-q-kernels-102x90.png 102w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-generating-code-executing-cuda-q-kernels-362x321.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-generating-code-executing-cuda-q-kernels-124x110.png 124w\" sizes=\"(max-width: 834px) 100vw, 834px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Asking Cursor a follow-up question on how to execute CUDA-Q kernels</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"824\" height=\"581\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-explanation-of-generated-code.png\" alt=\"Screenshot of Cursor\u2019s chat giving an explanation of the generated code\n\" class=\"wp-image-91122\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-explanation-of-generated-code.png 824w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-explanation-of-generated-code-300x212.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-explanation-of-generated-code-625x441.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-explanation-of-generated-code-163x115.png 163w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-explanation-of-generated-code-768x542.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-explanation-of-generated-code-645x455.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-explanation-of-generated-code-425x300.png 425w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-explanation-of-generated-code-128x90.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-explanation-of-generated-code-362x255.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-chat-explanation-of-generated-code-156x110.png 156w\" sizes=\"(max-width: 824px) 100vw, 824px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Cursor\u2019s explanation of the code it generated to execute a CUDA-Q kernel using the sample method</em></figcaption></figure>\n\n\n\n<p>To execute the code, click the \u2018Run as cell\u2019 button. This correctly outputs the following statistics of the Bell state:</p>\n\n\n\n<p><code>{ 00:504 11:496 }</code></p>\n\n\n\n<p><code>Measured 00: 504 times<br>Measured 11: 496 times</code></p>\n\n\n\n<p>You can also ask the chat to generate a more advanced example of a single layer of a quantum neural network with parameterized gates:</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"819\" height=\"800\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer.png\" alt=\"Screenshot of Cursor generating a quantum neural network layer\n\" class=\"wp-image-91126\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer.png 819w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer-300x293.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer-625x611.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer-118x115.png 118w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer-768x750.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer-645x630.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer-307x300.png 307w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer-92x90.png 92w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer-362x354.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-quantum-neural-network-layer-113x110.png 113w\" sizes=\"(max-width: 819px) 100vw, 819px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Cursor generating a more advanced CUDA-Q example with a quantum neural network layer with parameterized gates</em></figcaption></figure>\n\n\n\n<p>The chat correctly generates a kernel called <code>qnn_layer</code>, which rotates each qubit with three parameters, then applies a layer of entangling gates across four qubits. It provides the following helpful explanation:</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"819\" height=\"402\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-generated-quantum-neural-network-layer.png\" alt=\"Screenshot of Cursor\u2019s explanation for the generated quantum neural network layer\n\" class=\"wp-image-91128\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-generated-quantum-neural-network-layer.png 819w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-generated-quantum-neural-network-layer-300x147.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-generated-quantum-neural-network-layer-625x307.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-generated-quantum-neural-network-layer-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-generated-quantum-neural-network-layer-768x377.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-generated-quantum-neural-network-layer-645x317.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-generated-quantum-neural-network-layer-500x245.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-generated-quantum-neural-network-layer-160x79.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-generated-quantum-neural-network-layer-362x178.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-generated-quantum-neural-network-layer-224x110.png 224w\" sizes=\"(max-width: 819px) 100vw, 819px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Cursor\u2019s explanation of the generated quantum neural network layer</em></figcaption></figure>\n\n\n\n<p>The chat is a good way for beginners to generate examples from scratch. Users who are already familiar with CUDA-Q can proceed to code in the IDE and use Cursor\u2019s tab-complete feature, which already knows CUDA-Q syntax.</p>\n\n\n\n<h2 id=\"querying_the_codebase\"  class=\"wp-block-heading\">Querying the codebase<a href=\"#querying_the_codebase\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Use the chat to ask questions and explore CUDA-Q features directly by linking the docs and specific files explicitly with the @ command. Unfamiliar codebases can be challenging to navigate, and useful features can sometimes be hidden or hard to find. While reading the docs and combing through the source code is often the recommended way of familiarizing oneself with a framework, the reality is few people have the time or patience to do this comprehensively.&nbsp;</p>\n\n\n\n<p>Querying the codebase and docs through the chat directly is an amazingly effective way to understand a codebase\u2019s structure, capabilities, and hidden features. This approach enables interactive learning and encourages follow-up questions, enabling users to develop familiarity with CUDA-Q organically.&nbsp;</p>\n\n\n\n<p>You can use the chat to learn how to return the statevector instead of shots, and to ask about other simulation backends available with CUDA-Q.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"829\" height=\"764\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/querying-cursor-chat-cuda-q-backends.png\" alt=\"Screenshot of querying Cursor\u2019s chat on how to use other CUDA-Q backends\n\" class=\"wp-image-91129\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/querying-cursor-chat-cuda-q-backends.png 829w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/querying-cursor-chat-cuda-q-backends-300x276.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/querying-cursor-chat-cuda-q-backends-625x576.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/querying-cursor-chat-cuda-q-backends-125x115.png 125w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/querying-cursor-chat-cuda-q-backends-768x708.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/querying-cursor-chat-cuda-q-backends-645x594.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/querying-cursor-chat-cuda-q-backends-326x300.png 326w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/querying-cursor-chat-cuda-q-backends-98x90.png 98w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/querying-cursor-chat-cuda-q-backends-362x334.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/querying-cursor-chat-cuda-q-backends-119x110.png 119w\" sizes=\"(max-width: 829px) 100vw, 829px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. Using Cursor to query the CUDA-Q codebase on how to use other backends</em></figcaption></figure>\n\n\n\n<p>The chat correctly points to the <code>get_state</code> function, which when executed yields the real and imaginary components of the qubit in the superposition state:</p>\n\n\n\n<p><code>(0.707107,0)<br>(0.707107,0)</code></p>\n\n\n\n<p>For setting a different simulation target, the chat provides two other options, the <code>\u201ddensity-matrix-cpu\u201d</code> simulator and <code>\u201cnvidia\u201d</code> target for GPU-accelerated simulations. Additional helpful (and correct) examples include invoking the <code>cudaq.draw</code> function to visualize circuits:</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"820\" height=\"659\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-examples-gpu-accelerated-simulation-backends.png\" alt=\"Screenshot of Cursor giving additional examples of using GPU-accelerated simulation backends\n\" class=\"wp-image-91135\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-examples-gpu-accelerated-simulation-backends.png 820w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-examples-gpu-accelerated-simulation-backends-300x241.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-examples-gpu-accelerated-simulation-backends-625x502.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-examples-gpu-accelerated-simulation-backends-143x115.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-examples-gpu-accelerated-simulation-backends-768x617.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-examples-gpu-accelerated-simulation-backends-645x518.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-examples-gpu-accelerated-simulation-backends-373x300.png 373w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-examples-gpu-accelerated-simulation-backends-112x90.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-examples-gpu-accelerated-simulation-backends-362x291.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-examples-gpu-accelerated-simulation-backends-137x110.png 137w\" sizes=\"(max-width: 820px) 100vw, 820px\" /><figcaption class=\"wp-element-caption\"><em>Figure 8. Examples of using GPU-accelerated simulation backends</em></figcaption></figure>\n\n\n\n<p>Running the following cell (Figure 9, top) returns the circuit diagram (Figure 9, bottom):</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"806\" height=\"517\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-example-display-quantum-circuits-with-corresponding-output.png\" alt=\"An image of Cursor generating an example of how to display quantum circuits (above) with the corresponding output (below).\" class=\"wp-image-91147\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-example-display-quantum-circuits-with-corresponding-output.png 806w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-example-display-quantum-circuits-with-corresponding-output-300x192.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-example-display-quantum-circuits-with-corresponding-output-625x401.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-example-display-quantum-circuits-with-corresponding-output-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-example-display-quantum-circuits-with-corresponding-output-768x493.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-example-display-quantum-circuits-with-corresponding-output-645x414.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-example-display-quantum-circuits-with-corresponding-output-468x300.png 468w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-example-display-quantum-circuits-with-corresponding-output-140x90.png 140w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-example-display-quantum-circuits-with-corresponding-output-362x232.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-example-display-quantum-circuits-with-corresponding-output-171x110.png 171w\" sizes=\"(max-width: 806px) 100vw, 806px\" /><figcaption class=\"wp-element-caption\"><em>Figure 9. Cursor generating examples of displaying generated quantum circuits using the draw method (top) and the corresponding output (bottom)</em></figcaption></figure></div>\n\n\n\n<p>For other visualization tools, Cursor points out some of the new CUDA-Q features from the latest <a href=\"https://developer.nvidia.com/blog/performant-quantum-programming-even-easier-with-nvidia-cuda-q-v0-8/\">v0.8 release</a> to visualize qubit states on the Bloch sphere:</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"667\" height=\"519\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-code-visualize-qubit-state-bloch-sphere.png\" alt=\"Screenshot of Cursor generating code to visualize the qubit state on the Bloch sphere\n\" class=\"wp-image-91151\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-code-visualize-qubit-state-bloch-sphere.png 667w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-code-visualize-qubit-state-bloch-sphere-300x233.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-code-visualize-qubit-state-bloch-sphere-625x486.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-code-visualize-qubit-state-bloch-sphere-148x115.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-code-visualize-qubit-state-bloch-sphere-645x502.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-code-visualize-qubit-state-bloch-sphere-386x300.png 386w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-code-visualize-qubit-state-bloch-sphere-116x90.png 116w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-code-visualize-qubit-state-bloch-sphere-362x282.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-generating-code-visualize-qubit-state-bloch-sphere-141x110.png 141w\" sizes=\"(max-width: 667px) 100vw, 667px\" /><figcaption class=\"wp-element-caption\"><em>Figure 10. Cursor generating code to visualize the qubit state on the Bloch sphere, using features from the latest CUDA-Q v0.8 release</em></figcaption></figure>\n\n\n\n<p>This produces the following Bloch sphere visualization:</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"519\" height=\"519\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere.png\" alt=\"An image of the Bloch sphere \n\" class=\"wp-image-91152\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere.png 519w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere-300x300.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere-115x115.png 115w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere-90x90.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere-150x150.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere-362x362.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bloch-sphere-110x110.png 110w\" sizes=\"(max-width: 519px) 100vw, 519px\" /><figcaption class=\"wp-element-caption\"><em>Figure 11. Bloch sphere visualization in CUDA-Q of the generated Cursor code</em></figcaption></figure>\n\n\n\n<p>It represents the state:&nbsp;</p>\n\n\n\n<p><code>(-0.00150066,-0.00170142)<br>(0.996383,-0.084946)</code></p>\n\n\n\n<p>It is produced by the rotation angles: <code>[0.80783041, 3.13705533, 3.77932564]</code></p>\n\n\n\n<h2 id=\"porting_to_cuda-q\"  class=\"wp-block-heading\">Porting to CUDA-Q<a href=\"#porting_to_cuda-q\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>You can use Cursor to experiment with porting codes written in other quantum frameworks to CUDA-Q to leverage the excellent performance and scalability of CUDA-Q .&nbsp;</p>\n\n\n\n<p>For example, in response to the query, \u201cGive me an example of porting Pythonic code from another quantum framework to CUDA-Q,\u201d Cursor does an excellent job of translating the syntax of Qiskit to the syntax of CUDA-Q.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"790\" height=\"92\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-cuda-q-porting-example-query.png\" alt=\"Example Cursor CUDA-Q porting query.\" class=\"wp-image-91159\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-cuda-q-porting-example-query.png 790w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-cuda-q-porting-example-query-300x35.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-cuda-q-porting-example-query-625x73.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-cuda-q-porting-example-query-179x21.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-cuda-q-porting-example-query-768x89.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-cuda-q-porting-example-query-645x75.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-cuda-q-porting-example-query-500x58.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-cuda-q-porting-example-query-160x19.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-cuda-q-porting-example-query-362x42.png 362w\" sizes=\"(max-width: 790px) 100vw, 790px\" /></figure></div>\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1096\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q.png\" alt=\"An image of Cursor translating Qiskit code to CUDA-Q \n\" class=\"wp-image-91161\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q-300x164.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q-625x343.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q-179x98.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q-768x421.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q-1536x842.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q-645x354.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q-500x274.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q-160x88.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q-362x198.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q-201x110.png 201w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-translating-qiskit-code-cuda-q-1024x561.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 12. Cursor translating Qiskit code to CUDA-Q with the line-by-line translation highlighted</em></figcaption></figure>\n\n\n\n<p>The chat provides the following helpful explanation of the differences:</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"799\" height=\"501\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-difference-cuda-q-qiskit-code.png\" alt=\"An image of Cursor explaining the difference between CUDA-Q and Qiskit code that it has ported\n\" class=\"wp-image-91156\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-difference-cuda-q-qiskit-code.png 799w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-difference-cuda-q-qiskit-code-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-difference-cuda-q-qiskit-code-625x392.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-difference-cuda-q-qiskit-code-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-difference-cuda-q-qiskit-code-768x482.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-difference-cuda-q-qiskit-code-645x404.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-difference-cuda-q-qiskit-code-478x300.png 478w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-difference-cuda-q-qiskit-code-144x90.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-difference-cuda-q-qiskit-code-362x227.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cursor-explanation-difference-cuda-q-qiskit-code-175x110.png 175w\" sizes=\"(max-width: 799px) 100vw, 799px\" /><figcaption class=\"wp-element-caption\"><em>Figure 13. Cursor provides a helpful explanation of the syntactical difference of CUDA-Q versus Qiskit in the ported code</em></figcaption></figure>\n\n\n\n<p>You can now leverage the performance of CUDA-Q for this code.</p>\n\n\n\n<h2 id=\"verify_output\"  class=\"wp-block-heading\">Verify output<a href=\"#verify_output\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>While experimenting with these examples, the chat occasionally generates minor syntactical mistakes that cause errors when the code is executed. When this happens, the user may need to manually debug the error, although sometimes the chat can resolve the issue if the error is raised to it.</p>\n\n\n\n<p>Does this mean AI-assisted coding tools are useless? Of course not. Although they require human supervision to verify their output, AI-assisted coding tools significantly increase the speed and efficiency of generating code examples, providing immediate feedback and explanations for the code.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>AI coding assistants are a powerful way of improving quantum developer productivity and lowering the barrier to entry to developing scalable, high-performance hybrid quantum applications using CUDA-Q. This post has shown that coding assistants like Cursor do an excellent job generating CUDA-Q code, providing helpful explanations of the codebase, and enabling users of other frameworks to leverage CUDA-Q to accelerate their applications.&nbsp;</p>\n\n\n\n<p>Get started today with <a href=\"https://github.com/NVIDIA/cuda-quantum\">NVIDIA CUDA-Q</a> and <a href=\"https://www.cursor.com/\">Cursor</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>AI coding assistants have become ubiquitous across the software development landscape. Developers are increasingly using tools like GitHub Copilot, Amazon CodeWhisperer, and Cursor to boost productivity in computing tasks. These tools enable you to quickly generate, debug, and understand code; streamline workflows; and enhance collaboration across projects. While AI coding assistants are well-established in classical &hellip; <a href=\"https://developer.nvidia.com/blog/leverage-ai-coding-assistants-to-develop-quantum-applications-at-scale-with-nvidia-cuda-q/\">Continued</a></p>\n", "protected": false}, "author": 1757, "featured_media": 91115, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1514631", "discourse_permalink": "https://forums.developer.nvidia.com/t/leverage-ai-coding-assistants-to-develop-quantum-applications-at-scale-with-nvidia-cuda-q/312295", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 503], "tags": [453, 2735], "coauthors": [3359], "class_list": ["post-91111", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "category-simulation-modeling-design", "tag-featured", "tag-quantum-computing"], "acf": {"post_industry": ["HPC / Scientific Computing"], "post_products": ["CUDA-Q"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/quantum-computing-graphic.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nHx", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91111"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1757"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91111"}], "version-history": [{"count": 18, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91111/revisions"}], "predecessor-version": [{"id": 91411, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91111/revisions/91411"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91115"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91111"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91111"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91111"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91111"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91220, "date": "2024-11-04T09:39:18", "date_gmt": "2024-11-04T17:39:18", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91220"}, "modified": "2024-11-04T14:57:53", "modified_gmt": "2024-11-04T22:57:53", "slug": "discover-new-biological-insights-with-accelerated-pangenome-alignment-in-nvidia-parabricks", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/discover-new-biological-insights-with-accelerated-pangenome-alignment-in-nvidia-parabricks/", "title": {"rendered": "Discover New Biological Insights with Accelerated Pangenome Alignment in NVIDIA Parabricks"}, "content": {"rendered": "\n<p><a href=\"https://www.nvidia.com/en-us/clara/parabricks/\">NVIDIA Parabricks</a> is a scalable genomics analysis software suite that solves omics challenges with accelerated computing and deep learning to unlock new scientific breakthroughs. <a href=\"https://docs.nvidia.com/clara/parabricks/latest/index.html?ncid=em-anno-217927-vt12\">NVIDIA Parabricks v4.4</a> introduces new features and functionality including accelerated pangenome graph alignment, as announced at the American Society of Human Genetics (ASHG) national meeting.&nbsp;</p>\n\n\n\n<p>The core new feature of the Parabricks v4.4 release is single-end and paired-end support for <a href=\"https://www.science.org/doi/10.1126/science.abg8871\">Giraffe</a> for accelerated pangenome graph alignment. The release also includes additional functionality for Minimap2 and GATK HaplotypeCaller, as well as tool performance improvements. It also expands collaborations to support genomic sequencing and software platforms.&nbsp;</p>\n\n\n\n<p>Release highlights include the following:</p>\n\n\n\n<h3 id=\"new_features\"  class=\"wp-block-heading\">New features<a href=\"#new_features\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<ul class=\"wp-block-list\">\n<li>GPU-accelerated Giraffe, with single-end and paired-end support&nbsp;</li>\n\n\n\n<li>Pbmm2 wrapper for native PacBio input and output of Minimap2</li>\n\n\n\n<li>Allele option support in GATK HaplotypeCaller&nbsp;</li>\n\n\n\n<li>Support for unaligned BAMs: FQ2BAM (BWA-MEM) and Minimap2</li>\n</ul>\n\n\n\n<h3 id=\"improved_features\"  class=\"wp-block-heading\">Improved features<a href=\"#improved_features\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Faster Minimap2 for PacBio and Oxford Nanopore (ONT) data</li>\n\n\n\n<li>DeepVariant acceleration for ONT data</li>\n\n\n\n<li>Faster CRAM file writer (2x acceleration over CPU-only)</li>\n\n\n\n<li>&lt;30-minute end-to-end 30x whole genome sequencing (WGS) germline on a single-GPU system (NVIDIA Grace Hopper)</li>\n</ul>\n\n\n\n<h3 id=\"new_collaborations_and_benchmarks\"  class=\"wp-block-heading\">New collaborations and benchmarks<a href=\"#new_collaborations_and_benchmarks\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Complete Genomics data supported on Parabricks</li>\n\n\n\n<li>Parabricks now available on Basepair platform</li>\n\n\n\n<li>Updated benchmarks, including DeepSomatic and Giraffe</li>\n</ul>\n\n\n\n<p>The latest release of Parabricks v4.4 enables scientists and researchers to use Giraffe for pangenome alignment. By understanding genetic diversity from pangenomes and using the accelerated version of Giraffe available in Parabricks v4.4, scientists can discover new biological insights even faster.</p>\n\n\n\n<h2 id=\"understanding_genetic_diversity_from_pangenomes\"  class=\"wp-block-heading\">Understanding genetic diversity from pangenomes<a href=\"#understanding_genetic_diversity_from_pangenomes\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To understand the underlying cause of disease, individuals\u2019 genomes have historically been compared to a linear reference genome. Although a linear reference genome is not the DNA sequence of an individual, but is instead an average genome constructed from DNA of a few individuals, it serves as an accepted representation of a single consensus haplotype.&nbsp;</p>\n\n\n\n<p>Genome Reference Consortium Human Build 38 (GRCh38) is the current human reference genome that is most widely used across genetic studies as the comparison for different genetic studies. It inherently introduces biases and errors in variant calling, especially in repetitive or highly polymorphic regions. Additionally, it may inadequately represent genetic variation from minority populations, thereby limiting understanding of the complete spectrum of genetic diversity.</p>\n\n\n\n<p>In contrast, graph-based pangenomes offer a robust solution to this issue by integrating multiple reference genomes into a unified structure. This approach effectively captures the genetic diversity within a species, enabling more accurate detection and analysis of variations across different genomes. By representing genomic data as graphs, pangenome graphs enable comprehensive and unbiased genetic variation analysis, overcoming the limitations imposed by reliance on a single reference genome.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"815\" height=\"669\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/comparison-linear-reference-genome-pangenome-graph.png\" alt=\"The reference genome as a linear haploid sequence is limited in how well it can represent genetic diversity of populations, including single nucleotide polymorphisms (SNPs), indels and structural variants that are more common amongst specific subpopulations.\nAligning to a pangenome graph reference enables high accuracy genomic analysis by providing representation for many diverse subpopulations. \n\" class=\"wp-image-91227\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/comparison-linear-reference-genome-pangenome-graph.png 815w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/comparison-linear-reference-genome-pangenome-graph-300x246.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/comparison-linear-reference-genome-pangenome-graph-625x513.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/comparison-linear-reference-genome-pangenome-graph-140x115.png 140w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/comparison-linear-reference-genome-pangenome-graph-768x630.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/comparison-linear-reference-genome-pangenome-graph-645x529.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/comparison-linear-reference-genome-pangenome-graph-365x300.png 365w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/comparison-linear-reference-genome-pangenome-graph-110x90.png 110w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/comparison-linear-reference-genome-pangenome-graph-362x297.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/comparison-linear-reference-genome-pangenome-graph-134x110.png 134w\" sizes=\"(max-width: 815px) 100vw, 815px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. A linear reference genome compared to a pangenome graph</em></em></figcaption></figure>\n\n\n\n<h2 id=\"graph_genomes\"  class=\"wp-block-heading\">Graph genomes<a href=\"#graph_genomes\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To represent pangenome data, graph genomes provide a unified framework for representing the genetic variation of multiple genomes. The graph structure of the data provides easier understanding of structural changes, including insertions, deletions, and rearrangements.&nbsp;</p>\n\n\n\n<p>Graph genomes are particularly beneficial to improving accuracy in variant calling since they can help increase detection of genetic variants. However, the analysis becomes more&nbsp;challenging, particularly in alignment, since graph-based representations introduce more complexities than the linear sequences of single references. Additionally, as graph genomes grow in size and complexity, computational requirements and processing can become prohibitive.&nbsp;</p>\n\n\n\n<h2 id=\"accelerating_pangenome_alignment_with_giraffe\"  class=\"wp-block-heading\">Accelerating pangenome alignment with Giraffe<a href=\"#accelerating_pangenome_alignment_with_giraffe\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Giraffe is a software tool to support pangenome graph alignment. Built by the University of California, Santa Cruz (UCSC), it is used particularly in the context of large-scale genomic sequencing projects and helps with alignment, assembly, and variant calling. Giraffe enables new genomic sequences to be compared to a pangenome\u2014not just a single reference genome.&nbsp;</p>\n\n\n\n<p>With the latest v4.4 release, Parabricks now supports Giraffe for single-end and paired-end data to provide GPU-acceleration for pangenome alignment. Plus, results are fully equivalent to the <a href=\"https://github.com/vgteam/vg/releases/tag/v1.59.0\">open-source version of Giraffe</a> so that researchers can use Parabricks v4.4 to replicate an open-source tool. As a result, scientists and researchers can increase accuracy and improve variant calling\u2014particularly across genetic variations and diverse populations.</p>\n\n\n\n<p>\u201cThe current human reference genome has been the cornerstone of human genetics research for over twenty years,\u201d explains Dr. Benedict Paten, professor and associate director at the University of California, Santa Cruz Genomics Institute. \u201cHowever, it contains just a single representative sequence for each chromosome and so can\u2019t by definition capture the rich variation present in our population. To understand the common genetic diversity of our population a human pangenome is necessary.\u201d&nbsp;</p>\n\n\n\n<p>\u201cPangenomes encode hundreds or, in the future, even thousands of individual genomes in a reference structure,\u201d Dr. Paten adds. \u201cThey better represent us, ensuring research and future precision therapeutics account for our individual diversity. At UCSC, we have a research team dedicated to building tools to use the pangenome. This includes Giraffe, a tool for mapping a new sample to the pangenome. We are excited to be working with the NVIDIA team to accelerate Giraffe and make it a workhorse tool for future projects. This has potential to have a huge downstream impact.\u201d&nbsp;</p>\n\n\n\n<h2 id=\"new_collaborations\"  class=\"wp-block-heading\">New collaborations<a href=\"#new_collaborations\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In addition to the latest features of Parabricks v4.4, NVIDIA expands collaborations with genomic sequencing and software platforms\u2013including Complete Genomics and Basepair.&nbsp;</p>\n\n\n\n<h3 id=\"complete_genomics\"  class=\"wp-block-heading\">Complete Genomics<a href=\"#complete_genomics\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://www.completegenomics.com/\">Complete Genomics</a> is committed to driving genomics forward with complete sequencing solutions that improve lives. Offering a wide range of applications, including WGS, single-cell analysis, spatial transcriptomics, and microbiology, Complete Genomics leverages its proprietary DNBSEQ (DNA Nanoball Sequencing) technology. This technology produces deep sequencing coverage while ensuring high accuracy and low error rates. Parabricks germline workflows can now use data from Complete Genomicssequencers, including the DNBSEQ-T7 and DNBSEQ-G400.&nbsp;</p>\n\n\n\n<p>The integration of the DNBSEQ with Parabricks technology provides an accelerated and cost-effective solution for secondary genomic analysis. For example, processing a 30x WGS sample using fq2bam and haplotypecaller workflows on the DNBSEQ-T7 sequencer can be optimized for speed or cost depending on the GPU instance.</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Speed</strong>: 16-minute runtime on four NVIDIA L40 GPUs</li>\n\n\n\n<li><strong>Cost</strong>: $2.67 cost on four NVIDIA L4 GPUs</li>\n</ul>\n\n\n\n<p>&#8220;Our integration of NVIDIA Parabricks allows us to harness the full potential of our DNBSEQ-T7 sequencing platform,\u201d says Rob Tarbox, VP of Product and Marketing at Complete Genomics. \u201cBy combining our high-quality sequencing data with Parabricks\u2019 speed and accuracy, we\u2019re enabling researchers to uncover variants more efficiently and cost-effectively, ultimately advancing precision medicine and improving patient outcomes.</p>\n\n\n\n<p><a href=\"https://docs.nvidia.com/clara/parabricks/latest/tutorials/cloudguides/benchmarkingguide.html\">Explore the quick start guide</a> to learn more about benchmarking Parabricks germline workflows with Complete Genomics data.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"570\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-625x570.png\" alt=\"The Complete Genomics DNBSEQ-T7 sequencer. \n\" class=\"wp-image-91229\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-625x570.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-300x273.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-126x115.png 126w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-768x700.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-1536x1400.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-2048x1866.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-645x588.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-329x300.png 329w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-99x90.png 99w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-362x330.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-121x110.png 121w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/complete-genomics-sequencer-1024x933.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. The Complete Genomics DNBSEQ-T7 sequencer. Image credit: Complete Genomics</em></em></figcaption></figure>\n\n\n\n<h3 id=\"basepair\"  class=\"wp-block-heading\">Basepair<a href=\"#basepair\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://www.basepairtech.com/\">Basepair</a> is a next-generation sequencing (NGS) data analysis platform. Their point-and-click user interface helps make genomic data analysis and visualization more accessible to a broader range of scientists.&nbsp;</p>\n\n\n\n<p>Now, users can supercharge their genomic data analysis by using Parabricks on Basepair, powered by HealthOmics from AWS. Parabricks on Basepair gives users an intuitive graphical user interface (GUI) with interactive visualizations entirely provisioned within their own AWS account for compute and storage.&nbsp;</p>\n\n\n\n<p>\u201cWe are excited to support Parabricks on Basepair, bringing accelerated tools alongside a more comprehensive and visual way to analyze their genomic data,\u201d says Simon Valentine, chief commercial officer at Basepair. \u201cParabricks provides access to some of the most effective bioinformatics tools available today. By making them available through Basepair\u2019s intuitive point-and-click interface we can work together to make them accessible to an even broader range of scientists.\u201d&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1319\" height=\"971\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-parabricks-running-on-basepair.png\" alt=\"Screenshot of NVIDIA Parabricks running on the Basepair platform, with fields for pipeline, samples, analysis name, and omics.\" class=\"wp-image-91231\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-parabricks-running-on-basepair.png 1319w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-parabricks-running-on-basepair-300x221.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-parabricks-running-on-basepair-625x460.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-parabricks-running-on-basepair-156x115.png 156w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-parabricks-running-on-basepair-768x565.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-parabricks-running-on-basepair-645x475.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-parabricks-running-on-basepair-408x300.png 408w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-parabricks-running-on-basepair-122x90.png 122w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-parabricks-running-on-basepair-362x266.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-parabricks-running-on-basepair-149x110.png 149w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-parabricks-running-on-basepair-1024x754.png 1024w\" sizes=\"(max-width: 1319px) 100vw, 1319px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. NVIDIA Parabricks running on the Basepair platform. Image credit: Basepair</em></em></figcaption></figure></div>\n\n\n<h2 id=\"latest_parabricks_benchmarks\"  class=\"wp-block-heading\">Latest Parabricks benchmarks<a href=\"#latest_parabricks_benchmarks\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In addition to new features and upgrades for each release, NVIDIA continuously works to improve benchmark performance across instruments, tools, and GPUs.&nbsp;&nbsp;</p>\n\n\n\n<p>Table 1 outlines the latest benchmarks on the most popular NVIDIA GPUs for the fastest speed (NVIDIA H100) and lowest cost per sample (NVIDIA L4)\u2013including Giraffe from Parabricks v4.4 and <a href=\"https://developer.nvidia.com/blog/unlock-deeper-insights-of-somatic-mutations-with-deep-learning/\">DeepSomatic from v4.3.1</a>.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table class=\"has-fixed-layout\"><tbody><tr><td></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\"><strong>NVIDIA H100&nbsp; GPU</strong><br><strong>Fastest speed</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\"><strong>NVIDIA L4 GPU</strong><br><strong>Lowest cost per sample</strong></td></tr><tr><td></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>2 GPUs</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>4 GPUs</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>2 GPUs</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>4 GPUs</strong></td></tr><tr><td><strong>Giraffe</strong></td><td class=\"has-text-align-center\" data-align=\"center\">65.8</td><td class=\"has-text-align-center\" data-align=\"center\">42.1</td><td class=\"has-text-align-center\" data-align=\"center\">84.9</td><td class=\"has-text-align-center\" data-align=\"center\">44.7</td></tr><tr><td><strong>DeepSomatic</strong></td><td class=\"has-text-align-center\" data-align=\"center\">56.28</td><td class=\"has-text-align-center\" data-align=\"center\">35.13</td><td class=\"has-text-align-center\" data-align=\"center\">215.53</td><td class=\"has-text-align-center\" data-align=\"center\">108.55</td></tr><tr><td><strong>FQ2BAM (BWA-MEM)</strong></td><td class=\"has-text-align-center\" data-align=\"center\">13.8</td><td class=\"has-text-align-center\" data-align=\"center\">9.15</td><td class=\"has-text-align-center\" data-align=\"center\">48.15</td><td class=\"has-text-align-center\" data-align=\"center\">27.88</td></tr><tr><td><strong>BWA-Meth</strong></td><td class=\"has-text-align-center\" data-align=\"center\">27.43</td><td class=\"has-text-align-center\" data-align=\"center\">15.12</td><td class=\"has-text-align-center\" data-align=\"center\">77.35</td><td class=\"has-text-align-center\" data-align=\"center\">39.77</td></tr><tr><td><strong>DeepVariant</strong></td><td class=\"has-text-align-center\" data-align=\"center\">9.6</td><td class=\"has-text-align-center\" data-align=\"center\">5.82</td><td class=\"has-text-align-center\" data-align=\"center\">23.48</td><td class=\"has-text-align-center\" data-align=\"center\">13.10</td></tr><tr><td><strong>HaplotypeCaller</strong></td><td class=\"has-text-align-center\" data-align=\"center\">10.57</td><td class=\"has-text-align-center\" data-align=\"center\">4.90</td><td class=\"has-text-align-center\" data-align=\"center\">12.00</td><td class=\"has-text-align-center\" data-align=\"center\">7.73</td></tr><tr><td><strong>Mutect2</strong></td><td class=\"has-text-align-center\" data-align=\"center\">25.80</td><td class=\"has-text-align-center\" data-align=\"center\">13.60</td><td class=\"has-text-align-center\" data-align=\"center\">55.8</td><td class=\"has-text-align-center\" data-align=\"center\">32.50</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. The latest benchmarks on the most popular NVIDIA GPUs for the fastest speed and lowest cost per sample</em> <em>with performance time in minutes</em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\"><em>30x whole genome sequenced for FQ2BAM (BWA-Mem), BWA-Meth, DeepVariant, and Haplotype Caller with Illumina data. <br>50x tumor-normal whole genome sequenced for DeepSomatic and Mutect2 with Illumina data.</em></p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With the NVIDIA Parabricks v4.4 release, scientists and researchers using graph genomes can now access Giraffe for pangenome alignment. Parabricks v4.4 supports the groundbreaking tool from UCSC by powering an accelerated version of Giraffe to help discover new biological insights\u2014now even faster.&nbsp;</p>\n\n\n\n<p>Download <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/containers/clara-parabricks?nvid=nv-int-tblg-737311-vt12\">NVIDIA Parabricks</a> to get started with GPU-accelerated genomics analysis and join the conversation on the <a href=\"https://forums.developer.nvidia.com/c/healthcare/parabricks/290\">NVIDIA Parabricks Developer Forum</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA Parabricks is a scalable genomics analysis software suite that solves omics challenges with accelerated computing and deep learning to unlock new scientific breakthroughs. NVIDIA Parabricks v4.4 introduces new features and functionality including accelerated pangenome graph alignment, as announced at the American Society of Human Genetics (ASHG) national meeting.&nbsp; The core new feature of the &hellip; <a href=\"https://developer.nvidia.com/blog/discover-new-biological-insights-with-accelerated-pangenome-alignment-in-nvidia-parabricks/\">Continued</a></p>\n", "protected": false}, "author": 2135, "featured_media": 91236, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1513809", "discourse_permalink": "https://forums.developer.nvidia.com/t/discover-new-biological-insights-with-accelerated-pangenome-alignment-in-nvidia-parabricks/312178", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 3110], "tags": [1910, 2932, 1163], "coauthors": [3857], "class_list": ["post-91220", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-data-science", "category-generative-ai", "tag-bioinformatics-and-genomics", "tag-large-language-models", "tag-parabricks"], "acf": {"post_industry": ["Healthcare & Life Sciences"], "post_products": ["H100", "Parabricks"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/dna.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nJi", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91220"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2135"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91220"}], "version-history": [{"count": 17, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91220/revisions"}], "predecessor-version": [{"id": 91445, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91220/revisions/91445"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91236"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91220"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91220"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91220"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91220"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91234, "date": "2024-11-04T09:30:00", "date_gmt": "2024-11-04T17:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91234"}, "modified": "2024-11-04T09:36:43", "modified_gmt": "2024-11-04T17:36:43", "slug": "frictionless-collaboration-and-rapid-prototyping-in-hybrid-environments-with-nvidia-ai-workbench", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/frictionless-collaboration-and-rapid-prototyping-in-hybrid-environments-with-nvidia-ai-workbench/", "title": {"rendered": "Frictionless Collaboration and Rapid Prototyping in Hybrid Environments with NVIDIA AI Workbench"}, "content": {"rendered": "\n<p><a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/overview/introduction.html\">NVIDIA AI Workbench</a> is a free development environment manager that streamlines data science, AI, and machine learning (ML) projects on systems of choice. The goal is to provide a frictionless way to create, compute, and collaborate on and across PCs, workstations, data centers, and clouds. The basic user experience is straightforward:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Easy setup on single systems: </strong>Click through install in minutes on Windows, Ubuntu, and macOS, with a one-line install on remote systems.</li>\n\n\n\n<li><strong>Managed experience for decentralized deployment</strong>: A free, PaaS/SaaS type UX in truly hybrid contexts with no need for a centralized, service-based platform.&nbsp;</li>\n\n\n\n<li><strong>Seamless collaboration for experts and beginners:</strong> Friendly Git, container, and application management without limiting customization by power users.</li>\n\n\n\n<li><strong>Consistent across users and systems: </strong>Migrate workloads and applications across different systems while maintaining functionality and user experience.&nbsp;</li>\n\n\n\n<li><strong>Simplified GPU handling</strong>: Handles system dependencies like <a href=\"https://docs.nvidia.com/datacenter/tesla/driver-installation-guide/index.html#ubuntu\">NVIDIA drivers</a> and the <a href=\"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html\">NVIDIA Container Toolkit</a>, as well as <a href=\"https://docs.nvidia.com/deeplearning/frameworks/user-guide/index.html\">GPU-enabled container</a> runtime configuration.</li>\n</ul>\n\n\n\n<p>This post explores highlights of the October release of NVIDIA AI Workbench, which is the most significant since the product launch at GTC 2024 and is a big step closer to the full product vision.</p>\n\n\n\n<h2 id=\"release_highlights\"  class=\"wp-block-heading\">Release highlights<a href=\"#release_highlights\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This section will detail the major new capabilities and user-requested updates in the latest release. </p>\n\n\n\n<p>Major new capabilities include:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Enhance collaboration through expanded Git support, such as branching, merging, diffs, and finer-grained control for commits and gitignore.</li>\n\n\n\n<li>Create complex applications and workflows with multicontainer environments through Docker Compose support.</li>\n\n\n\n<li>Simple, fast, and secure rapid prototyping with application sharing with single-user URLs.</li>\n</ul>\n\n\n\n<p>User requested updates:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Dark mode for the Desktop App</li>\n\n\n\n<li>Improved installation on localized versions of Windows</li>\n</ul>\n\n\n\n<h3 id=\"expanded_git_support\"  class=\"wp-block-heading\">Expanded Git support<a href=\"#expanded_git_support\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Previously, AI Workbench supported only single, monolithic commits on the main branch. Users had to manage branches and merges manually, and this created various types of confusion, especially around resolving merge conflicts. Now, users can manage branches, merges, and conflicts directly in the Desktop App and the CLI. In addition, they can see and triage individual file diffs for commits. The UI is built to work seamlessly with manual Git operations and will update to reflect relevant changes.&nbsp;&nbsp;&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"3825\" height=\"2244\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1.jpg\" alt=\"A screenshot of the AI Workbench Desktop App tab for Git branching showing two different branches. \" class=\"wp-image-91258\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1.jpg 3825w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1-300x176.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1-625x367.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1-179x105.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1-768x451.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1-1536x901.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1-2048x1201.jpg 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1-645x378.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1-500x293.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1-153x90.jpg 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1-362x212.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1-188x110.jpg 188w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-desktop-app-git-branching-1-1024x601.jpg 1024w\" sizes=\"(max-width: 3825px) 100vw, 3825px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. AI Workbench Desktop App tab for Git branching</em></em></figcaption></figure></div>\n\n\n<p>These features are found in two new tabs on the Desktop App: Changes and Branches.</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Changes</strong>: Gives a line-by-line view of the diffs between the working tree and previous commits. Users can now select and commit file changes individually or in bulk based on visible file diffs tracked changes (addition, modification, or deletion), as well as being able to individually reject or add a file to git-ignore. The view also updates dynamically to reflect manual Git actions, for example manually staging a file and then following up with a change to the file in the working tree.</li>\n\n\n\n<li><strong>Branches</strong>: Provides branch management, including creation, switching, and merging, as well as visibility for remote branches on a Git server. Merging branches with a conflict initiates a conflict resolution flow that users can do within the UI, or move to a terminal or file editor of their choice.&nbsp;</li>\n</ul>\n\n\n\n<p><a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/git/git.html\">Learn more about how these advanced Git features work</a>.</p>\n\n\n\n<h3 id=\"multicontainer_support_with_docker_compose_stacks\"  class=\"wp-block-heading\">Multicontainer support with Docker Compose stacks<a href=\"#multicontainer_support_with_docker_compose_stacks\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>AI Workbench now supports <a href=\"https://docs.docker.com/compose/\">Docker Compose</a>. Users can work with multicontainer applications and workflows with the same ease of configuration, reproducibility, and portability that AI Workbench provides for single-container environments.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"3819\" height=\"2247\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1.jpg\" alt=\"Screenshot of a graphical UI showing affordances for adding a Docker Compose file to an AI Workbench Project.\" class=\"wp-image-91260\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1.jpg 3819w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1-300x177.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1-625x368.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1-179x105.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1-768x452.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1-1536x904.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1-2048x1205.jpg 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1-645x380.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1-500x294.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1-153x90.jpg 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1-362x213.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1-187x110.jpg 187w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-workbench-environment-management-tab-docker-compose-1-1024x602.jpg 1024w\" sizes=\"(max-width: 3819px) 100vw, 3819px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. The Docker Compose feature in the AI Workbench Environment Management tab</em></em></figcaption></figure></div>\n\n\n<p>The basic idea is to add a Docker Compose-based \u201cstack\u201d that is managed by AI Workbench and connects to the main development container. To add the stack, a user just needs to add the appropriate Docker Compose file to the project repository and do some configuration in the Desktop App or CLI.</p>\n\n\n\n<p>We\u2019re using Docker Compose for a few reasons. First, we didn\u2019t want to develop in a vacuum, and that\u2019s why we\u2019ve been <a href=\"https://www.docker.com/blog/optimizing-ai-application-development-docker-desktop-nvidia-ai-workbench/\">collaborating with the Docker team</a> on features like a <a href=\"https://developer.nvidia.com/blog/nvidia-ai-workbench-simplifies-using-gpus-on-windows/\">managed Docker Desktop install</a>.&nbsp;</p>\n\n\n\n<p>Second, we want users to be able to work with the multicontainer applications outside of AI Workbench, and Docker Compose is the easiest way to do that. The vision for this feature is to enable streamlined, powerful development and compute for multicontainer applications within AI Workbench that can then be stood up outside of AI Workbench with a simple <code>docker-compose</code> up command.&nbsp;</p>\n\n\n\n<p>This multicontainer feature is new and will continue to evolve. We would love to get feedback and help you sort out any issues through the <a href=\"https://forums.developer.nvidia.com/c/ai-data-science/nvidia-ai-workbench/671\">NVIDIA AI Workbench Developer Forum</a>.&nbsp;</p>\n\n\n\n<p><a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/projects/compose.html\">Learn more about how Docker Compose works</a>.</p>\n\n\n\n<h3 id=\"web_application_sharing_through_secure_urls\"  class=\"wp-block-heading\">Web application sharing through secure URLs<a href=\"#web_application_sharing_through_secure_urls\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>AI Workbench enables users to easily spin up managed web applications that are built into a project. The process is fairly simple: create or clone a project with the web app installed, start the project, then start the app, and it appears in your browser.&nbsp;</p>\n\n\n\n<p>This approach is great for a developer UX, but it wasn\u2019t good for rapid prototyping UX and collaboration. If you wanted another user to access and test your application, you either asked them to install AI Workbench, clone the project and run it, or you had to fully extract the application to run it and make it available to the user. The first is a speed bump for the user, and the second is a speed bump for the developer.&nbsp;</p>\n\n\n\n<p>We eliminated these speed bumps with a simple feature that enables you to set a remote AI Workbench to enable external access and to create single-use, secure URLs for running web applications in a project on that remote. You just need to make sure the user has access to port 10000 on the remote, and the application will be directly accessible. All they have to do is click the link and go to the app.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"3777\" height=\"2166\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1.jpg\" alt=\"A command line interface with AI Workbench commands showing how to open a project, start JupyterLab and then generate a URL to share JupyterLab with another user.\n\" class=\"wp-image-91262\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1.jpg 3777w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1-300x172.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1-625x358.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1-179x103.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1-768x440.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1-1536x881.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1-2048x1174.jpg 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1-645x370.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1-500x287.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1-157x90.jpg 157w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1-362x208.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1-192x110.jpg 192w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cli-ai-workbench-1-1024x587.jpg 1024w\" sizes=\"(max-width: 3777px) 100vw, 3777px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. Developers can now give end users direct access to applications running in an AI Workbench Project on a remote through secure, one-time-use URLs</em></em></figcaption></figure>\n\n\n\n<p>Enabling this kind of access is useful for rapid prototyping and collaboration. That\u2019s why various SaaS offerings provide this as a managed service. The difference with AI Workbench is that you can provide this access on your own resources and in your own network, for example on data center resources or a shared server. It doesn\u2019t have to be in the cloud.&nbsp;</p>\n\n\n\n<p>AI Workbench keeps things secure by restricting this access to a single browser and to a single application that\u2019s running in the project. This means a user can\u2019t share the URL with someone else, and they are constrained to the web app that you shared with them.</p>\n\n\n\n<p><a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/collaborate/app-sharing.html\">Learn more about how application sharing works.</a></p>\n\n\n\n<h3 id=\"dark_mode_and_localized_windows_installation\"  class=\"wp-block-heading\">Dark mode and localized Windows installation<a href=\"#dark_mode_and_localized_windows_installation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Many users requested a dark mode option because it\u2019s easier on the eyes. It\u2019s now available and can be selected through the Settings window that is now available directly from within the Desktop App. <a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/reference/settings.html\">Learn more about how dark mode works</a>.</p>\n\n\n\n<p>Windows users are by far our main demographic for the local installs, and not all Windows users are using the English language pack, and this blocked AI Workbench install due to how we handled some WSL commands. In particular, we\u2019ve had users working in Cyrillic or Chinese that were blocked on Windows. We adjusted how we handle non-English language packs, and it should work well now. If you were previously blocked by this, give it a try now. If it still doesn\u2019t work for you, let us know in the <a href=\"https://forums.developer.nvidia.com/c/ai-data-science/nvidia-ai-workbench/671\">NVIDIA AI Workbench Developer Forum</a> so we can continue to improve this capability.</p>\n\n\n\n<h2 id=\"new_ai_workbench_projects&nbsp;\"  class=\"wp-block-heading\">New AI Workbench projects&nbsp;<a href=\"#new_ai_workbench_projects&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This release introduces new example projects designed to jumpstart your AI development journey, detailed below.&nbsp; An <a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/overview/projects.html\">AI Workbench project</a> is a structured Git repository that defines a containerized development environment in AI Workbench. AI Workbench projects provide:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Effortless setup and GPU configuration:</strong> Simply clone a project from GitHub or GitLab, and AI Workbench handles the rest with automatic GPU configuration.&nbsp;</li>\n\n\n\n<li><strong>Development integrations: </strong>Seamless support for popular development environments such as Jupyter and VS Code, as well as support for user-configured web applications.</li>\n\n\n\n<li><strong>Containerized and customizable environments:</strong> Projects are containerized, isolated, and easily modifiable. Adapt example projects to suit your specific needs while ensuring consistency and reproducibility.&nbsp;</li>\n</ul>\n\n\n\n<p><a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/quickstart/example-projects.html\">Explore NVIDIA AI Workbench example projects</a>.</p>\n\n\n\n<h3 id=\"multimodal_virtual_assistant_example_project\"  class=\"wp-block-heading\"><strong>Multimodal virtual assistant</strong> <strong>example project</strong><a href=\"#multimodal_virtual_assistant_example_project\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>This project enables users to build their own virtual assistant using a multimodal <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation (RAG)</a> pipeline with fallback to web search. Users can interact with two RAG-based applications to learn more about AI Workbench, converse with the user documentation, troubleshoot their own installation, or even focus the RAG pipeline to their own, custom product.&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Control-Panel:</strong> Customizable Gradio app for working with product documentation allows uploading webpages, PDFs, images, and videos to a persistent vector store and query them. For inference, users can select between cloud endpoints like on the NVIDIA API Catalog or use self-hosted endpoints to run their own inference.&nbsp;</li>\n\n\n\n<li><strong>Public-Chat:</strong> With product documents loaded, the Gradio app is a simplified, &#8220;read-only&#8221; chatbot that you can share with end users through the new AI Workbench App Sharing feature.&nbsp;</li>\n</ul>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"800\" height=\"450\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/public-chat-web-app-query-virtual-assistant.gif\" alt=\"A GIF demonstrating how a user can submit a query to the virtual assistant and see the generated response.\" class=\"wp-image-91253\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. Using the Public-Chat web app, a read-only, pared down chat application that is meant to be more consumable and shareable to end users</em></em></figcaption></figure></div>\n\n\n\n<h3 id=\"competition-kernel_example_project\"  class=\"wp-block-heading\">Competition-Kernel example project<a href=\"#competition-kernel_example_project\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>This project provides an easy, local experience when working on Kaggle competitions. You can easily leverage your local machine or a cloud instance to work on competition datasets, write code, build out models, and submit results, all through AI Workbench. The Competition Kernel project offers:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>A managed experience to develop and test on your own GPUs and set up and customize in minutes.</li>\n\n\n\n<li>Easy version control and tracking of code through GitHub or GitLab and very easy collaboration.</li>\n\n\n\n<li>The power of using a local, dedicated IDE: robust debugging, intelligent code completion, extensive customization options.</li>\n\n\n\n<li>Easy plugin to existing data sources (external or your own).</li>\n\n\n\n<li>No Internet? No problem. Develop while offline.</li>\n</ul>\n\n\n\n<h2 id=\"get_started&nbsp;&nbsp;\"  class=\"wp-block-heading\">Get started&nbsp;&nbsp;<a href=\"#get_started&nbsp;&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This release of NVIDIA AI Workbench marks a significant step forward in providing a frictionless experience for AI development across GPU systems. New features from this release, including expanded Git support, support for multicontainer environments, and secure web app sharing, streamline developing and collaborating on AI workloads. Explore these features in the three new example projects available with this release or create your own projects.&nbsp;</p>\n\n\n\n<p>To get started with AI Workbench, <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/\">install the application from the webpage</a>. For more information about installing and updating, see the <a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/installation/windows.html#install-ai-workbench\">NVIDIA AI Workbench documentation</a>.\u00a0</p>\n\n\n\n<p>Explore a range of <a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/quickstart/example-projects.html\">NVIDIA AI Workbench example projects</a>, from data science to RAG.</p>\n\n\n\n<p>Visit the <a href=\"https://forums.developer.nvidia.com/c/ai-data-science/nvidia-ai-workbench/671\">NVIDIA AI Workbench Developer Forum</a> to report issues and learn more about how other developers are using AI Workbench.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA AI Workbench is a free development environment manager that streamlines data science, AI, and machine learning (ML) projects on systems of choice. The goal is to provide a frictionless way to create, compute, and collaborate on and across PCs, workstations, data centers, and clouds. The basic user experience is straightforward: This post explores highlights &hellip; <a href=\"https://developer.nvidia.com/blog/frictionless-collaboration-and-rapid-prototyping-in-hybrid-environments-with-nvidia-ai-workbench/\">Continued</a></p>\n", "protected": false}, "author": 1840, "featured_media": 91237, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1513803", "discourse_permalink": "https://forums.developer.nvidia.com/t/frictionless-collaboration-and-rapid-prototyping-in-hybrid-environments-with-nvidia-ai-workbench/312176", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 3110], "tags": [4134, 3613], "coauthors": [3466, 3685, 3227], "class_list": ["post-91234", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-data-science", "category-generative-ai", "tag-nim-agent-blueprint", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["General"], "post_products": ["AI Workbench", "NIM"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Best practice"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llm-visual.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nJw", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91234"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1840"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91234"}], "version-history": [{"count": 18, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91234/revisions"}], "predecessor-version": [{"id": 91438, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91234/revisions/91438"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91237"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91234"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91234"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91234"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91234"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 86011, "date": "2024-11-04T08:00:00", "date_gmt": "2024-11-04T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=86011"}, "modified": "2024-11-04T09:00:54", "modified_gmt": "2024-11-04T17:00:54", "slug": "build-a-video-search-and-summarization-agent-with-nvidia-ai-blueprint", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/build-a-video-search-and-summarization-agent-with-nvidia-ai-blueprint/", "title": {"rendered": "Build a Video Search and Summarization Agent with NVIDIA AI Blueprint"}, "content": {"rendered": "\n<p><em>This post was originally published July 29, 2024 but has been extensively revised with NVIDIA AI Blueprint information.</em></p>\n\n\n\n<p>Traditional video analytics applications and their development workflow are typically built on fixed-function, limited models that are designed to detect and identify only a select set of predefined objects.</p>\n\n\n\n<p>With <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI</a>, NVIDIA NIM microservices, and foundation models, you can now build applications with fewer models that have broad perception and rich contextual understanding.</p>\n\n\n\n<p>The new class of generative AI models, <a href=\"https://www.nvidia.com/en-us/glossary/vision-language-models/\">vision language models (VLM)</a>, powers <a href=\"https://www.nvidia.com/en-us/use-cases/visual-ai-agents/\">visual AI agents</a> that can understand natural language prompts and perform visual question answering. By combining VLMs, LLMs, and the latest Graph-RAG techniques, you can build a powerful visual AI agent that is capable of long-form video understanding.</p>\n\n\n\n<p>These visual AI agents will be deployed throughout factories, warehouses, retail stores, airports, traffic intersections, and more. They\u2019ll help operations teams make better decisions using richer insights generated from natural interactions.</p>\n\n\n\n<p>In this post, we show you how to seamlessly build an AI agent for long-form video understanding using <a href=\"https://build.nvidia.com/nvidia/video-search-and-summarization\">NVIDIA AI Blueprint for Video Search and Summarization</a>. You can apply for <a href=\"https://developer.nvidia.com/nim-agent-blueprint/video-search-and-summarization-early-access\">early access</a> to this new AI Blueprint.</p>\n\n\n\n<h2 id=\"releasing_nvidia_ai_blueprint_for_video_search_and_summarization\"  class=\"wp-block-heading\">Releasing NVIDIA AI Blueprint for Video Search and Summarization<a href=\"#releasing_nvidia_ai_blueprint_for_video_search_and_summarization\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/EAhe3aqcRQk?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Build Visual AI Agents with Vision Language Models</em></figcaption></figure>\n\n\n\n<p>NVIDIA AI Blueprints, powered by NVIDIA NIM, are reference workflows for canonical generative AI use cases. <a href=\"https://www.nvidia.com/en-us/ai/#referrer=ai-subdomain\">NVIDIA NIM</a> is a set of microservices that includes industry-standard APIs, domain-specific code, optimized inference engines, and enterprise runtime. It delivers multiple VLMs for building a visual AI agent that can process live or archived images or videos to extract actionable insight using natural language.</p>\n\n\n\n<p>The new <a href=\"https://build.nvidia.com/nvidia/video-search-and-summarization\">AI Blueprint for Video Search and Summarization</a> accelerates the development of visual AI agents by providing a recipe for long-form video understanding using VLMs, LLMs, and the latest RAG techniques.</p>\n\n\n\n<p>To interact with the agent, a set of easy-to-use REST APIs are available to enable video summarization, interactive Q&amp;A over videos, and custom alerts on live streams to find specific events. The REST APIs can be used to integrate the agent into your own application and are used by the reference UI for quick testing.</p>\n\n\n\n<p>The models used in the blueprint can come from the <a href=\"https://build.nvidia.com/explore/discover\">NVIDIA API Catalog</a> of model preview APIs and downloadable NIM microservices. For example, the AI Blueprint uses the NVIDIA-hosted <a href=\"https://build.nvidia.com/meta/llama-3_1-70b-instruct\">llama-3_1-70b-instruct</a> NIM microservice as the LLM for NVIDIA NeMo Guardrails, Context-Aware RAG (CA-RAG), and Graph-RAG modules. You can choose from a wide range of different LLMs and VLMs from the <a href=\"https://build.nvidia.com/explore/vision\">API Catalog</a>, either NVIDIA-hosted or locally deployed.</p>\n\n\n\n<h2 id=\"visual_ai_agent_for_video_search_and_summarization\"  class=\"wp-block-heading\">Visual AI agent for video search and summarization<a href=\"#visual_ai_agent_for_video_search_and_summarization\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Building a visual AI agent capable of understanding long-form videos requires a combination of VLMs and LLMs ensembled together with datastores. The blueprint provides a recipe for combining all of these components to enable scalable and GPU-accelerated video understanding agents that can perform several tasks such as summarization, Q&amp;A, and detecting events on live streaming video.</p>\n\n\n\n<p>The blueprint consists of the following components:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Stream handler:</strong> Manages the interaction and synchronization with the other components such as NeMo Guardrails, CA-RAG, the VLM pipeline, chunking, and the Milvus Vector DB.</li>\n\n\n\n<li><strong>NeMo Guardrails:</strong> Filters out invalid user prompts. It makes use of the REST API of an LLM NIM microservice.</li>\n\n\n\n<li><strong>VLM pipeline</strong> \u2013 Decodes video chunks generated by the stream handler, generates the embeddings for the video chunks using an NVIDIA Tensor RT-based visual encoder model, and then makes use of a VLM to generate per-chunk response for the user query. It is based on the NVIDIA DeepStream SDK.</li>\n\n\n\n<li><strong>VectorDB:</strong> Stores the intermediate per-chunk VLM response.</li>\n\n\n\n<li><strong>CA-RAG module:</strong> Extracts useful information from the per-chunk VLM response and aggregates it to generate a single unified summary. CA-RAG (Context Aware-Retrieval-Augmented Generation) uses the REST API of an LLM NIM microservice.</li>\n\n\n\n<li><strong>Graph-RAG module:</strong> Captures the complex relationships present in the video and stores important information in a graph database as sets of nodes and edges. This is then queried by an LLM for interactive Q&amp;A.</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/summarization-vision-ai-agent-architecture.png\"><img loading=\"lazy\" decoding=\"async\" width=\"1015\" height=\"696\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/summarization-vision-ai-agent-architecture.png\" alt=\"A diagram shows the architecture of the visual search and summarization agent. It includes the data flow of how videos are processed and used to generate summaries, alerts and Q&amp;A. \" class=\"wp-image-91153\" style=\"width:625px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/summarization-vision-ai-agent-architecture.png 1015w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/summarization-vision-ai-agent-architecture-300x206.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/summarization-vision-ai-agent-architecture-625x429.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/summarization-vision-ai-agent-architecture-168x115.png 168w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/summarization-vision-ai-agent-architecture-768x527.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/summarization-vision-ai-agent-architecture-645x442.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/summarization-vision-ai-agent-architecture-438x300.png 438w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/summarization-vision-ai-agent-architecture-131x90.png 131w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/summarization-vision-ai-agent-architecture-362x248.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/summarization-vision-ai-agent-architecture-160x110.png 160w\" sizes=\"(max-width: 1015px) 100vw, 1015px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 1. High-level architecture of the summarization vision AI agent</em></figcaption></figure></div>\n\n\n<p>Here\u2019s more information about the video ingestion and retrieval pipeline and how the blueprint is capable of summarization, Q&amp;A, and alerts over live streams and long videos.</p>\n\n\n\n<h3 id=\"video_ingestion\"  class=\"wp-block-heading\">Video ingestion<a href=\"#video_ingestion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To summarize a video or perform Q&amp;A, a comprehensive index of the video must be built that captures all the important information. This is done by combining VLMs and LLMs to produce dense captions and metadata to build a knowledge graph of the video. This video ingestion pipeline is GPU-accelerated and scales with more GPUs to lower processing time.</p>\n\n\n\n<h4 class=\"wp-block-heading\">VLM pipeline and CA-RAG</h4>\n\n\n\n<p>Most VLMs today accept only a limited number of frames, for example, 8/10/100. They also can\u2019t accurately generate captions for longer videos. For longer videos such as hour-long videos, sampled frames could be 10s of seconds apart or even longer. This can result in some details getting missed or actions not getting recognized.</p>\n\n\n\n<p>A solution to this problem is to create smaller chunks from long videos, analyze the chunks individually using VLMs to produce dense captions, and then summarize and aggregate results to generate a single summary for the entire file. This part of the ingestion process is the VLM pipeline and CA-RAG module.&nbsp;</p>\n\n\n\n<p>This strategy of chunking and captioning can also be applied to live streams. The blueprint includes a streaming pipeline that receives streaming data from an RTSP server. The NVIDIA AI Blueprint continuously generates video-chunk segments based on the user-configured chunk duration. The VLM pipeline then generates the captions for these chunks.</p>\n\n\n\n<p>The NVIDIA AI Blueprint keeps on gathering the captions from the VLM pipeline. When enough chunks are processed based on the user-configured summary duration, the chunks gathered are sent to CA-RAG for summarization and aggregation. The blueprint continues processing the next chunks. The summaries are streamed to the client using HTTP server-sent events.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Knowledge graph and Graph-RAG module&nbsp;</h4>\n\n\n\n<p>To capture the complex information produced by the VLM, a knowledge graph is built and stored during video ingestion. Use an LLM to convert the dense captions in a set of nodes, edges, and associated properties. This knowledge graph is stored in a graph database. By using Graph-RAG techniques, an LLMcan access this information to extract key insights for summarization, Q&amp;A, and alerts and go beyond what VLMs&nbsp;are capable of on their own.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"948\" height=\"568\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/knowledge-graph-warehouse-video.png\" alt=\"Diagram shows orange and purple circles connected by action words. Worker carrying box, worker dropped box, person inspects restricted zone, and box near pallets are some examples.\" class=\"wp-image-91145\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/knowledge-graph-warehouse-video.png 948w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/knowledge-graph-warehouse-video-300x180.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/knowledge-graph-warehouse-video-625x374.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/knowledge-graph-warehouse-video-179x107.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/knowledge-graph-warehouse-video-768x460.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/knowledge-graph-warehouse-video-645x386.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/knowledge-graph-warehouse-video-500x300.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/knowledge-graph-warehouse-video-150x90.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/knowledge-graph-warehouse-video-362x217.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/knowledge-graph-warehouse-video-184x110.png 184w\" sizes=\"(max-width: 948px) 100vw, 948px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. The knowledge graph produced from a short warehouse video</em></figcaption></figure></div>\n\n\n<h3 id=\"video_retrieval&nbsp;\"  class=\"wp-block-heading\">Video retrieval&nbsp;<a href=\"#video_retrieval&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>When the video has been ingested, the databases behind the CA-RAG and Graph-RAG modules contain an immense amount of information about the objects, events, and descriptions of what occurred in the video. This information can be queried and consumed by an LLM for several tasks, including summarization, Q&amp;A, and alerts. </p>\n\n\n\n<p>For each of these tasks, the blueprint exposes simple REST APIs that can be called to integrate with your application. A reference UI is also provided to enable you to quickly experiment with the features of the blueprint and tune the agent with several configuration options.</p>\n\n\n\n<h3 id=\"summarization\"  class=\"wp-block-heading\">Summarization<a href=\"#summarization\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>When a video file has been uploaded to the agent through the APIs, call the <code>summarize</code> endpoint to get a summary of the video. The blueprint takes care of all the heavy lifting while providing a lot of configurable parameters.</p>\n\n\n\n<p>When submitting the <code>summarize</code> request, there are prompts used to tune the outputs. This controls the VLM dense captioning and the LLM-based caption aggregation to produce the final summary.&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Prompt (VLM):</strong> Prompt given to the VLM to produce dense captions. This prompt can be tuned to tell the VLM exactly what type of objects, events, and actions it should pay attention to.&nbsp;&nbsp;</li>\n\n\n\n<li><strong>Caption summarization (LLM):</strong> An LLM prompt used to combine the VLM captions. This can be used to control how fine-grained the captions should be and the level of detail to include.&nbsp;</li>\n\n\n\n<li><strong>Summary aggregation (LLM):</strong> Produces the final summary output based on the aggregated captions. This prompt should be tuned to specify an output format, length of the summary, and a list of any key pieces of information that should be included in the output.&nbsp;</li>\n</ul>\n\n\n\n<p>In addition to the prompt configuration, the strategy to chunk the video is also important to tune based on your use case. There are a few different options depending on whether the summarization is over a video file or a live stream.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">Video files</h4>\n\n\n\n<ul class=\"wp-block-list\">\n<li><code>chunk_duration</code>: The entire video is divided into <code>chunk_duration</code> length segments, <em>N</em> (VLM-dependent) frames are sampled from this chunk and sent to VLM for inference. The chunk duration should be small enough that the <em>N</em> frames can capture the event.</li>\n\n\n\n<li><code>chunk_overlap</code>: If an event occurs at the chunk intersection, then the sampled frames might not capture the complete event and the model can\u2019t detect it. The NVIDIA AI Blueprint alleviates this problem by using a sliding window approach where <code>chunk_overlap</code> is the overlap duration between the chunks. (Default: <code>0</code>).</li>\n</ul>\n\n\n\n<h4 class=\"wp-block-heading\">Streams</h4>\n\n\n\n<ul class=\"wp-block-list\">\n<li><code>chunk_duration</code>: Similar to video files, the live stream is divided into segments of <code>chunk_duration</code> and sent to VLM for inference. The chunk duration should be small enough that the <em>N</em> frames can capture the event.</li>\n\n\n\n<li><code>summary_duration</code>: The duration for which the user wants a summary. This enables the user to control the duration of the stream for which the summary should be produced. For instance, if <code>chunk_duration</code> is 1 min and the summary duration is 30 min., then the stream is divided into 1-min. chunks for VLM inference. The VLM output of 30 chunks is aggregated to provide the user with a 30-min. concise summary.</li>\n</ul>\n\n\n\n<p>These are just guidelines and the actual parameter must be tuned for specific use cases. It\u2019s a tradeoff between accuracy and performance. Smaller chunk sizes result in better descriptions but take longer to process.</p>\n\n\n\n<h3 id=\"q&amp;a\"  class=\"wp-block-heading\">Q&amp;A<a href=\"#q&amp;a\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The knowledge graph built during video ingestion can be queried by an LLM to provide a natural language interface into the video. This enables users to ask open-ended questions over the input video and have a chatbot experience. In the reference UI, this feature is available after the video has been ingested.&nbsp;</p>\n\n\n\n<p>The LLM used to power Q&amp;A is configurable and can be adjusted through the blueprint configuration after deployment. It gives you the control to choose a model that best suits your local deployment or point it to an LLM deployed in the cloud.&nbsp;</p>\n\n\n\n<p>The prompts given to the LLM to retrieve the information needed from the knowledge are adjustable and can be tuned to improve the accuracy of the responses.&nbsp;</p>\n\n\n\n<h3 id=\"alerts\"  class=\"wp-block-heading\">Alerts<a href=\"#alerts\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In addition to video files, the blueprint can also accept a video live stream as input. For live streaming use cases, it is often critical to know when certain events take place in near real-time. To accomplish this, the blueprint enables live streams to be registered and alert rules can be set to monitor the stream. These alert rules are in natural language and are used to trigger notifications when user-defined events occur.&nbsp;</p>\n\n\n\n<p>For example, a camera set in a forest could be set with alert rules to detect when animals come into view or if a fire breaks out. When the stream is registered and the alert rules are set, the agent monitors the stream. If it detects that any of the alert rules are true, then it triggers a notification that can be received through the APIs.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/N1UOqr7Ga_A?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 2. Build Visual AI Agents for Video Search and Summarization</em></figcaption></figure>\n\n\n\n<h2 id=\"getting_started\"  class=\"wp-block-heading\">Getting started<a href=\"#getting_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Build powerful VLM-based AI agents using <a href=\"https://build.nvidia.com/nvidia/video-search-and-summarization\">NVIDIA AI Blueprint for Video Search and Summarization</a>. REST APIs provide ease of integration of this workflow and VLMs in existing customer applications. Apply for <a href=\"https://developer.nvidia.com/ai-blueprint-for-video-search-and-summarization-early-access/join\">early access to this AI Blueprint</a> now and see the <a href=\"https://forums.developer.nvidia.com/c/accelerated-computing/intelligent-video-analytics/visual-ai-agent/680\">Visual AI Agents forum</a> for technical questions.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>This post was originally published July 29, 2024 but has been extensively revised with NVIDIA AI Blueprint information. Traditional video analytics applications and their development workflow are typically built on fixed-function, limited models that are designed to detect and identify only a select set of predefined objects. With generative AI, NVIDIA NIM microservices, and foundation &hellip; <a href=\"https://developer.nvidia.com/blog/build-a-video-search-and-summarization-agent-with-nvidia-ai-blueprint/\">Continued</a></p>\n", "protected": false}, "author": 1925, "featured_media": 86302, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1457517", "discourse_permalink": "https://forums.developer.nvidia.com/t/build-vlm-powered-visual-ai-agents-using-nvidia-nim-and-nvidia-via-microservices/301586", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 3110, 63], "tags": [3965, 453, 2932, 3737, 3953], "coauthors": [3616, 3948, 3949, 3950, 3951, 3952, 995, 1095, 564], "class_list": ["post-86011", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-generative-ai", "category-robotics", "tag-ai-agent", "tag-featured", "tag-large-language-models", "tag-microservices", "tag-vlms"], "acf": {"post_industry": ["Healthcare & Life Sciences", "Manufacturing", "Smart Cities / Spaces"], "post_products": ["Metropolis", "NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/via-microservices-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mnh", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86011"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1925"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=86011"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86011/revisions"}], "predecessor-version": [{"id": 91431, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86011/revisions/91431"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/86302"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=86011"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=86011"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=86011"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=86011"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91412, "date": "2024-11-01T15:00:36", "date_gmt": "2024-11-01T22:00:36", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91412"}, "modified": "2024-11-05T18:23:28", "modified_gmt": "2024-11-06T02:23:28", "slug": "3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot/", "title": {"rendered": "3x Faster AllReduce with NVSwitch and TensorRT-LLM MultiShot"}, "content": {"rendered": "\n<p>Deploying generative AI workloads in production environments where user numbers can fluctuate from hundreds to hundreds of thousands \u2013 and where input sequence lengths differ with each request \u2013 poses unique challenges. To achieve low latency inference in these environments, multi-GPU setups are a must &#8211; irrespective of the GPU generation or its memory capacity. To enhance inference performance in production-grade setups, we&#8217;re excited to introduce TensorRT-LLM Multi-shot, a new multi-GPU communication protocol that leverages the <a href=\"https://www.nvidia.com/en-us/data-center/nvlink/?srsltid=AfmBOortYFyQ2kE7Y-h-aY4CEDLdD7bG2KMG71bGYLNCdIbCeoq9nns2\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NVLink Switch</a> to significantly increase communication speeds by up to 3x. This blog outlines this new feature and how it helps developers and solution architects address the limitations of traditional multi-GPU communication methods.</p>\n\n\n\n<h3 id=\"challenges_with_traditional_allreduce_algorithms\"  class=\"wp-block-heading\">Challenges with traditional AllReduce algorithms<a href=\"#challenges_with_traditional_allreduce_algorithms\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>For low latency inference, multi-GPU is critical, regardless of the memory capacity of a single GPU. However, at low concurrency, the time GPUs spend exchanging data can outweigh the time spent on compute. For optimal performance, an efficient <a href=\"https://docs.nvidia.com/doca/archive/doca-v1.3/allreduce/index.html#:~:text=Allreduce%20is%20a%20collective%20operation,Allreduce%20operates%20in%20stages.\">AllReduce</a> operation \u2013 a collective operation that combines partial results from each participating GPU \u2013 is critical.</p>\n\n\n\n<p>Traditional approaches use ring-based algorithms, where the partial values are passed around a ring of GPUs.&nbsp; Each GPU contributes its values and passes the result to its neighbor. This process is repeated 2N-2 times where N is the number of GPUs working together, and by the end of the process, every GPU has the same summed value. A second pass over the ring is required to propagate summed values from the last GPU to the rest.&nbsp;</p>\n\n\n\n<p>The Ring approach makes efficient use of available GPU-to-GPU bandwidth per communication step, but as the number of GPUs increases, so does the number of steps. This increases latency, as all GPUs need to stay synchronized at every step of the ring. \u200cThese synchronization latencies add significant latency overhead and can make it difficult to meet more stringent latency targets.&nbsp;</p>\n\n\n\n<p>The Ring AllReduce algorithm is described below:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Ring Algorithm:\u00a0 GPU-1 \u2192 GPU-2 \u2192 \u2026 \u2192 GPU-N \u2192 GPU-1 \u2192 GPU-2 \u2192 \u2026 \u2192 GPU-(N-1)</li>\n\n\n\n<li>2N-2 steps, with full tensor send/recv each step</li>\n\n\n\n<li>Latency: 2N-2 communication steps.\u00a0 (N: # of GPUs)</li>\n\n\n\n<li>Traffic: (4N-4)/N tensor bytes of send/recvs</li>\n</ul>\n\n\n\n<h3 id=\"addressing_allreduce_communication_challenges_with_tensorrt-llm_multishot\"  class=\"wp-block-heading\">Addressing AllReduce communication challenges with TensorRT-LLM MultiShot<a href=\"#addressing_allreduce_communication_challenges_with_tensorrt-llm_multishot\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>TensorRT-LLM MultiShot is a new algorithm that reduces the O(N) latency of Ring AllReduce by up to 3x leveraging multicast in NVSwitch. Multicast is a hardware acceleration feature in NVSwitch which allows a GPU to send data once and have that data sent simultaneously to all other GPUs, minimizing the number of communication steps to two inter-GPU synchronizations while remaining bandwidth efficient. Without NVSwitch, this would take N times the communication bandwidth.&nbsp;</p>\n\n\n\n<p>TensorRT-LLM Multishot separates the AllReduce into a ReduceScatter operation followed by an AllGather operation (for more detailed descriptions of collective operations, see this <a href=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html\">documentation</a>).</p>\n\n\n\n<p>Each GPU is responsible for accumulating only a portion of the result tensor.&nbsp;</p>\n\n\n\n<p>The first step (or \u201cshot\u201d) involves each GPU sending the different slices of the tensor to the respective GPU responsible for accumulating that slice of the tensor.</p>\n\n\n\n<p>After accumulating locally, each GPU now has the correct sum accumulators for its unique slice of the output.</p>\n\n\n\n<p>In the second step (or \u201cshot\u201d), each GPU broadcasts the result slice to all other GPUs using the NVSwitch multicast capability. This minimizes the per GPU bandwidth required as the NVSwitch itself performs data amplification; each GPU sends 1/N the data and receives the full result tensor in one step.</p>\n\n\n\n<p>The entire operation only takes two communication steps, regardless of the number GPUs performing tensor parallel inference.</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>TensorRT-LLM MultiShot Algorithm: GPU_N sends slices, Compute slice sum, broadcast result in single multicast operation.</li>\n\n\n\n<li>Latency: 2 communication steps (regardless of number of GPUs)</li>\n\n\n\n<li>Traffic: 2 tensor bytes of send/recv (regardless of number of GPUs)</li>\n</ul>\n\n\n\n<h3 id=\"why_this_matters\"  class=\"wp-block-heading\">Why this matters<a href=\"#why_this_matters\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Since this algorithm requires only two communication steps rather than 2N-2 (where N is the number of GPUs), MultiShot can be nearly 3x faster than Ring AllReduce. The benefits of this algorithm are particularly evident with smaller message sizes and high parallelism \u2013 the scenario needed when minimum latency is required for a great user experience.&nbsp;</p>\n\n\n\n<p>This can be used to either reduce minimum latency, or increase throughput at a given latency. In scenarios with more aggressive latency thresholds, this can lead to super-linear scaling with the number of GPUs.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1884\" height=\"1194\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X.png\" alt=\"A chart showing the reduction in latency that TensorRT-LLM MultiShot provides across message sizes.\u00a0\" class=\"wp-image-91413\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X.png 1884w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X-300x190.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X-625x396.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X-768x487.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X-1536x973.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X-645x409.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X-473x300.png 473w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X-142x90.png 142w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X-362x229.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X-174x110.png 174w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/11/Figure-1.-With-TensorRT-LLM-MultiShot-AllReduce-latency-is-reduced-by-up-to-3X-1024x649.png 1024w\" sizes=\"(max-width: 1884px) 100vw, 1884px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. With TensorRT-LLM MultiShot, AllReduce latency is reduced by up to 3x.</em></figcaption></figure></div>\n\n\n<p>Achieving optimal inference performance requires careful workload analysis and a deep understanding of performance bottlenecks. By gaining that understanding \u2013 both through internal engineering work as well as through close collaboration with external developers and researchers \u2013 we can quickly and frequently optimize many aspects of our platform to deliver great performance for users.</p>\n\n\n\n<p>As we continue to identify and implement new performance optimizations \u2013 some may be extensive, others might be narrower in scope \u2013&nbsp; we will be providing regular updates on these optimizations, providing both technical motivation and quantified benefits.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Deploying generative AI workloads in production environments where user numbers can fluctuate from hundreds to hundreds of thousands \u2013 and where input sequence lengths differ with each request \u2013 poses unique challenges. To achieve low latency inference in these environments, multi-GPU setups are a must &#8211; irrespective of the GPU generation or its memory capacity. &hellip; <a href=\"https://developer.nvidia.com/blog/3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot/\">Continued</a></p>\n", "protected": false}, "author": 2408, "featured_media": 88129, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1512982", "discourse_permalink": "https://forums.developer.nvidia.com/t/3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot/311952", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [4150, 3110, 1903], "tags": [296, 4159], "coauthors": [4157, 506, 2940, 2732, 3708], "class_list": ["post-91412", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-deployment", "category-generative-ai", "category-features", "tag-ai-inference-microservices", "tag-inference-performance"], "acf": {"post_industry": ["General", "Cloud Services"], "post_products": ["NVLink", "NVSwitch", "TensorRT", "TensorRT-LLM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-tech-blog-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nMo", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91412"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2408"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91412"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91412/revisions"}], "predecessor-version": [{"id": 91422, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91412/revisions/91422"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88129"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91412"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91412"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91412"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91412"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91198, "date": "2024-10-31T13:24:07", "date_gmt": "2024-10-31T20:24:07", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91198"}, "modified": "2024-11-04T08:55:56", "modified_gmt": "2024-11-04T16:55:56", "slug": "even-faster-and-more-scalable-umap-on-the-gpu-with-rapids-cuml", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/even-faster-and-more-scalable-umap-on-the-gpu-with-rapids-cuml/", "title": {"rendered": "Even Faster and More Scalable UMAP on the GPU with RAPIDS cuML"}, "content": {"rendered": "\n<p><a href=\"https://umap-learn.readthedocs.io/en/latest/\">UMAP</a> is a popular dimension reduction algorithm used in fields like bioinformatics, NLP topic modeling, and ML preprocessing. It works by creating a k-nearest neighbors (k-NN) graph, which is known in literature as an all-neighbors graph, to build a fuzzy topological representation of the data, which is used to embed high-dimensional data into lower dimensions.&nbsp;</p>\n\n\n\n<p><a href=\"https://github.com/rapidsai/cuml\">RAPIDS cuML</a> already contained an accelerated UMAP, which provided significant speed improvements over the original CPU-based UMAP. As we demonstrate in this post, there was still room for improvement.&nbsp;</p>\n\n\n\n<p>In this post, we explore how to use the new features introduced in RAPIDS cuML 24.10. We also dive into the details of the nn-descent algorithm and the batching process. Finally, we share benchmark results to highlight possible performance gains. By the end of this post, we hope you are excited about the benefits that RAPIDS\u2019 faster and scalable UMAP can provide.</p>\n\n\n\n<h2 id=\"challenges\"  class=\"wp-block-heading\">Challenges<a href=\"#challenges\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>One challenge we faced is that the all-neighbors graph-building phase takes a long time, especially in comparison to the other steps in the UMAP algorithm.&nbsp;</p>\n\n\n\n<p>cuML UMAP initially used only a <a href=\"https://docs.rapids.ai/api/cuvs/stable/python_api/neighbors_brute_force/\">brute-force</a> approach to compute the all-neighbors graph, which is usually referred to in literature as an <em>all-neighbors graph</em> because it involves an exhaustive vector search over all vectors in the dataset.&nbsp;</p>\n\n\n\n<p>Because it exhaustively computes distances for every pair of vectors in the dataset, brute force tends to have poor scaling. Thus, as the number of vectors in the dataset grows, the amount of time spent in this step grows quadratically (number of vectors to the power of 2) as compared to all the other steps in UMAP.&nbsp;</p>\n\n\n\n<p>Figure 1 shows the proportion of time spent in the all-neighbors graph construction for several popular datasets. The proportion spent in all-neighbors graph construction quickly becomes 99% and higher at the 1M and 5M vector scales.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1000\" height=\"250\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-building-all-neighbors.png\" alt=\"Four pie charts demonstrate the proportions of the amount of time the UMAP algorithm spends computing the all-neighbors graph compared to the time spent computing everything else. For small datasets like MNIST, over half the time (57%) is spent computing the all-neighbors graph, while larger datasets (with 1M and greater vectors) spend over 99% of the time computing the all-neighbors graph.\u00a0\" class=\"wp-image-91203\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-building-all-neighbors.png 1000w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-building-all-neighbors-300x75.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-building-all-neighbors-625x156.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-building-all-neighbors-179x45.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-building-all-neighbors-768x192.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-building-all-neighbors-645x161.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-building-all-neighbors-500x125.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-building-all-neighbors-160x40.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-building-all-neighbors-362x91.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-building-all-neighbors-440x110.png 440w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Time spent building all-neighbors graph</em></figcaption></figure></div>\n\n\n<p>The second challenge we faced is that, like many algorithms in cuML, the entire dataset had to fit into the memory of the GPU.&nbsp;</p>\n\n\n\n<p>Handling large datasets, such as those that are hundreds of GB in size, can be especially challenging when only a consumer-level NVIDIA RTX GPU is available for processing. Even though the NVIDIA H100 GPU offers 80 GB of memory, this may not be sufficient for an 80-GB dataset because algorithms like UMAP require many little temporary memory allocations that can add up over the course of the algorithm.</p>\n\n\n\n<h2 id=\"accelerating_and_scaling_umap\"  class=\"wp-block-heading\">Accelerating and scaling UMAP<a href=\"#accelerating_and_scaling_umap\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>We have solved these challenges with a novel batched approximate nearest neighbor (ANN) algorithm. While the general approach can apply to any algorithm capability of searching for nearest neighbors, we used a GPU-accelerated version of a fast algorithm called nearest neighbors descent (<a href=\"https://www.cs.princeton.edu/cass/papers/www11.pdf\">nn-descent</a>) from the <a href=\"https://github.com/rapidsai/cuvs\">RAPIDS cuVS</a> library, which is great for all-neighbors graph construction.\u00a0</p>\n\n\n\n<p>ANN algorithms accelerate the all-neighbors graph-building process by trading off quality for speed. In general, approximate methods aim to reduce the number of distances that need to be computed to find the nearest neighbors. As this algorithm can compute a single all-neighbors graph in pieces, we could place larger datasets in RAM memory and pull only what we need into the GPU memory when we needed it.&nbsp;</p>\n\n\n\n<p>As we demonstrate in this post, our new approach scales UMAP in RAPIDS cuML 24.10 to massive datasets at lightspeed. What\u2019s better is that it\u2019s enabled by default, so you don\u2019t have to make any changes to your code to reap the benefits!</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Matrix size</strong></td><td><strong>Running UMAP with brute-force</strong></td><td><strong>Running UMAP with nn-descent</strong></td></tr><tr><td>1M x 960</td><td>214.4s</td><td>9.9s (21.6x speedup)</td></tr><tr><td>8M x 384</td><td>2191.3s</td><td>34.0s (54.4x speedup)</td></tr><tr><td>10M x 96</td><td>2170.8s</td><td>53.4s (40.6x speedup)</td></tr><tr><td>20M x 384</td><td>38350.7s</td><td>122.9 (312x speedup)</td></tr><tr><td>59M x 768</td><td>Error: out of memory</td><td>575.1</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Comparison of end-to-end runtime (seconds) of running UMAP with nn-descent and brute force as its all-neighbors graph-building algorithm</em></figcaption></figure>\n\n\n\n<p>Table 1 shows that UMAP can now run with datasets that don\u2019t fit on the device (50M, 768 is 153 GB). Speedup gain increases for large datasets. What used to take 10 hours to run on the GPU can be run in 2 minutes.</p>\n\n\n\n<h2 id=\"using_faster_and_scalable_umap_in_rapids_cuml\"  class=\"wp-block-heading\">Using faster and scalable UMAP in RAPIDS cuML<a href=\"#using_faster_and_scalable_umap_in_rapids_cuml\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>As mentioned earlier, no code changes are required as of cuML 24.10 to take advantage of this new feature.&nbsp;</p>\n\n\n\n<p>However, for more control, the UMAP estimator now accepts two more parameters during initialization:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><code>build_algo</code>: Algorithm to build the all-neighbors graph. It can be one of the following three values:\n<ul class=\"wp-block-list\">\n<li><code>auto</code>: Decides to build with brute force or nn-descent during runtime depending on the dataset size (>50K data samples uses nn-descent). Default value.</li>\n\n\n\n<li><code>brute_force_knn</code>: Builds all-neighbors graph using brute force.</li>\n\n\n\n<li><code>nn_descent</code>: Builds all-neighbors graph using nn-descent.</li>\n</ul>\n</li>\n\n\n\n<li><code>build_kwds</code>: Python dictionary type for passing parameters related to all-neighbors graph building, with the following parameters:\n<ul class=\"wp-block-list\">\n<li><code>nnd_graph_degree</code>: Graph degree when building k-nn with nn-descent. Default: <code>64</code>.</li>\n\n\n\n<li><code>nnd_intermediate_graph_degree</code>: Intermediate graph degree when building k-NN with nn-descent. Default: <code>128</code>.</li>\n\n\n\n<li><code>nnd_max_iterations</code>: Maximum number of iterations to run nn-descent. Default: <code>20</code>.</li>\n\n\n\n<li><code>nnd_termination_threshold</code>: Termination threshold to early stop nn-descent iterations. Default: <code>0.0001</code>.</li>\n\n\n\n<li><code>nnd_return_distances</code>: Whether to return distances from nn-descent. This should be set to true to use nn-descent with UMAP. Default: <code>True.</code></li>\n\n\n\n<li><code>nnd_n_clusters</code>: Number of clusters to use for the batching approach. A larger number of clusters reduces memory usage when running with larger datasets. Default: <code>2</code>.</li>\n\n\n\n<li><code>nnd_do_batch</code>: Should be set to <code>True</code> for batching. Default: <code>False</code>.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>You can also choose to put the data on the host instead of putting the entire data on the device using the <code>data_on_host</code> option which defaults to <code>False</code>. This is only compatible with <code>build_algo=\u201dnn_descent\u201d</code> and is not supported for building with the brute-force algorithm. </p>\n\n\n\n<p>We recommend that you put data on the host to get the most out of our batching algorithm for large datasets.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom cuml.manifold.umap import UMAP\n\ndata = generate_data()\n\n# running default. Runs with NN Descent if data has more than 50K points\numap = UMAP(n_neighbors=16)\nemb\u00a0 = umap.fit_transform(data)\n\n# explicitly set build algo. Runs with this regardless of the data size. Data can be put on host\numap = UMAP(n_neighbors=16, build_algo=&quot;nn_descent&quot;, build_kwds={&quot;nnd_graph_degree&quot;: 32})\nemb = umap.fit_transform(data, data_on_host=True)\n\n# batching NN Descent with 4 clusters\numap = UMAP(n_neighbors=16, build_algo=&quot;nn_descent&quot;, build_kwds={&quot;nnd_do_batch&quot;: True, &quot;nnd_n_clusters&quot;: 4})\nemb = umap.fit_transform(data, data_on_host=True)\n</pre></div>\n\n\n<h2 id=\"why_approximate_nearest_neighbors\"  class=\"wp-block-heading\">Why approximate nearest neighbors?<a href=\"#why_approximate_nearest_neighbors\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Brute-force is an exact and exhaustive algorithm. In contrast, ANN algorithms don\u2019t guarantee finding the exact closest neighbors but they do efficiently navigate the search space to construct an approximation to the nearest neighbors much faster, trading off search speed for accuracy.</p>\n\n\n\n<p>Nearest neighbors descent (nn-descent) is an ANN algorithm that can directly approximate an all-neighbors graph. The algorithm begins by randomly initializing nearest neighbors for each data point before iteratively improving nearest neighbor approximations by exploring each point\u2019s neighbors\u2019 neighbors.\u00a0</p>\n\n\n\n<p>As noted in the <a href=\"https://dl.acm.org/doi/10.1145/1963405.1963487\">original paper</a>, nn-descent \u201ctypically converges to above 90% recall with each point comparing only to several percent of the whole dataset on average\u201d. In short, ANN algorithms generally find clever ways to reduce the number of distances that must be computed.</p>\n\n\n\n<p>We used nn-descent from the <a href=\"https://github.com/rapidsai/cuvs\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA cuVS library</a> to construct all-neighbors graphs for UMAP. For large datasets, this method accelerates the all-neighbors graph-building process by hundreds of times, while still maintaining functionally equivalent results.</p>\n\n\n\n<h2 id=\"using_batching_to_scale_all-neighbors_graph_construction\"  class=\"wp-block-heading\">Using batching to scale all-neighbors graph construction<a href=\"#using_batching_to_scale_all-neighbors_graph_construction\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Managing a large dataset by keeping it on the host and processing it in batches on the device may seem straightforward. However, a key challenge when building k-NN subgraphs with a certain subset of the dataset is that data samples with similar indices are not guaranteed to be close in distance. This means you can\u2019t simply slice the dataset into batches.</p>\n\n\n\n<p>We solved this problem with a batching approach that is inspired by literature on the popular <a href=\"https://www.microsoft.com/en-us/research/publication/diskann-fast-accurate-billion-point-nearest-neighbor-search-on-a-single-node/\">DiskANN</a> algorithm. We first perform a balanced k-means clustering on a subsample of the dataset to extract centroids for a predefined number of clusters. Then, using this information, we partition the dataset into batches based on their closest clusters.&nbsp;</p>\n\n\n\n<p>This approach ensures that data points in each batch are more likely to be closed to each other, improving the likelihood that nearest neighbors are found within the same batch. The remaining part of this section explains each step of the batching process in detail:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Extract the cluster centroids</li>\n\n\n\n<li>Find data points for each cluster</li>\n\n\n\n<li>Build subgraphs of cluster data points</li>\n\n\n\n<li>Merge the k-NN subgraph with the global all-neighbors graph</li>\n</ul>\n\n\n\n<h3 id=\"extract_the_cluster_centroids\"  class=\"wp-block-heading\">Extract the cluster centroids<a href=\"#extract_the_cluster_centroids\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>We first extracted the cluster centroids from the dataset. Because we assumed that a large dataset doesn\u2019t fit on a GPU device, we left the data in host memory and randomly subsampled a set of points to ensure that the subset fits in the GPU device memory. Usually, 10% of the dataset is a large enough subsample to find a usable set of centroids.\u00a0</p>\n\n\n\n<p>Using the <code>nnd_n_clusters</code> parameters provided by the user, we ran balanced k-means on the sampled subset to identify the specified number of cluster centers.</p>\n\n\n\n<h3 id=\"find_data_points_for_each_cluster\"  class=\"wp-block-heading\">Find data points for each cluster<a href=\"#find_data_points_for_each_cluster\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Next, we determined the top two closest cluster centers for each data point and then inverted the indices to find the data points that belonged to each cluster. This process resulted in each data point being assigned to two separate clusters.&nbsp;</p>\n\n\n\n<p>This approach ensures that there is overlap in the neighborhoods for each cluster, increasing the likelihood that the final neighborhoods will include at least an acceptable number of the neighbors that we might have expected if we had computed the exact results.</p>\n\n\n\n<h3 id=\"build_subgraphs_of_cluster_data_points\"  class=\"wp-block-heading\">Build subgraphs of cluster data points<a href=\"#build_subgraphs_of_cluster_data_points\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>When we knew the data points that belonged to each cluster, we proceeded to iteratively build subgraphs on the data points for each cluster. This means that for each cluster, we gathered the data points for that cluster in the GPU\u2019s memory and ran NN-descent on this subset to construct the all-neighbors graph for that cluster.</p>\n\n\n\n<h3 id=\"merge_the_k-nn_subgraph_with_the_global_all-neighbors_graph\"  class=\"wp-block-heading\">Merge the k-NN subgraph with the global all-neighbors graph<a href=\"#merge_the_k-nn_subgraph_with_the_global_all-neighbors_graph\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>After the all-neighbors graph for a cluster was built, we merged this k-NN subgraph with the global all-neighbors graph. To do this efficiently, we used a custom CUDA kernel that merged the two subgraphs without allocating additional device memory.&nbsp;</p>\n\n\n\n<p>After iterating through all the clusters in this way, the global all-neighbors graph was returned as the final result. As this graph is generally much smaller than the input dataset, it could be copied safely into the GPU\u2019s memory space even when the input dataset was much too large to fit.</p>\n\n\n\n<h2 id=\"performance_improvements\"  class=\"wp-block-heading\">Performance improvements<a href=\"#performance_improvements\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>We evaluated the performance impact of using cuML UMAP and the new batched all-neighbors graph construction method.\u00a0</p>\n\n\n\n<p>For these experiments, we used an NVIDIA H100 GPU with 80 GB of memory. These comparisons are against the GPU version of UMAP, and so these speedups are not from a CPU-to-GPU comparison but improvements to the existing GPU implementation.\u00a0</p>\n\n\n\n<p>Figure 2 illustrates the total runtime of UMAP in cuML, comparing the new NN-descent strategy with the brute-force all-neighbors graph construction strategy.<strong> </strong>For a dataset with 20M points and 384 dimensions, we gained 311x speedup using NN-descent, reducing UMAP\u2019s total runtime on the GPU from 10 hours to just 2 minutes!</p>\n\n\n\n<p>Figure 2 is in log scale because the speedups are so high.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1000\" height=\"573\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-compute-umap-embeddings.png\" alt=\"A bar chart shows the time to compute UMAP embeddings when computing the all-neighbors graph with brute-force compared to NN-descent. The chart demonstrates that our new batching approach to constructing the all-neighbors graph results in massive speedups.\" class=\"wp-image-91205\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-compute-umap-embeddings.png 1000w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-compute-umap-embeddings-300x172.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-compute-umap-embeddings-625x358.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-compute-umap-embeddings-179x103.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-compute-umap-embeddings-768x440.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-compute-umap-embeddings-645x370.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-compute-umap-embeddings-500x287.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-compute-umap-embeddings-157x90.png 157w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-compute-umap-embeddings-362x207.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/time-compute-umap-embeddings-192x110.png 192w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Time to compute UMAP embeddings</em></figcaption></figure></div>\n\n\n<p>We also observe that the UMAP algorithm for a dataset as large as 50M points with 768 dimensions is now able to be run on the GPU, even though this dataset is 150 GB\u2013 much larger than the amount of memory in the GPU.\u00a0</p>\n\n\n\n<p>This feat is achieved with the batching algorithm by partitioning the dataset into five clusters. In contrast, the brute-force all-neighbors graph building algorithm runs out of memory because it attempts to load the entire dataset onto the device at one time.<s>&nbsp;</s></p>\n\n\n\n<p>While this new technique can improve UMAP\u2019s speed and scalability, we need to maintain quality to ensure the low-dimensional embeddings can be used effectively. To measure quality, we turn to the <a href=\"https://scikit-learn.org/dev/modules/generated/sklearn.manifold.trustworthiness.html\">trustworthiness score</a>. Trustworthiness is a score between 0 and 1 that indicates how well the local nearest neighbors structure is retained in the low-dimensional UMAP embedded space as compared to the nearest neighbors of the original vectors before running UMAP. In this metric, higher is better.</p>\n\n\n\n<p>Figure 3 shows that these significant speedups and benefits come without sacrificing the quality of the UMAP embedding results. We can see that there are no significant changes in the trustworthiness score as we increase the numbers of batches.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"640\" height=\"480\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/changes-umap-embedding-quality.png\" alt=\"A line plot shows the original non-batched version of the k-NN graph construction algorithm and three batches of 5, 10, and 15 batches. There is no substantial impact to the trustworthiness scores even when the vectors are chunked across 15 batches.\" class=\"wp-image-91206\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/changes-umap-embedding-quality.png 640w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/changes-umap-embedding-quality-300x225.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/changes-umap-embedding-quality-625x469.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/changes-umap-embedding-quality-153x115.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/changes-umap-embedding-quality-400x300.png 400w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/changes-umap-embedding-quality-120x90.png 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/changes-umap-embedding-quality-362x272.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/changes-umap-embedding-quality-147x110.png 147w\" sizes=\"(max-width: 640px) 100vw, 640px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Evaluation of the changes to UMAP embedding quality over increasing numbers of batches</em></figcaption></figure></div>\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>We are excited to share these performance results with the data science community. Given UMAP\u2019s popularity across various domains, we believe that these new features in RAPIDS cuML will significantly accelerate workflows and help computational scientists uncover insights that are only possible by processing large-scale datasets on the GPU.</p>\n\n\n\n<p>To get started with cuML and install the <code>conda</code> and <code>pip</code> packages, as well as ready-to-go Docker containers, see the <a href=\"https://docs.rapids.ai/install\">RAPIDS Installation Guide</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>UMAP is a popular dimension reduction algorithm used in fields like bioinformatics, NLP topic modeling, and ML preprocessing. It works by creating a k-nearest neighbors (k-NN) graph, which is known in literature as an all-neighbors graph, to build a fuzzy topological representation of the data, which is used to embed high-dimensional data into lower dimensions.&nbsp; &hellip; <a href=\"https://developer.nvidia.com/blog/even-faster-and-more-scalable-umap-on-the-gpu-with-rapids-cuml/\">Continued</a></p>\n", "protected": false}, "author": 2407, "featured_media": 91307, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1512398", "discourse_permalink": "https://forums.developer.nvidia.com/t/even-faster-and-more-scalable-umap-on-the-gpu-with-rapids-cuml/311851", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [145], "coauthors": [4156, 1793, 2497], "class_list": ["post-91198", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "tag-graph-algorithms"], "acf": {"post_industry": "", "post_products": ["cuML", "RAPIDS"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Benchmark"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/umap-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nIW", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91198"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2407"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91198"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91198/revisions"}], "predecessor-version": [{"id": 91443, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91198/revisions/91443"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91307"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91198"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91198"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91198"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91198"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90989, "date": "2024-10-31T13:20:01", "date_gmt": "2024-10-31T20:20:01", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90989"}, "modified": "2024-11-06T21:07:52", "modified_gmt": "2024-11-07T05:07:52", "slug": "build-multimodal-visual-ai-agents-powered-by-nvidia-nim", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/build-multimodal-visual-ai-agents-powered-by-nvidia-nim/", "title": {"rendered": "Build Multimodal Visual AI Agents Powered by NVIDIA NIM"}, "content": {"rendered": "\n<p>The exponential growth of visual data\u2014ranging from images to PDFs to streaming videos\u2014has made manual review and analysis virtually impossible. Organizations are struggling to transform this data into actionable insights at scale, leading to missed opportunities and increased risks.</p>\n\n\n\n<p>To solve this challenge, vision-language models (VLMs) are emerging as powerful tools, combining visual perception of images and videos with text-based reasoning. Unlike traditional <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models</a> (LLMs) that only process text, VLMs empower you to build <a href=\"https://www.nvidia.com/en-us/use-cases/visual-ai-agents/\">visual AI agents</a> that understand and act on complex multimodal data, enabling real-time decision-making and automation.</p>\n\n\n\n<p>Imagine having an intelligent AI agent that can analyze remote camera footage to detect early signs of wildfires or scan business documents to extract critical information buried within charts, tables, and images\u2014all autonomously. </p>\n\n\n\n<p>With <a href=\"https://build.nvidia.com/explore/vision\">NVIDIA NIM microservices</a>, building these advanced visual AI agents is easier and more efficient than ever. Offering flexible customization, streamlined API integration, and smooth deployment, NIM microservices enable you to create dynamic agents tailored to your unique business needs.</p>\n\n\n\n<p>In this post, we guide you through the process of designing and building intelligent visual AI agents using NVIDIA NIM microservices. We introduce the different types of vision AI models available, share four sample applications\u2014streaming video alerts, structured text extraction, multimodal search, and few-shot classification\u2014and provide Jupyter notebooks to get you started. For more information about bringing these models to life, see the <a href=\"https://github.com/NVIDIA/metropolis-nim-workflows\">/NVIDIA/metropolis-nim-workflows</a> GitHub repo.&nbsp;</p>\n\n\n\n<h2 id=\"types_of_vision_ai_models\"  class=\"wp-block-heading\">Types of vision AI models<a href=\"#types_of_vision_ai_models\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To build a robust visual AI agent, you have the following core types of vision models at your disposal: </p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>VLMs</li>\n\n\n\n<li>Embedding models</li>\n\n\n\n<li>Computer vision (CV) models</li>\n</ul>\n\n\n\n<p>These models serve as essential building blocks for developing intelligent visual AI agents. While the VLM functions as the core engine of each agent, CV and embedding models can enhance its capabilities, whether by improving accuracy for tasks like object detection or parsing complex documents.</p>\n\n\n\n<p>In this post, we use <a href=\"https://build.nvidia.com/explore/vision\">vision NIM microservices</a> to access these models. Each vision NIM microservice can be easily integrated into your workflows through simple REST APIs, allowing for efficient model inference on text, images, and videos. To get started, you can experiment with hosted preview APIs on <a href=\"http://build.nvidia.com\">build.nvidia.com</a>, without needing a local GPU.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"557\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llama-3.2-vision-90b-model-1024x557.gif\" alt=\"GIF shows the llama-3.2-vision-90b model summarizing an image.\u00a0\" class=\"wp-image-91046\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llama-3.2-vision-90b-model-1024x557.gif 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llama-3.2-vision-90b-model-300x163.gif 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llama-3.2-vision-90b-model-625x340.gif 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llama-3.2-vision-90b-model-179x97.gif 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llama-3.2-vision-90b-model-768x418.gif 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llama-3.2-vision-90b-model-1536x836.gif 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llama-3.2-vision-90b-model-645x351.gif 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llama-3.2-vision-90b-model-500x272.gif 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llama-3.2-vision-90b-model-160x87.gif 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llama-3.2-vision-90b-model-362x197.gif 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llama-3.2-vision-90b-model-202x110.gif 202w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1.The llama-3.2-vision-90b model on build.nvidia.com</em></figcaption></figure></div>\n\n\n<h3 id=\"vision_language_models\"  class=\"wp-block-heading\">Vision language models<a href=\"#vision_language_models\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>VLMs bring a new dimension to language models by adding vision capabilities, making them multimodal. These models can process images, videos, and text, enabling them to interpret visual data and generate text-based outputs. VLMs are versatile and can be fine-tuned for specific use cases or prompted for tasks such as Q&amp;A based on visual inputs.&nbsp;</p>\n\n\n\n<p>NVIDIA and its partners offer several VLMs as NIM microservices each differing in size, latency, and capabilities (Table 1).&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Company</strong></td><td><strong>Model</strong></td><td><strong>Size</strong></td><td><strong>Description</strong></td></tr><tr><td>NVIDIA</td><td><a href=\"https://build.nvidia.com/nvidia/vila\">VILA</a></td><td>40B</td><td>A powerful general-purpose model built on SigLIP and Yi that is suitable for nearly any use case.&nbsp;</td></tr><tr><td>NVIDIA</td><td><a href=\"https://build.nvidia.com/nvidia/neva-22b\">Neva</a></td><td>22B</td><td>A medium-sized model combining NVGPT and CLIP and offering the functionality of much larger multimodal models.&nbsp;</td></tr><tr><td>Meta</td><td><a href=\"https://build.nvidia.com/meta/llama-3.2-90b-vision-instruct\">Llama 3.2</a></td><td>90B/11B</td><td>The first vision-capable Llama model in two sizes, excelling in a range of vision-language tasks and supporting higher-resolution input.&nbsp;</td></tr><tr><td>Microsoft</td><td><a href=\"https://build.nvidia.com/microsoft/phi-3_5-vision-instruct\">phi-3.5-vision</a></td><td>4.2B</td><td>A small, fast model that excels at OCR and is capable of processing multiple images.&nbsp;</td></tr><tr><td>Microsoft</td><td><a href=\"https://build.nvidia.com/microsoft/microsoft-florence-2\">Florence-2</a></td><td>0.7B</td><td>A multi-task model capable of captioning, object detection, and segmentation using simple text prompts.&nbsp;</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. VLM NIM microservices</em></figcaption></figure>\n\n\n\n<h3 id=\"embedding_models\"  class=\"wp-block-heading\">Embedding models<a href=\"#embedding_models\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Embedding models convert input data (such as images or text) into dense feature-rich vectors known as embeddings. These embeddings encapsulate the essential properties and relationships within the data, enabling tasks like similarity search or classification. Embeddings are typically stored in <a href=\"https://www.nvidia.com/en-us/glossary/vector-database/\">vector databases</a> where GPU-accelerated search can quickly retrieve relevant data.&nbsp;</p>\n\n\n\n<p>Embedding models play a crucial role in creating intelligent agents. For example, they support <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation</a> (RAG) workflows, enabling agents to pull relevant information from diverse data sources and improve accuracy through in-context learning.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Company</strong></td><td><strong>Model</strong></td><td><strong>Description</strong></td><td><strong>Use Cases</strong></td></tr><tr><td>NVIDIA</td><td><a href=\"https://build.nvidia.com/nvidia/nvclip\">NV-CLIP</a></td><td>Multimodal foundation model generating text and image embeddings</td><td>Multimodal search, Zero-shot classification</td></tr><tr><td>NVIDIA</td><td><a href=\"https://build.nvidia.com/nvidia/nv-dinov2\">NV-DINOv2</a></td><td>Vision foundation model generating high-resolution image embeddings</td><td>Similarity search, Few-shot classification</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2.</em><strong><em> </em></strong><em>Embedding NIM microservices</em></figcaption></figure>\n\n\n\n<h3 id=\"computer_vision_models\"  class=\"wp-block-heading\">Computer vision models<a href=\"#computer_vision_models\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>CV models focus on specialized tasks like image classification, object detection, and optical character recognition (OCR). These models can augment VLMs by adding detailed metadata, improving the overall intelligence of AI agents.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Company</strong></td><td><strong>Model</strong></td><td><strong>Description</strong></td><td><strong>Use Cases</strong></td></tr><tr><td>NVIDIA</td><td><a href=\"https://build.nvidia.com/nvidia/nv-grounding-dino\">Grounding Dino</a></td><td>Open-vocabulary object detection</td><td>Detect anything</td></tr><tr><td>NVIDIA</td><td><a href=\"https://build.nvidia.com/nvidia/ocdrnet\">OCDRNet</a></td><td>Optical character detection and recognition</td><td>Document parsing</td></tr><tr><td>NVIDIA</td><td><a href=\"https://build.nvidia.com/nvidia/visual-changenet\">ChangeNet</a></td><td>Detects pixel-level changes between two images&nbsp;</td><td>Defect detection,&nbsp;satellite imagery analysis</td></tr><tr><td>NVIDIA</td><td><a href=\"https://build.nvidia.com/nvidia/retail-object-detection\">Retail Object Detection</a></td><td>Pretrained to detect common retail items&nbsp;</td><td>Loss prevention</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 3.</em><strong><em> </em></strong><em>Computer vision NIM microservices</em></figcaption></figure>\n\n\n\n<h2 id=\"build_visual_ai_agents_with_vision_nim_microservices\"  class=\"wp-block-heading\">Build visual AI agents with vision NIM microservices<a href=\"#build_visual_ai_agents_with_vision_nim_microservices\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Here are real-world examples of how the vision NIM microservices can be applied to create powerful visual AI agents.&nbsp;</p>\n\n\n\n<p>To make application development with NVIDIA NIM microservices more accessible, we have published a collection of examples on GitHub. These examples demonstrate how to use NIM APIs to build or integrate them into your applications. Each example includes a Jupyter notebook tutorial and demo that can be easily launched, even without GPUs.</p>\n\n\n\n<p>On the <a href=\"https://build.nvidia.com/explore/discover\">NVIDIA API Catalog</a>, select a model page, such as <a href=\"https://build.nvidia.com/meta/llama-3_1-405b-instruct\">Llama 3.1 405B</a>. Choose <strong>Get API Key</strong> and <a href=\"https://enterpriseproductregistration.nvidia.com/?LicType=EVAL&amp;ProductFamily=NVAIEnterprise\">enter your business email</a> for a 90-day <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> license, or use your personal email to <a href=\"https://developer.nvidia.com/blog/access-to-nvidia-nim-now-available-free-to-developer-program-members/\">access NIM</a> through the <a href=\"https://developer.nvidia.com/developer-program\">NVIDIA Developer Program</a>.</p>\n\n\n\n<p>On the <a href=\"https://github.com/NVIDIA/metropolis-nim-workflows/tree/main\">/NVIDIA/metropolis-nim-workflows</a> GitHub repo, explore the Jupyter notebook tutorials and demos. These workflows showcase how vision NIM microservices can be combined with other components, like vector databases and LLMs to build powerful AI agents that solve real-world problems. With your API key, you can easily recreate the workflows showcased in this post, giving you hands-on experience with Vision NIM microservices.</p>\n\n\n\n<p>Here are a few example workflows:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://github.com/NVIDIA/metropolis-nim-workflows/tree/main/nim_workflows/vlm_alerts\">VLM streaming video alerts agent</a></li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/metropolis-nim-workflows/tree/main/nim_workflows/vision_text_extraction\">Structured text extraction agent</a></li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/metropolis-nim-workflows/tree/main/nim_workflows/nvdinov2_few_shot\">Few-shot classification with NV-DINOv2 agent</a></li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/metropolis-nim-workflows/tree/main/nim_workflows/nvclip_multimodal_search\">Multimodal search with NV-CLIP agent</a></li>\n</ul>\n\n\n\n<h3 id=\"vlm_streaming_video_alerts_agent\"  class=\"wp-block-heading\">VLM streaming video alerts agent<a href=\"#vlm_streaming_video_alerts_agent\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>With vast amounts of video data generated every second, it\u2019s impossible to manually review footage for key events like package deliveries, forest fires, or unauthorized access.&nbsp;</p>\n\n\n\n<p>This workflow shows how to use VLMs, Python, and OpenCV to build an AI agent that <a href=\"https://github.com/NVIDIA/metropolis-nim-workflows/blob/main/nim_workflows/vlm_alerts/README.md\">autonomously monitors live streams for user-defined events</a>. When an event is detected, an alert is generated, saving countless hours of manual video review. Thanks to the flexibility of VLMs, new events can be detected by changing the prompt\u2014no need for custom CV models to be built and trained for each new scenario..</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/spzj4JxLcs8?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Visual AI Agent Powered by NVIDIA NIM</em></figcaption></figure>\n\n\n\n<p>In Figure 2, the VLM runs in the cloud while the video streaming pipeline operates locally. This setup enables the demo to run on almost any hardware, with the heavy computation offloaded to the cloud through NIM microservices.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/streaming-video-alert-agent-architecture-1.png\"><img loading=\"lazy\" decoding=\"async\" width=\"956\" height=\"851\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/streaming-video-alert-agent-architecture-1.png\" alt=\"An architecture diagram shows the input of a video stream to frame decode and subsampling step, while a user alert creates a request for the VLM NIM microservice. The response is parsed and goes to overlay generation and a WebSocket server for the alert notification.\" class=\"wp-image-91314\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/streaming-video-alert-agent-architecture-1.png 956w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/streaming-video-alert-agent-architecture-1-300x267.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/streaming-video-alert-agent-architecture-1-625x556.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/streaming-video-alert-agent-architecture-1-129x115.png 129w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/streaming-video-alert-agent-architecture-1-768x684.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/streaming-video-alert-agent-architecture-1-645x574.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/streaming-video-alert-agent-architecture-1-337x300.png 337w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/streaming-video-alert-agent-architecture-1-101x90.png 101w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/streaming-video-alert-agent-architecture-1-362x322.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/streaming-video-alert-agent-architecture-1-124x110.png 124w\" sizes=\"(max-width: 956px) 100vw, 956px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 2. Streaming video alert agent architecture</em></figcaption></figure></div>\n\n\n<p>Here are the steps for building this agent:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li><strong>Load and process the video stream</strong>: Use OpenCV to load a video stream or file, decode it, and subsample frames.</li>\n\n\n\n<li><strong>Create REST API endpoints:</strong> Use FastAPI to create control REST API endpoints where users can input custom prompts.</li>\n\n\n\n<li><strong>Integrate with the VLM API:</strong> A wrapper class handles interactions with the VLM API by sending video frames and user prompts. It forms the NIM API requests and parses the response.&nbsp;</li>\n\n\n\n<li><strong>Overlay responses on video:</strong> The VLM response is overlaid onto the input video, streamed out using OpenCV for real-time viewing.&nbsp;</li>\n\n\n\n<li><strong>Trigger alerts:</strong> Send the parsed response over a WebSocket server to integrate with other services, triggering notifications based on detected events.&nbsp;</li>\n</ol>\n\n\n\n<p>For more information about building a VLM-powered streaming video alert agent, see the <a href=\"https://github.com/NVIDIA/metropolis-nim-workflows/blob/main/nim_workflows/vlm_alerts/README.md\">/NVIDIA/metropolis-nim-workflows</a> notebook tutorial and demo on GitHub. You can experiment with different VLM NIM microservices to find the best model for your use case.&nbsp;</p>\n\n\n\n<p>For more information about how VLMs can transform edge applications with <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/\">NVIDIA Jetson</a> and <a href=\"https://developer.nvidia.com/embedded-computing\">Jetson Platform Services</a>, see <a href=\"https://developer.nvidia.com/blog/develop-generative-ai-powered-visual-ai-agents-for-the-edge/\">Develop Generative AI-Powered Visual AI Agents for the Edge</a> and explore additional resources on the <a href=\"https://developer.nvidia.com/embedded/jetpack/jetson-platform-services-get-started\">Jetson Platform Services</a> page.</p>\n\n\n\n<h3 id=\"structured_text_extraction_agent\"  class=\"wp-block-heading\">Structured text extraction agent<a href=\"#structured_text_extraction_agent\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Many business documents are stored as images rather than searchable formats like PDFs. This presents a significant challenge when it comes to searching and processing these documents, often requiring manual review, tagging, and organizing.&nbsp;</p>\n\n\n\n<p>While optical character detection and recognition (OCDR) models have been around for a while, they often return cluttered results that fail to retain the original formatting or interpret its visual data. This becomes especially challenging when working with documents in irregular formats, such as photo IDs, which come in various shapes and sizes.&nbsp;</p>\n\n\n\n<p>Traditional CV models make processing such documents time-consuming and costly. However, by combining the flexibility of VLMs and LLMs with the precision of OCDR models, you can build a <a href=\"https://github.com/NVIDIA/metropolis-nim-workflows/tree/main/nim_workflows/vision_text_extraction\">powerful text-extraction pipeline to autonomously parse documents</a> and store user-defined fields in a database.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-agent-architecture-1.png\"><img loading=\"lazy\" decoding=\"async\" width=\"951\" height=\"830\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-agent-architecture-1.png\" alt=\"An architecture diagram shows form requests and responses to and from the OCDR, VLM, and LLM NIM microservices, plus the steps of combining OCD metadata with the prompt, the LLM formatting prompt, and the parsing of the formatted response.\" class=\"wp-image-91315\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-agent-architecture-1.png 951w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-agent-architecture-1-300x262.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-agent-architecture-1-625x545.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-agent-architecture-1-132x115.png 132w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-agent-architecture-1-768x670.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-agent-architecture-1-645x563.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-agent-architecture-1-344x300.png 344w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-agent-architecture-1-103x90.png 103w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-agent-architecture-1-362x316.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-agent-architecture-1-126x110.png 126w\" sizes=\"(max-width: 951px) 100vw, 951px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 3. Structured text extraction agent architecture</em></figcaption></figure></div>\n\n\n<p>Here are the structured text-extraction pipeline building steps:&nbsp;</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li><strong>Document input:</strong> Provide an image of the document to an OCDR model, such as OCDRNet or Florence, which returns metadata for all the detected characters in the document.&nbsp;</li>\n\n\n\n<li><strong>VLM integration: </strong>The VLM processes the user\u2019s prompt specifying the desired fields and analyzes the document. It uses the detected characters from the OCDR model to generate a more accurate response.&nbsp;</li>\n\n\n\n<li><strong>LLM formatting:</strong> The response of the VLM is passed to an LLM, which formats the data into JSON, presenting it as a table.&nbsp;</li>\n\n\n\n<li><strong>Output and storage:</strong> The extracted fields are now in a structured format, ready to be inserted into a database or stored for future use.&nbsp;</li>\n</ol>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id.png\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"768\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id-1024x768.png\" alt=\"Screenshots of the microservice extracting text from a photo ID.\u00a0The screenshots include specifying the model options for VLM, OCDR, and LLM, plus the user-defined fields and the structured output, filled out with the information from the actual ID.\" class=\"wp-image-91049\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id-1024x768.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id-300x225.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id-625x469.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id-153x115.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id-768x576.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id-1536x1152.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id-645x484.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id-400x300.png 400w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id-120x90.png 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id-362x271.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id-147x110.png 147w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/structured-text-extraction-example-photo-id.png 1667w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 4. Structured text extraction example with vision NIM microservices</em></figcaption></figure></div>\n\n\n<p>The preview APIs make it easy to experiment by combining multiple models to build complex pipelines. From the demo UI, you can switch between different VLMs, OCDR, and LLM models available on <a href=\"http://build.nvidia.com\">build.nvidia.com</a> for quick experimentation.&nbsp;</p>\n\n\n\n<h3 id=\"few-shot_classification_with_nv-dinov2&nbsp;\"  class=\"wp-block-heading\">Few-shot classification with NV-DINOv2&nbsp;<a href=\"#few-shot_classification_with_nv-dinov2&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>NV-DINOv2 generates embeddings from high-resolution images, making it ideal for tasks requiring detailed analysis, such as defect detection with only a few sample images. This workflow demonstrates how to build a <a href=\"https://github.com/NVIDIA/metropolis-nim-workflows/tree/main/workflows/nvdinov2_few_shot\">scalable few-shot classification pipeline</a> using NV-DINOv2 and a Milvus vector database.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"531\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1-1024x531.png\" alt=\"An architecture diagram shows how to embed and store few-shot examples and how to inference new images.\" class=\"wp-image-91316\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1-1024x531.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1-300x156.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1-625x324.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1-179x93.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1-768x398.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1-1536x797.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1-645x335.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1-500x259.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1-160x83.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1-362x188.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1-212x110.png 212w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/few-shot-classification-nv-dinov2-1.png 1822w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Few-shot classification with NV-DINOv2</em></figcaption></figure></div>\n\n\n<p>Here is how the few-shot classification pipeline works:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li><strong>Define classes and upload samples:</strong> Users define classes and upload a few sample images for each. NV-DINOv2 generates embeddings from these images, which are then stored in a Milvus vector database along with the class labels.&nbsp;</li>\n\n\n\n<li><strong>Predict new classes: </strong>When a new image is uploaded, NV-DINOv2 generates its embedding, which is compared with the stored embeddings in the vector database. The closest neighbors are identified using the k-nearest neighbors (k-NN) algorithm, and the majority class among them is predicted.\u00a0</li>\n</ol>\n\n\n\n<h3 id=\"multimodal_search_with_nv-clip\"  class=\"wp-block-heading\">Multimodal search with NV-CLIP<a href=\"#multimodal_search_with_nv-clip\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>NV-CLIP offers a unique advantage: the ability to embed both text and images, enabling <a href=\"https://github.com/NVIDIA/metropolis-nim-workflows/tree/main/nim_workflows/nvclip_multimodal_search\">multimodal search</a>. By converting text and image inputs into embeddings within the same vector space, NV-CLIP facilitates the retrieval of images that match a given text query. This enables highly flexible and accurate search results.\u00a0</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"749\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nv-clip-multimodal-search-1024x749.gif\" alt=\"GIF shows a search for school bus images using natural language prompts with NV-CLIP.\u00a0\" class=\"wp-image-91051\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nv-clip-multimodal-search-1024x749.gif 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nv-clip-multimodal-search-300x220.gif 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nv-clip-multimodal-search-625x457.gif 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nv-clip-multimodal-search-157x115.gif 157w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nv-clip-multimodal-search-768x562.gif 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nv-clip-multimodal-search-645x472.gif 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nv-clip-multimodal-search-410x300.gif 410w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nv-clip-multimodal-search-123x90.gif 123w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nv-clip-multimodal-search-362x265.gif 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nv-clip-multimodal-search-150x110.gif 150w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Multimodal search (image and text) with NV-CLIP</em></figcaption></figure></div>\n\n\n<p>In this workflow, users upload a folder of images, which are embedded and stored in a vector database. Using the UI, they can type a query, and NV-CLIP retrieves the most similar images based on the input text.&nbsp;</p>\n\n\n\n<p>More advanced agents can be built using this approach with VLMs to create multimodal RAG workflows, enabling visual AI agents to build on past experiences and improve responses.\u00a0</p>\n\n\n\n<h2 id=\"get_started_with_visual_ai_agents_today&nbsp;\"  class=\"wp-block-heading\">Get started with visual AI agents today&nbsp;<a href=\"#get_started_with_visual_ai_agents_today&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Ready to dive in and start building your own visual AI agents? Use the code provided in the <a href=\"https://github.com/NVIDIA/metropolis-nim-workflows\">/NVIDIA/metropolis-nim-workflows</a> GitHub repo as a foundation to develop your own custom workflows and AI solutions powered by NIM microservices. Let the example inspire new applications that solve your specific challenges.</p>\n\n\n\n<p>For any technical questions or support, join our community and engage with experts in the <a href=\"https://forums.developer.nvidia.com/c/accelerated-computing/intelligent-video-analytics/visual-ai-agent/680\">NVIDIA Visual AI Agent forum</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The exponential growth of visual data\u2014ranging from images to PDFs to streaming videos\u2014has made manual review and analysis virtually impossible. Organizations are struggling to transform this data into actionable insights at scale, leading to missed opportunities and increased risks. To solve this challenge, vision-language models (VLMs) are emerging as powerful tools, combining visual perception of &hellip; <a href=\"https://developer.nvidia.com/blog/build-multimodal-visual-ai-agents-powered-by-nvidia-nim/\">Continued</a></p>\n", "protected": false}, "author": 1925, "featured_media": 91320, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1512397", "discourse_permalink": "https://forums.developer.nvidia.com/t/build-multimodal-visual-ai-agents-powered-by-nvidia-nim/311850", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 3110], "tags": [3965, 1950, 354, 3953], "coauthors": [3616], "class_list": ["post-90989", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-generative-ai", "tag-ai-agent", "tag-image-recognition", "tag-image-segmentation", "tag-vlms"], "acf": {"post_industry": ["Manufacturing", "Retail / Consumer Packaged Goods", "Smart Cities / Spaces"], "post_products": ["AI Enterprise", "Metropolis", "NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/metropolis-and-iva-ngc-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nFz", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90989"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1925"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90989"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90989/revisions"}], "predecessor-version": [{"id": 91328, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90989/revisions/91328"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91320"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90989"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90989"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90989"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90989"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91133, "date": "2024-10-31T09:06:07", "date_gmt": "2024-10-31T16:06:07", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91133"}, "modified": "2024-10-31T09:23:44", "modified_gmt": "2024-10-31T16:23:44", "slug": "deep-learning-ai-model-identifies-breast-cancer-spread-without-surgery", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/deep-learning-ai-model-identifies-breast-cancer-spread-without-surgery/", "title": {"rendered": "Deep Learning AI Model Identifies Breast Cancer Spread without Surgery"}, "content": {"rendered": "\n<p>A new <a href=\"https://pubs.rsna.org/doi/10.1148/rycan.230107\">deep learning model</a> could reduce the need for surgery when diagnosing whether cancer cells are spreading, including to nearby lymph nodes\u2014also known as metastasis. Developed by researchers from the University of Texas Southwestern Medical Center, the AI tool analyzes time-series MRIs and clinical data to identify metastasis, providing crucial, noninvasive support for doctors in treatment planning. The advancement could lead to more timely and accurate cancer assessments, helping many patients avoid unnecessary surgery and improve outcomes.&nbsp;</p>\n\n\n\n<p>Metastatic breast cancer is responsible for the majority of breast cancer-related deaths. About one in three women in the US diagnosed with early-stage breast cancer develops metastatic cancer. However, early detection and treatment can slow disease progression, help doctors and patients manage symptoms, and maximize the effectiveness of treatments.</p>\n\n\n\n<p>Doctors often rely on sentinel lymph node biopsies (SLNB) when checking whether cancer has spread to the lymph nodes. The procedure involves injecting dye and a radioactive solution near the cancer site to identify the sentinel nodes, which drain into the tumor area first. These nodes are then surgically removed and biopsied. If cancer cells are found in the sentinel nodes, it shows that the cancer is spreading to the lymphatic system and could spread further. This information helps doctors determine the most appropriate treatment for the patient.&nbsp;</p>\n\n\n\n<p>While SLNB is a proven method, it&#8217;s invasive and comes with risks related to anesthesia, radiation exposure, swelling, pain, and limited movement near the incision.&nbsp;</p>\n\n\n\n<p>To create a noninvasive and reliable alternative to SLNB, the researchers developed a custom four-dimensional convolutional neural network (4D CNN). They trained the model using dynamic contrast-enhanced MRI (DCE-MRI) along with clinical datasets from 350 women recently diagnosed with breast cancer that spread to lymph nodes.</p>\n\n\n\n<p>The researchers used the <a href=\"https://portal.biohpc.swmed.edu/content/about/systems/\">Nucleus Compute Cluster,</a> part of the University of Texas Southwestern Medical Center&#8217;s high-performance computing infrastructure, to build and train the complex 4D deep learning model employing <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 Tensor Core</a> and <a href=\"https://www.nvidia.com/en-us/data-center/v100/\">NVIDIA V100 Tensor Core GPUs</a> </p>\n\n\n\n<p>\u201cThe deep learning model we built was a complex 4D model and GPUs were essential for us to achieve high training throughput as well as for our data preprocessing pipeline for image enhancement and noise reduction,\u201d said NVIDIA Senior HPC Engineer Paniz Karbasi, a study coauthor and former Computational Scientist at the University of Texas Southwestern Medical Center.</p>\n\n\n\n<div class=\"wp-block-image aligncenter\">\n<figure class=\"size-full is-resized\"><img decoding=\"async\" class=\"wp-image-75722\" style=\"width: 602px;\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/MRI-breast-cancer.png\" alt=\"Four boxes showing how the model processes breast cancer MRIs .\" height=\"576\" />\n<figcaption class=\"wp-element-caption\"><em>Figure 1. A volumetric dynamic contrast-enhanced MR image involves tumor delineation, cropping to a cuboidal volume, and enhancing the tumor by calculating difference images between time points</em></figcaption>\n</figure>\n</div>\n\n\n\n<p>This AI model processes data in four dimensions, examining data from 3D MRI scans while accounting for changes over time. The model learns \u200cfeatures of tumors and nearby lymph nodes by analyzing multiple images over time and integrating clinical data such as age, tumor grade, and breast cancer markers. By doing so, it can accurately identify patterns associated with cancer-free or cancer-affected lymph nodes.</p>\n\n\n\n<p>\u201cThe most important aspect of our study is that for imaging data we solely focus on data related to the primary tumor, without any additional axillary imaging,\u201d said study lead author Dogan Polat, an Interventional Radiology Resident at Mount Sinai Health Systems. Dr. Polat led the study while at the University of Texas Southwestern Medical Center. \u201cWe aim to decrease the need for additional imaging and reduce the number of invasive procedures for patients,\u201d said Dr. Polat.&nbsp;</p>\n\n\n\n<p>It identifies lymph node metastasis with 89% accuracy, outperforming radiologists and other imaging-based models. It also has the potential to prevent breast cancer patients from undergoing unnecessary sentinel node biopsies, and axillary lymph node dissection (ALND), reducing the risks, complications, and resources associated with the procedure.&nbsp;</p>\n\n\n\n<p>According to Polat, the next steps for the researchers include deploying the model to gather real-world data, which will help validate its effectiveness and identify areas for further refinement and broader application.&nbsp;</p>\n\n\n\n<p>Read the study <a href=\"https://pubs.rsna.org/doi/10.1148/rycan.230107?_gl=1*he76ww*_gcl_au*MTU2MjI0MzI4Ni4xNzI5NTQyODAw*corpRollup_ga*NjI2MDQ1ODAuMTcyOTU0MjgwMA..*corpRollup_ga_EQ32SZ84M3*MTcyOTgwODk2My42LjEuMTcyOTgxMTYxOS41Ny4wLjA.\"><em>Machine Learning Prediction of Lymph Node Metastasis in Breast Cancer: Performance of a Multi-institutional MRI-based 4D Convolutional Neural Network</em></a><em>.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>A new deep learning model could reduce the need for surgery when diagnosing whether cancer cells are spreading, including to nearby lymph nodes\u2014also known as metastasis. Developed by researchers from the University of Texas Southwestern Medical Center, the AI tool analyzes time-series MRIs and clinical data to identify metastasis, providing crucial, noninvasive support for doctors &hellip; <a href=\"https://developer.nvidia.com/blog/deep-learning-ai-model-identifies-breast-cancer-spread-without-surgery/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 91144, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1512318", "discourse_permalink": "https://forums.developer.nvidia.com/t/deep-learning-ai-model-identifies-breast-cancer-spread-without-surgery/311833", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 696, 4146, 1903], "tags": [3941, 453, 90, 1877], "coauthors": [2315], "class_list": ["post-91133", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-data-science", "category-development", "category-features", "tag-ai-impact", "tag-featured", "tag-medical-imaging", "tag-research"], "acf": {"post_industry": ["Healthcare & Life Sciences"], "post_products": ["A100", "V100"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Breast-cancer-cells-e1730236106594.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nHT", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91133"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91133"}], "version-history": [{"count": 19, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91133/revisions"}], "predecessor-version": [{"id": 91618, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91133/revisions/91618"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91144"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91133"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91133"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91133"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91133"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91186, "date": "2024-10-30T12:57:15", "date_gmt": "2024-10-30T19:57:15", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91186"}, "modified": "2024-10-31T09:21:05", "modified_gmt": "2024-10-31T16:21:05", "slug": "teaching-robots-to-tackle-household-chores", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/teaching-robots-to-tackle-household-chores/", "title": {"rendered": "Teaching Robots to Tackle Household Chores"}, "content": {"rendered": "\n<p>Robotics could make everyday life easier by taking on repetitive or time-consuming tasks. At NVIDIA GTC 2024, researchers from Stanford University unveiled BEHAVIOR-1K, a major benchmark designed to train robots to perform 1,000 real-world-inspired activities\u2014such as folding laundry, cooking breakfast, and cleaning up after a party.\u00a0</p>\n\n\n\n<p>Using OmniGibson, a cutting-edge simulation environment for accelerating embodied AI research built on the <a href=\"https://www.nvidia.com/en-us/omniverse/\">NVIDIA Omniverse</a> platform, they focus on training robots in practical skills that can be directly applied to real-world settings\u2014from assisting in homes to workplaces and beyond.</p>\n\n\n\n<p>Part of a broader initiative to make robotics practical for everyday assistance, the BEHAVIOR-1K benchmark focuses on bringing advanced robotic capabilities closer to reality and freeing up time for people to engage in activities they enjoy.</p>\n\n\n\n<script src=\"https://api-prod.nvidia.com/search/nvidia-search-library.js\"></script>\n \n\n<div id=\"nvidia-event-details-widget\"></div>\n<style>\n.nvidia-search-widget .cleanslate , .nvidia-search-widget .player-overlay {\ndisplay:none;\n}\n</style>\n \n\n<script>\n \n NvidiaSearchLibrary.EventSessionDetailsWidget.mount({\n          site: 'https://www.nvidia.com',\n          language: 'en-us',\n          sessionId: 'gtc24-s62698',\n          jwtToken: '',\n \u2002\u2002\u2002\u2002voltronApiUrl:  'https://api-prod.nvidia.com/services/nod/api/v1/',\n          apiUrl: 'https://api-prod.nvidia.com/search/graphql',\n           onLogin: () => { },\n          onLogout: () => { },\n       \n          onSeeAllSessions: (speakerName) => {\n            window.location.href =  'https://www.nvidia.com/en-us/on-demand/search/?q=\"' + speakerName+'\"';\n          },\n          searchApiUrl: 'https://api-prod.nvidia.com/search/graphql',\n          searchToken: '',\n          uiConfId: '50468382',\n          showSessionRating: false,\n          anonToken: '',\n        });\n \n</script>\n\n\n\n<p>Follow along with a <a href=\"https://developer.download.nvidia.com/devblogs/BEHAVIOR-1K-GTC-2024.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">PDF of the session</a>, which provides a detailed look at how BEHAVIOR-1K leverages insights from surveys involving over 1,400 participants to define meaningful everyday activities.\u00a0</p>\n\n\n\n<p>The session covers how the benchmark enables robots to learn tasks that people want help with, optimizing for performance and for practical impact on daily living. Key highlights include:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Activity definitions and OmniGibson simulation</strong>: Insights into how BEHAVIOR-1K combines human-defined tasks with realistic simulation environments to ensure robots learn in everyday life settings.</li>\n\n\n\n<li><strong>Scaling robotics training</strong>: Techniques for large-scale training across 50 fully interactive environments, incorporating over 1,200 object categories and 5,000+ 3D models to provide robots with diverse and realistic experiences.</li>\n\n\n\n<li><strong>Improving realism in AI training</strong>: Incorporating various object states, complex interactions, and realistic physical properties, making sure that robots are ready for real-world applications.</li>\n\n\n\n<li><strong>Survey insights and human-centered task design</strong>: Understanding what people want robots to help with through participant data, ensuring that BEHAVIOR-1K remains aligned with human needs.</li>\n</ul>\n\n\n\n<p>Watch the session <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s62698/about:blank\" target=\"_blank\" rel=\"noreferrer noopener\">Tired of Household Chores? Teach Robots to Perform 1,000 Everyday Activities With BEHAVIOR</a>, explore more videos on NVIDIA On-Demand, and gain valuable skills and insights from industry experts by joining the <a href=\"https://developer.nvidia.com/developer-program\">NVIDIA Developer Program</a>.</p>\n\n\n\n<p><em>This content was partially crafted with the assistance of generative AI and LLMs. It underwent careful review and was edited by the NVIDIA Technical Blog team to ensure precision, accuracy, and quality.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Robotics could make everyday life easier by taking on repetitive or time-consuming tasks. At NVIDIA GTC 2024, researchers from Stanford University unveiled BEHAVIOR-1K, a major benchmark designed to train robots to perform 1,000 real-world-inspired activities\u2014such as folding laundry, cooking breakfast, and cleaning up after a party.\u00a0 Using OmniGibson, a cutting-edge simulation environment for accelerating embodied &hellip; <a href=\"https://developer.nvidia.com/blog/teaching-robots-to-tackle-household-chores/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 91211, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1511945", "discourse_permalink": "https://forums.developer.nvidia.com/t/teaching-robots-to-tackle-household-chores/311744", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 63, 503, 1903], "tags": [453, 3986], "coauthors": [2315], "class_list": ["post-91186", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "category-robotics", "category-simulation-modeling-design", "category-features", "tag-featured", "tag-nvidia-on-demand"], "acf": {"post_industry": ["General", "Academia / Education"], "post_products": ["Omniverse"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Video"], "post_collections": ["GTC March 2024"]}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/robots-chores-BEHAVIOR1K-e1730318187631.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nIK", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Robotics", "link": "https://developer.nvidia.com/blog/category/robotics/", "id": 63}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91186"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91186"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91186/revisions"}], "predecessor-version": [{"id": 91215, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91186/revisions/91215"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91211"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91186"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91186"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91186"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91186"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90642, "date": "2024-10-30T10:54:45", "date_gmt": "2024-10-30T17:54:45", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90642"}, "modified": "2024-11-06T19:39:34", "modified_gmt": "2024-11-07T03:39:34", "slug": "high-throughput-ai-driven-drug-discovery-pipeline", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/high-throughput-ai-driven-drug-discovery-pipeline/", "title": {"rendered": "High Throughput AI-Driven Drug Discovery Pipeline"}, "content": {"rendered": "\n<p>The integration of AI in drug discovery is revolutionizing the way researchers approach the development of new treatments for various diseases. Traditional methods are often time-consuming and costly, with the process of bringing a new drug to market taking up to 15 years and costing between $1\u20132B.&nbsp;</p>\n\n\n\n<p>By using AI and advanced computational tools, researchers can now accelerate the identification of new drugs, significantly reducing both the time and cost involved in the drug discovery process.&nbsp;</p>\n\n\n\n<h2 id=\"challenges_of_traditional_drug_discovery\"  class=\"wp-block-heading\">Challenges of traditional drug discovery<a href=\"#challenges_of_traditional_drug_discovery\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In conventional drug discovery workflows, researchers first identify a biological target, such as a protein involved in disease progression, and then search for molecules that can modulate this target. The complexity of biological systems, combined with the vast number of potential chemical structures, estimated at around 10<sup>60</sup>, makes this a daunting task.&nbsp;</p>\n\n\n\n<p>Traditional computer-aided drug discovery (CADD) methods often rely on simplified models and assumptions that fail to capture the intricacies of drug-target interactions, leading to high attrition rates in clinical trials.</p>\n\n\n\n<h2 id=\"an_ai-driven_approach_to_virtual_screening\"  class=\"wp-block-heading\">An AI-driven approach to virtual screening<a href=\"#an_ai-driven_approach_to_virtual_screening\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.innoplexus.com/\">Innoplexus</a> is a registered NVIDIA Inception startup.<strong> </strong>Their proprietary deep learning method uses <a href=\"https://www.nvidia.com/en-us/ai/#referrer=ai-subdomain\">NVIDIA NIM microservices</a> to streamline the drug discovery process. They also use NVIDIA H100 GPU clusters featuring the following components:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Accelerator:</strong> NVIDIA H100 Tensor Core GPU</li>\n\n\n\n<li><strong>Memory:</strong> 80-GB HBM3 (High-Bandwidth Memory)</li>\n\n\n\n<li><strong>Interconnect: </strong>NVIDIA NVLink 4.0</li>\n\n\n\n<li><strong>Cluster configuration:</strong> Scalable, multi-node clusters with high-speed interconnects for distributed training and inference</li>\n</ul>\n\n\n\n<p>This approach is informed by the NVIDIA NIM Agent Blueprint for generative virtual screening, which enables the rapid, AI-driven generation of novel molecular structures for accelerated molecular simulations and docking with NIM microservices.&nbsp;</p>\n\n\n\n<p>Combining Innoplexus&#8217; expertise with NVIDIA&#8217;s cutting-edge AI technology fundamentally transforms how innovative treatments are discovered and brought to market\u2014making this process faster, more efficiently, and more precise.</p>\n\n\n\n<p>To address the urgent need for novel therapies for neurodegenerative diseases associated with TDP-43 aggregation, Innoplexus developed an AI-driven drug discovery pipeline.&nbsp;</p>\n\n\n\n<h2 id=\"innoplexus\u2019s_deep_learning_method\"  class=\"wp-block-heading\">Innoplexus\u2019s deep learning method<a href=\"#innoplexus\u2019s_deep_learning_method\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Innoplexus\u2019 method employs custom-designed artificial neural networks (ANNs) for protein target prediction, trained on large-scale datasets of protein sequences, structural information, and molecular interactions.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"661\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1-1024x661.png\" alt=\"Diagram shows the AG workflow pipeline from structure-based and ligand-based drug discovery, to using NIM microservices for protein prediction.\" class=\"wp-image-91065\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1-1024x661.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1-300x194.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1-625x404.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1-768x496.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1-1536x992.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1-645x416.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1-465x300.png 465w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1-139x90.png 139w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1-362x234.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1-170x110.png 170w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-structure-ligand-based-drug-discovery-1.png 1744w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure></div>\n\n\n<p class=\"has-text-align-center\"><em>Figure 1.</em> <em>The workflow for structure and ligand-based drug discovery using NVIDIA NIM microservices</em></p>\n\n\n\n<p>Innoplexus uses the following NVIDIA NIM microservices:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://build.nvidia.com/deepmind/alphafold2\">AlphaFold2</a> for protein structure prediction</li>\n\n\n\n<li><a href=\"https://build.nvidia.com/nvidia/molmim-generate\">MolMIM</a> for optimized lead generation</li>\n\n\n\n<li><a href=\"https://build.nvidia.com/mit/diffdock\">DiffDock</a> for molecular docking</li>\n</ul>\n\n\n\n<p>By combining these advanced AI tools, Innoplexus aims to streamline the drug discovery process and identify promising candidates that can effectively target TDP-43 and mitigate the progression of these debilitating diseases. This innovative approach has the potential to accelerate the development of new treatments and improve the lives of patients affected by neurodegenerative conditions.</p>\n\n\n\n<h3 id=\"alphafold2_for_protein_structure_prediction\"  class=\"wp-block-heading\">AlphaFold2 for protein structure prediction<a href=\"#alphafold2_for_protein_structure_prediction\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>A protein sequence provided by the user is processed through the <a href=\"https://build.nvidia.com/deepmind/alphafold2\">AlphaFold2</a> NIM microservice, which accurately determines the 3D structure of the target protein. This step involves aligning the sequence with known proteins, offering multiple alignment configurations for improved accuracy.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"440\" height=\"394\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-structure-protein.png\" alt=\"Image shows the 3D structure of a target protein.\" class=\"wp-image-90981\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-structure-protein.png 440w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-structure-protein-300x269.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-structure-protein-128x115.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-structure-protein-335x300.png 335w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-structure-protein-101x90.png 101w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-structure-protein-362x324.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-structure-protein-123x110.png 123w\" sizes=\"(max-width: 440px) 100vw, 440px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. AlphaFold2 predicts the 3D structure of the protein from its amino acid sequence</em></figcaption></figure></div>\n\n\n<h3 id=\"molmim_for_optimized_lead_generation\"  class=\"wp-block-heading\">MolMIM for optimized lead generation<a href=\"#molmim_for_optimized_lead_generation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>An initial chemical structure is passed through the <a href=\"https://build.nvidia.com/nvidia/molmim-generate\">MolMIM</a> NIM microservice, which generates new molecular structures optimized for specific properties such as drug-likeness (QED), solubility (penalized log P), and molecular similarity.\u00a0</p>\n\n\n\n<p>The generated molecules are iteratively optimized in multiple cycles, depending on your requirements.</p>\n\n\n\n<h3 id=\"diffdock_for_molecular_docking\"  class=\"wp-block-heading\">DiffDock for molecular docking<a href=\"#diffdock_for_molecular_docking\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Molecular docking helps in determining the optimum site on the target protein where the drug binds. The optimized molecules and the target protein structure are processed by <a href=\"https://build.nvidia.com/mit/diffdock\">DiffDock</a>, which predicts the binding poses of the molecules to the protein.&nbsp;</p>\n\n\n\n<p>You can define the number of poses and other docking constraints, enabling a comprehensive analysis of potential drug-target interactions.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"570\" height=\"369\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-molecule-protein-interaction.png\" alt=\"Image shows 3D structure of a molecule interacting with a protein.\" class=\"wp-image-90982\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-molecule-protein-interaction.png 570w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-molecule-protein-interaction-300x194.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-molecule-protein-interaction-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-molecule-protein-interaction-463x300.png 463w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-molecule-protein-interaction-139x90.png 139w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-molecule-protein-interaction-362x234.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3d-molecule-protein-interaction-170x110.png 170w\" sizes=\"(max-width: 570px) 100vw, 570px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Diffdock predicts the 3D structure of how the molecule interacts with the protein</em></figcaption></figure></div>\n\n\n<h2 id=\"post-processing_admet_pipeline\"  class=\"wp-block-heading\">Post-processing ADMET pipeline<a href=\"#post-processing_admet_pipeline\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>After DiffDock, the top 1K small molecules are further screened using the proprietary ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) pipeline, which assesses the pharmacokinetic and pharmacodynamic properties of the molecules.&nbsp;</p>\n\n\n\n<p>This pipeline includes the following components:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>ADMET Prediction:</strong> The proprietary model predicts the ADMET properties of the molecules, including solubility, permeability, metabolism, and toxicity.</li>\n\n\n\n<li><strong>Filtering and Ranking:</strong> Molecules are filtered and ranked based on their predicted ADMET properties, ensuring that only the most promising candidates are selected for further development.</li>\n</ul>\n\n\n\n<h2 id=\"innoplexus_admet_model\"  class=\"wp-block-heading\">Innoplexus ADMET model<a href=\"#innoplexus_admet_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The ADMET model is a custom-designed neural network that uses a large dataset of molecular structures and their corresponding ADMET properties.&nbsp;</p>\n\n\n\n<p>The model is trained using advanced techniques:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Multi-task learning:</strong> The model is trained on multiple ADMET tasks simultaneously, improving its overall performance and accuracy.</li>\n\n\n\n<li><strong>Transfer learning:</strong> The model is fine-tuned on a large dataset of molecular structures, enabling it to generalize well to new, unseen molecules.</li>\n</ul>\n\n\n\n<h2 id=\"workflow_optimization\"  class=\"wp-block-heading\">Workflow optimization<a href=\"#workflow_optimization\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The pipeline is optimized for performance:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Data parallelism:</strong> Distributed training and inference across multiple GPUs and nodes.</li>\n\n\n\n<li><strong>Model parallelism:</strong> Splitting large models across multiple GPUs and nodes.</li>\n\n\n\n<li><strong>Pipeline parallelism:</strong> Overlapping computation and communication between pipeline stages.</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"786\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1-1024x786.png\" alt=\"Bar chart shows the time taken in hours for GPUs on measures such as ADMET profiling for 10K molecules; molecular docking of generated molecules; and optimizing molecule generation towards drug-like properties.\" class=\"wp-image-91067\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1-1024x786.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1-300x230.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1-625x480.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1-150x115.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1-768x590.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1-1536x1179.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1-645x495.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1-391x300.png 391w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1-117x90.png 117w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1-362x278.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1-143x110.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-stats-1.png 1594w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. GPU-accelerated computing enables fast, efficient compute-intensive operations</em></figcaption></figure></div>\n\n\n<p>The application of GPUs and the approaches for accelerated computing facilitated in performing the compute-intensive operations of this solution fast, and making it feasible to complete within practical timelines.</p>\n\n\n\n<h2 id=\"real-world_applications_and_implications\"  class=\"wp-block-heading\">Real-world applications and implications<a href=\"#real-world_applications_and_implications\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Rapid compound identification with Innoplexus\u2019AI-driven pipeline powered with NVIDIA H100 clusters, accelerates virtual screening of generated molecules in addition to molecular docking up to 10x, enabling researchers to perform the following tasks:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Screen 5.8M small molecules in 5\u20138 hours duration.</li>\n\n\n\n<li>Identify the top 1% of compounds with high therapeutic potential from ADMET profiling in a few hours for a million compounds.&nbsp;</li>\n\n\n\n<li>Optimize lead compounds with 90% accuracy.</li>\n</ul>\n\n\n\n<p>By harnessing the power of AI and high-performance computing, you can rapidly explore vast chemical spaces and pinpoint promising candidates for therapeutic development, significantly accelerating the drug discovery process.</p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>AI and high-performance computing are set to transform the field of drug discovery, enabling faster, more accurate identification of potential drug candidates.&nbsp;</p>\n\n\n\n<p>By combining cutting-edge neural network algorithms, generative models, and advanced molecular docking techniques, the Innoplexus virtual screening pipeline offers a powerful tool for accelerating the discovery of new drugs, ultimately improving patient outcomes and reducing the cost and time associated with bringing new therapies to market.</p>\n\n\n\n<p><a href=\"https://build.nvidia.com/nvidia/generative-virtual-screening-for-drug-discovery/blueprintcard\">Get started</a> with the NVIDIA NIM Agent Blueprint for generative virtual screening and learn more about <a href=\"https://www.innoplexus.com/\">Innoplexus</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The integration of AI in drug discovery is revolutionizing the way researchers approach the development of new treatments for various diseases. Traditional methods are often time-consuming and costly, with the process of bringing a new drug to market taking up to 15 years and costing between $1\u20132B.&nbsp; By using AI and advanced computational tools, researchers &hellip; <a href=\"https://developer.nvidia.com/blog/high-throughput-ai-driven-drug-discovery-pipeline/\">Continued</a></p>\n", "protected": false}, "author": 2410, "featured_media": 90644, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1511908", "discourse_permalink": "https://forums.developer.nvidia.com/t/high-throughput-ai-driven-drug-discovery-pipeline/311739", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [2385, 453], "coauthors": [4160, 4161, 4162, 4163, 3527], "class_list": ["post-90642", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-drug-discovery", "tag-featured"], "acf": {"post_industry": ["Healthcare & Life Sciences"], "post_products": ["NIM"], "post_learning_levels": ["General Interest"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/innoplexus-admet-pipeline-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nzY", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90642"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2410"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90642"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90642/revisions"}], "predecessor-version": [{"id": 91068, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90642/revisions/91068"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90644"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90642"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90642"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90642"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90642"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91056, "date": "2024-10-29T15:01:56", "date_gmt": "2024-10-29T22:01:56", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91056"}, "modified": "2024-10-31T12:07:37", "modified_gmt": "2024-10-31T19:07:37", "slug": "protect-your-network-with-secure-boot-in-sonic", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/protect-your-network-with-secure-boot-in-sonic/", "title": {"rendered": "Protect Your Network with Secure Boot in SONiC"}, "content": {"rendered": "\n<p>NVIDIA technology helps organizations build and maintain secure, scalable, and high-performance network infrastructure. Advances in AI, with NVIDIA at the forefront, contribute every day to security advances. One way NVIDIA has taken a more direct approach to network security is through a secure network operating system (NOS).</p>\n\n\n\n<p>A secure network operating system (NOS) is a specialized type of NOS focused on robust security features to protect network infrastructure from a wide range of threats.&nbsp;</p>\n\n\n\n<p>Different systems offer various security features. Some provide built-in firewalls, VPNs, or monitoring tools. Some offer advanced threat detection and response features. Some offer hardened security at the boot level, preventing attacks before the operating system even loads. One of these features is called<strong> </strong>Secure Boot.</p>\n\n\n\n<h2 id=\"secure_boot\"  class=\"wp-block-heading\">Secure Boot<a href=\"#secure_boot\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA is increasingly supporting the security standard Secure Boot in more platforms. Secure Boot is a security UEFI (Unified Extensible Firmware Interface) feature that aims to protect against unauthorized firmware or software from running during the boot process and during firmware updates. <a href=\"https://www.nvidia.com/en-us/networking/spectrumx/\">NVIDIA Spectrum-4</a> switches and <a href=\"https://www.nvidia.com/en-us/networking/products/data-processing-unit/\">NVIDIA BlueField-2</a> DPUs and up now fully support UEFI Secure Boot.</p>\n\n\n\n<p>Unsigned or improperly signed code is prevented from executing at the boot level, preventing rootkits, bootkits, firmware attacks, and other malicious activity being loaded before the OS or security mechanisms are initialized, where an attacker could potentially gain full control of the core system. Gaining such a level of access allows an attacker to do almost anything.&nbsp;</p>\n\n\n\n<p>Secure Boot also significantly raises the barrier for attackers attempting to exploit physical access to devices. Even if an attacker can physically access the device, they cannot alter the boot components without the proper keys, protecting against tangible modifications such as replacing CPUs or hard drives.</p>\n\n\n\n<p>Secure Boot works by establishing a \u201cchain of trust\u201d starting from the hardware level and extending through the firmware and bootloader. Each component in the boot process verifies the next, and must be signed and checked before execution. If the signatures are valid and match known trusted keys, the system proceeds with the boot process. Otherwise, all unsigned code will be rejected by firmware and the system either halts or provides a warning. This includes an attacker attempting to install their own operating system outright.</p>\n\n\n\n<h2 id=\"secure_boot_in_the_sonic_network_operating_system\"  class=\"wp-block-heading\">Secure Boot in the SONiC network operating system<a href=\"#secure_boot_in_the_sonic_network_operating_system\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Secure Boot is supported within <a href=\"https://sonicfoundation.dev/\">SONiC</a> (Software for Open Networking in the Cloud), the Linux-based, open-source, hardware-agnostic network operating system. NVIDIA is the second-largest contributor to the SONiC project, behind only Microsoft, and supports SONiC in many ways. Learn more about <a href=\"https://www.nvidia.com/en-us/networking/ethernet-switching/sonic/\">NVIDIA and SONiC</a>. </p>\n\n\n\n<p>The big advantage of SONiC Secure Boot functionality over other systems is autonomy. Being open-source, SONiC enables customizable boot processes, unlike many traditional or proprietary systems, where you are only able to modify so much if at all.&nbsp;</p>\n\n\n\n<p>Running SONiC is not dependent on any vendors as signing entities. You\u2019re free to sign your image with your own private keys, so you know only the firmware you explicitly authorize can be installed. This also adds an extra layer against vendor lock-in. You can design your distribution to only run with certain vendors or boxes, applying one more knowledge barrier for an attacker to cross, as many boxes often require proprietary or special knowledge to access and use.&nbsp;</p>\n\n\n\n<p>Figure 1 shows the high-level architecture flow design for Secure Boot in SONiC. The production sign process works slightly differently from development, in which components are signed in an external signing server rather than within its own. An external signing server provides an isolated environment for extra security, scalability in large environments and controlled updates and management. At runtime, boot components are verified throughout the process.&nbsp;&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1159\" height=\"881\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/build-signing-process-sonic.jpg\" alt=\"This diagram shows the high-level development and production signing flow during the build process, and runtime flow when the system is booted. \n\" class=\"wp-image-91061\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/build-signing-process-sonic.jpg 1159w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/build-signing-process-sonic-300x228.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/build-signing-process-sonic-625x475.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/build-signing-process-sonic-151x115.jpg 151w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/build-signing-process-sonic-768x584.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/build-signing-process-sonic-645x490.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/build-signing-process-sonic-395x300.jpg 395w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/build-signing-process-sonic-118x90.jpg 118w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/build-signing-process-sonic-362x275.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/build-signing-process-sonic-145x110.jpg 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/build-signing-process-sonic-1024x778.jpg 1024w\" sizes=\"(max-width: 1159px) 100vw, 1159px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Flow of the SONiC build signing process</em></figcaption></figure>\n\n\n\n<p><a href=\"https://github.com/sonic-net/SONiC/blob/master/doc/secure_boot/hld_secure_boot.md\">Read more about how Secure Boot works in SONiC</a> and how to implement it.</p>\n\n\n\n<h2 id=\"get_started_securing_your_boxes\"  class=\"wp-block-heading\">Get started securing your boxes<a href=\"#get_started_securing_your_boxes\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA strongly recommends using UEFI secure boot in any case due the increased security it enables. Reach out to your NVIDIA sales representative or <a href=\"https://www.nvidia.com/en-us/networking/support/\">NVIDIA Networking Support</a> for more information about how to implement Secure Boot.&nbsp;</p>\n\n\n\n<p>To learn more, check out the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://www.nvidia.com/en-us/on-demand/session/other2024-sonic/\">NVIDIA Pure SONiC Virtual Workshop</a></li>\n\n\n\n<li><a href=\"https://nvdam.widen.net/s/rcnnmcp8xg/networking-sonic-workshop-lab-guide-oct-2024\">NVIDIA SONiC Workshop Lab Walkthrough Guide</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/building-pure-sonic-image/\">Building your own SONiC Image</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/exploring-sonic-on-nvidia-air/\">Try SONiC on NVIDIA Air</a></li>\n\n\n\n<li><a href=\"https://github.com/sonic-net/SONiC/blob/master/doc/secure_boot/hld_secure_boot.md\">Secure Boot in SONiC Guide</a></li>\n\n\n\n<li><a href=\"https://github.com/sonic-net/sonic-buildimage/blob/master/rules/config\">SONiC Build Configuration Options</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/networking/display/bluefielddpubspv422/uefi+secure+boot\">Secure Boot on NVIDIA BlueField DPUs</a></li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA technology helps organizations build and maintain secure, scalable, and high-performance network infrastructure. Advances in AI, with NVIDIA at the forefront, contribute every day to security advances. One way NVIDIA has taken a more direct approach to network security is through a secure network operating system (NOS). A secure network operating system (NOS) is a &hellip; <a href=\"https://developer.nvidia.com/blog/protect-your-network-with-secure-boot-in-sonic/\">Continued</a></p>\n", "protected": false}, "author": 2162, "featured_media": 91058, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1511166", "discourse_permalink": "https://forums.developer.nvidia.com/t/protect-your-network-with-secure-boot-in-sonic/311616", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1205], "tags": [1466, 1634, 453, 3566, 1641], "coauthors": [3885], "class_list": ["post-91056", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-networking-communications", "tag-bluefield", "tag-ethernet", "tag-featured", "tag-network-security", "tag-sonic"], "acf": {"post_industry": ["Hardware / Semiconductor"], "post_products": ["General"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Best practice"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/data-center-sonic-logo.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nGE", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Networking / Communications", "link": "https://developer.nvidia.com/blog/category/networking-communications/", "id": 1205}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91056"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2162"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91056"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91056/revisions"}], "predecessor-version": [{"id": 91326, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91056/revisions/91326"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91058"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91056"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91056"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91056"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91056"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91077, "date": "2024-10-29T10:56:55", "date_gmt": "2024-10-29T17:56:55", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91077"}, "modified": "2024-10-31T09:21:07", "modified_gmt": "2024-10-31T16:21:07", "slug": "ai-powered-devices-track-howls-to-save-wolves", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/ai-powered-devices-track-howls-to-save-wolves/", "title": {"rendered": "AI-Powered Devices Track Howls to Save Wolves"}, "content": {"rendered": "\n<p>A new cell-phone-sized device\u2014which can be deployed in vast, remote areas\u2014is using AI to identify and geolocate wildlife to help conservationists track endangered species, including wolves around Yellowstone National Park.&nbsp;</p>\n\n\n\n<p>The battery-powered devices\u2014dubbed GrizCams\u2014are designed by a small Montana startup, Grizzly Systems. Together with biologists, they\u2019re deploying a constellation of the devices across the Greater Yellowstone ecosystem to record audio and video of when and where wolves or wolf packs howl.&nbsp;</p>\n\n\n\n<p>Once fully deployed, the data can help scientists and conservationists better understand wolf behavior and create new strategies for deterring wolves from attacking livestock.</p>\n\n\n\n<p>Conservationists retrieve audio data from SD cards on remote recorders every few months. That data is fed into and analyzed by AI models trained using terabytes of data of howling wolves. The model\u2014a convolutional neural network\u2014converts the audio into a spectrogram, which analyzes the data, identifying different aspects of a wolf\u2019s howl and geolocating where the sounds originated.\u00a0</p>\n\n\n\n<p>Grizzly Systems trained the model using <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 Tensor Core GPUs</a> in the Azure cloud and PyTorch framework running <a href=\"https://developer.nvidia.com/gpu-accelerated-libraries\">NVIDIA CUDA-X libraries</a>. For inferencing, they use <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a> and ONNX Runtime for model optimization, with an <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4090/\">NVIDIA RTX 4090</a> for on-prem storage of sensitive data and local inference.\u00a0</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/3bu8lN0uvtI?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;start=7&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. A wolf pack recorded in 2023 in Yellowstone National Park vocalizes in chorus and asynchronously</em></figcaption></figure>\n\n\n\n<p class=\"has-text-align-left\">Grizzly Systems CEO, Jeff Reed, PhD, highlighted how the system monitors large tracts of land 24 hours a day, every day of the year. The devices can help perennially under-resourced wildlife managers and state and federal agencies monitor lands that often lack personnel.</p>\n\n\n\n<p>The AI model can identify varied pitches and intonations of wolf vocalizations, which can carry more than six kilometers from where they originate. Knowing where a pack moves by tracking their howls can help conservationists identify a wolf\u2019s territorial boundaries.\u00a0</p>\n\n\n\n<p>While the model can\u2019t yet identify individual wolves from their howls, Reed said future iterations of the technology will have that capability.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"975\" height=\"548\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Diagram-GrizCam.png\" alt=\"A workflow showing pictures of animals, representing data the GrizCam picks up, and, next to those images, a sketch representing how the GrizCam hardware and software process that data and share it with cloud-based LLMs for analysis.\" class=\"wp-image-91078\" style=\"width:841px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Diagram-GrizCam.png 975w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Diagram-GrizCam-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Diagram-GrizCam-625x351.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Diagram-GrizCam-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Diagram-GrizCam-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Diagram-GrizCam-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Diagram-GrizCam-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Diagram-GrizCam-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Diagram-GrizCam-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Diagram-GrizCam-362x203.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Diagram-GrizCam-196x110.png 196w\" sizes=\"(max-width: 975px) 100vw, 975px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. GrizCam collects sounds or video, runs through an on-device thin model layer, and analyzes data on a cloud-based LLM</em></figcaption></figure></div>\n\n\n<p>Today, the GrizCams make up one part of a larger conservation effort aimed at balancing competing interests in the land.&nbsp;</p>\n\n\n\n<p>These include the small but growing wolf population in Montana, which needs wild prey for food; the billion-dollar Yellowstone eco-tourism economy, which relies upon healthy wildlife populations; and the ranchers who need to protect their livestock, and whose land offers critical habitat for wildlife.</p>\n\n\n\n<p>\u201cWolves, grizzlies, elk can be a hassle to a rancher because they might kill their livestock, or tear down their fences,\u201d said Reed, who before starting up Grizzly Systems three years ago, spent his career working in the tech industry. \u201cOn the flip side, those ranches also provide critical habitat for wildlife on private lands around Yellowstone.</p>\n\n\n\n<p>\u201cIf our devices can detect a lone wolf coming through a ranchland because we have AI on it, then we can playback the sound of guardian dogs barking, or a gunshot, or a large territorial wolf pack, which can \u201cencourage\u201d that wolf to move out of that area. But that requires vigilance throughout the day and the night\u2014and nobody is sitting outside 24/7\u2014which is where AI comes in.\u201d</p>\n\n\n\n<p>Another way AI is helping conservationists is by streamlining the data collection process.&nbsp;</p>\n\n\n\n<p>The remote recorders\u2014which can also be deployed with video capabilities\u2014run a very thin-layer AI on-device, which weeds out most motion that would otherwise trigger false-positive recordings. The recorders can ignore wind rustling through grass or trees, or bright light reflecting off snow\u2014two common stimuli that trigger false-positive recordings on remote devices.</p>\n\n\n\n<p>As a result, the GrizCam\u2019s batteries last longer and require less servicing by wildlife managers and landowners.</p>\n\n\n\n<p>AI is also useful to conservationists as it quickly sifts through terabytes of recorded data to quickly identify and flag relevant audio or visual signatures.&nbsp;</p>\n\n\n\n<p>While the on-device AI cuts down on unwanted recordings, it nevertheless records sounds and imagery of biological activity\u2014including birds, elk, or bears moving across terrain and making noises.&nbsp;&nbsp;</p>\n\n\n\n<p>\u201cThese acoustic recorders are gathering data with AI, they\u2019re recording 24-7, every day for a year across 50 or so recorders,\u201d said Reed. \u201cWith AI, we can crunch through the data, go through and identify wolves or other endangered species if they\u2019re there, and then work with conservationists to say, \u2018okay, we gotta go protect this area and do some additional conservation over there.\u2019\u201d</p>\n\n\n\n<p>Grizzly Systems plans to continue its close collaboration with conservationists. It also foresees its rugged edge devices&#8217; relevance for a variety of industrial use cases, including remote surveillance.</p>\n\n\n\n<p>Reed points out that 97% of the Earth\u2019s surface lacks access to an electrical outlet. A rugged recorder can monitor oil and gas rigs, as well as remote electrical transformers, which, in very rural areas, can attract vandals who take them offline.&nbsp;&nbsp;</p>\n\n\n\n<p>\u201cAI is a great example of how, if we can get it right, with battery life and ruggedness, we can monitor illegal activity that hurts us all,\u201d Reed said. \u201cPoaching, illegal wildlife trafficking, illegal logging or mining in the Amazon\u2014 this is activity that ends up hurting the vast majority of people and the planet\u2013and which technology can help prevent.\u201d</p>\n\n\n\n<p>Read more about the <a href=\"https://www.thelanguagesoflife.com/crywolf\">partnership</a> between Grizzly Systems and Yellowstone National Park.</p>\n\n\n\n<p>Check out additional reporting on <a href=\"https://www.washingtonpost.com/climate-solutions/2024/10/12/yellowstone-wolves-howls-bioacoustics/\">wolf conservation</a> and <a href=\"https://yellowstonian.org/the-secret-chorus-language-of-yellowstone-wolves/\">decoding wolf verbalizations</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>A new cell-phone-sized device\u2014which can be deployed in vast, remote areas\u2014is using AI to identify and geolocate wildlife to help conservationists track endangered species, including wolves around Yellowstone National Park.&nbsp; The battery-powered devices\u2014dubbed GrizCams\u2014are designed by a small Montana startup, Grizzly Systems. Together with biologists, they\u2019re deploying a constellation of the devices across the Greater &hellip; <a href=\"https://developer.nvidia.com/blog/ai-powered-devices-track-howls-to-save-wolves/\">Continued</a></p>\n", "protected": false}, "author": 2156, "featured_media": 91093, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1511091", "discourse_permalink": "https://forums.developer.nvidia.com/t/ai-powered-devices-track-howls-to-save-wolves/311595", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 3110, 1903], "tags": [3941, 453], "coauthors": [3876], "class_list": ["post-91077", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-edge-computing", "category-generative-ai", "category-features", "tag-ai-impact", "tag-featured"], "acf": {"post_industry": ["Smart Cities / Spaces"], "post_products": ["A100", "RTX GPU", "Triton Inference Server"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/wolf-howling-e1730224047800.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nGZ", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91077"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2156"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91077"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91077/revisions"}], "predecessor-version": [{"id": 91097, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91077/revisions/91097"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91093"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91077"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91077"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91077"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91077"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90647, "date": "2024-10-29T09:00:00", "date_gmt": "2024-10-29T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90647"}, "modified": "2024-10-31T11:32:20", "modified_gmt": "2024-10-31T18:32:20", "slug": "enhanced-security-and-streamlined-deployment-of-ai-agents-with-nvidia-ai-enterprise", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/enhanced-security-and-streamlined-deployment-of-ai-agents-with-nvidia-ai-enterprise/", "title": {"rendered": "Enhanced Security and Streamlined Deployment of AI Agents with NVIDIA AI Enterprise"}, "content": {"rendered": "\n<p>AI agents are emerging as the newest way for organizations to increase efficiency, improve productivity, and accelerate innovation. These agents are more advanced than prior AI applications, with the ability to autonomously reason through tasks, call out to other tools, and incorporate both enterprise data and employee knowledge to produce valuable business outcomes. They\u2019re being embedded into applications customized for each organization&#8217;s needs.&nbsp;</p>\n\n\n\n<p>The latest release of <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> includes several new features that help make AI agents more secure, stable, and easier to deploy.</p>\n\n\n\n<h2 id=\"simplified_management_of_ai_agent_pipelines\"  class=\"wp-block-heading\">Simplified management of AI agent pipelines<a href=\"#simplified_management_of_ai_agent_pipelines\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The newly launched <a href=\"https://developer.nvidia.com/blog/managing-ai-inference-pipelines-on-kubernetes-with-nvidia-nim-operator/\">NVIDIA NIM Operator</a> simplifies the deployment and management of <a href=\"https://www.nvidia.com/en-us/ai/\">NIM microservices</a> used to deploy AI pipelines on Kubernetes. NIM Operator automates the deployment of AI pipelines and enhances performance with capabilities such as intelligent model pre-caching for lower initial inference latency and faster autoscaling.&nbsp;</p>\n\n\n\n<p>You can choose to autoscale based on CPU, GPU, or NIM-specific metrics, such as NIM max requests, KVcache, and so on.&nbsp;</p>\n\n\n\n<p>It also simplifies the upgrade process by providing easy rolling upgrades. Change the version number of the NIM microservice and the NIM Operator updates the deployments in the cluster.&nbsp;</p>\n\n\n\n<p>NVIDIA now offers the following deployment paths to deploy NIM microservices for production AI pipelines:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://docs.nvidia.com/nim/large-language-models/latest/deploy-helm.html\">Helm</a></li>\n\n\n\n<li><a href=\"https://blogs.nvidia.com/blog/kserve-nim-inference/\">KServe</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/nim-operator/latest/index.html\">NIM Operator</a>&nbsp;</li>\n</ul>\n\n\n\n<h2 id=\"security_and_api_stability_for_ai_models\"  class=\"wp-block-heading\">Security and API stability for AI models<a href=\"#security_and_api_stability_for_ai_models\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA AI Enterprise includes monthly feature branch releases for AI and data science software which contain top-of-tree software updates and are ideal for AI developers who want the latest features.&nbsp;</p>\n\n\n\n<p>This software is maintained by NVIDIA for one month until the next version is released, and available security fixes are applied before each release. Although this is great for customers who want to stay on the leading edge with the newest capabilities, there\u2019s no guarantee that APIs will not change from month to month. This can make it challenging to build enterprise solutions that need to be both secure and reliable over time, as developers may need to adjust applications after an update.</p>\n\n\n\n<p>To address this need, NVIDIA AI Enterprise also includes <a href=\"https://docs.nvidia.com/ai-enterprise/planning-resource/release-branches/latest/release-branches.html\">production branches</a> of AI software. Production branches ensure API stability and regular security updates and are meant for deploying AI in production when stability is required. Production branches are released every 6 months and have a 9-month lifecycle.&nbsp;</p>\n\n\n\n<p>Throughout the 9-month lifecycle of each production branch, NVIDIA continuously monitors critical and high common vulnerabilities and exposures (CVEs) and releases monthly security patches. By doing so, the AI frameworks, libraries, models, and tools included in NVIDIA AI Enterprise can be updated for security fixes while eliminating the risk of breaking an API.&nbsp;</p>\n\n\n\n<p>The new release is expected to add these NIM microservices to production branches:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Meta\u2019s Llama 3.1 family of models:\n<ul class=\"wp-block-list\">\n<li>Llama-3.1-Instruct-8B</li>\n\n\n\n<li>Llama-3.1-Instruct-70B</li>\n\n\n\n<li>Llama-3.1-Instruct-405B</li>\n</ul>\n</li>\n\n\n\n<li>Mistral AI\u2019s Mistral 7B and mixture of experts (MoE) 8x7B and 8x22B models:\n<ul class=\"wp-block-list\">\n<li>Mixtral-8x7B</li>\n\n\n\n<li>Mixtral-8x22B</li>\n\n\n\n<li>Mistral-7B</li>\n</ul>\n</li>\n\n\n\n<li>NVIDIA Nemotron-4-340B family of models for synthetic data generation:\n<ul class=\"wp-block-list\">\n<li>Nemotron-4-340B-Instruct</li>\n\n\n\n<li>Nemotron-4-340B-Reward</li>\n</ul>\n</li>\n\n\n\n<li>NVIDIA NeMo Retriever QA E5 Embedding v5 text embedding model:\n<ul class=\"wp-block-list\">\n<li>NV-EmbedQA-E5-v5</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>You can build AI agents using these microservices with the confidence that NVIDIA will secure and maintain them without breaking any application dependencies during the lifetime of that production branch.</p>\n\n\n\n<p>These NIM microservices join numerous other AI libraries and frameworks already on a production branch:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>PyTorch</li>\n\n\n\n<li>TensorFlow</li>\n\n\n\n<li>RAPIDS</li>\n\n\n\n<li>NVIDIA TensorRT</li>\n\n\n\n<li>NVIDIA Triton Inference Server</li>\n\n\n\n<li>NVIDIA Morpheus</li>\n\n\n\n<li>NVIDIA Holoscan</li>\n</ul>\n\n\n\n<p>Other AI frameworks that are new to a production branch with this release include the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Deepstream for AI-based video and image understanding and multi-sensor processing</li>\n\n\n\n<li>DGL and PyG for training graph neural networks</li>\n</ul>\n\n\n\n<h2 id=\"ai_for_healthcare\"  class=\"wp-block-heading\">AI for healthcare<a href=\"#ai_for_healthcare\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Customers in highly regulated industries often require software to be supported for even longer periods. For these customers, NVIDIA AI Enterprise also includes long-term support branches (LTSB), which are supported with stable APIs for 3 years.</p>\n\n\n\n<p>LTSB 1 coincided with the first release of NVIDIA AI Enterprise in 2021 and includes foundational AI components:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>PyTorch</li>\n\n\n\n<li>TensorFlow</li>\n\n\n\n<li>RAPIDS</li>\n\n\n\n<li>TensorRT</li>\n\n\n\n<li>Triton Inference Server</li>\n\n\n\n<li>Infrastructure software, such as vGPU driver</li>\n</ul>\n\n\n\n<p>LTSB 2, as part of this latest release of NVIDIA AI Enterprise, adds <a href=\"https://www.nvidia.com/en-us/clara/holoscan/\">Holoscan</a>, which includes Holoscan SDK and Holoscan Deployment Stack.</p>\n\n\n\n<p>Holoscan is the NVIDIA AI sensor processing platform that combines hardware systems for low-latency sensor and network connectivity, optimized libraries for data processing and AI, and core capabilities to run real-time streaming, imaging, and other applications.&nbsp; Holoscan SDK includes C++ and Python APIs to create sensor processing workflows with inherent support for sensor I/O, compute, AI inferencing, and visualization, while leveraging NVIDIA GPU acceleration.&nbsp;</p>\n\n\n\n<p>One of the most prevalent uses of Holoscan is for medical devices, such as those for medical imaging and robotic surgery. As medical devices have strict requirements for long-term supportability, the addition of Holoscan to long-term support combined with long-life hardware enables device manufacturers to build the next generation of intelligent AI-enabled medical devices, with faster time to market and lower cost of maintenance.&nbsp;</p>\n\n\n\n<p>The Holoscan platform with LTSB is an effective solution for other industries beyond medical devices, wherever an industrial-grade production-ready platform is needed to build AI-enabled sensor processing products.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"399\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options-1024x399.png\" alt=\"The diagram shows the relationship between feature, production, and long-term support branch by month.\" class=\"wp-image-90652\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options-1024x399.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options-300x117.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options-625x243.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options-179x70.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options-768x299.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options-1536x598.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options-645x251.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options-500x195.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options-160x62.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options-362x141.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options-283x110.png 283w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-support-branch-options.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Software support branch options in NVIDIA AI Enterprise</em></figcaption></figure></div>\n\n\n<h2 id=\"more_ways_to_deploy_nim_microservices\"  class=\"wp-block-heading\">More ways to deploy NIM microservices<a href=\"#more_ways_to_deploy_nim_microservices\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA AI Enterprise is supported o both on-premises and public cloud services. You can deploy NIM microservices and other software containers into self-managed Kubernetes running on cloud instances, but many prefer to use Kubernetes managed by the cloud provider.&nbsp;</p>\n\n\n\n<p>Google Cloud has now integrated <a href=\"https://developer.nvidia.com/blog/scale-high-performance-ai-inference-with-google-kubernetes-engine-and-nvidia-nim/\">NVIDIA NIM into Google Kubernetes Engine</a> to provide enterprise customers with a simplified path for deploying optimized models directly from the Google Cloud Marketplace.</p>\n\n\n\n<h2 id=\"availability\"  class=\"wp-block-heading\">Availability<a href=\"#availability\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The next version of NVIDIA AI Enterprise is available now. License holders can download production branch versions of most AI software containers right away but the NIM microservices are expected to be added to the production branch at the end of November. As always, you also get the benefit of enterprise support, which includes guaranteed response times and access to NVIDIA experts for timely issue resolution.\u00a0</p>\n\n\n\n<p>For more information, see <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/get-started/\">NVIDIA AI Enterprise Getting Started</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>AI agents are emerging as the newest way for organizations to increase efficiency, improve productivity, and accelerate innovation. These agents are more advanced than prior AI applications, with the ability to autonomously reason through tasks, call out to other tools, and incorporate both enterprise data and employee knowledge to produce valuable business outcomes. They\u2019re being &hellip; <a href=\"https://developer.nvidia.com/blog/enhanced-security-and-streamlined-deployment-of-ai-agents-with-nvidia-ai-enterprise/\">Continued</a></p>\n", "protected": false}, "author": 1379, "featured_media": 90651, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1511021", "discourse_permalink": "https://forums.developer.nvidia.com/t/enhanced-security-and-streamlined-deployment-of-ai-agents-with-nvidia-ai-enterprise/311576", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [4150, 3110], "tags": [3965, 453], "coauthors": [2784], "class_list": ["post-90647", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-deployment", "category-generative-ai", "tag-ai-agent", "tag-featured"], "acf": {"post_industry": ["General"], "post_products": ["AI Enterprise", "NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvaie-kv-press-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nA3", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90647"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1379"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90647"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90647/revisions"}], "predecessor-version": [{"id": 91070, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90647/revisions/91070"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90651"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90647"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90647"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90647"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90647"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 91036, "date": "2024-10-28T12:23:49", "date_gmt": "2024-10-28T19:23:49", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=91036"}, "modified": "2024-10-31T11:32:58", "modified_gmt": "2024-10-31T18:32:58", "slug": "upcoming-webinar-enhance-generative-ai-model-accuracy-through-high-quality-data-processing", "status": "publish", "type": "post", "link": "https://nvda.ws/3UreHhQ", "title": {"rendered": "Upcoming Webinar: Enhance Generative AI Model Accuracy Through High-Quality Data Processing"}, "content": {"rendered": "\n<p>Learn how to build scalable data processing pipelines to create high-quality datasets.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn how to build scalable data processing pipelines to create high-quality datasets.</p>\n", "protected": false}, "author": 1921, "featured_media": 91037, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1510279", "discourse_permalink": "https://forums.developer.nvidia.com/t/upcoming-webinar-enhance-generative-ai-model-accuracy-through-high-quality-data-processing/311428", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3UreHhQ", "_links_to_target": "_blank"}, "categories": [3110], "tags": [791, 453, 1958], "coauthors": [3612], "class_list": ["post-91036", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-data-preprocessing", "tag-featured", "tag-news"], "acf": {"post_industry": ["General"], "post_products": ["NeMo", "NeMo Curator"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llm-social-nemo-retriever-webinar-content-3450414-1600x900-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nGk", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91036"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1921"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=91036"}], "version-history": [{"count": 3, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91036/revisions"}], "predecessor-version": [{"id": 91041, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/91036/revisions/91041"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/91037"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=91036"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=91036"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=91036"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=91036"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90842, "date": "2024-10-28T11:30:00", "date_gmt": "2024-10-28T18:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90842"}, "modified": "2024-10-31T11:33:13", "modified_gmt": "2024-10-31T18:33:13", "slug": "an-introduction-to-model-merging-for-llms", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/an-introduction-to-model-merging-for-llms/", "title": {"rendered": "An Introduction to Model Merging for LLMs"}, "content": {"rendered": "\n<p>One challenge organizations face when customizing <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models (LLMs)</a> is the need to run multiple experiments, which produces only one useful model. While the cost of experimentation is typically low, and the results well worth the effort, this experimentation process does involve \u201cwasted\u201d resources, such as compute assets spent without their product being utilized, dedicated developer time, and more.</p>\n\n\n\n<p>Model merging combines the weights of multiple customized LLMs, increasing resource utilization and adding value to successful models. This approach provides two key solutions:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Reduces experimentation waste by repurposing &#8220;failed experiments&#8221;</li>\n\n\n\n<li>Offers a cost-effective alternative to join training</li>\n</ul>\n\n\n\n<p>This post explores how models are customized, how model merging works, different types of model merging, and how model merging is iterating and evolving.</p>\n\n\n\n<h2 id=\"revisiting_model_customization&nbsp;\"  class=\"wp-block-heading\">Revisiting model customization&nbsp;<a href=\"#revisiting_model_customization&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This section provides a brief overview of how models are customized and how this process can be leveraged to help build an intuitive understanding of model merging.&nbsp;</p>\n\n\n\n<p>Note that some of the concepts discussed are oversimplified for the purpose of building this intuitive understanding of model merging. It is suggested that you familiarize yourself with customization techniques, transformer architecture, and training separately before diving into model merging. See, for example, <a href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\">Mastering LLM Techniques: Customization</a>.&nbsp;</p>\n\n\n\n<h3 id=\"the_role_of_weight_matrices_in_models\"  class=\"wp-block-heading\">The role of weight matrices in models<a href=\"#the_role_of_weight_matrices_in_models\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Weight matrices are essential components in many popular model architectures, serving as large grids of numbers (weights, or parameters) that store the information necessary for the model to make predictions.</p>\n\n\n\n<p>As data flows through a model, it passes through multiple layers, each containing its own weight matrix. These matrices transform the input data through mathematical operations, enabling the model to learn from and adapt to the data.</p>\n\n\n\n<p>To modify a model\u2019s behavior, the weights within these matrices must be updated. Although the specifics of weight modification are not essential, it\u2019s crucial to understand that each customization of a base model results in a unique set of updated weights.</p>\n\n\n\n<h3 id=\"task_customization\"  class=\"wp-block-heading\">Task customization<a href=\"#task_customization\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>When fine-tuning an LLM for a specific task, such as summarization or math, the updates made to the weight matrices are targeted towards improving performance on that particular task. This implies that the modifications to the weight matrices are localized to specific regions, rather than being uniformly distributed.</p>\n\n\n\n<p>To illustrate this concept, consider a simple analogy where the weight matrices are represented as a sports field that is 100 yards in length. When customizing the model for summarization, the updates to the weight matrices might concentrate on specific areas, such as the 10-to-30 yard lines. In contrast, customizing the model for math might focus updates on a different region, like the 70-to-80 yard lines.</p>\n\n\n\n<p>Interestingly, when customizing the model for a related task, such as summarization in the French language, the updates might overlap with the original summarization task, affecting the same regions of the weight matrices (the 25-to-35 yard lines, for example). This overlap suggests an important insight: different task customizations can significantly impact the same areas of the weight matrices.</p>\n\n\n\n<p>While the previous example is purposefully oversimplified, the intuition is accurate. Different task customizations will lead to different parts of the weight matrices being updated, and customization for similar tasks might lead to changing the same parts of their respective weight matrices.</p>\n\n\n\n<p>This understanding can inform strategies for customizing LLMs and leveraging knowledge across tasks.</p>\n\n\n\n<h2 id=\"model_merging\"  class=\"wp-block-heading\">Model merging<a href=\"#model_merging\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Model merging is a loose grouping of strategies that relates to combining two or more models, or model updates, into a single model for the purpose of saving resources or improving task-specific performance.&nbsp;</p>\n\n\n\n<p>This discussion focuses primarily on the implementation of these techniques through an open-source library developed by <a href=\"https://www.arcee.ai/\">Arcee AI</a> called <a href=\"https://github.com/arcee-ai/mergekit\">mergekit</a>. This library simplifies the implementation of various merging strategies.&nbsp;</p>\n\n\n\n<p>Many methods are used to merge models, in various levels of complexity. Here, we\u2019ll focus on four main merging methods:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Model Soup</li>\n\n\n\n<li>Spherical Linear Interpolation (SLERP)</li>\n\n\n\n<li>Task Arithmetic (using Task Vectors)</li>\n\n\n\n<li>TIES leveraging DARE</li>\n</ol>\n\n\n\n<h3 id=\"model_soup\"  class=\"wp-block-heading\">Model Soup<a href=\"#model_soup\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The Model Soup method involves averaging the resultant model weights created by hyperparameter optimization experiments, as explained in <a href=\"https://arxiv.org/abs/2203.05482\">Model Soups: Averaging Weights of Multiple Fine-Tuned Models Improves Accuracy Without Increasing Inference Time</a>.</p>\n\n\n\n<p>Originally tested and verified through computer vision models, this method has shown promising results for LLMs as well. In addition to generating some additional value out of the experiments, this process is simple and not compute intensive.&nbsp;</p>\n\n\n\n<p>There are two ways to create Model Soup: naive and greedy. The naive approach involves merging all models sequentially, regardless of their individual performance. In contrast, the greedy implementation follows a simple algorithm:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Rank models by performance on the desired task</li>\n\n\n\n<li>Merge the best performing model with the second best performing model</li>\n\n\n\n<li>Evaluate the merged model\u2019s performance on the desired task</li>\n\n\n\n<li>If the merged model performs better, continue with the next model; otherwise, skip the current model and try again with the next best model</li>\n</ul>\n\n\n\n<p>This greedy approach ensures that the resulting Model Soup is at least as good as the best individual model.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"452\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-soup-method-625x452.png\" alt=\"Graph showing performance along two axes, accuracy and generalization.\" class=\"wp-image-90964\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-soup-method-625x452.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-soup-method-300x217.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-soup-method-159x115.png 159w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-soup-method-768x556.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-soup-method-645x467.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-soup-method-415x300.png 415w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-soup-method-124x90.png 124w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-soup-method-362x262.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-soup-method-152x110.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-soup-method.png 807w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. The Model Soup method outperforms the constituent models using the greedy model Soup Model merging technique&nbsp;</em></em></figcaption></figure></div>\n\n\n<p>Each step of creating a Model Soup is implemented by simple weighted and normalized linear averaging of two or more model weights. Both the weighting and normalization are optional, though recommended. The implementation of this from the <code>mergekit</code> library is as follows:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nres = (weights * tensors).sum(dim=0)\nif self.normalize:\n\tres = res / weights.sum(dim=0)\n</pre></div>\n\n\n<p>While this method has shown promising results in the computer vision and language domains, it faces some serious limitations. Specifically, there is no guarantee that the model will be more performant. The linear averaging can lead to degraded performance or loss of generalizability.&nbsp;</p>\n\n\n\n<p>The next method, SLERP, addresses some of those specific concerns.</p>\n\n\n\n<h3 id=\"slerp&nbsp;\"  class=\"wp-block-heading\">SLERP&nbsp;<a href=\"#slerp&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Spherical Linear Interpolation, or SLERP, is a method introduced in a 1985 paper titled <a href=\"https://www.cs.cmu.edu/~kiranb/animation/p245-shoemake.pdf\">Animating Rotation with Quaternion Curves</a>. It\u2019s a \u201csmarter\u201d way of computing the average between two vectors. In a technical sense, it helps compute the shortest path between two points on a curved surface.&nbsp;</p>\n\n\n\n<p>This method excels at combining two models. The classic example is imagining the shortest path between two points on the Earth. Technically, the shortest path would be a straight line that goes through the Earth, but in reality it\u2019s a curved path on the surface of the Earth. SLERP computes this smooth path to use for averaging two models together while maintaining their unique model weight \u201csurfaces.\u201d</p>\n\n\n\n<p>The following code snippet is the core of the SLERP algorithm, and is what provides such a good interpolation between the two models:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Calculate initial angle between v0 and v1\ntheta_0 = np.arccos(dot)\nsin_theta_0 = np.sin(theta_0)\n\n# Angle at timestep t\ntheta_t = theta_0 * t\nsin_theta_t = np.sin(theta_t)\n\n# Finish the slerp algorithm\ns0 = np.sin(theta_0 - theta_t) / sin_theta_0\ns1 = sin_theta_t / sin_theta_0\nres = s0 * v0_copy + s1 * v1_copy\n\nreturn maybe_torch(res, is_torch)\n</pre></div>\n\n\n<h3 id=\"task_arithmetic_using_task_vectors\"  class=\"wp-block-heading\">Task Arithmetic (using Task Vectors)<a href=\"#task_arithmetic_using_task_vectors\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>This group of model merging methods utilizes Task Vectors to combine models in various ways, increasing in complexity.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Task Vectors: Capturing customization updates</h4>\n\n\n\n<p>Recalling how models are customized, updates are made to the model\u2019s weights, and those updates are captured in the base model matrices. Instead of considering the final matrices as a brand new model, they can be viewed as the difference (or delta) between the base weights and the customized weights. This introduces the concept of a task vector,a structure containing the delta between the base and customized weights.</p>\n\n\n\n<p>This is the same intuition behind Low Rank Adaptation (LoRA), but without the further step of factoring the matrices representing the weight updates.&nbsp;</p>\n\n\n\n<p>Task Vectors can be simply obtained from customization weights by subtracting out the base model weights.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">Task Interference: Conflicting updates</h4>\n\n\n\n<p>Recalling the sports field example, there is a potential for overlap in the updated weights between different customizations. There is some intuitive understanding that customization done for the same task would lead to a higher rate of conflicting updates than customization done for two, or more, separate tasks.</p>\n\n\n\n<p>This \u201cconflicting update\u201d idea is more formally defined as Task Interference and it relates to the potential collision of important updates between two, or more, Task Vectors.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">Task Arithmetic</h4>\n\n\n\n<p>As introduced in the paper <a href=\"https://arxiv.org/abs/2212.04089\">Editing Models with Task Arithmetic</a>, Task Arithmetic represents the simplest implementation of a task vector approach. The process is as follows:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Obtain two or more task vectors and merge them linearly as seen in Model Soup.&nbsp;</li>\n\n\n\n<li>After the resultant merged task vector is obtained, it is added into the base model.</li>\n</ol>\n\n\n\n<p>This process is simple and effective, but has a key weakness: no attention is paid to the potential interference between the task vectors intended to be merged.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">TIES-Merging</h4>\n\n\n\n<p>As introduced in the paper <a href=\"https://arxiv.org/abs/2306.01708\">TIES-Merging: Resolving Interference When Merging Models</a>, TIES (TrIm Elect Sign and Merge) is a method that takes the core ideas of Task Arithmetic and combines it with heuristics for resolving potential interference between the Task Vectors.&nbsp;</p>\n\n\n\n<p>The general procedure is to consider, for each weight in the Task Vectors being merged, the magnitude of each incoming weight, then the sign of each incoming weight, and then averaging the remaining weights.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"759\" height=\"290\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ties-process-diagram.png\" alt=\"Diagram of the TIES process, including examples of each step and the final set of Task Vectors\n\" class=\"wp-image-90847\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ties-process-diagram.png 759w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ties-process-diagram-300x115.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ties-process-diagram-625x239.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ties-process-diagram-179x68.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ties-process-diagram-645x246.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ties-process-diagram-500x191.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ties-process-diagram-160x61.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ties-process-diagram-362x138.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ties-process-diagram-288x110.png 288w\" sizes=\"(max-width: 759px) 100vw, 759px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. A visual representation of the TIES process</em></em></figcaption></figure>\n\n\n\n<p>This method seeks to resolve interference by enabling the models that had the most significant weight updates for any given weight update take precedence during the merging process. In essence, the models that \u201ccared\u201d more about that weight would be prioritized over the models that did not.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">DARE</h4>\n\n\n\n<p>Introduced in the paper <a href=\"https://arxiv.org/abs/2311.03099\">Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch</a>, DARE isn\u2019t directly a model merging technique. Rather, it\u2019s an augment that can be considered alongside other approaches. DARE derives from the following:</p>\n\n\n\n<p><strong>D</strong>rops delta parameters with a ratio p <strong>A</strong>nd <strong>RE</strong>scales the remaining ones by 1/(1 &#8211; p) to approximate the original embeddings.</p>\n\n\n\n<p>Instead of trying to address the problem of interference through heuristics, DARE approaches it from a different perspective. In essence, it randomly drops a large number of the updates found in a specific task vector by setting them to 0, and then rescales the remaining weight proportional to the ratio of the dropped weights.</p>\n\n\n\n<p>DARE has been shown to be effective even when dropping upwards of 90%, or even 99% of the task vector weights.&nbsp;</p>\n\n\n\n<h2 id=\"increase_model_utility_with_model_merging\"  class=\"wp-block-heading\">Increase model utility with model merging<a href=\"#increase_model_utility_with_model_merging\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The concept of model merging offers a practical way to maximize the utility of multiple LLMs, including task-specific fine-tuning done by a larger community. Through techniques like Model Soup, SLERP, Task Arithmetic, TIES-Merging, and DARE, organizations can effectively merge multiple models in the same family in order to reuse experimentation and cross-organizational efforts.&nbsp;</p>\n\n\n\n<p>As the techniques behind model merging are better understood and further developed, they are poised to become a cornerstone of the development of performant LLMs. While this post has only scratched the surface, more techniques are constantly under development, including some <a href=\"https://arxiv.org/abs/2403.13187\">evolution-based methods</a>. Model merging is a budding field in the generative AI landscape, as more applications are being tested and proven.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>One challenge organizations face when customizing large language models (LLMs) is the need to run multiple experiments, which produces only one useful model. While the cost of experimentation is typically low, and the results well worth the effort, this experimentation process does involve \u201cwasted\u201d resources, such as compute assets spent without their product being utilized, &hellip; <a href=\"https://developer.nvidia.com/blog/an-introduction-to-model-merging-for-llms/\">Continued</a></p>\n", "protected": false}, "author": 2077, "featured_media": 90844, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1510264", "discourse_permalink": "https://forums.developer.nvidia.com/t/an-introduction-to-model-merging-for-llms/311425", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 4147], "tags": [453, 3650], "coauthors": [3791, 3328], "class_list": ["post-90842", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "category-models", "tag-featured", "tag-llm-techniques"], "acf": {"post_industry": ["General"], "post_products": ["General"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llm-icons.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nDc", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90842"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2077"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90842"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90842/revisions"}], "predecessor-version": [{"id": 90965, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90842/revisions/90965"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90844"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90842"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90842"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90842"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90842"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90872, "date": "2024-10-28T09:00:00", "date_gmt": "2024-10-28T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90872"}, "modified": "2024-11-11T12:00:23", "modified_gmt": "2024-11-11T20:00:23", "slug": "creating-rag-based-question-and-answer-llm-workflows-at-nvidia", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/creating-rag-based-question-and-answer-llm-workflows-at-nvidia/", "title": {"rendered": "Creating RAG-Based Question-and-Answer LLM Workflows at NVIDIA"}, "content": {"rendered": "\n<p>The rapid development of solutions using retrieval augmented generation (RAG) for question-and-answer LLM workflows has led to new types of system architectures. Our work at NVIDIA using AI for internal operations has led to several important findings for finding alignment between system capabilities and user expectations.&nbsp;</p>\n\n\n\n<p>We found that regardless of the intended scope or use case, users generally want to be able to execute non-RAG tasks like performing document translation, editing emails, or even writing code. A vanilla RAG application might be implemented so that it executes a retrieval pipeline on every message, leading to excess usage of tokens and unwanted latency as irrelevant results are included.&nbsp;</p>\n\n\n\n<p>We also found that users really appreciate having access to a web search and summarization capability, even if the application is designed for accessing internal private data. As a example, we used <a href=\"https://docs.perplexity.ai/home\">Perplexity\u2019s search API </a>to meet this need.</p>\n\n\n\n<p>In this post, we share a basic architecture for addressing these issues, using routing and multi-source RAG to produce a chat application that is capable of answering a broad range of questions. This is a slimmed-down version of an application and there are many ways to build a RAG-based application, but this can help get you going. For more information, see the <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/tree/main/community/routing-multisource-rag\">/NVIDIA/GenerativeAIExamples</a> GitHub repo.</p>\n\n\n\n<p>In particular, we highlight how to use LlamaIndex, NVIDIA NIM microservices, and Chainlit to rapidly deploy this application. You can use this project as inspiration for the <a href=\"https://developer.nvidia.com/llamaindex-developer-contest\">NVIDIA and LlamaIndex Developer Contest</a>, showcasing innovative uses of these technologies in real-world applications and a chance to win exciting prizes.</p>\n\n\n\n<p>We found a tremendous amount of synergy between these technologies. NVIDIA NIM microservices and their LlamaIndex connectors make it effortless to develop LLM applications with self-managed or hosted LLMs. Chainlit and LlamaIndex <code>Workflow</code> events fit together nicely due to their shared event-driven architecture, which makes it easy to provide a user interface with detailed information on the full trace of an LLM response. We outline more of the system details in this post.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"700\" height=\"507\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/system-architecture-rag-workflow.png\" alt=\"A flowchart shows components including Chainlit for user interface and chat history, and LlamaIndex for dense retrieval, web search, and query routing.\" class=\"wp-image-90920\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/system-architecture-rag-workflow.png 700w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/system-architecture-rag-workflow-300x217.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/system-architecture-rag-workflow-625x453.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/system-architecture-rag-workflow-159x115.png 159w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/system-architecture-rag-workflow-645x467.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/system-architecture-rag-workflow-414x300.png 414w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/system-architecture-rag-workflow-124x90.png 124w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/system-architecture-rag-workflow-362x262.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/system-architecture-rag-workflow-152x110.png 152w\" sizes=\"(max-width: 700px) 100vw, 700px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. System architecture for the chat application</em></figcaption></figure></div>\n\n\n<h2 id=\"nim_inference_microservices_for_llm_deployment\"  class=\"wp-block-heading\">NIM inference microservices for LLM deployment<a href=\"#nim_inference_microservices_for_llm_deployment\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Our project was built around NVIDIA NIM microservices for several models, including the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://build.nvidia.com/meta/llama-3_1-70b-instruct\">Meta\u2019s llama-3.1-70b-instruct</a></li>\n\n\n\n<li><a href=\"https://build.nvidia.com/nvidia/nv-embed-v1/modelcard\">NVIDIA&#8217;s nv-embed-v1</a> for text embeddings</li>\n\n\n\n<li><a href=\"https://build.nvidia.com/nvidia/nv-rerankqa-mistral-4b-v3\">Mistral&#8217;s nv-rerankqa-mistral-4b-v3</a> for reranking</li>\n</ul>\n\n\n\n<p>Despite not having any machine learning engineers or LLM inference specialists on our team, we requested and deployed our own instance of llama-3.1-70b-instruct using a NIM container running on an NVIDIA A100-equipped node (8 GPUs) in just a few hours. This helped us circumvent issues with availability and latency that we found with some enterprise LLM APIs.&nbsp;</p>\n\n\n\n<p>To try out the NIM APIs, sign up for an account at <a href=\"http://build.nvidia.com\">build.nvidia.com</a> and obtain an API key. To use the API key in this project, make sure it is available in a <code>.env</code> file located in the project directory. LlamaIndex connectors for NVIDIA models and APIs are available in the <a href=\"https://docs.llamaindex.ai/en/stable/examples/llm/nvidia/\">Python package</a> <code>llama-index-llms-nvidia</code>. For more information about the performance benefits for NIM-based LLM deployment, see <a href=\"https://developer.nvidia.com/blog/optimizing-inference-efficiency-for-llms-at-scale-with-nvidia-nim-microservices/\">Optimizing Inference Efficiency for LLMs at Scale with NVIDIA NIM Microservices</a>.&nbsp;</p>\n\n\n\n<h3 id=\"llamaindex_workflow_events\"  class=\"wp-block-heading\">LlamaIndex <code>Workflow</code> events<a href=\"#llamaindex_workflow_events\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Our first version of this application was built around <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/\">LlamaIndex\u2019s ChatEngine</a> class, which provided a turnkey solution for deploying a conversational AI assistant backed by a vector database. While this worked well, we found that we wanted to inject additional steps to augment context and toggle features in a way that required more extensibility.&nbsp;</p>\n\n\n\n<p>Fortunately, LlamaIndex <code>Workflow</code> events provided precisely the solution that we needed with its event-driven, step-based approach for controlling an application&#8217;s execution flow. We found it much easier and faster to extend our application as <code>Workflow</code> events while still retaining key LlamaIndex functionality such as vector stores and retrievers when necessary.</p>\n\n\n\n<p>Figure 2 shows our <code>Workflow</code> event, which we explain more about later in this post.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1408\" height=\"911\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow_graph.png\" alt=\"A diagram shows the connections between different steps of the Workflow event, beginning with query routing and ending with final response synthesis.\" class=\"wp-image-90945\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow_graph.png 1408w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow_graph-300x194.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow_graph-625x404.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow_graph-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow_graph-768x497.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow_graph-645x417.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow_graph-464x300.png 464w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow_graph-139x90.png 139w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow_graph-362x234.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow_graph-170x110.png 170w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow_graph-1024x663.png 1024w\" sizes=\"(max-width: 1408px) 100vw, 1408px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. LlamaIndex Workflow event used to answer user questions&nbsp;</em></figcaption></figure></div>\n\n\n<h3 id=\"user_interface_via_chainlit\"  class=\"wp-block-heading\">User interface via Chainlit<a href=\"#user_interface_via_chainlit\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://github.com/Chainlit/chainlit\">Chainlit</a> includes several features that helped speed up our development and deployment. It supports progress indicators and step summaries using the <code>chainlit.Step</code> decorator, and <code>LlamaIndexCallbackHandler</code> enables automatic tracing. We used a <code>Step</code> decorator for each LlamaIndex <code>Workflow</code> event to expose the application&#8217;s inner workings without overwhelming the user.\u00a0</p>\n\n\n\n<p>Chainlit&#8217;s support for enterprise authentication and PostgreSQL data layer was also crucial for production.&nbsp;</p>\n\n\n\n<h2 id=\"setting_up_the_project_environment_dependencies_and_installation\"  class=\"wp-block-heading\">Setting up the project environment, dependencies, and installation<a href=\"#setting_up_the_project_environment_dependencies_and_installation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To deploy this project, clone the repository located at <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/tree/main/community/routing-multisource-rag\">/NVIDIA/GenerativeAIExamples</a> and create a virtual Python environment, running the following commands to create and activate the environment before installing dependencies:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nmkdir .venv\npip -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</pre></div>\n\n\n<h3 id=\"configuration\"  class=\"wp-block-heading\">Configuration<a href=\"#configuration\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>After you\u2019ve installed the dependencies, make sure that you have a <code>.env</code> file located in the top-level directory of the project with values for the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><code>NVIDIA_API_KEY</code>: Required. You can get an API key for NVIDIA\u2019s services from <a href=\"http://build.nvidia.com\">build.nvidia.com</a>.</li>\n\n\n\n<li><code>PERPLEXITY_API_KEY</code>. Optional. If it is not provided, then the application runs without using Perplexity\u2019s search API. To obtain an API key for Perplexity, follow the <a href=\"https://docs.perplexity.ai/home\">instructions</a>.</li>\n</ul>\n\n\n\n<h3 id=\"project_structure\"  class=\"wp-block-heading\">Project structure<a href=\"#project_structure\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>We organized the project code into separate files:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>LlamaIndex <code>Workflow</code> (<code>workflow.py</code>): Routes queries and aggregates responses from multiple sources.&nbsp;</li>\n\n\n\n<li>Document ingestion (<code>ingest.py</code>): Loads documents into a Milvus Lite database, which is a simple way to start with Milvus without containers. Milvus Lite\u2019s main limitation is inefficient vector lookup so consider switching to a dedicated cluster when document collections grow. The ingestion module uses LlamaIndex\u2019s <code>SimpleDirectoryReader</code> to parse and load PDFs.</li>\n\n\n\n<li>Chainlit application (<code>chainlit_app.py</code>): The Chainlit application contains functions triggered by events, with the main function (<code>on_message</code>) activating on user messages.</li>\n\n\n\n<li>Configuration (<code>config.py</code>): To play around with different model types, edit the default values. Here, you can select different models for routing and chat completion as well as the number of past messages used from chat history for each completion, and the type of model used by Perplexity for web search and summarization.</li>\n</ul>\n\n\n\n<p>You can also tweak the prompts listed in <code>prompts.py</code> to fit your use case.</p>\n\n\n\n<h2 id=\"building_the_core_functionality\"  class=\"wp-block-heading\">Building the core functionality<a href=\"#building_the_core_functionality\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This application integrates LlamaIndex and NIM microservices via Chainlit. To show how to implement this logic, we\u2019ll work through the following steps:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Creating the user interface</li>\n\n\n\n<li>Implementing the <code>Workflow</code> event</li>\n\n\n\n<li>Integrating NIM microservices</li>\n</ul>\n\n\n\n<h3 id=\"creating_the_user_interface\"  class=\"wp-block-heading\">Creating the user interface<a href=\"#creating_the_user_interface\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Here\u2019s how this project is implemented, beginning with the Chainlit application in <code>chainlit_app.py</code>. Create a list of <code>Starter</code> objects in the <code>set_starter</code> function to prepopulate initial questions as clickable buttons. These help guide users on possible actions or questions and can route them to specific features.</p>\n\n\n\n<p>The main chat functionality, managed in the main function, handles message history using the <code>cl.user_session</code> variable. This isn\u2019t required for Chainlit to show conversation history but enabled us to keep state on the client side rather than within LlamaIndex objects.&nbsp;</p>\n\n\n\n<p>This approach made prototyping more straightforward and facilitated transitioning to a traditional user-frontend-backend application, unlike the stateful LlamaIndex <code>ChatEngine</code>, which complicates REST API deployment.</p>\n\n\n\n<p>When the Workflow is invoked using <code>workflow.run</code>, a series of asynchronous function calls is triggered through the Workflow, which requires only the current user query and the past chat messages as inputs. When a streaming response is generated, use the <code>stream_token</code> method on Chainlit\u2019s <code>Message</code> class to show it in the user interface. We also added a small amount of HTML with styling to show the token count and time elapsed.</p>\n\n\n\n<h3 id=\"implementing_the_workflow_event\"  class=\"wp-block-heading\">Implementing the Workflow event<a href=\"#implementing_the_workflow_event\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The RAG logic is contained within the <code>QueryFlow</code> class in <code>workflow.py</code>, composed of multiple steps defined as methods of <code>QueryFlow</code>. Each method is triggered when the events in its signature occur. Passing lists of nodes between steps using the nodes attribute was an easy way to structure the Workflow. A node represents a discrete unit of information within LlamaIndex.</p>\n\n\n\n<p>Here are the Workflow steps:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><code>workflow_start</code>: Adds the user query and chat history to the workflow\u2019s context (<code>ctx.data</code>) and routes between RAG and non-RAG queries using <code>LLMTextCompletionProgram</code>. Depending on the result, it generates either <code>RawQueryEvent</code> (triggers RAG logic) or <code>ShortcutEvent</code> (triggers immediate response synthesis).</li>\n\n\n\n<li><code>rewrite_query</code>: Transforms the user\u2019s query for better search results by removing instruction keywords like &#8220;email&#8221; and &#8220;table&#8221; that can hinder document lookup. It triggers T<code>ransformedQueryEvent</code> for the Milvus retrieval and Perplexity search steps.</li>\n\n\n\n<li><code>embed_query</code>: Produces a vector embedding for the transformed query.</li>\n\n\n\n<li><code>milvus_retrieve</code>: Uses the vector embedding for a vector search.</li>\n\n\n\n<li><code>pplx_retrieve</code>: Uses the LlamaIndex connector for the Perplexity search API to get web search results, summarized as a single node.</li>\n\n\n\n<li><code>collect_nodes</code>: Combines results from Milvus and Perplexity retrievals. This step triggers once both retrieval events are completed. Adding a reranker here could prioritize high-value nodes.</li>\n</ul>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nready = ctx.collect_events(\n            qe,\n            expected=&#x5B;\n                MilvusQueryEvent,\n                PerplexityQueryEvent,\n            ],\n        )\n\n        if ready is None:\n            logger.info(&quot;Still waiting for all input events!&quot;)\n            return None\n</pre></div>\n\n\n<ul class=\"wp-block-list\">\n<li><code>response_synthesis</code>: Builds a prompt string with past chat history context and retrieved documents. We manually form this string, though LlamaIndex templating could also be used. This step triggers <code>StopEvent</code>, ending the <code>Workflow</code> event and returning a response to the Chainlit application by generating <code>CompletionResponse</code> objects for each token produced by the LLM.</li>\n</ul>\n\n\n\n<p>To summarize, a user\u2019s query first goes through a routing step in which the LLM decides if it is worthwhile to use retrieval to look up documents to answer the query. If not, a follow-up completion call is used to produce an answer.&nbsp;</p>\n\n\n\n<p>This branch is triggered when users want to use the LLM to perform tasks that don\u2019t need retrieval, such as editing an email or summarizing a passage of existing text. If retrieval is selected, the user\u2019s query is transformed into a more search-appropriate form. This is then used for vector lookup of ingested documents using the NVIDIA embedding model as well as a Milvus vector store.&nbsp;</p>\n\n\n\n<p>The texts returned from these steps are then augmented with the results of a search using Perplexity\u2019s API, which can find data from the web to form its answers. Finally, these results are used for response synthesis. Figure 2 shows the diagram generated using <code>llama-index-utils-workflow</code>.</p>\n\n\n\n<h3 id=\"integrating_nim_microservices\"  class=\"wp-block-heading\">Integrating NIM microservices<a href=\"#integrating_nim_microservices\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Using NVIDIA NIM microservices for LLM and embedding functionality was quick, thanks to the connectors available from the <code>llama-index-llms-nvidia</code> and <code>llama-index-embeddings-nvidia</code> packages.&nbsp;</p>\n\n\n\n<p>As there\u2019s a range of models available from build.nvidia.com, we could pick the small, fast-executing model Meta\u2019s meta/llama-3.1-8b-instruct for routing queries while also using a larger model, Mistral\u2019s mistralai/mistral-large-2-instruct with superior reasoning abilities to produce the final response.&nbsp;</p>\n\n\n\n<p>Another good choice for a high-performing, large model would be Meta\u2019s meta/llama-3.1-405b-instruct.</p>\n\n\n\n<p>A great advantage for using NIM microservices is that if you want to move to an on-premises or self-managed LLM inference deployment, there are no code changes required beyond setting the base_url parameter for the LLM creation. Otherwise, it\u2019s identical!&nbsp;</p>\n\n\n\n<p>You can toggle between the NVIDIA inference public-facing APIs documented at <a href=\"https://build.nvidia.com\">build.nvidia.com</a> or a self-managed Llama 3.1 deployments. This gives a great deal of flexibility to try several models for prototyping before deciding what type of NIM microservice that you want to manage and deploy yourselves.\u00a0</p>\n\n\n\n<h2 id=\"extra_features\"  class=\"wp-block-heading\">Extra features<a href=\"#extra_features\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>While some of these are beyond the scope of this post, here are a few more features that are easy to add on to enhance value:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Multimodal ingestion </strong>by using vision-language models (VLMs) to read tables, perform optical character recognition, and caption images. You can find many of these at <a href=\"https://build.nvidia.com/explore/vision\">Vision Language Models</a>.</li>\n\n\n\n<li><strong>User chat history </strong>with Chainlit\u2019s Postgres connector. To persist user conversations, you can supply PostgreSQL connection details to Chainlit using the functionality of <code>chainlit.data_layer</code>.</li>\n\n\n\n<li><strong>RAG reranking </strong>with the <a href=\"https://build.nvidia.com/nvidia/rerank-qa-mistral-4b\">NVIDIA Mistral-based reranker</a>.</li>\n\n\n\n<li><strong>Adding citations</strong> by prompting the LLM to use HTML styling to show hyperlinked citations with answers.</li>\n\n\n\n<li><strong>Error handling and timeout management</strong> to enhance reliability. While APIs like Perplexity are powerful for answering a broad range of queries, their execution time can be highly variable due to the complexity of the underlying components involved. Setting reasonable timeouts and gracefully recovering when such answers are not available rapidly is an important step towards a production-ready application.</li>\n</ul>\n\n\n\n<h2 id=\"explore_advanced_chat_functionality_with_the_nvidia_and_llamaindex_developer_contest\"  class=\"wp-block-heading\">Explore advanced chat functionality with the NVIDIA and LlamaIndex Developer Contest<a href=\"#explore_advanced_chat_functionality_with_the_nvidia_and_llamaindex_developer_contest\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>We hope this post has been a useful resource for you as you learn more about generative AI and the ways that NIM microservices and LlamaIndex <code>Workflow</code> events can be used together for the fast development of advanced chat functionality.&nbsp;</p>\n\n\n\n<p>If you\u2019re inspired by this project, consider participating in the <a href=\"https://developer.nvidia.com/llamaindex-developer-contest\">NVIDIA and LlamaIndex Developer Contest</a> to build your own AI-powered solutions for a chance to win cash prizes, an NVIDIA GeForce RTX 4080 SUPER GPU, <a href=\"https://www.nvidia.com/en-us/training/\">DLI</a> credits, and more.</p>\n\n\n\n<p>If you\u2019re interested in learning more or exploring similar applications, consider diving into the code here, or experiment with other similar reference applications from the <a href=\"https://github.com/NVIDIA/GenerativeAIExamples\">GenerativeAI Examples</a> repo on GitHub.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The rapid development of solutions using retrieval augmented generation (RAG) for question-and-answer LLM workflows has led to new types of system architectures. Our work at NVIDIA using AI for internal operations has led to several important findings for finding alignment between system capabilities and user expectations.&nbsp; We found that regardless of the intended scope or &hellip; <a href=\"https://developer.nvidia.com/blog/creating-rag-based-question-and-answer-llm-workflows-at-nvidia/\">Continued</a></p>\n", "protected": false}, "author": 2402, "featured_media": 90947, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1510181", "discourse_permalink": "https://forums.developer.nvidia.com/t/creating-rag-based-question-and-answer-llm-workflows-at-nvidia/311405", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [453, 3737, 3613], "coauthors": [4141, 2362], "class_list": ["post-90872", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "tag-featured", "tag-microservices", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["Consumer Internet"], "post_products": ["NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llamaindex-workflow-chat-app-featured-1.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-nDG", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90872"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2402"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90872"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90872/revisions"}], "predecessor-version": [{"id": 91681, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90872/revisions/91681"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90947"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90872"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90872"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90872"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90872"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90877, "date": "2024-10-28T08:30:00", "date_gmt": "2024-10-28T15:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90877"}, "modified": "2024-10-31T11:36:06", "modified_gmt": "2024-10-31T18:36:06", "slug": "supercharging-fraud-detection-in-financial-services-with-graph-neural-networks", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/supercharging-fraud-detection-in-financial-services-with-graph-neural-networks/", "title": {"rendered": "Supercharging Fraud Detection in Financial Services with Graph Neural Networks"}, "content": {"rendered": "\n<p>Fraud in financial services is a massive problem. According to <a href=\"https://verafin.com/nasdaq-verafin-global-financial-crime-report/\">NASDAQ</a>, in 2023, banks faced $442 billion in projected losses from payments, checks, and credit card fraud. It\u2019s not just about the money, though. Fraud can tarnish a company\u2019s reputation and frustrate customers when legitimate purchases are blocked. This is called a <em>false positive</em>. Unfortunately, these errors happen more often than you\u2019d think because traditional fraud detection methods simply aren\u2019t keeping up with how sophisticated fraud has become.</p>\n\n\n\n<p>This post focuses on credit card transaction fraud, one of the most prevalent forms of financial fraud. While other types of fraud, such as identity theft, account takeover, and money laundering, are also significant concerns, credit card fraud poses a unique challenge due to its high transaction volume and broad attack surface, making it a key target for fraudsters. Financial institutions are estimated to lose $43 billion by 2026 in annual credit card losses, according to <a href=\"https://nilsonreport.com/articles/card-fraud-losses-worldwide/\">Nilson</a>.&nbsp;</p>\n\n\n\n<p>Traditional fraud detection methods, which rely on rules-based systems, or statistical methods, are reactive and increasingly ineffective in identifying sophisticated fraudulent activities. As data volumes grow and fraud tactics evolve, financial institutions need more proactive, intelligent approaches to detect and prevent fraudulent transactions.&nbsp;</p>\n\n\n\n<p>AI offers essential tools for analyzing vast amounts of transactional data, identifying abnormal behaviors, and recognizing patterns that indicate fraud. But while steps have been taken to improve detection, even more advanced techniques are needed to improve accuracy, reduce false positives, and enhance operational efficiency in fraud detection.&nbsp;</p>\n\n\n\n<p>This post introduces an end-to-end AI workflow that uses graph neural networks (GNNs) offering a flexible, high-performance solution for fraud detection. It also walks through how you can get started with model building and inference with this fraud detection workflow.</p>\n\n\n\n<h2 id=\"graph_neural_networks_for_fraud_detection\"  class=\"wp-block-heading\">Graph neural networks for fraud detection<a href=\"#graph_neural_networks_for_fraud_detection\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Traditional machine learning (ML) models, such as <a href=\"https://www.nvidia.com/en-us/glossary/xgboost/\">XGBoost</a>, have been widely used for fraud detection and have proven effective at identifying anomalous behavior in individual transactions. However, fraud detection is rarely a problem of isolated events. Fraudsters operate within complex networks, often using connections between accounts and transactions to hide their activities. This is where GNNs come in.</p>\n\n\n\n<p>GNNs are designed to work with graph-structured data, making them particularly suited for fraud detection in financial services. Imagine each account, transaction, and device as a node within a network. Instead of analyzing individual transactions only, GNNs consider the connections between these nodes\u2014revealing patterns of suspicious activity across the network.&nbsp;</p>\n\n\n\n<p>For example, suppose an account has a relationship with known fraudulent entities or is similar to other high-risk entities, GNNs can pick up on that connection and flag it for further investigation, even if the account itself seems fine.</p>\n\n\n\n<p>Combining GNNs with XGBoost offers the best of both worlds:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Higher accuracy</strong>: GNNs don\u2019t just focus on individual transactions\u2014they consider how everything is connected, catching fraud that might otherwise go undetected.</li>\n\n\n\n<li><strong>Fewer false positives</strong>: With more context, GNNs help reduce false alarms, so legitimate transactions don\u2019t get flagged unnecessarily.</li>\n\n\n\n<li><strong>Better scalability</strong>: GNNs model building scales to handle massive networks of data efficiently. But combining GNNs with XGBoost real-time fraud detection (inference) is possible even at large scales.</li>\n\n\n\n<li><strong>Explainability</strong>: Combining GNNs with XGBoost provides the power of deep learning with the explainability of decision trees.</li>\n</ul>\n\n\n\n<h2 id=\"an_end-to-end_fraud_detection_ai_workflow_using_gnns\"  class=\"wp-block-heading\">An end-to-end fraud detection AI workflow using GNNs<a href=\"#an_end-to-end_fraud_detection_ai_workflow_using_gnns\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA has built an end-to-end fraud detection workflow that combines traditional ML with the power of GNNs. This process builds on a standard XGBoost approach but augments it with GNN embeddings to significantly boost accuracy. While exact numbers are confidential, even a small improvement\u2014such as 1%\u2014could translate into millions of dollars in savings, making GNNs a critical part of fraud detection systems.</p>\n\n\n\n<p>The general architecture includes two main parts: the model building step and the inference process, as shown in Figure 1.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"907\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow.png\" alt=\"Payment fraud detection AI workflow. Left to right: internet, data stream, model building, tagged data stream, data lake, periodic model building, inference.\" class=\"wp-image-90910\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow-300x136.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow-625x284.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow-179x81.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow-768x348.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow-1536x697.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow-645x293.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow-500x227.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow-160x73.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow-362x164.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow-242x110.png 242w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/payment-fraud-detection-ai-workflow-1024x465.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Payment fraud detection AI workflow, including inference and model building</em></em></figcaption></figure></div>\n\n\n<h3 id=\"model_building_with_gnns_and_xgboost\"  class=\"wp-block-heading\">Model building with GNNs and XGBoost<a href=\"#model_building_with_gnns_and_xgboost\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The process starts with the<strong> </strong>model building<strong> </strong>phase, since a model needs to be available for inference in the workflow above, where GNNs are used to create features (embeddings) that are fed into an XGBoost model (Figure 2).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"484\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection.png\" alt=\"Flowchart of the GNN training into XGBoost workflow; (left to right): data cleaning and prep, graph creation, feature store/graph store, GNN embeddings, XGBoost, model for deployment into NVIDIA Morpheus.\" class=\"wp-image-90912\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection-300x73.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection-625x151.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection-179x43.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection-768x186.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection-1536x372.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection-645x156.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection-500x121.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection-160x39.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection-362x88.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection-454x110.png 454w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/model-building-ai-workflow-payment-fraud-detection-1024x248.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. The model building portion of the AI workflow for payment fraud detection</em></em></figcaption></figure></div>\n\n\n<h4 class=\"wp-block-heading\"><strong>Step 1: Data preparation</strong>&nbsp;</h4>\n\n\n\n<p>Incoming transaction data is cleaned and prepared, typically using tools like <a href=\"https://developer.nvidia.com/rapids\">RAPIDS</a> for efficiency. Data preparation and feature engineering have a significant impact on the performance of model building. This step requires a detailed understanding of the data and could take multiple tries to get the best results.&nbsp;</p>\n\n\n\n<p>Once a script for data preparation has been created, it can be automated in the workflow. The data preparation process should be evaluated as new data is added or periodically as data grows. The next iteration of this workflow will leverage <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/apache-spark-3/\">NVIDIA RAPIDS Accelerator for Apache Spark</a> to accelerate the data processing piece of this workflow.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Step 2: Graph creation&nbsp;</h4>\n\n\n\n<p>For large datasets, typical of FSI, the graph creation process converts the prepared data into a Feature Store (tabular data) and a Graph Store (structural data). This enables better host and device memory usage and peak performance. The two stores are optimized for GNN frameworks like PyG (PyTorch Geometric) and DGL (Deep Graph Library). A key benefit of using this workflow is ensuring the stores are optimized for the selected GNN framework.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# load the edge data\nedge_data = cudf.read_csv(edge_path, header=None,\n\tnames=&#x5B;edge_src_col, edge_dst_col, edge_att_col], dtype=&#x5B;&#039;int32&#039;,&#039;int32&#039;,&#039;float&#039;])\n\n# convert to tensors\nnum_nodes = max(edge_data&#x5B;edge_src_col].max(), edge_data&#x5B; edge_dst_col].max()) + 1\nsrc_tensor = torch.as_tensor(edge_data&#x5B;edge_src_col], device=&#039;cuda&#039;)\ndst_tensor = torch.as_tensor(edge_data&#x5B;edge_dst_col], device=&#039;cuda&#039;)\n\n# save in a GraphStore\ngraph_store = cugraph_pyg.data.GraphStore()\ngraph_store&#x5B;(&quot;n&quot;, &quot;e&quot;, &quot;n&quot;), &quot;coo&quot;, False, (num_nodes, num_nodes)] = &#x5B;src_tensor, dst_tensor]\n\n...\n# load the features\nfeature_data = cudf.read_csv(feature_path)\n\n# convert to tensors\ncol_tensors = &#x5B;]\nfor c in feature_columns:\n\tt = torch.as_tensor(feature_data&#x5B;c].values, device=&#039;cuda&#039;)\n\tcol_tensors.append(t)\n\nx_feature_tensor = torch.stack(col_tensors).T\n\n\nfeature_store = cugraph_pyg.data.TensorDictFeatureStore()\nfeature_store&#x5B;&quot;node&quot;, &quot;x&quot;] = x_feature_tensor\nfeature_store&#x5B;&quot;node&quot;, &quot;y&quot;] = y_label_tensor\n</pre></div>\n\n\n<h4 class=\"wp-block-heading\">Step 3: GNN embedding generation</h4>\n\n\n\n<p>Rather than having the GNN produce a classification, the last layer of the GNN is extracted as embeddings. Those GNN embeddings are passed into XGBoost and a model is created. That model is then saved for use in inferencing.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ndef extract_embeddings(model, loader):\n\tmodel.eval()\n\tembeddings = &#x5B;]\n\tlabels = &#x5B;]\n\twith torch.no_grad():\n    \tfor batch in loader:\n        \tbatch_size = batch.batch_size\n        \thidden = model(batch.x&#x5B;:,:].to(torch.float32), batch.edge_index, return_hidden=True)&#x5B;:batch_size]\n        \tembeddings.append(hidden)  # Keep embeddings on GPU\n        \tlabels.append(batch.y&#x5B;:batch_size].view(-1).to(torch.long))\n    \n\tembeddings = torch.cat(embeddings, dim=0)  # Concatenate embeddings on GPU\n\tlabels = torch.cat(labels, dim=0)  # Concatenate labels on GPU\n\treturn embeddings, labels\n\n\n... in main code ....\n\n# Define the model\nmodel = GraphSAGE( ....)\n\nfor epoch in range(best_params&#x5B;&#039;num_epochs&#039;]):\n\ttrain_loss = train_gnn(model, train_loader, optimizer, criterion)\n\n...\n# Extract embeddings from the second-to-last layer and keep them on GPU\nembeddings, labels = extract_embeddings(model, train_loader)\n</pre></div>\n\n\n<p>By using GPU-accelerated versions of GNN frameworks\u2014such as cuGraph-pyg and cuGraph-dgl\u2014this workflow can handle large datasets with complex graph structures efficiently.</p>\n\n\n\n<h3 id=\"inference_for_real-time_fraud_detection\"  class=\"wp-block-heading\">Inference for real-time fraud detection<a href=\"#inference_for_real-time_fraud_detection\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Once the model is trained, it can be served for real-time fraud detection using <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a>, an open-source AI model-serving platform that streamlines and accelerates the deployment of AI inference workloads in production. NVIDIA Triton helps enterprises reduce the complexity of model-serving infrastructure, shorten the time needed to deploy new AI models in production, and increase AI inferencing and prediction capacity.&nbsp;</p>\n\n\n\n<p>The trained model can also be deployed using <a href=\"https://developer.nvidia.com/morpheus-cybersecurity\">NVIDIA Morpheus</a>, an open-source cybersecurity AI framework that enables developers to create optimized applications for filtering, processing, and classifying large volumes of streaming data. The <a href=\"https://github.com/nv-morpheus/MRC\">Morpheus Runtime Core (MRC)</a> orchestrating this workflow accelerates massive data processing and analysis and helps with inferencing by periodically triggering the process to build a new model.&nbsp;</p>\n\n\n\n<p>As shown in Figure 3, the inference process involves:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Transforming the raw input data using the same process used during model building (that is, training).</li>\n\n\n\n<li>Feeding the data into the GNN model to convert the transaction into an embedding. This is needed since the XGBoost model was trained on the embedding.</li>\n\n\n\n<li>Feeding the embeddings into the XGBoost model to predict if the transactions are fraudulent.</li>\n</ol>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"722\" height=\"160\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-inference-gnn-nvidia-triton.png\" alt=\"Inference workflow diagram; (left to right): data stream, data cleaning and prep, data to GNN embedding, NVIDIA Triton Inference Server.\" class=\"wp-image-90958\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-inference-gnn-nvidia-triton.png 722w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-inference-gnn-nvidia-triton-300x66.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-inference-gnn-nvidia-triton-625x139.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-inference-gnn-nvidia-triton-179x40.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-inference-gnn-nvidia-triton-645x143.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-inference-gnn-nvidia-triton-500x111.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-inference-gnn-nvidia-triton-160x35.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-inference-gnn-nvidia-triton-362x80.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-inference-gnn-nvidia-triton-496x110.png 496w\" sizes=\"(max-width: 722px) 100vw, 722px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. Inference workflow</em></em></figcaption></figure></div>\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Load GNN model for creating node embeddings\ngnn_model = torch.load(gnn_model_path)\ngnn_model.eval()  # Set the model to evaluation mode\n\n# Load xgboost model for node classification\nloaded_bst = xgb.Booster()\nloaded_bst.load_model(xgb_model_path)\n\n# Generate node embedding using the GNN model\ntransaction_embeddings = gnn_model(X.to(device), ....)\n\n# Convert embeddings to cuDF DataFrame\nembeddings_cudf = cudf.DataFrame(cp.from_dlpack(to_dlpack(embeddings)))\n\n# Create DMatrix for the test embeddings\ndtest = xgb.DMatrix(embeddings_cudf)\n\n# Predict using XGBoost on GPU\npreds = bst.predict(dtest)\n</pre></div>\n\n\n<p>By incorporating both GNNs and XGBoost, this AI workflow offers a flexible, high-performance solution for fraud detection. Enterprises can customize the configuration of GNNs and adjust model-building processes based on their unique needs, ensuring the system stays optimized over time.</p>\n\n\n\n<h2 id=\"ecosystem_using_the_ai_workflow_to_enhance_fraud_detection\"  class=\"wp-block-heading\">Ecosystem using the AI workflow to enhance fraud detection<a href=\"#ecosystem_using_the_ai_workflow_to_enhance_fraud_detection\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Amazon Web Services (AWS) is the first cloud service provider to integrate this <a href=\"https://www.youtube.com/watch?v=fZJLfruV-1w\">end-to-end fraud detection workflow</a> with their highly secure accelerated computing capabilities. With this simple workflow integration, developers who build fraud detection models can use NVIDIA RAPIDS within Amazon EMR for data processing, leverage RAPIDS and GNN libraries within Amazon SageMaker and Amazon EC2 services for model training. This integration flexibly scales low latency and high throughput predictions using NVIDIA Morpheus and NVIDIA Triton Inference Server through Amazon SageMaker or Amazon Elastic Kubernetes Service endpoint.</p>\n\n\n\n<p>As these efforts continue to develop, this workflow will be available across the NVIDIA partner ecosystem for enterprises and developers to prototype and take it to production through <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>.</p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>As fraud tactics evolve, traditional detection methods fall short. Combining XGBoost with GNNs offers a powerful solution\u2014boosting accuracy, reducing false positives, and improving real-time detection. This <a href=\"https://www.nvidia.com/en-us/ai-data-science/ai-workflows/fraud-detection/\">AI workflow</a> is designed to help enterprises stay ahead of sophisticated fraud attempts and adapt quickly to new threats.</p>\n\n\n\n<p>To learn more about using GNNs to transform your approach to fraud detection, check out the <a href=\"https://github.com/nv-morpheus/morpheus-experimental/tree/branch-25.02/ai-credit-fraud-workflow\">AI Credit Card Fraud Workflow</a>. You can also explore the NVIDIA LaunchPad lab <a href=\"https://www.nvidia.com/en-us/launchpad/data-science/deploy-a-fraud-detection-xgboost-model-using-triton/\">Deploy a Fraud Detection XGBoost Model with NVIDIA Triton</a> and the <a href=\"https://www.nvidia.com/en-us/use-cases/ai-for-fraud-detection/\">AI for Fraud Detection Use Case</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Fraud in financial services is a massive problem. According to NASDAQ, in 2023, banks faced $442 billion in projected losses from payments, checks, and credit card fraud. It\u2019s not just about the money, though. Fraud can tarnish a company\u2019s reputation and frustrate customers when legitimate purchases are blocked. This is called a false positive. Unfortunately, &hellip; <a href=\"https://developer.nvidia.com/blog/supercharging-fraud-detection-in-financial-services-with-graph-neural-networks/\">Continued</a></p>\n", "protected": false}, "author": 2033, "featured_media": 90918, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1510171", "discourse_permalink": "https://forums.developer.nvidia.com/t/supercharging-fraud-detection-in-financial-services-with-graph-neural-networks/311401", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 696], "tags": [453, 3550, 2127, 1177], "coauthors": [3738, 2092, 4139], "class_list": ["post-90877", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-cybersecurity", "category-data-science", "tag-featured", "tag-fraud-detection", "tag-morpheus", "tag-triton"], "acf": {"post_industry": ["Financial Services"], "post_products": ["AI Enterprise", "Morpheus", "RAPIDS", "Triton Inference Server"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/fraud-alert-mobile.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nDL", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90877"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2033"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90877"}], "version-history": [{"count": 13, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90877/revisions"}], "predecessor-version": [{"id": 91084, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90877/revisions/91084"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90918"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90877"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90877"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90877"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90877"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]