[{"id": 90672, "date": "2024-10-23T22:30:00", "date_gmt": "2024-10-24T05:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90672"}, "modified": "2024-11-12T10:43:31", "modified_gmt": "2024-11-12T18:43:31", "slug": "three-building-blocks-for-creating-ai-virtual-assistants-for-customer-service-with-an-nvidia-nim-agent-blueprint", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/three-building-blocks-for-creating-ai-virtual-assistants-for-customer-service-with-an-nvidia-nim-agent-blueprint/", "title": {"rendered": "Three Building Blocks for Creating AI Virtual Assistants for Customer Service with an NVIDIA AI Blueprint"}, "content": {"rendered": "\n<p>In today&#8217;s fast-paced business environment, providing exceptional customer service is no longer just a nice-to-have\u2014it&#8217;s a necessity. Whether addressing technical issues, resolving billing questions, or providing service updates, customers expect quick, accurate, and personalized responses at their convenience. However, achieving this level of service comes with significant challenges.&nbsp;</p>\n\n\n\n<p>Legacy approaches, such as static scripts or manual processes, often fall short when it comes to delivering personalized and real-time support. Additionally, many customer service operations rely on sensitive and fragmented data, which is subject to strict data governance and privacy regulations. With the rise of generative AI, companies aim to revolutionize customer service by enhancing operational efficiency, cutting costs, and maximizing ROI.&nbsp;</p>\n\n\n\n<p>Integrating AI into existing systems presents challenges related to transparency, accuracy, and security, which can impede adoption and disrupt workflows. To overcome these hurdles, companies are leveraging generative AI-powered virtual assistants to manage a wide range of tasks, ultimately improving response times and freeing up resources.</p>\n\n\n\n<p>This post outlines how developers can use the <a href=\"https://build.nvidia.com/nvidia/ai-virtual-assistant-for-customer-service\">NVIDIA AI Blueprint for AI virtual assistants</a> to scale operations with generative AI. By leveraging this information, including sample code, businesses can meet the growing demands for exceptional customer service while ensuring data integrity and governance. Whether improving existing systems or creating new ones, this blueprint empowers teams to meet customer needs with efficient and meaningful interactions.\u00a0</p>\n\n\n\n<h2 id=\"smarter_ai_virtual_assistants_with_an_ai_query_engine_using_retrieval-augmented_generation\"  class=\"wp-block-heading\">Smarter AI virtual assistants with an AI query engine using retrieval-augmented generation<a href=\"#smarter_ai_virtual_assistants_with_an_ai_query_engine_using_retrieval-augmented_generation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>When building an AI virtual assistant, it\u2019s important to align with the unique use case requirements, institutional knowledge, and needs of the organization. Traditional bots, however, often rely on rigid frameworks and outdated methods that struggle to meet the evolving demands of today\u2019s customer service landscape.&nbsp;</p>\n\n\n\n<p>Across every industry, AI-based assistants can be transformational. For example, telecommunications companies, and the majority of retail and service providers, can use AI virtual assistants to enhance customer experience by offering support 24 hours a day, 7 days a week while handling a wide range of customer queries in multiple languages and providing dynamic, personalized interactions that streamline troubleshooting and account management. This helps reduce wait times and ensures consistent service across diverse customer needs.</p>\n\n\n\n<p>Another example is within the healthcare insurance payor industry, where ensuring a positive member experience is critical. Virtual assistants enhance this experience by providing personalized support to members, addressing their claims, coverage inquiries, benefits, and payment issues, all while ensuring compliance with healthcare regulations. This also helps reduce the administrative burden on healthcare workers.</p>\n\n\n\n<p>With the NVIDIA AI platform, organizations can create an AI query engine that uses <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation\">retrieval-augmented generation (RAG)</a> to connect AI applications to enterprise data. The AI virtual assistant blueprint enables developers to quickly get started building solutions that provide enhanced customer experiences. It is built using the following <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM </a>microservices:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>NVIDIA NIM for LLM:</strong> Brings the power of state-of-the-art large language models (LLMs) to applications, providing unmatched natural language processing with remarkable efficiency.\n<ul class=\"wp-block-list\">\n<li><strong><a href=\"https://build.nvidia.com/meta/llama-3_1-70b-instruct\">Llama 3.1 70B Instruct NIM</a>:</strong> Powers complex conversations with superior contextual understanding, reasoning, and text generation.</li>\n</ul>\n</li>\n\n\n\n<li><strong><a href=\"https://build.nvidia.com/explore/retrieval\">NVIDIA NeMo</a> Retriever NIM:</strong> This collection provides easy access to state-of-the-art models that serve as foundational building blocks for RAG pipelines. These pipelines, when integrated into virtual assistant solutions, enable seamless access to enterprise data, unlocking institutional knowledge via fast, accurate, and scalable answers.\n<ul class=\"wp-block-list\">\n<li><strong>NeMo <a href=\"https://build.nvidia.com/nvidia/embed-qa-4\">Retriever Embedding NIM</a>:</strong> Boosts text question-answering retrieval performance, providing high-quality embeddings for the downstream virtual assistant.</li>\n\n\n\n<li><strong>NeMo <a href=\"https://build.nvidia.com/nvidia/rerank-qa-mistral-4b\">Retriever Reranking NIM</a>:</strong> Enhances the retrieval performance further with a fine-tuned reranker, finding the most relevant passages to provide as context when querying an LLM.&nbsp;</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>The blueprint is designed to integrate seamlessly with existing customer service applications without breaking information security mandates. Thanks to the portability of NVIDIA NIM, organizations can integrate data wherever it resides. By bringing generative AI to the data, this architecture enables AI virtual assistants to provide more personalized experiences tailored to each customer by leveraging their unique profiles, user interaction histories, and other relevant data.&nbsp;</p>\n\n\n\n<p>A blueprint is a starting point that can be customized for an enterprise\u2019s unique use case.\u00a0 For example, integrate other NIM microservices, such as the <a href=\"https://build.nvidia.com/nvidia/nemotron-4-mini-hindi-4b-instruct\">Nemotron 4 Hindi 4B Instruct</a>, to enable an AI virtual assistant to communicate in the local language. Other microservices can enable additional capabilities such as synthetic data generation and model fine-tuning to better align with your specific use case requirements. Give the AI virtual assistant a humanlike interface when connected to the digital human AI Blueprint.</p>\n\n\n\n<p>With the implementation of a RAG backend with proprietary data (both company and user profile and their specific data), the AI virtual assistant can engage in highly contextual conversations, addressing the specifics of each customer\u2019s needs in real-time. Additionally, the solution operates securely within your existing governance frameworks, ensuring compliance with privacy and security protocols especially when working with sensitive data.&nbsp;</p>\n\n\n\n<h2 id=\"three_building_blocks_for_creating_your_own_ai_virtual_assistant\"  class=\"wp-block-heading\">Three building blocks for creating your own AI virtual assistant<a href=\"#three_building_blocks_for_creating_your_own_ai_virtual_assistant\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>As a developer, you can build your own AI virtual assistant that retrieves the most relevant and up-to-date information, in real time, with ever-improving humanlike responses.&nbsp;Figure 1 shows the AI virtual assistant architecture diagram which includes three functional components.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1733\" height=\"1999\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-customer-service-nvidia-nim-agent-blueprint.png\" alt=\"Architecture diagram showing the AI virtual assistant workflow for customer service, with ingest (top), virtual assistant (middle), customer service operations (bottom).\n\" class=\"wp-image-90691\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-customer-service-nvidia-nim-agent-blueprint.png 1733w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-customer-service-nvidia-nim-agent-blueprint-260x300.png 260w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-customer-service-nvidia-nim-agent-blueprint-625x721.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-customer-service-nvidia-nim-agent-blueprint-100x115.png 100w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-customer-service-nvidia-nim-agent-blueprint-768x886.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-customer-service-nvidia-nim-agent-blueprint-1332x1536.png 1332w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-customer-service-nvidia-nim-agent-blueprint-645x744.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-customer-service-nvidia-nim-agent-blueprint-78x90.png 78w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-customer-service-nvidia-nim-agent-blueprint-362x418.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-customer-service-nvidia-nim-agent-blueprint-95x110.png 95w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-customer-service-nvidia-nim-agent-blueprint-1024x1181.png 1024w\" sizes=\"(max-width: 1733px) 100vw, 1733px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The NVIDIA AI Blueprint for AI virtual assistants</em></figcaption></figure>\n\n\n\n<h3 id=\"1_data_ingestion_and_retrieval_pipeline\"  class=\"wp-block-heading\">1. Data ingestion and retrieval pipeline<a href=\"#1_data_ingestion_and_retrieval_pipeline\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Pipeline administrators use the ingestion pipeline to load structured and unstructured data into the databases. Examples of structured data include customer profiles, order history, and order status. Unstructured data includes product manuals, the product catalog, and supporting material such as FAQ documents.</p>\n\n\n\n<h3 id=\"2_ai_agent\"  class=\"wp-block-heading\">2. AI agent<a href=\"#2_ai_agent\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The AI virtual assistant is the second functional component. Users interact with the virtual assistant through a user interface. An AI agent, implemented in the LangGraph agentic LLM programming framework, plans how to handle complex customer queries and solves recursively. The LangGraph agent uses the tool calling feature of the <a href=\"https://build.nvidia.com/meta/llama-3_1-70b-instruct\">Llama 3.1 70B Instruct NIM</a> to retrieve information from both the unstructured and structured data sources, then generates an accurate response.</p>\n\n\n\n<p>The AI agent also uses short-term and long-term memory functions to enable multi-turn conversation history. The active conversation queries and responses are embedded so they can be retrieved later in the conversation as additional context. This allows more human-like interactions and eliminates the need for customers to repeat information they\u2019ve already shared with the agent.</p>\n\n\n\n<p>Finally, at the end of the conversation, the AI agent summarizes the discussion along with a sentiment determination and stores the conversation history in the structured database. Subsequent interactions from the same user can be retrieved as additional context in future conversations. Call summarization and conversation history retrieval can reduce call time and improve customer experience. Sentiment determination can provide valuable insights to the customer service administrator regarding the agent&#8217;s effectiveness.</p>\n\n\n\n<h3 id=\"3_operations_pipeline\"  class=\"wp-block-heading\">3. Operations pipeline<a href=\"#3_operations_pipeline\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The customer operations pipeline is the third functional component of the overall solution. This pipeline provides important information and insight to the customer service operators. Administrators can use the operations pipeline to review chat history, user feedback, sentiment analysis data, and call summaries. The analytics microservice, which leverages the Llama 3.1 70B Instruct NIM, can be used to generate analytics such as average call time, time to resolution, and customer satisfaction. The analytics are also leveraged as user feedback to retrain the LLM models to improve accuracy.&nbsp;</p>\n\n\n\n<h2 id=\"get_to_production_with_nvidia_partners\"  class=\"wp-block-heading\">Get to production with NVIDIA partners<a href=\"#get_to_production_with_nvidia_partners\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA consulting partners are helping enterprises adopt world-class AI virtual assistants built using NVIDIA accelerated computing and <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise software</a>, which includes NeMo, NIM microservices, and AI Blueprints.\u00a0</p>\n\n\n\n<h3 id=\"accenture\"  class=\"wp-block-heading\">Accenture<a href=\"#accenture\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://newsroom.accenture.com/news/2024/accenture-and-nvidia-lead-enterprises-into-era-of-ai\">The Accenture AI Refinery</a> built on <a href=\"https://newsroom.accenture.com/news/2024/accenture-pioneers-custom-llama-llm-models-with-nvidia-ai-foundry\">NVIDIA AI Foundry</a> helps design autonomous, intent-driven customer interactions, enabling businesses to tailor the journey to the individual through innovative channels such as digital humans or interaction agents. Specific use cases can be tailored to meet the needs of each industry, for example, telco call centers, insurance policy advisors, pharmaceutical interactive agents or automotive dealer network agents.</p>\n\n\n\n<h3 id=\"deloitte\"  class=\"wp-block-heading\">Deloitte<a href=\"#deloitte\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Deloitte Frontline AI enhances the customer service experience with digital avatars and LLM agents built with NVIDIA AI Blueprints that are accelerated by NVIDIA technologies such as NVIDIA ACE, NVIDIA Omniverse, NVIDIA Riva, and NIM.</p>\n\n\n\n<h3 id=\"wipro\"  class=\"wp-block-heading\">Wipro<a href=\"#wipro\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Wipro Enterprise Generative AI (WeGA) Studio accelerates industry-specific use cases including contact center agents across healthcare, financial services, retail, and more.</p>\n\n\n\n<h3 id=\"tech_mahindra\"  class=\"wp-block-heading\">Tech Mahindra<a href=\"#tech_mahindra\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Tech Mahindra is leveraging the NVIDIA AI Blueprint for digital humans to build solutions for customer service. Using RAG and NVIDIA NeMo, the solution provides the ability for a trainee to stop an agent during a conversation by raising a hand to ask clarifying questions. The system is designed to connect with microservices on the backend with a refined learning management system) which can be deployed across many industry use cases.</p>\n\n\n\n<h3 id=\"infosys\"  class=\"wp-block-heading\">Infosys<a href=\"#infosys\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://www.infosys.com/products-and-platforms/cortex.html\">Infosys Cortex</a>, part of <a href=\"https://www.infosys.com/services/data-ai-topaz.html\">Infosys Topaz</a>, is an AI-driven customer engagement platform that integrates NVIDIA AI Blueprints and the NVIDIA NeMo, Riva, and ACE technologies for generative AI, speech AI, and digital human capabilities to deliver specialized and individualized, proactive, and on-demand assistance to every member of a customer service organization, consequently playing a pivotal role in enhancing customer experience, improving operational efficiency, and reducing costs.</p>\n\n\n\n<h3 id=\"tata_consultancy_services\"  class=\"wp-block-heading\">Tata Consultancy Services<a href=\"#tata_consultancy_services\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The Tata Consultancy Services (TCS) virtual agent, powered by NVIDIA NIM, and integrated with ServiceNow\u2019s IT Virtual Agent is designed to optimize IT and HR support. This solution uses prompt-tuning and RAG to improve response times, accuracy, and provide multi-turn conversational capabilities. Benefits include reduced service desk costs, fewer support tickets, enhanced knowledge utilization, faster deployment, and a better overall employee and customer experience.</p>\n\n\n\n<h3 id=\"quantiphi\"  class=\"wp-block-heading\">Quantiphi<a href=\"#quantiphi\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://quantiphi.com/quantiphi-empowers-enterprise-ai-transformation-with-nvidia-nim-agent-blueprints/\">Quantiphi</a> is integrating NVIDIA AI Blueprints into its conversational AI solutions to enhance customer service with lifelike digital avatars. These state-of-the-art avatars, powered by NVIDIA Tokkio and ACE technologies, <a href=\"https://www.nvidia.com/en-us/ai/#referrer=ai-subdomain\">NVIDIA NIM microservices</a> and <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo</a>, seamlessly integrate with existing enterprise applications, enhancing operations and customer experiences with increased realism. Fine-tuned NIM deployments for digital avatar workflows have proven to be highly cost-effective, reducing enterprise spending on tokens.</p>\n\n\n\n<h3 id=\"softserve\"  class=\"wp-block-heading\">SoftServe<a href=\"#softserve\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://www.softserveinc.com/en-us/our-partners/nvidia/digital-concierge\">SoftServe Digital Concierge</a>, accelerated by NVIDIA AI Blueprints and NVIDIA NIM microservices, uses NVIDIA ACE, NVIDIA Riva, and the NVIDIA Audio2Face NIM microservice to deliver a highly realistic virtual assistant. Thanks to the Character Creator tool, it delivers speech and facial expressions with remarkable accuracy and lifelike detail.</p>\n\n\n\n<p>With RAG capabilities from NVIDIA NeMo Retriever, SoftServe Digital Concierge can intelligently respond to customer queries by referencing context and delivering specific, up-to-date information. It simplifies complex queries into clear, concise answers and can also provide detailed explanations when needed.</p>\n\n\n\n<h3 id=\"exl\"  class=\"wp-block-heading\">EXL<a href=\"#exl\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>EXL&#8217;s Smart Agent Assist offering is a contact center AI solution leveraging NVIDIA Riva, NVIDIA NeMo, and NVIDIA NIM microservices. EXL plans to augment their solution using the NVIDIA AI Blueprint for AI virtual agents.</p>\n\n\n\n<p>This week at <a href=\"https://blogs.nvidia.com/blog/accelerating-india-ai-adoption\">NVIDIA AI Summit India</a>, NVIDIA consulting partners announced a collaboration with NVIDIA to transform India into a Front Office for AI. Using NVIDIA technologies, these consulting giants can help customers tailor the customer service agent blueprint to build unique virtual assistants using their preferred AI model\u2014including sovereign LLMs from India-based model makers\u2014and run it in production efficiently on the infrastructure of their choice.</p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To try the blueprint for free, and to see system requirements, navigate to the <a href=\"https://build.nvidia.com/nvidia/ai-virtual-assistant-for-customer-service\">Blueprint Card</a>.</p>\n\n\n\n<p>To start building applications using those microservices, visit the <a href=\"https://build.nvidia.com/explore/discover\">NVIDIA API catalog</a>. To <a href=\"https://build.nvidia.com/explore/discover?signin=true\">sign in</a>, you\u2019ll be prompted to enter a personal or business email address to access different options for building with NIM. For more information, see the <a href=\"https://forums.developer.nvidia.com/t/nim-faq/300317\">NVIDIA NIM FAQ</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In today&#8217;s fast-paced business environment, providing exceptional customer service is no longer just a nice-to-have\u2014it&#8217;s a necessity. Whether addressing technical issues, resolving billing questions, or providing service updates, customers expect quick, accurate, and personalized responses at their convenience. However, achieving this level of service comes with significant challenges.&nbsp; Legacy approaches, such as static scripts or &hellip; <a href=\"https://developer.nvidia.com/blog/three-building-blocks-for-creating-ai-virtual-assistants-for-customer-service-with-an-nvidia-nim-agent-blueprint/\">Continued</a></p>\n", "protected": false}, "author": 1784, "featured_media": 90688, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1507909", "discourse_permalink": "https://forums.developer.nvidia.com/t/three-building-blocks-for-creating-ai-virtual-assistants-for-customer-service-with-an-nvidia-nim-agent-blueprint/310951", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [3266, 453, 2932, 4134, 3613], "coauthors": [3395, 3966], "class_list": ["post-90672", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "tag-chatbot", "tag-featured", "tag-large-language-models", "tag-nim-agent-blueprint", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["General"], "post_products": ["NeMo", "NeMo Microservices", "NeMo Retriever", "NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-virtual-assistant-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nAs", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90672"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1784"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90672"}], "version-history": [{"count": 21, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90672/revisions"}], "predecessor-version": [{"id": 91777, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90672/revisions/91777"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90688"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90672"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90672"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90672"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90672"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90706, "date": "2024-10-23T14:00:00", "date_gmt": "2024-10-23T21:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90706"}, "modified": "2024-10-31T09:21:19", "modified_gmt": "2024-10-31T16:21:19", "slug": "accelerating-quantum-algorithms-for-solar-energy-prediction-with-nvidia-cuda-q-and-nvidia-cudnn", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-quantum-algorithms-for-solar-energy-prediction-with-nvidia-cuda-q-and-nvidia-cudnn/", "title": {"rendered": "Accelerating Quantum Algorithms for Solar Energy Prediction with NVIDIA CUDA-Q and NVIDIA cuDNN"}, "content": {"rendered": "\n<p>Improving sources of sustainable energy is a worldwide problem with environmental and economic security implications. Ying-Yi Hong, distinguished professor of Power Systems and Energy at Chung Yuan Christian University in Taiwan, researches hybrid quantum-classical&nbsp; methods. These approaches leverage <a href=\"https://www.nvidia.com/en-us/glossary/quantum-computing/\">quantum computing</a> to solve challenging problems in power systems and sustainable energy.&nbsp;</p>\n\n\n\n<p>Solar irradiance prediction is a key focus of Professor Hong\u2019s research group. The goal is to use geographical and historical data to forecast the power generation of photovoltaic farms, enabling power utilities to optimally schedule traditional fossil fuel-based power generation.</p>\n\n\n\n<p>Professor Hong and his student, Dylan Lopez have used the <a href=\"https://developer.nvidia.com/cuda-q\">NVIDIA CUDA-Q platform</a> to predict solar irradiance through calculations run by hybrid quantum neural networks (HQNNs). This work was recently published in the paper, <a href=\"https://ieeexplore.ieee.org/document/10703035\">Solar Irradiance Forecasting Using a Hybrid Quantum Neural Network: A Comparison on GPU-Based Workflow Development Platforms.</a></p>\n\n\n\n<p>This work on HQNN made use of CUDA-Q interoperability with the <a href=\"https://developer.nvidia.com/cudnn\">NVIDIA cuDNN</a> library to achieve a 2.7x model training speedup and a 3.4x reduction in test set error compared to other leading quantum simulators.</p>\n\n\n\n<h2 id=\"what_is_a_hybrid_quantum_neural_network\"  class=\"wp-block-heading\">What is a hybrid quantum neural network?<a href=\"#what_is_a_hybrid_quantum_neural_network\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Classical neural networks (NNs) are trainable <a href=\"https://www.nvidia.com/en-us/glossary/machine-learning/\">machine learning (ML)</a> models built from layers of mathematical operations that resemble the connectivity of neurons in the brain. Each layer is made up of neurons which are connected to neurons in adjacent layers through trainable weights. A standard NN consists of an input layer to receive the raw data, hidden layers that apply various transformations, and an output layer that produces a final prediction.\u00a0</p>\n\n\n\n<p>An NN is an ML model trained with a data set to find the optimal parameters that minimize a cost function. The trained model can then make predictions based on new data in a process known as <a href=\"https://developer.nvidia.com/topics/ai/ai-inference\">inference</a>. NNs have proved remarkably capable when modeling complex systems.</p>\n\n\n\n<p>An HQNN shares the same objective, but instead replaces one or more layers of the traditional NN with a parameterized <em>quantum</em> circuit within a so-called \u201cquantum layer.\u201d&nbsp; A quantum layer consists of a few important sublayers (Figure 1).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"293\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network-625x293.png\" alt=\"Diagram of a standard quantum layer within a hybrid quantum neural network showing encoding layer, parametric unitary, classical data, and more.\" class=\"wp-image-90712\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network-625x293.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network-300x141.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network-768x360.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network-1536x720.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network-645x302.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network-500x234.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network-160x75.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network-362x170.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network-235x110.png 235w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network-1024x480.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/standard-quantum-layer-in-hybrid-quantum-neural-network.png 1774w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. A standard quantum layer within a hybrid quantum neural network</em></figcaption></figure></div>\n\n\n<p>First, the input data is encoded into the quantum circuit with an encoding layer. Then, a set of parameterized single qubit gates act on each qubit. The structure of these gates is generally called an ansatz. Next, an entangling layer is applied with a cascade of controlled NOT (CNOT) gates. Finally, a quantum circuit is measured and the measurement results are either used to compute a cost function or are fed forward as inputs to another layer.\u00a0</p>\n\n\n\n<p>HQNNs are a promising approach because the unique properties of quantum entanglement allow the opportunity for a more expressive model that can capture complex patterns with fewer trainable parameters. However, many challenges remain, particularly regarding the best way to encode classical data into a quantum circuit.&nbsp;</p>\n\n\n\n<h2 id=\"a_cuda-q_hqnn_for_solar_irradiance\"  class=\"wp-block-heading\">A CUDA-Q HQNN for solar irradiance<a href=\"#a_cuda-q_hqnn_for_solar_irradiance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>HQNNs require CPUs, GPUs, and QPUs all working in concert (Figure 2). Data preprocessing takes place on a traditional CPU, GPUs run the classical layers of the HQNN, and the QPU runs the circuits that compose the quantum layers. Professor Hong and Dylan used the CUDA-Q development platform to construct and train an HQNN with data from the National Solar Radiation Database including a multitude of weather related features from across Taiwan.</p>\n\n\n\n<p>Figure 2 shows a typical HQNN workflow. Most of the workflow is accelerated with CUDA and additional acceleration is realized using the <a href=\"https://developer.nvidia.com/cudnn\">cuDNN</a> and <a href=\"https://developer.nvidia.com/cuquantum-sdk\">cuQuantum</a> libraries.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"2000\" height=\"547\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow.png\" alt=\"A typical HQNN workflow showing dataset preparation; classical, hybrid, quantum; and CPU-powered, GPU-powered sections.\" class=\"wp-image-90714\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow.png 2000w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow-300x82.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow-625x171.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow-179x49.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow-768x210.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow-1536x420.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow-645x176.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow-500x137.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow-160x44.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow-362x99.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow-402x110.png 402w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-workflow-1024x280.png 1024w\" sizes=\"(max-width: 2000px) 100vw, 2000px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. A typical HQNN workflow</em></figcaption></figure></div>\n\n\n<p>A classical NN was implemented in PyTorch, with the NN layers designed using Bayesian optimization as described in the <a href=\"https://ieeexplore.ieee.org/document/10703035\">Methodology section</a> of the paper. The resulting architecture served as the classical component of an HQNN, where a final dense layer was replaced with a quantum layer (Figure 3).&nbsp;</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"229\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-architecture-625x229.png\" alt=\"Diagram showing (left to right): locations, multidimensional input, quantum NN, and output vector.\" class=\"wp-image-90716\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-architecture-625x229.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-architecture-300x110.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-architecture-179x66.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-architecture-768x281.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-architecture-1536x562.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-architecture-645x236.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-architecture-500x183.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-architecture-160x59.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-architecture-362x133.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-architecture-1024x375.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hqnn-architecture.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. The HQNN is similar to the NN design with the final (magenta) layer replaced by a quantum layer. Both NNs process data with various weather features to generate corresponding predictions</em></figcaption></figure></div>\n\n\n\n<p>Working together, NVIDIA CUDA-Q, CUDA, and cuDNN tools were able to accelerate the whole workflow in this HQNN. CUDA-Q ensures acceleration of both the quantum and classical layers in the network, enabling quantum and classical resources to work together seamlessly. The PyTorch training is automatically accelerated with CUDA.</p>\n\n\n\n<p>Two NVIDIA libraries provide even further acceleration for specific tasks. cuDNN ensures highly efficient NN operations like convolution, while in cases where the quantum layers are simulated (rather than running on actual quantum hardware), cuQuantum accelerates all quantum circuit simulations.\u00a0\u00a0</p>\n\n\n\n<h2 id=\"cuda-q_improves_hqnn_speed_and_accuracy\"  class=\"wp-block-heading\">CUDA-Q improves HQNN speed and accuracy<a href=\"#cuda-q_improves_hqnn_speed_and_accuracy\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Professor Hong and Dylan trained their HQNN model to predict solar irradiance for all four seasons of the year using two NVIDIA RTX 3070 GPUs. They compared their results to a classical baseline and benchmarked the impact of different simulators and methods of accelerating the classical NN part of the hybrid workflow. The data suggests the importance of using GPU acceleration <em>and</em> CUDA-Q to realize the greatest performance gains.</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"227\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts-625x227.png\" alt=\"Two side-by-side graphs: average epoch latency (left) and HQNN test set error (right).\" class=\"wp-image-90718\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts-625x227.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts-300x109.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts-179x65.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts-768x278.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts-1536x557.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts-645x234.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts-500x181.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts-160x58.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts-362x131.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts-303x110.png 303w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts-1024x371.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cuda-q-optimization-charts.png 1876w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. CUDA-Q is optimized to leverage CUDA and other libraries like cuDNN for accelerating hybrid quantum-classical applications such as HQNNs</em></figcaption></figure></div>\n\n\n\n<p>The utility of the GPU is made clear for simulating both the quantum and the classical parts of an HQNN. Regardless of the simulator, GPU-accelerated quantum circuit simulations lowered the epoch latency (time for each training step) by at least 3x. The classical NN steps could also be accelerated with CUDA or CUDA plus cuDNN (Figure 4, left).</p>\n\n\n\n<p>CUDA-Q&nbsp; is uniquely optimized to take advantage of the GPU better than any other simulator. Compared to other leading GPU simulators, when CUDA and cuDNN accelerated the classical NN steps, CUDA-Q was 2.7x faster (Figure 4, left) and trained a model that was 3.4x more accurate (Figure 4, right) in terms of the test set RMSE.</p>\n\n\n\n<p>Professor Hong and Dylan were able to successfully predict the seasonal solar irradiance in Taiwan with competitive accuracy to classical approaches. Professor Hong noted that the outcomes of this study indicate that \u201cCUDA-Q provides a great means to stage hybrid quantum operations for energy research during the NISQ-era and beyond. Accelerating both the classical and quantum tasks allows us to explore best-case and worst-case solutions for integrating HPCs and quantum computers in solution pipelines.\u201d</p>\n\n\n\n<h2 id=\"get_started_with_cuda-q&nbsp;\"  class=\"wp-block-heading\">Get started with CUDA-Q&nbsp;<a href=\"#get_started_with_cuda-q&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://developer.nvidia.com/cuda-q\">CUDA-Q</a> is a platform for hybrid quantum-classical computing, not just a quantum simulator. CUDA-Q orchestrates all aspects of a hybrid CPU, GPU, and QPU workflow enabling acceleration of the quantum and classical components of the HQNN presented in this work. Code developed on the CUDA-Q platform has longevity and is designed to seamlessly scale as accelerated quantum computers scale to solve practical problems.</p>\n\n\n\n<p>To get started with CUDA-Q, check out the following resources:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Quickly set up your environment with the <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/quick_start.html\">CUDA-Q Quick Start guide</a>.\u202f&nbsp;</li>\n\n\n\n<li>Write your first CUDA-Q application using <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/basics/basics.html\">CUDA-Q Basics</a>.&nbsp;</li>\n\n\n\n<li>Learn more with the <a href=\"https://nvidia.github.io/cuda-quantum/latest/applications/python/hybrid_qnns.html\">CUDA-Q HQNN Tutorial</a>.</li>\n\n\n\n<li>Get inspiration for your own quantum application development with <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/examples/examples.html\">CUDA-Q By Example</a> and <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/tutorials.html\">CUDA-Q Tutorials</a>. \u202f\u00a0</li>\n\n\n\n<li>Visit the<a href=\"https://github.com/NVIDIA/cuda-quantum\"> NVIDIA/cuda-quantum</a> GitHub repo to provide feedback and suggestions.&nbsp;</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>Improving sources of sustainable energy is a worldwide problem with environmental and economic security implications. Ying-Yi Hong, distinguished professor of Power Systems and Energy at Chung Yuan Christian University in Taiwan, researches hybrid quantum-classical&nbsp; methods. These approaches leverage quantum computing to solve challenging problems in power systems and sustainable energy.&nbsp; Solar irradiance prediction is a &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-quantum-algorithms-for-solar-energy-prediction-with-nvidia-cuda-q-and-nvidia-cudnn/\">Continued</a></p>\n", "protected": false}, "author": 2390, "featured_media": 90831, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1507720", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-quantum-algorithms-for-solar-energy-prediction-with-nvidia-cuda-q-and-nvidia-cudnn/310923", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [503], "tags": [3941, 1937, 453, 2735], "coauthors": [4126, 4127, 3645, 4128], "class_list": ["post-90706", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-simulation-modeling-design", "tag-ai-impact", "tag-oil-and-gas", "tag-featured", "tag-quantum-computing"], "acf": {"post_industry": ["Energy", "HPC / Scientific Computing"], "post_products": ["CUDA-Q", "cuDNN"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/solar-panels-wind-turbines.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nB0", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90706"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2390"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90706"}], "version-history": [{"count": 21, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90706/revisions"}], "predecessor-version": [{"id": 90979, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90706/revisions/90979"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90831"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90706"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90706"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90706"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90706"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90780, "date": "2024-10-23T10:28:49", "date_gmt": "2024-10-23T17:28:49", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90780"}, "modified": "2024-10-31T09:21:20", "modified_gmt": "2024-10-31T16:21:20", "slug": "optimizing-drug-discovery-with-cuda-graphs-coroutines-and-gpu-workflows", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/optimizing-drug-discovery-with-cuda-graphs-coroutines-and-gpu-workflows/", "title": {"rendered": "Optimizing Drug Discovery with CUDA Graphs, Coroutines, and GPU Workflows"}, "content": {"rendered": "\n<p>Pharmaceutical research demands fast, efficient simulations to predict how molecules interact, speeding up drug discovery. Jiqun Tu, a senior developer technology engineer at NVIDIA, and Ellery Russell, tech lead for the Desmond engine at Schr\u00f6dinger, explore advanced GPU optimization techniques designed to accelerate molecular dynamics simulations. </p>\n\n\n\n<p>In this NVIDIA GTC 2024 session, they present practical strategies for improving workload efficiency and throughput, giving pharmaceutical researchers the tools to enhance computational drug discovery.\u00a0Building on existing CUDA workflows, they cover innovations such as CUDA Graphs, C++ coroutines, and mapped memory to overcome scaling challenges and bottlenecks.</p>\n\n\n\n<script src=\"https://api-prod.nvidia.com/search/nvidia-search-library.js\"></script>\n \n\n<div id=\"nvidia-event-details-widget\"></div>\n<style>\n.nvidia-search-widget .cleanslate , .nvidia-search-widget .player-overlay {\ndisplay:none;\n}\n</style>\n \n\n<script>\n \n NvidiaSearchLibrary.EventSessionDetailsWidget.mount({\n          site: 'https://www.nvidia.com',\n          language: 'en-us',\n          sessionId: 'gtc24-s61156',\n          jwtToken: '',\n \u2002\u2002\u2002\u2002voltronApiUrl:  'https://api-prod.nvidia.com/services/nod/api/v1/',\n          apiUrl: 'https://api-prod.nvidia.com/search/graphql',\n           onLogin: () => { },\n          onLogout: () => { },\n       \n          onSeeAllSessions: (speakerName) => {\n            window.location.href =  'https://www.nvidia.com/en-us/on-demand/search/?q=\"' + speakerName+'\"';\n          },\n          searchApiUrl: 'https://api-prod.nvidia.com/search/graphql',\n          searchToken: '',\n          uiConfId: '50468382',\n          showSessionRating: false,\n          anonToken: '',\n        });\n \n</script>\n\n\n\n<p>Follow along with a <a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/S61156.pdf\">PDF of the session</a>, which equips attendees with actionable techniques to optimize performance, minimize latency, and fully harness GPU capabilities for molecular simulations. Topics include:&nbsp;</p>\n\n\n\n<p><strong>CUDA Graphs:</strong> How grouping kernel launches into dependency trees reduces overhead and enables more efficient execution.&nbsp;&nbsp;</p>\n\n\n\n<p><strong>GPU throughput optimization:</strong> Focus on throughput by scheduling multiple independent simulations on the same GPU to mask serial bottlenecks.&nbsp;&nbsp;</p>\n\n\n\n<p><strong>Mapped memory:</strong> Using direct memory access between host and device to eliminate data transfer delays.&nbsp;&nbsp;</p>\n\n\n\n<p><strong>C++ coroutines: </strong>Strategies to overlap computations and yield control across multiple simulations, improving GPU utilization without complex code restructuring.&nbsp;&nbsp;</p>\n\n\n\n<p><strong>FEP+ and Desmond engine performance:</strong> Case studies on how these tools are used in Schr\u00f6dinger\u2019s molecular dynamics engine, achieving up to 2.02x speedup in key workloads.&nbsp;&nbsp;</p>\n\n\n\n<p>Watch the session <a href=\"http://Accelerating Drug Discovery: Optimizing Dynamic GPU Workflows with CUDA Graphs, Mapped Memory, C++ Coroutines, and More\">Accelerating Drug Discovery: Optimizing Dynamic GPU Workflows with CUDA Graphs, Mapped Memory, C++ Coroutines, and More</a>, explore more videos on NVIDIA On-Demand, and gain valuable skills and insights from industry experts by joining the&nbsp;<a href=\"https://developer.nvidia.com/developer-program\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Developer Program</a>.</p>\n\n\n\n<p><em>This content was partially crafted with the assistance of generative AI and LLMs. It underwent careful review and was edited by the NVIDIA Technical Blog team to ensure precision, accuracy, and quality.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Pharmaceutical research demands fast, efficient simulations to predict how molecules interact, speeding up drug discovery. Jiqun Tu, a senior developer technology engineer at NVIDIA, and Ellery Russell, tech lead for the Desmond engine at Schr\u00f6dinger, explore advanced GPU optimization techniques designed to accelerate molecular dynamics simulations. In this NVIDIA GTC 2024 session, they present practical &hellip; <a href=\"https://developer.nvidia.com/blog/optimizing-drug-discovery-with-cuda-graphs-coroutines-and-gpu-workflows/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 90794, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1507613", "discourse_permalink": "https://forums.developer.nvidia.com/t/optimizing-drug-discovery-with-cuda-graphs-coroutines-and-gpu-workflows/310916", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 503, 1903], "tags": [3941, 13, 2499, 2385, 453, 157, 3986], "coauthors": [2315], "class_list": ["post-90780", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-simulation-modeling-design", "category-features", "tag-ai-impact", "tag-c", "tag-cuda-graphs", "tag-drug-discovery", "tag-featured", "tag-molecular-dynamics", "tag-nvidia-on-demand"], "acf": {"post_industry": ["Healthcare & Life Sciences"], "post_products": ["General"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Video"], "post_collections": ["GTC March 2024"]}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/drug-discovery-GTC-2024.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nCc", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90780"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90780"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90780/revisions"}], "predecessor-version": [{"id": 90832, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90780/revisions/90832"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90794"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90780"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90780"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90780"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90780"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90646, "date": "2024-10-23T06:00:00", "date_gmt": "2024-10-23T13:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90646"}, "modified": "2024-10-31T09:21:21", "modified_gmt": "2024-10-31T16:21:21", "slug": "optimizing-the-cv-pipeline-in-automotive-vehicle-development-using-the-pva-engine", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/optimizing-the-cv-pipeline-in-automotive-vehicle-development-using-the-pva-engine/", "title": {"rendered": "Optimizing the CV Pipeline in Automotive Vehicle Development Using the PVA Engine"}, "content": {"rendered": "\n<p>In the field of automotive vehicle software development, more large-scale AI models are being integrated into autonomous vehicles. The models range from vision AI models to end-to-end AI models for autonomous driving.&nbsp; Now the demand for computing power is sharply increasing, leading to higher system loads that can have a negative impact on system stability and latency.</p>\n\n\n\n<p>To address these challenges, <a href=\"https://developer.nvidia.com/docs/drive/drive-os/6.0.10/public/drive-os-linux-sdk/common/topics/pva/ProgrammableVisionAccelerator20.html?hl=pva\">Programmable Vision Accelerator (PVA)</a>, a low-power and efficient hardware engine available on NVIDIA DRIVE SoCs, can be used to improve energy efficiency and overall system performance. By using PVA, tasks typically handled by the GPU or other hardware engines can be offloaded, thereby reducing their load and enabling them to manage other critical tasks more efficiently.</p>\n\n\n\n<p>In this post, we provide a brief introduction to the PVA hardware engine and the SDK on the DRIVE platform. We showcase the typical use cases of the PVA engine in the computer vision (CV) pipeline, including preprocessing, postprocessing, and other CV algorithms, highlighting its effectiveness and efficiency. Finally, as an example, we detail how NIO uses the NVIDIA PVA engine and the optimized algorithms)within its data pipeline to offload GPU or video image compositor (VIC) tasks and enhance the overall performance of the autonomous vehicle systems.</p>\n\n\n\n<h2 id=\"overview_of_pva_hardware\"  class=\"wp-block-heading\">Overview of PVA hardware<a href=\"#overview_of_pva_hardware\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The PVA engine is an advanced very long instruction word (VLIW), single instruction multiple data (SIMD) digital signal processor. It is optimized for the tasks of image processing and computer vision algorithm acceleration.&nbsp;</p>\n\n\n\n<p>PVA provides excellent performance with extremely low power consumption. PVA can be used asynchronously and concurrently with the CPU, GPU, and other accelerators on the DRIVE platform as part of a heterogeneous compute pipeline.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"934\" height=\"578\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pva-architecture.png\" alt=\"Chart shows the components of PVA, including fault control, debug control, and host control.\" class=\"wp-image-90746\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pva-architecture.png 934w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pva-architecture-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pva-architecture-625x387.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pva-architecture-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pva-architecture-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pva-architecture-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pva-architecture-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pva-architecture-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pva-architecture-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pva-architecture-178x110.png 178w\" sizes=\"(max-width: 934px) 100vw, 934px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. PVA hardware architecture</em></figcaption></figure></div>\n\n\n<p>There is one PVA instance in the CV cluster on NVIDIA Orin, which is a high-performance system-on-chip (SoC) designed for advanced AI applications, particularly in autonomous vehicles and robotics.&nbsp;</p>\n\n\n\n<p>In each PVA, there are two vector processing subsystems (VPS). Each VPS includes the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>1 vector processing unit (VPU) core</li>\n\n\n\n<li>1 decoupled look-up unit (DLUT)</li>\n\n\n\n<li>1 vector memory (VMEM)</li>\n\n\n\n<li>1 instruction cache (I-cache)</li>\n</ul>\n\n\n\n<p>The VPU core is the main processing unit. It is a vector SIMD VLIW DSP optimized for computer vision. It fetches instructions through the I-cache, and accesses data through the VMEM.</p>\n\n\n\n<p>The DLUT is a specialized hardware component developed to enhance the efficiency of parallel lookup operations. It enables parallel lookups using a single copy of the lookup table by executing these lookups in a decoupled pipeline, independent of the primary processor pipeline. By doing so, the DLUT minimizes memory usage and enhances throughput while avoiding data-dependent memory bank conflicts, ultimately leading to improved overall system performance.</p>\n\n\n\n<p>VPU VMEM provides local data storage for the VPU, enabling efficient implementation of various image processing and computer vision algorithms. It supports access from outside-VPS hosts such as DMA and R5, facilitating data exchange with R5 and other system-level components.</p>\n\n\n\n<p>The VPU I-cache supplies instruction data to the VPU when requested, requests missing instruction data from system memory, and maintains temporary instruction storage for the VPU.</p>\n\n\n\n<p>For each VPU task, R5 configures DMA, optionally prefetches the VPU program into VPU I-cache, and kicks off each VPU-DMA pair to process a task. Orin PVA also includes an L2 SRAM memory to be shared between the two sets of VPS and DMA.</p>\n\n\n\n<p>Two DMA devices are used to move data among external memory, PVA L2 memory, the two VMEMs (one in each VPS), R5 TCM (tightly coupled memory), DMA descriptor memory, and PVA-level config registers.</p>\n\n\n\n<p>In a lightly loaded system, two parallel DMA accesses to DRAM can achieve a read/write bandwidth of up to 15 GB/s each. In a heavily loaded system, this bandwidth can reach up to 10 GB/s each.</p>\n\n\n\n<p>Regarding computing capacities, the INT8 GMACs (Giga Multiply-Accumulate Operations per Second) is 2048, excluding the DLUT. The FP32 GMACs is 32 per PVA instance.</p>\n\n\n\n<h2 id=\"introduction_to_the_pva_sdk\"  class=\"wp-block-heading\">Introduction to the PVA SDK<a href=\"#introduction_to_the_pva_sdk\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Similar as the CUDA toolkit is to GPUs, the NVIDIA PVA SDK is designed for crafting computer vision algorithms that harness the PVA hardware&#8217;s capabilities. The PVA SDK provides runtime APIs, tools and tutorials for the development, deployment, and safety certification of CV and DL/ML algorithms. It offers a seamless build-to-deploy framework, enabling the cross-compilation of code into a binary executable on the Tegra PVA.</p>\n\n\n\n<p>The PVA SDK bolsters software development with a variety of resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>A comprehensive getting started guide</li>\n\n\n\n<li>An x86 native emulator that mimics a real VPU, enabling the development and functional debugging of VPU kernels on x86-64 platforms</li>\n\n\n\n<li>A full suite of code generation tools including an optimizing C/C++ compiler, debuggers, and integrated development environment</li>\n\n\n\n<li>Profiling utilities, such as NVIDIA Nsight Systems for visual performance analysis and APIs for detailed VPU code performance metrics</li>\n\n\n\n<li>Step-by-step tutorials introduce PVA concept by concept, ranging from basic examples to advanced optimizations for VPU, DMA, and interop with other Tegra engines</li>\n\n\n\n<li>Extensive documentation and reference manuals that provide detailed information on VPU intrinsics, enabling you to write optimized code while abstracting the complexities of DMA programming</li>\n</ul>\n\n\n\n<p>The PVA SDK provides numerous ready-to-use algorithms to support common computer vision use cases in autonomous driving and robotics. It enables you to use these algorithms by default (with access to the source code) in a production environment or use the PVA SDK features to develop custom algorithms.</p>\n\n\n\n<p>NVIDIA has predeveloped many algorithms based on the PVA SDK according to the common CV use cases. Use the PVA algorithms, with access to the code, in production or just use the different algorithms as a reference to develop your own valuable algorithms.</p>\n\n\n\n<h2 id=\"typical_pva_use_cases\"  class=\"wp-block-heading\">Typical PVA use cases<a href=\"#typical_pva_use_cases\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Many developers of autonomous vehicles are facing the challenge of insufficient computing resources on their SoCs, resulting in high loads on the CPU, GPU, VIC, and DLA. To address this issue, the PVA hardware is being considered for offloading processing tasks from these heavily used hardware engines on the SoC.&nbsp;</p>\n\n\n\n<p>Here are some examples of processing tasks that can be offloaded:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Image processing:</strong> Some image-processing and CV tasks can be ported to the PVA to offload GPU, CPU, VIC, and even DLA.</li>\n\n\n\n<li><strong>Deep learning operations: </strong>Within deep learning networks, certain layers or computationally intensive operators (such as ROI aligns) can be offloaded to the PVA. In specific cases, small deep-learning networks can be entirely ported to the PVA.</li>\n\n\n\n<li><strong>Math computation:</strong> The PVA, being a vector SIMD VLIW DSP, can handle math computations efficiently, such as matrix computation, FFT, and so on.</li>\n</ul>\n\n\n\n<p>The following two use cases are offered in detail for reference:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Offloading preprocessing and postprocessing in AI pipelines to PVA</li>\n\n\n\n<li>Moving pure CV or compute-bound pipelines to PVA</li>\n</ul>\n\n\n\n<h3 id=\"offloading_preprocessing_and_postprocessing_in_ai_pipelines_to_pva\"  class=\"wp-block-heading\">Offloading preprocessing and postprocessing in AI pipelines to PVA<a href=\"#offloading_preprocessing_and_postprocessing_in_ai_pipelines_to_pva\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1000\" height=\"61\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-inference-pipeline.png\" alt=\"Workflow diagram shows an image going through preprocessing, AI inference, and postprocessing.\" class=\"wp-image-90748\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-inference-pipeline.png 1000w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-inference-pipeline-300x18.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-inference-pipeline-625x38.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-inference-pipeline-179x11.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-inference-pipeline-768x47.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-inference-pipeline-645x39.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-inference-pipeline-500x31.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-inference-pipeline-160x10.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ai-inference-pipeline-362x22.png 362w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. AI inference pipeline</em></figcaption></figure></div>\n\n\n<p>This is a typical use case of the CV pipeline. The input images come from either a live camera in real-time scenarios or a decoder in offline scenarios. The pipeline consists of three stages:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Preprocessing</li>\n\n\n\n<li>AI inference</li>\n\n\n\n<li>Postprocessing</li>\n</ul>\n\n\n\n<p>The PVA hardware engine can play a crucial role in all stages of the CV pipeline, from preprocessing to postprocessing, ensuring efficient and effective handling of image processing and computer vision tasks.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Preprocessing</h4>\n\n\n\n<p>Preprocessing involves basic CV tasks to align or normalize the input for the model. This includes operations such as remapping (undistortion), cropping, resizing, and color conversion (from YUV to RGB).&nbsp;</p>\n\n\n\n<p>In some cases, when the images are from NVDEC (the decoder hardware engine on the Tegra SoC), the image layout is block linear. In that case, more steps are needed in the preprocessing stage to convert the block linear image to a pitch linear image.&nbsp;</p>\n\n\n\n<p>The PVA hardware engine is well-suited for these tasks. However, in memory-bound cases, consider merging adjacent PVA operations to fully use the PVA&#8217;s computational power.</p>\n\n\n\n<h4 class=\"wp-block-heading\">AI inference</h4>\n\n\n\n<p>AI inference performs the core CV tasks required by business needs based on the state-of-the-art AI models. This step can be executed on the GPU or DLA (<a href=\"https://developer.nvidia.com/deep-learning-accelerator\">Deep Learning Accelerator</a>) for better performance.&nbsp;</p>\n\n\n\n<p>The PVA runtime APIs support both NvSciSync and native CUDA streams, enabling efficient execution of heterogeneous pipelines involving GPU or DLA without incurring the latency associated with reverting to the CPU for scheduling.&nbsp;</p>\n\n\n\n<p>Depending on the use case, the AI model could be YOLO or R-CNN for object detection, logistic regression or <em>k</em>-nearest neighbor (KNN) for classification, and any other models, and so on.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Post-processing</h4>\n\n\n\n<p>Postprocessing refines the detection results. This may involve using a median filter to remove outliers, blending operations to fuse different candidates, or applying non-maximum suppression (NMS) to select the best target. The PVA hardware can effectively handle these tasks.</p>\n\n\n\n<h3 id=\"moving_pure_cv_or_compute-bound_pipelines_to_pva\"  class=\"wp-block-heading\">Moving pure CV or compute-bound pipelines to PVA<a href=\"#moving_pure_cv_or_compute-bound_pipelines_to_pva\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1000\" height=\"154\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tracker-pipeline.png\" alt=\"Workflow diagram shows an image going through the image pyramid, feature detector, and feature tracker and tracking information informing earlier steps. The output is tracked feature points or sparse optical flow.\" class=\"wp-image-90745\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tracker-pipeline.png 1000w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tracker-pipeline-300x46.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tracker-pipeline-625x96.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tracker-pipeline-179x28.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tracker-pipeline-768x118.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tracker-pipeline-645x99.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tracker-pipeline-500x77.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tracker-pipeline-160x25.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tracker-pipeline-362x56.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tracker-pipeline-714x110.png 714w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Tracker pipeline</em></figcaption></figure></div>\n\n\n<p>This is a more specific and complex use case, where all steps can be performed on the PVA. It primarily involves detecting and tracking feature points in input images or computing sparse optical flow in certain scenarios:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>An image pyramid extends the image along the scale space.&nbsp;</li>\n\n\n\n<li>A specific detection algorithm identifies feature points or corners in the image.&nbsp;</li>\n\n\n\n<li>A tracking algorithm tracks these feature points frame by frame.</li>\n</ul>\n\n\n\n<p>Compared to the previous use case, this scenario differs in key aspects:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Compute-bound processing</strong>: Each step of data processing is compute-bound and involves processing 2D images. These algorithms can be well-vectorized and executed highly efficiently on the PVA hardware. Most importantly, the computation capability of PVA is fully used.</li>\n\n\n\n<li><strong>Tightly coupled steps</strong>: There is an additional data loop that transfers tracking information back to previous steps to refine subsequent tracking results. This makes the steps more tightly coupled.</li>\n\n\n\n<li><strong>Pure CV pipeline</strong>: This use case is a pure computer vision pipeline that does not involve machine learning networks. Each step is predictable and explainable, focusing solely on traditional CV algorithms.</li>\n</ul>\n\n\n\n<p>By using PVA for these tasks, you can alleviate the load on the GPU, VIC, CPU, and DLA, leading to a more stable and efficient system.</p>\n\n\n\n<h2 id=\"nio\u2019s_data_pipeline_optimization\"  class=\"wp-block-heading\">NIO\u2019s data pipeline optimization<a href=\"#nio\u2019s_data_pipeline_optimization\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NIO Inc. is a prominent Chinese multinational automobile manufacturer, specializing in the design, development, and production of premium smart electric vehicles (EVs).&nbsp;</p>\n\n\n\n<p>The following data pipeline from NIO involves de-identifying, masking, or replacing interested regions and objects in video streams from live cameras or H.264 videos using specialized algorithms and techniques.</p>\n\n\n\n<h3 id=\"original_scheme_of_the_data_pipeline\"  class=\"wp-block-heading\">Original scheme of the data pipeline<a href=\"#original_scheme_of_the_data_pipeline\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1010\" height=\"268\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nio-data-pipeline.png\" alt=\"Workflow diagram shows a data pipeline that uses specialized algorithms and techniques.\" class=\"wp-image-90744\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nio-data-pipeline.png 1010w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nio-data-pipeline-300x80.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nio-data-pipeline-625x166.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nio-data-pipeline-179x47.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nio-data-pipeline-768x204.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nio-data-pipeline-645x171.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nio-data-pipeline-500x133.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nio-data-pipeline-160x42.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nio-data-pipeline-362x96.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nio-data-pipeline-415x110.png 415w\" sizes=\"(max-width: 1010px) 100vw, 1010px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. NIO data pipeline</em></figcaption></figure></div>\n\n\n<p>Figure 4 represents the original version of NIO\u2019s data pipeline. The NVDEC is used to decode H264 videos, producing block linear YUV images. As block linear is an internal format specific to NVIDIA, external users cannot process these images directly. The VIC engine is used to convert the block linear images to pitch linear format for further processing.</p>\n\n\n\n<p>Next, a color conversion (from YUV to RGB) is performed using the VIC engine to produce RGB images. These images are then analyzed by AI models to detect interested objects. After the AI model generates bounding boxes for the object, postprocessing steps using VIC or CUDA apply mosaics or masks to the original YUV pitch linear images.</p>\n\n\n\n<p>Finally, the processed frames are converted back from pitch linear to block linear format using the VIC engine and then encoded back to H264 videos with NVENC.</p>\n\n\n\n<h3 id=\"replacing_cv_operations_with_pva\"  class=\"wp-block-heading\">Replacing CV operations with PVA<a href=\"#replacing_cv_operations_with_pva\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In NIO\u2019s case, both the GPU and VIC are heavily loaded. According to this pipeline, several CV operators are involved, including the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Layout conversion between the block linear and pitch linear formats</li>\n\n\n\n<li>Color conversion from YUV to RGB</li>\n\n\n\n<li>Mosaicing and masking</li>\n</ul>\n\n\n\n<p>These operators can be offloaded to the PVA to save resources on the GPU and VIC.</p>\n\n\n\n<p>Both layout conversion and color conversion are memory-bound tasks for the PVA, with DMA bandwidth being the bottleneck. Other computing resources in the PVA might be used for mosaicing or masking based on the bounding box and the YUV PL image.&nbsp;</p>\n\n\n\n<p>To further accelerate execution, you can also run PVA algorithms in parallel, as each PVA instance contains two VPUs and each VPU has a standalone DMA controller for data exchange with DRAM.</p>\n\n\n\n<p>Several other techniques can also be employed when implementing PVA kernels to enhance overall performance. These include the DLUT, hardware-based loop address generation (AGEN), ping-pong buffers, and loop unrolling.</p>\n\n\n\n<h3 id=\"data_pipeline_optimization\"  class=\"wp-block-heading\">Data pipeline optimization<a href=\"#data_pipeline_optimization\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In a traditional data processing pipeline, latency may arise from two sources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Overhead of copying data between different function modules or hardware accelerators,&nbsp; such as PVA and DLA in this use case.</li>\n\n\n\n<li>Additional synchronization overhead required for performing and synchronizing multiple algorithmic processes.</li>\n</ul>\n\n\n\n<p>These overheads can be reduced by using the<a href=\"https://developer.nvidia.com/docs/drive/drive-os/6.0.8.1/public/drive-os-linux-sdk/common/topics/nvsci/NvStreams1.html\"> NvStreams</a> framework provided by the NVIDIA DriveOS SDK. The PVA hardware accelerator can work efficiently with NvStreams using the NvSci interoperability APIs in the PVA SDK to achieve zero-copy data transitions and asynchronous task submission to minimize the overhead.</p>\n\n\n\n<h3 id=\"zero-copy_interface\"  class=\"wp-block-heading\">Zero-copy interface<a href=\"#zero-copy_interface\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Different hardware components (for example, PVA and CPU) and applications have their own access restrictions or requirements for memory buffers. To achieve the goal of zero-copy, a unified memory architecture enables accelerators and different applications to share the same physical memory on the NVIDIA DRIVE SoC.&nbsp;</p>\n\n\n\n<p>Before allocating the memory buffer, detailed requirements are collected and reconciled to ensure the allocated memory buffer is shareable across necessary modules. This feature is realized through the NvStreams APIs.&nbsp;</p>\n\n\n\n<p>After the shareable memory buffer is successfully allocated, data transitions between different hardware modules or applications can work in a zero-copy manner. This solution applies to scenarios involving inter-process communication (IPC) or cross-virtual machines (VMs). For data transfers between chips, high-speed PCIe can be used under the same NvStreams framework.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"688\" height=\"404\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-drive-soc-orin.png\" alt=\"Chart shows components of the NVIDIA DRIVE SoC, including CPU clusters, CPU Switch Fabric, System Control Fabric, PVA, DLA, System MMU and arbitration, and the GPU module.\" class=\"wp-image-90743\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-drive-soc-orin.png 688w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-drive-soc-orin-300x176.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-drive-soc-orin-625x367.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-drive-soc-orin-179x105.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-drive-soc-orin-645x379.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-drive-soc-orin-500x294.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-drive-soc-orin-153x90.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-drive-soc-orin-362x213.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-drive-soc-orin-187x110.png 187w\" sizes=\"(max-width: 688px) 100vw, 688px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. NVIDIA DRIVE SoC (Orin) architecture</em></figcaption></figure></div>\n\n\n<h3 id=\"hardware_accelerator-based_scheduling\"  class=\"wp-block-heading\">Hardware accelerator-based scheduling<a href=\"#hardware_accelerator-based_scheduling\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>NIO&#8217;s data pipeline incorporates multiple hardware accelerators and you can use NvSciSync to manage synchronization between these engines. NvSciSync, part of the <a href=\"https://docs.nvidia.com/drive/drive_os_5.1.6.1L/nvvib_docs/index.html#page/DRIVE_OS_Linux_SDK_Development_Guide/Graphics/nvsci.html\">NVIDIA NvStreams</a> library, coordinates the execution of operations across different hardware components by managing synchronization objects.</p>\n\n\n\n<p>First, insert the sync points between tasks running on the accelerators. When a task begins, subsequent hardware accelerators wait at the sync point until the preceding tasks are completed. When a task finishes, its corresponding hardware accelerator releases the sync point, automatically triggering the next accelerator to proceed with its task. This process minimizes CPU involvement, requiring only some initial setup, and ensures efficient synchronization across hardware engines.</p>\n\n\n\n<h3 id=\"pva_task_level\u2013based_scheduling\"  class=\"wp-block-heading\">PVA task level\u2013based scheduling<a href=\"#pva_task_level\u2013based_scheduling\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In the original pipeline, all task submissions and synchronizations are controlled by the CPU one by one. That means the CPU submits the tasks to the computing engines and then waits synchronously on each of the algorithm tasks to be completed.&nbsp;</p>\n\n\n\n<p>The PVA supports submitting multiple tasks at one time and waiting only on the last task. All submitted PVA tasks are computed in the specified orders until all tasks are completed. Submitting multiple tasks in a batch optimizes performance by reducing the CPU load associated with PVA task submissions. This frees up the CPU for other valuable tasks and decreases overall system latency.</p>\n\n\n\n<p>Using the PVA SDK, you can also specify scheduling strategies for PVA algorithms to fully leverage the two VPUs on a PVA instance. For example, you can specify that certain algorithms be performed on a single VPU.&nbsp;</p>\n\n\n\n<p>When using both VPUs, if there are sequential requirements between tasks, you can set the tasks to be performed one after the other on the two VPUs. If there are no sequential requirements, PVA tasks are performed as soon as the VPUs are free. This significantly reduces the multiple-task execution latency.</p>\n\n\n\n<h3 id=\"production_readiness\"  class=\"wp-block-heading\">Production readiness<a href=\"#production_readiness\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Figure 6 shows the production-ready NIO data pipeline after replacing the CV operations with PVA and porting the DL model to the DLA engine. For more information, see <a href=\"https://developer.nvidia.com/blog/deploying-yolov5-on-nvidia-jetson-orin-with-cudla-quantization-aware-training-to-inference/\">Deploying YOLOv5 on NVIDIA Jetson Orin with cuDLA: Quantization-Aware Training to Inference</a><em>.</em></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1001\" height=\"157\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-for-production.png\" alt=\"Workflow diagram shows a block linear image going through preprocessing with PVA, DL inference with DLA, and postprocessing (mosaic or masking) with PVA. Input is decoded with NVDEC and output is encoded with NVENC.\" class=\"wp-image-90742\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-for-production.png 1001w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-for-production-300x47.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-for-production-625x98.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-for-production-179x28.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-for-production-768x120.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-for-production-645x101.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-for-production-500x78.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-for-production-160x25.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-for-production-362x57.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-for-production-701x110.png 701w\" sizes=\"(max-width: 1001px) 100vw, 1001px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Data pipeline for production</em></figcaption></figure></div>\n\n\n<p>In this optimized pipeline, the PVA and DLA solution effectively meets the business requirements. This approach is feasible and highly efficient. As a result, overall GPU resource usage is reduced by 10%, and the VIC engine is freed up for other high-priority tasks within the system. There is no need to pre-allocate extra memory for temporary variables during block linear and pitch linear conversions, leading to significant memory savings.</p>\n\n\n\n<p>According to the NIO internal review, the PVA operates at about 50% load on one VPU instance when running this pipeline in the system. As one PVA contains two VPUs, the total load of PVA is about 25% in the NIO data pipeline. This shows that the PVA still has available computational capacity for additional tasks within this pipeline.</p>\n\n\n\n<h2 id=\"further_optimization\"  class=\"wp-block-heading\">Further optimization<a href=\"#further_optimization\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1000\" height=\"202\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/further-optimization.png\" alt=\"Workflow diagram shows that the image may merge into one single PVA kernel to save DMA BW based on workload through preprocessing, DL inference, and postprocessing.\" class=\"wp-image-90740\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/further-optimization.png 1000w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/further-optimization-300x61.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/further-optimization-625x126.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/further-optimization-179x36.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/further-optimization-768x155.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/further-optimization-645x130.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/further-optimization-500x101.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/further-optimization-160x32.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/further-optimization-362x73.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/further-optimization-545x110.png 545w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. Data pipeline for further optimization</em></figcaption></figure></div>\n\n\n<p>To further optimize this pipeline, you can take one or both of the following steps:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Use the PVA to replace the DLA with a simple deep learning model, as the PVA is currently only about 25% utilized. Internal tests have shown that a Yolo-Fastest network can be successfully ported to the PVA, detecting objects as expected.</li>\n\n\n\n<li>Consider merging the preprocessing, deep learning inference, and postprocessing stages into one single PVA kernel to reduce overall DMA bandwidth by eliminating the need for additional DMA transmissions between kernels.</li>\n</ul>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The PVA-based optimization solution significantly improves the performance of the NIO pipeline and is extensively used in NIO&#8217;s mass production vehicles. By offloading tasks to the PVA, GPU computational resources can be freed up, enabling an increase in deep learning TOPs and enabling you to implement more complex deep learning networks.&nbsp;</p>\n\n\n\n<p>NIO is actively developing more efficient algorithms on the PVA using the PVA SDK to leverage the additional computational capabilities of the NVIDIA DRIVE platform, thereby enhancing the intelligence and competitiveness of their products.</p>\n\n\n\n<p>In conclusion, the PVA provides powerful tools for addressing computations in autonomous vehicle development, enabling more efficient and effective processing of complex visual tasks and improving overall system performance.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In the field of automotive vehicle software development, more large-scale AI models are being integrated into autonomous vehicles. The models range from vision AI models to end-to-end AI models for autonomous driving.&nbsp; Now the demand for computing power is sharply increasing, leading to higher system loads that can have a negative impact on system stability &hellip; <a href=\"https://developer.nvidia.com/blog/optimizing-the-cv-pipeline-in-automotive-vehicle-development-using-the-pva-engine/\">Continued</a></p>\n", "protected": false}, "author": 2395, "featured_media": 90736, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1507507", "discourse_permalink": "https://forums.developer.nvidia.com/t/optimizing-the-cv-pipeline-in-automotive-vehicle-development-using-the-pva-engine/310896", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724], "tags": [2892, 453], "coauthors": [4130, 4129, 4135], "class_list": ["post-90646", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "tag-autonomous-vehicles", "tag-featured"], "acf": {"post_industry": ["Automotive / Transportation"], "post_products": ["DRIVE", "NvSci"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nio-av-development-pva-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nA2", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90646"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2395"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90646"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90646/revisions"}], "predecessor-version": [{"id": 90824, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90646/revisions/90824"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90736"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90646"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90646"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90646"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90646"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90753, "date": "2024-10-22T11:00:00", "date_gmt": "2024-10-22T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90753"}, "modified": "2024-10-31T09:21:22", "modified_gmt": "2024-10-31T16:21:22", "slug": "networkx-introduces-zero-code-change-acceleration-using-nvidia-cugraph", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/networkx-introduces-zero-code-change-acceleration-using-nvidia-cugraph/", "title": {"rendered": "NetworkX Introduces Zero Code Change Acceleration Using NVIDIA cuGraph"}, "content": {"rendered": "\n<p>NetworkX accelerated by NVIDIA cuGraph is a newly released backend co-developed with the NetworkX team. NVIDIA cuGraph provides GPU acceleration for popular graph algorithms such as PageRank, Louvain, and betweenness centrality. Depending on the algorithm and graph size, it can significantly accelerate NetworkX workflows, up to 50x, even 500x over NetworkX on CPU.&nbsp;</p>\n\n\n\n<p>In this post, we share details about NetworkX and its new backend accelerator powered by cuGraph. We also discuss how to use the cuGraph backend for NetworkX, speed up your graph analytic workflows, and overcome the limitations NetworkX presents when working with large-scale graphs.&nbsp;</p>\n\n\n\n<p>If you&#8217;re even remotely familiar with the field of graph data science, then you&#8217;ve probably used NetworkX.&nbsp; NetworkX is one of the most widely known open-source graph analytics libraries today and has over 80M downloads a month (as of 10/24). Since its initial release in 2005, NetworkX has built a reputation for its easy-to-use, intuitive API, friendly and supportive community, and wide breadth of algorithms that address almost every graph analytics use case imaginable.</p>\n\n\n\n<p>However, if you\u2019re responsible for production workflows operating on real-world<a>\u2013</a>sized data, then you&#8217;ve probably realized that when a graph reaches a certain size, NetworkX must be replaced with a more performant but also more complex graph solution.&nbsp;</p>\n\n\n\n<p>NetworkX\u2019s pure-Python, single-threaded implementation restricts its ability to scale. Many workflows using graphs typically larger than 100K nodes and 1M edges can result in significant slowdowns that often justify abandoning NetworkX altogether. These workloads are common with high data volumes and comparisons between millions of entities and relationships, as done in fraud detection, recommendation systems, social network analysis, supply chain optimization, and others.</p>\n\n\n\n<p>Since its initial release in 2019, the cuGraph open-source project has been building solutions for data scientists looking for an easy-to-use Python graph analytics library similar to NetworkX. Now, with the addition of the cuGraph backend for NetworkX, which is the first production-ready GPU backend, you don\u2019t have to leave NetworkX to experience material speedups.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/3EsbU1gcH5c?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Achieve up to 500x Faster NetworkX with Zero Code Changes Using NVIDIA cuGraph</em></figcaption></figure>\n\n\n\n<h2 id=\"unlocking_speed_and_scale\"  class=\"wp-block-heading\">Unlocking speed and scale<a href=\"#unlocking_speed_and_scale\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The backend for NetworkX is powered by cuGraph, a CUDA-based library, and enables NetworkX workflows running cuGraph-supported algorithms to see a range of speedups depending on the algorithm and the graph size.</p>\n\n\n\n<p>You can see up to 10x, and for other algorithms and larger graphs up to 500x faster processing. The resulting speedups not only generate results faster but unlock use cases previously considered unrealistic for NetworkX.&nbsp;&nbsp;</p>\n\n\n\n<p>Now graph data scientists can finally have both ease of use and speed at scale using NetworkX accelerated by NVIDIA cuGraph.</p>\n\n\n\n<h2 id=\"zero-code-change_gpu_acceleration\"  class=\"wp-block-heading\">Zero-code-change GPU acceleration<a href=\"#zero-code-change_gpu_acceleration\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA cuGraph accelerates NetworkX through its backend dispatching system, which provides a standard mechanism for NetworkX to use third-party accelerators.&nbsp;</p>\n\n\n\n<p>Install the cuGraph backend <code>nx-cugraph</code> using the <code>pip</code> or <code>conda</code><strong> </strong>package managers or from the source on a system with an NVIDIA GPU. The backend is activated through an environment variable that enables NetworkX automatic function dispatching, enabling NetworkX to send supported algorithms to the GPU backend and all others to the default NetworkX implementation on the CPU.</p>\n\n\n\n<p>The result is automatic GPU acceleration for many of the most common graph algorithms used by data scientists without code changes, and without workarounds for missing functionality:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><code>pagerank</code></li>\n\n\n\n<li><code>betweenness_centrality</code></li>\n\n\n\n<li><code>louvain_communities</code></li>\n\n\n\n<li><code>shortest_path</code></li>\n\n\n\n<li>And many more, approximately 60 in total</li>\n</ul>\n\n\n\n<p>You don\u2019t need to modify your workflows to see massive speedups on large graph data that would normally make unaccelerated NetworkX virtually unusable. Those same workflows can also be used on non-GPU systems, where smaller data and longer runtimes can be tolerated for prototyping and experimentation.</p>\n\n\n\n<p>From a command line, you can experience GPU speedup by setting an environment variable in your shell, where the same algorithm call runs nearly 87x faster when cuGraph is enabled.</p>\n\n\n\n<p>File: demo.ipy</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nimport pandas as pd\nimport networkx as nx\n\nurl = &quot;https://data.rapids.ai/cugraph/datasets/cit-Patents.csv&quot;\ndf = pd.read_csv(url, sep=&quot; &quot;, names=&#x5B;&quot;src&quot;, &quot;dst&quot;], dtype=&quot;int32&quot;)\nG = nx.from_pandas_edgelist(df, source=&quot;src&quot;, target=&quot;dst&quot;)\n\n%time result = nx.betweenness_centrality(G, k=10)\n</pre></div>\n\n\n<p>Output:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nuser@machine:/# ipython demo.ipy\nCPU times: user 7min 36s, sys: 5.22 s, total: 7min 41s\nWall time: 7min 41s\n\nuser@machine:/# NX_CUGRAPH_AUTOCONFIG=True ipython demo.ipy\nCPU times: user 4.14 s, sys: 1.13 s, total: 5.27 s\nWall time: 5.32 s\n</pre></div>\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Software:</strong> NetworkX 3.4.1, cuGraph/nx-cugraph 24.10</li>\n\n\n\n<li><strong>CPU:</strong> Intel Xeon Gold 6128 CPU @ 3.40GHz 45GB RAM</li>\n\n\n\n<li><strong>GPU:</strong> NVIDIA Quadro RTX 8000 50GB RAM</li>\n</ul>\n\n\n\n<p>In a notebook, set the environment variable using the <code>%env</code> magic anywhere before the initial <code>networkx</code> import:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n%env NX_CURGAPH_AUTOCONFIG=True\nimport networkx as nx\n</pre></div>\n\n\n<h2 id=\"graph_workloads_big_and_small\"  class=\"wp-block-heading\">Graph workloads big and small<a href=\"#graph_workloads_big_and_small\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The flexibility of NetworkX is unmatched among other graph analytics libraries, and the resulting performance is acceptable for smaller graph data used for teaching, prototyping, and generally smaller problems.&nbsp;</p>\n\n\n\n<p>But graphs exceeding 100K nodes and 1M edges are becoming much more common as the volume of data generated continues to grow, with <a href=\"https://www.idc.com/getdoc.jsp?containerId=US50554523\">enterprises expected to produce 20 ZB of data by 2027</a>.&nbsp; The cuGraph backend for NetworkX lets you have both ease of use for rapid development and the performance needed for modern workloads.</p>\n\n\n\n<p>High volumes, along with multimodal data in modern workloads can be used by graphs to enrich the context and depth of analysis. Use cases where NetworkX accelerated by cuGraph is an ideal choice, involve large-scale graphs and fast processing requirements, outside the bounds of NetworkX on CPUs.&nbsp;</p>\n\n\n\n<p>Graph algorithms boosted by GPU acceleration can be applied to use cases such as community or fraud detection systems or recommendation systems based on millions of consumers and their relationships to tens of millions of consumer profiles, transaction patterns, and inventory data points.</p>\n\n\n\n<p>The following examples show the speedups using NetworkX accelerated by cuGraph.</p>\n\n\n\n<p>Louvain community detection run on a large network graph of Hollywood actors to find those appearing in movies together (1M nodes, 58M edges) is 60x faster than NetworkX on CPU.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"431\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection-1024x431.png\" alt=\"Horizontal bar chart showing Louvain community detection algorithm run on a large network graph of Hollywood actors to find those appearing in movies together (1M nodes, 58M edges) is 60x faster than NetworkX on CPU.\" class=\"wp-image-90763\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection-1024x431.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection-300x126.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection-625x263.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection-179x75.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection-768x323.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection-1536x646.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection-645x271.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection-500x210.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection-160x67.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection-362x152.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection-262x110.png 262w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/louvain-community-detection.png 1586w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Louvain community detection algorithm run on a large network graph of Hollywood actors to find those appearing in movies together</em></figcaption></figure></div>\n\n\n<p>PageRank algorithm used to compute values for a citation graph of U.S. patents (4M nodes, 16M edges) is 70x faster than NetworkX on CPU</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"484\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pagerank-for-patents-2-1024x484.png\" alt=\"Horizontal bar chart showing PageRank algorithm used to compute values for a citation graph of U.S. patents (4M nodes, 16M edges) is 70x faster than NetworkX on CPU.\u00a0\" class=\"wp-image-90762\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pagerank-for-patents-2-1024x484.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pagerank-for-patents-2-300x142.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pagerank-for-patents-2-625x295.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pagerank-for-patents-2-179x85.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pagerank-for-patents-2-768x363.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pagerank-for-patents-2-645x305.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pagerank-for-patents-2-500x236.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pagerank-for-patents-2-160x76.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pagerank-for-patents-2-362x171.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pagerank-for-patents-2-233x110.png 233w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pagerank-for-patents-2.png 1461w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. PageRank algorithm used to compute values for a citation graph of U.S. patents</em></figcaption></figure></div>\n\n\n<p>Betweenness-centrality algorithm used to compute values for the Live Journal social network (5M nodes, 69M edges, k=100) is 485x faster than NetworkX on CPU.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"466\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/betweenness-centrality-1024x466.png\" alt=\"Horizontal bar chart showing betweenness centrality algorithm used to compute values for the Live Journal social network (5M nodes, 69M edges, k=100) is 485x faster than NetworkX on CPU.\" class=\"wp-image-90761\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/betweenness-centrality-1024x466.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/betweenness-centrality-300x137.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/betweenness-centrality-625x284.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/betweenness-centrality-179x81.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/betweenness-centrality-768x350.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/betweenness-centrality-645x294.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/betweenness-centrality-500x228.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/betweenness-centrality-160x73.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/betweenness-centrality-362x165.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/betweenness-centrality-242x110.png 242w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/betweenness-centrality.png 1483w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Betweenness centrality algorithm used to compute values for the Live Journal social network</em></figcaption></figure></div>\n\n\n<p>All benchmarks in this post used the following configuration:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Software:</strong> NetworkX 3.4.1, cuGraph/nx-cugraph 24.10</li>\n\n\n\n<li><strong>CPU:</strong> Intel Xeon w9-3495X (56 cores) 250GB</li>\n\n\n\n<li><strong>GPU:</strong> NVIDIA A100 80-GB</li>\n</ul>\n\n\n\n<p>For more information about the code for the benchmarks, see the <a href=\"https://github.com/rapidsai/cugraph/tree/HEAD/benchmarks/nx-cugraph/pytest-based\">/rapidsai/cugraph</a> GitHub repo.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>While NetworkX has earned a reputation for being the most popular graph analytics library available, you still need performance that keeps pace with modern workflows, especially as they get more complex and datasets continue to grow rapidly.</p>\n\n\n\n<p>With a single environment variable, you can use the cuGraph/nx-cugraph package to accelerate NetworkX up to 500x using NVIDIA cuGraph to tackle modern workloads without leaving the flexibility and ease of use of NetworkX.</p>\n\n\n\n<p>Experience it for yourself now in the <a href=\"https://nvda.ws/4drM4re\">NetworkX &#8211; Easy Graph Analytics</a> test notebook hosted on Google Colab or explore the <a href=\"https://github.com/rapidsai-community/showcase/blob/main/accelerated_data_processing_examples/nxcg_wikipedia_e2e.ipynb\">Running Pagerank on Wikipedia with vs. without nx-cugraph</a> end-to-end demo.</p>\n\n\n\n<p>For more information, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://docs.rapids.ai/api/cugraph/stable/nx_cugraph/\">RAPIDS<strong> </strong>cuGraph</a></li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s61674/\">Accelerating NetworkX: The Future of Easy Graph Analytics</a> (GTC session)</li>\n\n\n\n<li><a href=\"https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+T-DS-03+V1\">Accelerate Data Science Workflows with Zero Code Changes</a> (DLI self-paced course)</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-gb/training/instructor-led-workshops/fundamentals-of-accelerated-data-science/\">Fundamentals of Accelerated Data Science</a> (DLI instructor-led workshop)</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>NetworkX accelerated by NVIDIA cuGraph is a newly released backend co-developed with the NetworkX team. NVIDIA cuGraph provides GPU acceleration for popular graph algorithms such as PageRank, Louvain, and betweenness centrality. Depending on the algorithm and graph size, it can significantly accelerate NetworkX workflows, up to 50x, even 500x over NetworkX on CPU.&nbsp; In this &hellip; <a href=\"https://developer.nvidia.com/blog/networkx-introduces-zero-code-change-acceleration-using-nvidia-cugraph/\">Continued</a></p>\n", "protected": false}, "author": 1908, "featured_media": 90770, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1506421", "discourse_permalink": "https://forums.developer.nvidia.com/t/networkx-introduces-zero-code-change-acceleration-using-nvidia-cugraph/310793", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [3564, 453], "coauthors": [3597, 4131], "class_list": ["post-90753", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "tag-ai-networking", "tag-featured"], "acf": {"post_industry": ["General"], "post_products": ["cuGraph", "RAPIDS"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/rapids-wholegraph-24-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nBL", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90753"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1908"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90753"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90753/revisions"}], "predecessor-version": [{"id": 90967, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90753/revisions/90967"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90770"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90753"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90753"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90753"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90753"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90412, "date": "2024-10-22T09:53:55", "date_gmt": "2024-10-22T16:53:55", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90412"}, "modified": "2024-10-31T11:39:29", "modified_gmt": "2024-10-31T18:39:29", "slug": "scaling-llms-with-nvidia-triton-and-nvidia-tensorrt-llm-using-kubernetes", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/scaling-llms-with-nvidia-triton-and-nvidia-tensorrt-llm-using-kubernetes/", "title": {"rendered": "Scaling LLMs with NVIDIA Triton and NVIDIA TensorRT-LLM Using Kubernetes"}, "content": {"rendered": "\n<p><a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">Large language models (LLMs)</a> have been widely used for chatbots, content generation, summarization, classification, translation, and more. State-of-the-art LLMs and&nbsp; foundation models, such as <a href=\"https://llama.meta.com/\">Llama</a>, <a href=\"https://ai.google.dev/gemma\">Gemma</a>, <a href=\"https://platform.openai.com/docs/models\">GPT</a>, and <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/nemotron/index.html\">Nemotron</a>, have demonstrated human-like understanding and generative abilities. Thanks to these models, AI developers do not need to go through the expensive and time consuming training process from scratch.&nbsp;</p>\n\n\n\n<p>Techniques such as <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation (RAG)</a>, prompt running, and fine-tuning can be applied to customize foundation models and achieve higher accuracy for specific tasks in much shorter time. The customized models can be deployed in production quickly and serve inference requests for a wide range of use cases.</p>\n\n\n\n<p>This post provides step-by-step instructions on how to optimize LLMs using <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">NVIDIA TensorRT-LLM</a>, deploy the optimized models with <a href=\"https://github.com/triton-inference-server\">NVIDIA Triton Inference Server</a>, and autoscale LLM deployment in a Kubernetes environment.&nbsp;</p>\n\n\n\n<p>NVIDIA TensorRT-LLM is an easy-to-use Python API that defines and optimizes LLMs. NVIDIA Triton Inference Server is an open-source inference serving software that supports multiple frameworks and hardware platforms. TensorRT-LLM provides multiple optimizations such as kernel fusion, quantization, in-flight batch, and paged attention, so that inference using the optimized models can be performed efficiently on <a href=\"https://nvidia.github.io/TensorRT-LLM/performance/perf-overview.html\">NVIDIA GPUs</a>.&nbsp;</p>\n\n\n\n<p>Triton Inference Server supports multiple deep learning and machine learning frameworks, including <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a>, TensorFlow, PyTorch, and ONNX. It supports multiple query types, including real time, batched, ensembles, or streaming. It can run across cloud, data center, edge and embedded devices on NVIDIA GPUs, x86 and ARM CPUs. As a developer, you can first build TensorRT engines that contain the model optimizations, then deploy the optimized models with Triton in production.&nbsp;</p>\n\n\n\n<p>You can scale up the deployment of the optimized LLMs from a single GPU to multiple GPUs with Kubernetes, to handle thousands of real-time inference requests with low latency and high accuracy, and scale down the number of GPUs when the volume of inference requests decreases. This is especially useful for enterprises, such as online shopping and call centers, when handling different volumes of inference requests during both peak and non-peak hours with flexibility, while benefiting from the reduced total cost compared to purchasing the maximum number of hardware resources to handle the peak workloads.</p>\n\n\n\n<p>Triton metrics are scraped by Prometheus and sent to Horizontal Pod Autoscaler (HPA) to make decisions on scaling up or down the number of deployment and GPUs based on the amount of the inference requests. To see the code and steps for this optimization and deployment, visit <a href=\"https://github.com/triton-inference-server/tutorials/tree/main/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing\">triton-inference-server / tutorials</a> on GitHub.&nbsp;</p>\n\n\n\n<h2 id=\"hardware_and_software_requirements\"  class=\"wp-block-heading\">Hardware and software requirements<a href=\"#hardware_and_software_requirements\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>For optimizing and deploying your models, you need to have NVIDIA GPUs that support TensorRT-LLM and Triton Inference Server. It is recommended to use the new generations of NVIDIA GPUs. You can find the list of supported GPUs in the hardware section of the <a href=\"https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html\">support matrix</a> for TensorRT-LLM. You can also deploy your models on public cloud compute instances with the appropriate GPU resources such as <a href=\"https://aws.amazon.com/eks/\">AWS EKS</a>, <a href=\"https://azure.microsoft.com/en-us/products/kubernetes-service\">Azure AKS</a>, <a href=\"https://cloud.google.com/kubernetes-engine\">GCP GKE</a>, or <a href=\"https://www.oracle.com/cloud/cloud-native/kubernetes-engine/\">OCI OKE</a>.</p>\n\n\n\n<p><a href=\"https://kubernetes.io/\">Kubernetes</a> can be used to scale your deployment automatically to handle large-scale real-time inference requests with low latency. To enable Kubernetes to discover which nodes have GPUs and make them available to containers running on those nodes, install the <a href=\"https://github.com/kubernetes-sigs/node-feature-discovery\">Kubernetes node feature discovery service</a>, <a href=\"https://github.com/NVIDIA/k8s-device-plugin\">NVIDIA device plugin for Kubernetes</a>, <a href=\"https://github.com/NVIDIA/gpu-feature-discovery\">GPU Feature Discovery service</a>, and <a href=\"https://github.com/NVIDIA/dcgm-exporter\">NVIDIA DCGM Exporter</a>. You also need to install <a href=\"https://prometheus.io/\">Prometheus</a> to collect metrics for autoscaling. <a href=\"https://github.com/triton-inference-server/tutorials/tree/main/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing#cluster-setup\">See detailed installation steps</a>.&nbsp;</p>\n\n\n\n<h2 id=\"optimize_llms_with_tensorrt-llm&nbsp;\"  class=\"wp-block-heading\">Optimize LLMs with TensorRT-LLM&nbsp;<a href=\"#optimize_llms_with_tensorrt-llm&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main/tensorrt_llm/models\">TensorRT-LLM supports a variety of state-of-the-art models</a>. You can <a href=\"https://huggingface.co/docs/hub/en/security-tokens\">download the model checkpoints</a> from Hugging Face, then use TensorRT-LLM to build engines that contain the model optimizations. To download the LLMs, you will need an <a href=\"https://huggingface.co/docs/hub/en/security-tokens\">access token</a>. You can then create a <a href=\"https://github.com/triton-inference-server/tutorials/tree/main/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing#hugging-face-authorization\">Kubenetes secret</a> with the access token, which will be used in a later step of Kubernetes Deployment to download the models.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: yaml; title: ; notranslate\" title=\"\">\n$ kubectl create secret generic hf-model-pull &#039;--from-literal=password=&lt;HF_access_token&gt;&#039;\n</pre></div>\n\n\n<p>To understand more about how TensorRT-LLM works, explore <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples\">examples</a> of how to build the engines of the popular models with optimizations to get better performance, for example, adding <code>gpt_attention_plugin</code>, <code>paged_kv_cache</code>, <code>gemm_plugin</code>, <code>quantization</code>.&nbsp;</p>\n\n\n\n<p>To generate TensorRT engine files, you can use the Docker container image of Triton Inference Server with TensorRT-LLM provided on <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags\">NVIDIA GPU Cloud (NGC)</a>. To pull the container image from NGC, you need to generate an <a href=\"https://org.ngc.nvidia.com/setup/api-key\">API key</a> on NGC that enables you access to the NGC containers. Next, log in to NGC using the API key to pull the container image.&nbsp;</p>\n\n\n\n<p>After pulling the NGC image of Triton with TensorRT-LLM\u2014for example, the base image nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\u2014generate TensorRT-LLM engine files referencing the <a href=\"https://github.com/triton-inference-server/tutorials/tree/ibhosale_update/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing#model-preparation-steps\">Model Preparation Steps</a>. Based on the model size and GPU memory size, you can configure TP tensor parallelism (TP) and pipeline parallelism (PP). Note that you need a minimum number of GPUs, TP*PP, for generating the engine files.&nbsp;</p>\n\n\n\n<p>Create a custom Triton-TensorRT-LLM image following the <a href=\"https://github.com/triton-inference-server/tutorials/tree/main/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing#custom-container-image\">Custom Container Image</a> steps and scripts. After building the custom image, you can push it to a repository that your cluster can access. To pull this customized image from a private registry during deployment, you need to create a Kubernetes <a href=\"https://github.com/triton-inference-server/tutorials/tree/main/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing#kubernetes-pull-secrets\">docker-registry secret</a> with the API key and let Kubernetes use the secret to pull the image from the private registry.&nbsp;</p>\n\n\n\n<p>During deployment, the generated TensorRT engine and plan files are stored in the host node and remapped to all the Kubernetes Pods on the same node. This eliminates the need to generate the same files if more Pods are scaled up later.&nbsp;</p>\n\n\n\n<h2 id=\"autoscale_deployment_of_llms_with_kubernetes\"  class=\"wp-block-heading\">Autoscale deployment of LLMs with Kubernetes<a href=\"#autoscale_deployment_of_llms_with_kubernetes\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>After optimizing your LLMs with TensorRT-LLM, you can deploy the models using Triton and autoscale the deployment with Kubernetes. Three main steps are required to deploy LLMs for AI inference:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Create a&nbsp;Kubernetes&nbsp;Deployment for Triton servers</li>\n\n\n\n<li>Create a Kubernetes Service to expose the Triton servers as a network service</li>\n\n\n\n<li>Autoscale the Deployment using Horizontal Pod Autoscaler (HPA) based on Triton metrics scraped by Prometheus</li>\n</ul>\n\n\n\n<h3 id=\"helm_chart_for_llm_deployment\"  class=\"wp-block-heading\">Helm chart for LLM deployment<a href=\"#helm_chart_for_llm_deployment\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>You can use a Helm chart for deployment, as it is easy to modify and deploy across different environments. To find the Helm chart, see <a href=\"https://github.com/triton-inference-server/tutorials/tree/main/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing\">Autoscaling and Load Balancing Generative AI with Triton Server and TensorRT-LLM</a>. Inside the <a href=\"https://github.com/triton-inference-server/tutorials/tree/main/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing/chart\">chart directory</a>, Helm expects the files as below:</p>\n\n\n\n<p><code>chart.yaml</code> holds all the information about the chart you are packaging; for example, version number and name.</p>\n\n\n\n<p><code>values.yaml</code> defines all the values you want to inject into your templates directory, including the supported GPUs, LLMs, the container image of Triton, image pull secrets, and so on. You can create a custom values file that is specific to your model, custom image, and GPU type to overwrite <code>values.yaml</code>. The following example is a <code>gpt2_values.yaml</code><em> </em>for deploying a GPT-2 model on an NVIDIA A10G GPU.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: yaml; title: ; notranslate\" title=\"\">\ngpu:\n- NVIDIA-A10G\n\nmodel:\n  name: gpt2\n  tensorrtLlm:\n    parallelism:\n      tensor: 1\n\ntriton:\n  image:\n    pullSecrets:\n    -name: ngc-container-pull\n    name: &lt;your custom image&gt;\n</pre></div>\n\n\n<p>If you have a larger model that does not fit on a single GPU, you can configure TP based on the model and GPU size. For example, you can set <code>model.tensorrtLlm.parallelism.tensor</code> to 2 for a model that needs two GPUs, and each Kubernetes Pod has two GPUs in Deployment.</p>\n\n\n\n<h3 id=\"create_a_kubernetes_deployment&nbsp;\"  class=\"wp-block-heading\">Create a Kubernetes Deployment&nbsp;<a href=\"#create_a_kubernetes_deployment&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>A <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\">Kubernetes&nbsp;Deployment</a>&nbsp;provides declarative updates for&nbsp;<a href=\"https://kubernetes.io/docs/concepts/workloads/pods/\">Kubernetes Pods</a>&nbsp;and&nbsp;<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/\">ReplicaSets</a>. The <a href=\"https://github.com/triton-inference-server/tutorials/blob/main/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing/chart/templates/deployment.yaml\"><code>deployment.yaml</code></a> creates a set of replicated Pods for Triton servers. You can specify how many Pods to start your deployment with, indicated by the&nbsp;<code>.spec.replicas</code>&nbsp;field.&nbsp;</p>\n\n\n\n<p>The <code>.spec.containers</code> field tells each Kubernetes Pod to have a Triton server container running on a GPU. Port numbers 8000 and 8001 are specified for Triton servers to receive inference requests from clients by HPPP and GRPC, respectively. Port 8002 is for collecting <a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/metrics.md\">Triton metrics</a>. You may change the <code>.resources.ephemeral-storage</code> field to adapt to the size of your model; for example, a GPT-2 model can fit in 24 GB memory of NVIDIA A10G GPU.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: yaml; title: ; notranslate\" title=\"\">\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  &#x5B;\u2026]\nspec:\n  selector:\n  &#x5B;\u2026]\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: {{ $.Release.Name }}\n        app.kubernetes.io/component: server\n\u2026\u2026\nspec:\n      &#x5B;\u2026]\n      containers:\n        - name: triton\n          &#x5B;...]\n          image: {{ $image_name }}\n          imagePullPolicy: IfNotPresent\n          &#x5B;...]\n          ports:\n          - containerPort: 8000\n            name: http\n          - containerPort: 8001\n            name: grpc\n          - containerPort: 8002\n            name: metrics\n</pre></div>\n\n\n<h3 id=\"create_a_kubernetes_service&nbsp;\"  class=\"wp-block-heading\">Create a Kubernetes Service&nbsp;<a href=\"#create_a_kubernetes_service&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>A <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Kubernetes Service</a> is an abstract way to expose an application running on a set of&nbsp;Pods&nbsp;as a network service. The <a href=\"https://github.com/triton-inference-server/tutorials/blob/main/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing/chart/templates/service.yaml\"><code>service.yaml</code></a> exposes Triton servers as a network service, so the servers are ready to receive inference requests from the clients.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: yaml; title: ; notranslate\" title=\"\">\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ $.Release.Name }}\n  labels:\n    app: {{ $.Release.Name }}\n    app.kubernetes.io/component: service\n\u2026\u2026\nspec:\n  ports:\n  - name: http\n    port: 8000\n    targetPort: http\n  - name: grpc\n    port: 8001\n    targetPort: grpc\n  - name: metrics\n    port: 8002\n    targetPort: metrics\n  selector:\n    app: {{ $.Release.Name }}\n  type: ClusterIP\n</pre></div>\n\n\n<h3 id=\"autoscale_llm_deployment&nbsp;\"  class=\"wp-block-heading\">Autoscale LLM deployment&nbsp;<a href=\"#autoscale_llm_deployment&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To autoscale the application, you need to monitor the application\u2019s performance by examining the containers, Pods, Services. You can use either a <a href=\"https://docs.openshift.com/container-platform/4.12/rest_api/monitoring_apis/podmonitor-monitoring-coreos-com-v1.html\">PodMonitor</a> or a ServiceMonitor to monitor Kubernetes Pods or Service for target discovery by Prometheus. Kube-Prometheus can help deploy Prometheus and link Prometheus to metric endpoints. You can expose NVIDIA Triton metrics to Prometheus using PodMonitor and Kube-Prometheus.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1208\" height=\"435\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/example-kubernetes-deployment-nvidia-triton.png\" alt=\"On the left, kube-prometheus set up the collection targets for Prometheus and link Prometheus to metrics endpoints. In the middle, Prometheus scrapes the Triton metrics from Pods at port number 8002. On the right, four Triton servers running on four GPUs.\n\" class=\"wp-image-90427\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/example-kubernetes-deployment-nvidia-triton.png 1208w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/example-kubernetes-deployment-nvidia-triton-300x108.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/example-kubernetes-deployment-nvidia-triton-625x225.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/example-kubernetes-deployment-nvidia-triton-179x64.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/example-kubernetes-deployment-nvidia-triton-768x277.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/example-kubernetes-deployment-nvidia-triton-645x232.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/example-kubernetes-deployment-nvidia-triton-500x180.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/example-kubernetes-deployment-nvidia-triton-160x58.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/example-kubernetes-deployment-nvidia-triton-362x130.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/example-kubernetes-deployment-nvidia-triton-305x110.png 305w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/example-kubernetes-deployment-nvidia-triton-1024x369.png 1024w\" sizes=\"(max-width: 1208px) 100vw, 1208px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. An example of Kubernetes Deployment with four replicated Pods, each Pod having a Triton server running on a GPU</em></figcaption></figure>\n\n\n\n<p>The <code>pod-monitor.yaml</code> file below uses PodMonitor to monitor Pods, each having a Triton server:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: yaml; title: ; notranslate\" title=\"\">\napiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: {{ $.Release.Name }}\n  labels:\n    app: {{ $.Release.Name }}\n    app.kubernetes.io/component: autoscaler\n    release: prometheus\n\u2026\u2026\nspec:\n  selector:\n    matchLabels:\n      app: {{ $.Release.Name }}\n      app.kubernetes.io/component: server\n  podMetricsEndpoints:\n  - port: metrics\n    path: /metrics\n</pre></div>\n\n\n<p>Prometheus can scrape <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/metrics.html#gpu-metrics\">Triton metrics</a> from all the Kubernetes Pods at Port number 8002. You can choose a Triton for HPA, or define a new custom metric using the collected metrics based on your requirement. Prometheus adapter communicates with both Kubernetes and Prometheus, acting as a translator between the two. With the help from the Prometheus adapter, HPA can use the selected metric to autoscale the replica number of Kubernetes Pods based on the volume of the inference requests.&nbsp;</p>\n\n\n\n<p>In this application, a custom metric, called queue-to-compute ratio<em>,</em> is used as the metric for HPA. The queue-to-compute ratio reflects the response time of inference requests. It\u2019s defined as the queue time divided by the compute time for an inference request in the <a href=\"https://github.com/triton-inference-server/tutorials/blob/main/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing/triton-metrics_prometheus-rule.yaml\"><code>triton-metrics_prometheus-rule.yaml</code></a> below.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: yaml; title: ; notranslate\" title=\"\">\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: triton-metrics\n  labels:\n    app.kubernetes.io/component: autoscaler\n    release: prometheus\nspec:\n  groups:\n  - name: autoscaling\n    interval: 6s\n    rules:\n    # Average percentage of time inference requests spend in queue (not including cache hits).\n    - expr: rate(nv_inference_queue_duration_us&#x5B;1m])/clamp_min(rate(nv_inference_compute_infer_duration_us&#x5B;1m]),1)\n      record: triton:queue_compute:ratio\n</pre></div>\n\n\n<p>Prometheus scrapes Triton metrics with an interval of 6 seconds, and calculates the value of custom metric using the Triton metrics. Then HPA scales up and down the number of replicas based on the value of the custom metric.</p>\n\n\n\n<p>The <code>hpa.yaml</code> below specifies the maximum and minimum numbers of replicas for deployment; for example, up to 4 Pods and at least 1 Pod. Use <code>Pods</code> for <code>metrics-type</code> to take the average of the queue-to-compute ratio across all the Pods. An average queue-to-compute ratio that is higher than the desired value 1,000 (milliunit) means the queue time is longer than the compute time, or the number of replica(s) is not enough to respond quickly to the inference requests. In this case, HPA should increase the number of replicas to reduce the queue-to-compute ratio until the custom metric is lower than the desired value, and vice versa if the volume of inference requests is reduced. This target value can be set at any number based on your requirements.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: yaml; title: ; notranslate\" title=\"\">\n{{- $metric_name := &quot;triton:queue_compute:ratio&quot; }}\n{{- $metric_value := &quot;1000m&quot; }}\n{{- $replicasMax := 4 }}\n{{- $replicasMin := 1 }}\n\u2026\u2026\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: {{ $.Release.Name }}\n  labels:\n    app: {{ $.Release.Name }}\n    app.kubernetes.io/component: autoscaler\n    release: prometheus\n\u2026\u2026\nspec:\n  maxReplicas: {{ $replicasMax }}\n  minReplicas: {{ $replicasMin }}\n  metrics:\n  - type: Pods\n    pods:\n      metric:\n        name: {{ $metric_name }}\n      target:\n        type: AverageValue\n        averageValue: {{ $metric_value }}\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: {{ $.Release.Name }}\n</pre></div>\n\n\n<p>After configuring your .yaml files for Kubernetes Deployment, Service, PodMonitor, and HPA, you can deploy your model with Triton server and Kubernetes using the following command:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n$ helm install gpt2 --values ./chart/values.yaml    --values ./chart/gpt2_values.yaml   --set &#039;triton.image.name=&lt;your custom image&gt;&#039; ./chart/.\nNAME: gpt2\nLAST DEPLOYED: Mon Aug 26 23:04:42 2024\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\ntriton_trt-llm_aslb-example (0.1.0) installation complete.\n\nRelease Name: gpt2\nNamespace: default\nDeployment Name: gpt2\nService Name: gpt2\n</pre></div>\n\n\n<p>Verify that everything is working as expected. You should be able to see the output information, such as <code>NAME</code><em>, </em><code>READY</code><em>, </em>and<em> </em><code>STATUS</code> for Pod; <code>NAME</code><em>, </em><code>TYPE</code><em>, </em>and <code>PORTS</code> for Service; and <code>NAME</code><em>, </em><code>MINPODS</code><em>, </em><code>MAXPODS</code><em>, </em>and <code>REPLICAS</code> for HPA, and so on.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n$ kubectl get deployments,pods,hpa,services,podmonitors --selector=&#039;app=gpt2&#039;\nNAME                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/gpt2   1/1     1            1           11m\n\nNAME                        READY   STATUS    RESTARTS   AGE\npod/gpt2-85cfd5b6d5-4v87s   1/1     Running   0          11m\n\nNAME                                       REFERENCE         TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/gpt2   Deployment/gpt2   0/1       1         4         1          11m\n\nNAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nservice/gpt2   ClusterIP   10.100.56.211   &lt;none&gt;        8000/TCP,8001/TCP,8002/TCP   11m\n\nNAME                                    AGE\npodmonitor.monitoring.coreos.com/gpt2   11m\n</pre></div>\n\n\n<p>If the status of Pod is not running successfully, for example (the status shows <code>Init:CrashLoopBackOff</code> or <code>ImagePullBackOff</code>), you can try the following commands to debug the issue. Potential reasons for failure could be the model or custom image was not downloaded successfully, Kubernetes secrets are not working properly, out of memory, and so on.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n$ kubectl describe pod &lt;pod name&gt;\n$ kubectl logs &lt;pod name&gt; -c init \n$ kubectl logs &lt;pod name&gt; -c init --previous\n</pre></div>\n\n\n<h3 id=\"load_balancer&nbsp;\"  class=\"wp-block-heading\">Load balancer&nbsp;<a href=\"#load_balancer&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>You also need a load balancer to distribute the workload among all the running Pods. There are mainly two types of load balancers: Layer 4 and Layer 7. Layer 4 load balancing operates at the transport level and manages traffic based on network information, while Layer 7 load balancing operates at the application level and uses protocols to make decisions based on the content of each message.&nbsp;</p>\n\n\n\n<p>You can use a third-party load balancer such as <a href=\"https://traefik.io/solutions/kubernetes-ingress/?utm_term=traefik&amp;utm_campaign=sitelink&amp;utm_source=adwords&amp;utm_medium=paid_search&amp;hsa_acc=4064213719&amp;hsa_grp=153025400283&amp;hsa_ad=673595752752&amp;hsa_src=g&amp;hsa_tgt=kwd-468823377738&amp;hsa_kw=traefik&amp;hsa_net=adwords&amp;hsa_ver=3&amp;gad_source=1&amp;gclid=EAIaIQobChMI8tK3uImFiAMVFQGtBh29QSI5EAAYASABEgKs9vD_BwE\">Traefik</a> ingress controller and load balancer,&nbsp; or <a href=\"https://www.nginx.com/blog/load-balancing-kubernetes-services-nginx-plus/\">NGINX Plus</a>, which are both Layer 7. See more information about how to deploy <a href=\"https://developer.nvidia.com/blog/autoscaling-nvidia-riva-deployment-with-kubernetes-for-speech-ai-in-production/\">Traefik</a> and <a href=\"https://resources.nvidia.com/en-us-ai-inference-cloud-deployments/deploying-nvidia-tri\">NGINX Plus</a>. You can also use a cloud load balancer if you\u2019re using a cloud service. For example, the <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html\">AWS load balancer controller</a> can provision both application and network load balancers. After installing the controller, you can ask the cloud controller to provision a <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/network-load-balancing.html\">network load balancer</a> if the Service has the <code>loadBalancer</code> type, or an <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html\">application load balancer</a> if you create a Kubernetes ingress.</p>\n\n\n\n<h2 id=\"send_test_inference_requests&nbsp;&nbsp;\"  class=\"wp-block-heading\">Send test inference requests&nbsp;&nbsp;<a href=\"#send_test_inference_requests&nbsp;&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Finally, you can test the Triton servers by sending inference requests from clients. A <a href=\"https://github.com/triton-inference-server/tutorials/tree/main/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing#what-is-the-client-folder-for\">sample client</a> folder is provided from which you can build a client container image:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n$ docker build -f ./containers/client.containerfile -t &lt;name&gt; ./containers/.\n</pre></div>\n\n\n<p>Next, modify the &lt;model&gt;.yaml file in the client folder to the name of the client container image that you built. Use the following commands to create the Deployment of the client with one replica:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n $ kubectl apply -f ./clients/gpt2.yaml\n deployment.apps/client-gpt2 created\n</pre></div>\n\n\n<p>Let the client increase or decrease the volume of inference requests by changing the number of replicas of the client, and check the Pods:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n$ kubectl scale deployment/client-gpt2   --replicas=10\ndeployment.apps/client-gpt2 scaled\n\n$ kubectl get pods\nNAME                                READY   STATUS    RESTARTS   AGE\nclient-gpt2-cb4bf7b74-6g82l   1/1     Running   0          16s\nclient-gpt2-cb4bf7b74-6lt8x   1/1     Running   0          16s\nclient-gpt2-cb4bf7b74-6nnvn   1/1     Running   0          16s\nclient-gpt2-cb4bf7b74-b7s88   1/1     Running   0          16s\nclient-gpt2-cb4bf7b74-fl5c6   1/1     Running   0          36s\nclient-gpt2-cb4bf7b74-j88ld   1/1     Running   0          16s\nclient-gpt2-cb4bf7b74-jdmkm   1/1     Running   0          16s\nclient-gpt2-cb4bf7b74-lqptv   1/1     Running   0          16s\nclient-gpt2-cb4bf7b74-m66cx   1/1     Running   0          16s\nclient-gpt2-cb4bf7b74-nt7b7   1/1     Running   0          16s\ngpt2-85cfd5b6d5-65ftt         1/1     Running   0          7m57s\n</pre></div>\n\n\n<p>You can see 10 clients running, which increases the number of inference requests significantly. This will increase the value of the custom metric, therefore causing HPA to increase the number of replicas of the Triton servers with GPT-2 models:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n$ kubectl get pods\nNAME                             READY   STATUS    RESTARTS   AGE\nclient-gpt2-cb4bf7b74-6g82l   1/1     Running   0          56s\nclient-gpt2-cb4bf7b74-6lt8x   1/1     Running   0          56s\nclient-gpt2-cb4bf7b74-6nnvn   1/1     Running   0          56s\nclient-gpt2-cb4bf7b74-b7s88   1/1     Running   0          56s\nclient-gpt2-cb4bf7b74-fl5c6   1/1     Running   0          76s\nclient-gpt2-cb4bf7b74-j88ld   1/1     Running   0          56s\nclient-gpt2-cb4bf7b74-jdmkm   1/1     Running   0          56s\nclient-gpt2-cb4bf7b74-lqptv   1/1     Running   0          56s\nclient-gpt2-cb4bf7b74-m66cx   1/1     Running   0          56s\nclient-gpt2-cb4bf7b74-nt7b7   1/1     Running   0          56s\ngpt2-85cfd5b6d5-65ftt         1/1     Running   0          8m37s\ngpt2-85cfd5b6d5-65wg4         1/1     Running   0          22s\ngpt2-85cfd5b6d5-kh9j4         1/1     Running   0          22s\ngpt2-85cfd5b6d5-pdg5m         1/1     Running   0          22s\n</pre></div>\n\n\n<p>Similarly, if you decrease the volume of inference requests, for example, reducing the number of client to one replica, HPA reduces the number of the Triton servers to one replica accordingly after a few minutes:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n$ kubectl get pods\nNAME                            READY   STATUS    RESTARTS   AGE\nclient-gpt2-cb4bf7b74-6g82l   1/1     Running   0          11m\ngpt2-85cfd5b6d5-65ftt         1/1     Running   0          19m\n</pre></div>\n\n\n<p>You can also use <a href=\"https://github.com/triton-inference-server/tutorials/tree/4f166c50f9485a936986f701eed057f25fa2608c/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing#monitoring-triton-in-kubernetes\">Grafana</a> to query NVIDIA Triton metrics and your custom metric, visualizing the results in time series by navigating to its metrics endpoint: <code><a href=\"http://localhost:9090/metrics\">localhost:</a>8080</code>.&nbsp;</p>\n\n\n\n<p>Figure 2 shows the GPU Utilization and Queue:Compute Ratio reflecting this change. At the beginning, only one replica of the Triton server was running on one GPU. As a result, the GPU utilization (orange line) and Queue:Compute Ratio increased while the inference requests were increased significantly by the number of clients.&nbsp;</p>\n\n\n\n<p>To reduce the custom metric, HPA increased the number of Triton servers to four replicas running on four GPUs, as shown by the four lines in the GPU Utilization,<em> </em>which reduced the<em> </em>Queue:Compute Ratio efficiently. In the end, the volume of inference requests reduced to one client, so the GPU utilization reduced accordingly.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1509\" height=\"1271\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grafana-graphic-ui.png\" alt=\"Screenshot of graphic user interface for Grafana showing the metric in time series.\n\" class=\"wp-image-90430\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grafana-graphic-ui.png 1509w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grafana-graphic-ui-300x253.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grafana-graphic-ui-625x526.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grafana-graphic-ui-137x115.png 137w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grafana-graphic-ui-768x647.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grafana-graphic-ui-645x543.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grafana-graphic-ui-356x300.png 356w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grafana-graphic-ui-107x90.png 107w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grafana-graphic-ui-362x305.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grafana-graphic-ui-131x110.png 131w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/grafana-graphic-ui-1024x862.png 1024w\" sizes=\"(max-width: 1509px) 100vw, 1509px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Grafana graphic user interface showing the metrics in time series based on the client&#8217;s query</em></figcaption></figure>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This post provides step-by-step instructions for deploying LLMs and autoscaling the deployment in a Kubernetes environment. LLMs can be optimized using NVIDIA TensorRT-LLM, then deployed using NVIDIA Triton Inference Server. Prometheus collects the Triton metrics and communicates with Kubernetes. HPA can use a custom metric to autoscale the Pod numbers, depending on the amount of inference requests from clients.</p>\n\n\n\n<p>Ready to get started? Visit <a href=\"https://github.com/triton-inference-server/tutorials/tree/main/Deployment/Kubernetes/TensorRT-LLM_Autoscaling_and_Load_Balancing\">triton-inference-server / tutorials</a> on GitHub. Learn more about <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend/tree/main\">Triton with TensorRT-LLM</a>. Docker containers can be pulled from <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags\">NGC</a> if you want to make your own customized image.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLMs) have been widely used for chatbots, content generation, summarization, classification, translation, and more. State-of-the-art LLMs and&nbsp; foundation models, such as Llama, Gemma, GPT, and Nemotron, have demonstrated human-like understanding and generative abilities. Thanks to these models, AI developers do not need to go through the expensive and time consuming training process &hellip; <a href=\"https://developer.nvidia.com/blog/scaling-llms-with-nvidia-triton-and-nvidia-tensorrt-llm-using-kubernetes/\">Continued</a></p>\n", "protected": false}, "author": 636, "featured_media": 90417, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1506377", "discourse_permalink": "https://forums.developer.nvidia.com/t/scaling-llms-with-nvidia-triton-and-nvidia-tensorrt-llm-using-kubernetes/310787", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [453, 572, 3650, 2932], "coauthors": [993, 4114, 4105, 4115], "class_list": ["post-90412", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "tag-featured", "tag-kubernetes", "tag-llm-techniques", "tag-large-language-models"], "acf": {"post_industry": ["General"], "post_products": ["TensorRT-LLM", "Triton Inference Server"], "post_learning_levels": ["Intermediate Technical", "Advanced Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/llm-graphic-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nwg", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Conversational AI", "link": "https://developer.nvidia.com/blog/category/conversational-ai/", "id": 1050}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90412"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/636"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90412"}], "version-history": [{"count": 29, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90412/revisions"}], "predecessor-version": [{"id": 90561, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90412/revisions/90561"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90417"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90412"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90412"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90412"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90412"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90495, "date": "2024-10-22T09:00:00", "date_gmt": "2024-10-22T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90495"}, "modified": "2024-11-11T20:32:34", "modified_gmt": "2024-11-12T04:32:34", "slug": "multi-agent-ai-and-gpu-powered-innovation-in-sound-to-text-technology", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/multi-agent-ai-and-gpu-powered-innovation-in-sound-to-text-technology/", "title": {"rendered": "Multi-Agent AI and GPU-Powered Innovation in Sound-to-Text Technology"}, "content": {"rendered": "\n<p>The <a href=\"https://dcase.community/challenge2024/task-automated-audio-captioning\">Automated Audio Captioning</a> task centers around generating natural language descriptions from audio inputs. Given the distinct modalities between the input (audio) and the output (text), AAC systems typically rely on an audio encoder to extract relevant information from the sound, represented as feature vectors, which a decoder then uses to generate text descriptions.&nbsp;</p>\n\n\n\n<p>This area of research is critical for developing systems that enable machines to better interpret and interact with the surrounding acoustic environment. Recognizing its importance, the <a href=\"https://dcase.community/\">Detection and Classification of Acoustic Scenes and Events (DCASE)</a> community has hosted annual AAC competitions since 2020, attracting over 26 teams globally from both academia and industry.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"351\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-625x351.png\" alt=\"Diagram shows a picture of a forest labeled Recording Environment (Hidden). The enriched audio caption, which is the result of the audio input going through the audio model agent and text model agent, says, &quot;this audio .. birds-bees .. faint environmental noises like rustling leaves .. outdoor setting&quot;.\" class=\"wp-image-90629\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-625x351.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration-1024x576.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/multi-agent-collaboration.png 1636w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Example of the proposed multi-agent collaboration between audio and text agents</em></figcaption></figure></div>\n\n\n<p>Listen to the result at<em> </em><a href=\"https://freesound.org/people/SamuelGremaud/sounds/466073/\" target=\"_blank\" rel=\"noreferrer noopener\">Audio Example of a Recording Environment in a Forest</a>.</p>\n\n\n\n<p>In this post, we dive into the core innovations behind our winning submission to the <a href=\"https://dcase.community/challenge2024/task-automated-audio-captioning\">DCASE 2024 AAC Challenge</a>, to be hosted in Tokyo, Japan, October 23\u201325.&nbsp;</p>\n\n\n\n<p>The <a href=\"https://dcase.community/documents/challenge2024/technical_reports/DCASE2024_Jung_74_t6.pdf\">CMU-NVIDIA solution report</a>:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Enhances the encoder-decoder architecture by employing multiple audio encoders.</li>\n\n\n\n<li>Uses an LM-based task-activating prompting for information post-editing enrichment.&nbsp;</li>\n</ul>\n\n\n\n<p>This architecture improves the system\u2019s ability to capture diverse audio features by using encoders with different granularities. The multi-encoder approach enables us to deliver richer, more complementary information to the decoder, significantly enhancing performance.</p>\n\n\n\n<p>Professor <a href=\"https://lti.cmu.edu/people/faculty/watanabe-shinji.html\">Shinji Wanatabe</a> from the <a href=\"https://lti.cmu.edu/index.html\">Language Technologies Institute</a> (LTI) at Carnegie Mellon University (CMU) said, &#8220;This is a cool way to showcase our team\u2019s efforts, collaborating with open-source researchers, to contribute to advancements in the audio\u2013 and language-understanding communities.&#8221;</p>\n\n\n\n<h2 id=\"multi-agent_collaboration_for_enhanced_performance\"  class=\"wp-block-heading\">Multi-agent collaboration for enhanced performance<a href=\"#multi-agent_collaboration_for_enhanced_performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>One of the most innovative aspects of our approach lies in the multi-agent collaboration between distinct encoder models, which proved to be a key factor in boosting performance. By integrating multiple encoders with varying granularities, such as <a href=\"https://proceedings.mlr.press/v202/chen23ag.html\">BEATs</a> and <a href=\"https://arxiv.org/abs/2201.03545\">ConvNeXt</a>, we achieved enhanced coverage of audio features.&nbsp;</p>\n\n\n\n<p>This strategy of fusing encoders has similarities with recent breakthroughs in multimodal AI research, such as <a href=\"https://arxiv.org/pdf/2309.17352\">MERL and CMU\u2019s 2023 solution</a>, where the combination of distinct agents\u2014each specializing in different aspects of a task\u2014yields superior results.</p>\n\n\n\n<p>In our system, we adopted an encoder fusion strategy similar to the concepts used in those papers, enabling us to use the strengths of each encoder. We further considered textual hypotheses-based enrichment with the recent <a href=\"https://research.nvidia.com/index.php/publication/2024-08_gentranslate-large-language-models-are-generative-multilingual-speech-and\">GenTranslate</a><strong> </strong>in<strong> </strong><a href=\"https://2024.aclweb.org/\">ACL 2024</a> and <a href=\"https://research.nvidia.com/publication/2024-11_descriptive-richness-bias-unveiling-dark-side-generative-image-caption\">Generative Image Captioning (GIC)</a> evaluation in <a href=\"https://2024.emnlp.org/\">EMNLP 2024</a> works from <a href=\"https://www.nvidia.com/en-us/research/\">NVIDIA Research</a> in <a href=\"https://www.nvidia.com/zh-tw/\">Taiwan</a>, which enable descriptive richness customization.<br><br>For example, GenTranslate and GIC both demonstrate how multiple language models can cooperate to improve speech translation accuracy across languages, while GenTranslate highlights the efficacy of multi-agent systems in generative speech translation tasks.&nbsp;</p>\n\n\n\n<p>Both examples underscore the value of integrating complementary models for complex tasks, reinforcing the potential of our approach to significantly elevate AAC performance. We introduce how the core techniques have been used on GPU-based pretraining pipelines and post-editing pipelines.&nbsp;</p>\n\n\n\n<p>Advanced NVIDIA computer technology, such as <a href=\"https://en.wikipedia.org/wiki/Taipei-1_(supercomputer)\">Taipei-1</a> (the world&#8217;s 38th-ranked supercomputer cluster on the <a href=\"https://top500.org/lists/top500/list/2024/06/\">top 500 list</a>), also played an important role in accelerating this state-of-the-art exploration and research development with the <a href=\"https://www.nvidia.com/en-us/data-center/dgx-platform/\">NVIDIA DGX</a> and <a href=\"https://www.nvidia.com/en-us/data-center/products/ovx/\">NVIDIA OVX</a> platforms.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"210\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system-625x210.png\" alt=\"Workflow diagram starts with audio input, moves through encoder fusion and layer aggregation (BEATs, CovNext, Conformer), and to a decoder (BART). This produces 64 captions, which are filtered and reranked. The top k captions go to LLM summarization, which produces the caption output.\" class=\"wp-image-90630\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system-625x210.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system-300x101.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system-179x60.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system-768x258.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system-1536x516.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system-645x217.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system-500x168.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system-160x54.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system-362x122.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system-327x110.png 327w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system-1024x344.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/automated-audio-captioning-system.png 1690w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. An automated audio captioning (AAC) system</em></figcaption></figure></div>\n\n\n<p>Figure 2 shows modeling based on encoder fusion, caption filtering, and generative summarization. The generative summarization part builds on NVIDIA Research\u2019s previous work on <a href=\"https://research.nvidia.com/index.php/publication/2024-08_gentranslate-large-language-models-are-generative-multilingual-speech-and\">GenTranslate</a>.</p>\n\n\n\n<h2 id=\"core_acoustic_modeling_techniques_behind_the_model\"  class=\"wp-block-heading\">Core acoustic modeling techniques behind the model<a href=\"#core_acoustic_modeling_techniques_behind_the_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The architecture of our system is inspired by CMU and MERL\u2019s <a href=\"https://github.com/slSeanWU/beats-conformer-bart-audio-captioner\">last year&#8217;s winning open source model</a> and introduces several improvements:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Multi-encoder fusion</strong>: We employed two pretrained audio encoders (BEATs and ConvNeXt) to generate complementary audio representations. This fusion enables the decoder to attend to a wider pool of feature sets, leading to more accurate and detailed captions.</li>\n\n\n\n<li><strong>Multi-layer aggregation</strong>: Different layers of the encoders capture varying aspects of the input audio, and by aggregating outputs across all layers, we further enriched the information fed into the decoder.</li>\n\n\n\n<li><strong>Generative caption modeling</strong>: To optimize the generation of natural language descriptions, we applied a <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language model (LLM)</a>\u2013based summarization process, similar to techniques used in <a href=\"https://research.nvidia.com/index.php/publication/2024-05_large-language-models-are-efficient-learners-noise-robust-speech-recognition\">RobustGER</a>. This step consolidates multiple candidate captions into a single, fluent output, using LLMs to ensure both grammatical coherence and a human-like feel to the descriptions.</li>\n</ul>\n\n\n\n<h2 id=\"multi-agent_collaboration_with_audio-text-llm_integration\"  class=\"wp-block-heading\">Multi-agent collaboration with audio-text-LLM Integration<a href=\"#multi-agent_collaboration_with_audio-text-llm_integration\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Beyond the multi-encoder architecture, we also developed a new multi-agent collaboration<strong> </strong>inference pipeline. Inspired by recent research showing the benefits of nucleus sampling in AAC tasks, we improved upon traditional beam search methods.&nbsp;</p>\n\n\n\n<p>Our inference process follows a three-stage pipeline:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>CLAP-based caption filtering</strong>: We generate multiple candidate captions and filter out less relevant ones using a <a href=\"https://arxiv.org/pdf/2206.04769\">Contrastive Language-Audio Pretraining (CLAP)</a> model, reducing the number of candidates by half.</li>\n\n\n\n<li><strong>Hybrid reranking</strong>: The remaining captions are then ranked using our <a href=\"https://arxiv.org/pdf/2011.11715\">hybrid reranking</a> method to select the top k-best captions.</li>\n\n\n\n<li><strong>LLM summarization</strong>: Finally, we use a task-activated (that is, [conditional prompt] do you know audio captioning?) LLM to summarize the k-best captions into a single, coherent caption, ensuring the final output captures all critical aspects of the audio.</li>\n</ul>\n\n\n\n<p>This novel inference pipeline leverages the strengths of both audio processing and language modeling, significantly improving the model\u2019s ability to contextually accurate captions as refined decoding text as a form of feature maps for the downstream text-agent</p>\n\n\n\n<h2 id=\"impact_and_performance\"  class=\"wp-block-heading\">Impact and performance<a href=\"#impact_and_performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Our multi-encoder system achieved a <a href=\"https://arxiv.org/pdf/2110.04684\">Fluency Enhanced Sentence-BERT Evaluation</a> (FENSE) score of 0.5442, outperforming the baseline score of 0.5040. By incorporating multi-agent systems, we have opened new avenues for further improving AAC tasks.&nbsp;</p>\n\n\n\n<p>Future work will explore integrating more advanced fusion techniques and examining how further collaboration between specialized agents can enhance both the granularity and quality of the generated captions.</p>\n\n\n\n<p>We hope that our contributions inspire continued exploration in multi-agent AI systems and encourage other teams to adopt similar strategies for fusing diverse models to handle complex multimodal tasks like AAC.</p>\n\n\n\n<p>In Figure 3, the higher score means more details and richer information captured from the audio context.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"302\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score-1024x302.png\" alt=\"Bar chart shows the CMU-NVIDIA 1st place winning audio captioning system as the leftmost bar, with the FENSE score shown in blue near the other end.\" class=\"wp-image-90631\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score-1024x302.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score-300x88.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score-625x184.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score-179x53.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score-768x226.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score-1536x452.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score-645x190.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score-500x147.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score-160x47.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score-362x107.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score-374x110.png 374w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cmu-dcase-fense-score.png 1922w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Audio Caption 2024 DCASE Challenge result evaluating on the fluency-aware FENSE score</em></figcaption></figure></div>\n\n\n<h2 id=\"using_gpu_technology_for_performance_and_scalability\"  class=\"wp-block-heading\">Using GPU technology for performance and scalability<a href=\"#using_gpu_technology_for_performance_and_scalability\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Our solution outperformed other participants in the challenge by over (relative) 10% in semantic understanding scores, thanks to the synergy between multi-encoder fusion and LLM-driven summarization. This success underscores the potential of multi-agent, multi-modality systems in advancing general-purpose understanding.</p>\n\n\n\n<p>The use of LLM-based many-to-one textual correction was a critical innovation in this process, enabling the model to better use the computational power of the text modeling agent. This method retrieves and refines hidden information embedded in the audio, improving the system\u2019s overall performance.</p>\n\n\n\n<p>This approach builds on NVIDIA\u2019s state-of-the-art work in multimodal AI, such as the <a href=\"https://research.nvidia.com/index.php/publication/2024-08_gentranslate-large-language-models-are-generative-multilingual-speech-and\">GenTranslate</a> model, which excels in multilingual speech and text translation. Similarly, our recent <a href=\"https://audioflamingo.github.io/\">Audio Flamingo</a> project, <a href=\"https://arxiv.org/pdf/2410.02056\">Synthio</a> project, and <a href=\"https://arxiv.org/abs/2406.15487\">dataset</a> from <a href=\"https://nv-adlr.github.io/\">NVIDIA applied deep learning research</a> (ADLR) also demonstrated the power of advanced pretraining techniques for audio encoders.&nbsp;</p>\n\n\n\n<p>These systems, alongside our winning AAC solution, have all benefited from <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100</a> and <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100</a> GPUs, accelerating AI development and pushing the boundaries of what\u2019s possible in multimodal learning. <a href=\"https://research.nvidia.com/person/huck-yang\">Huck Yang</a> from NVIDIA Research has been invited to join the Technical Panel Discussion on audio-language technologies during the <a href=\"https://dcase.community/workshop2024/\">DCASE Workshop</a> 2024 program.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The Automated Audio Captioning task centers around generating natural language descriptions from audio inputs. Given the distinct modalities between the input (audio) and the output (text), AAC systems typically rely on an audio encoder to extract relevant information from the sound, represented as feature vectors, which a decoder then uses to generate text descriptions.&nbsp; This &hellip; <a href=\"https://developer.nvidia.com/blog/multi-agent-ai-and-gpu-powered-innovation-in-sound-to-text-technology/\">Continued</a></p>\n", "protected": false}, "author": 2384, "featured_media": 90677, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1506336", "discourse_permalink": "https://forums.developer.nvidia.com/t/multi-agent-ai-and-gpu-powered-innovation-in-sound-to-text-technology/310780", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [453, 1976, 549], "coauthors": [4120, 4121], "class_list": ["post-90495", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-speech-and-audio-processing", "tag-telecommunications"], "acf": {"post_industry": ["Telecommunications"], "post_products": ["DGX", "OGX"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/audio-captioning-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nxB", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90495"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2384"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90495"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90495/revisions"}], "predecessor-version": [{"id": 90752, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90495/revisions/90752"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90677"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90495"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90495"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90495"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90495"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90569, "date": "2024-10-22T00:00:00", "date_gmt": "2024-10-22T07:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90569"}, "modified": "2024-10-31T11:40:06", "modified_gmt": "2024-10-31T18:40:06", "slug": "how-to-calibrate-sensors-with-msa-calibration-anywhere-for-nvidia-isaac-perceptor", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/how-to-calibrate-sensors-with-msa-calibration-anywhere-for-nvidia-isaac-perceptor/", "title": {"rendered": "How to Calibrate Sensors with MSA Calibration Anywhere for NVIDIA Isaac Perceptor"}, "content": {"rendered": "\n<p>Multimodal sensor calibration is critical for achieving sensor fusion for robotics, autonomous vehicles, mapping, and other perception-driven applications. Traditional calibration methods, which rely on structured environments with checkerboards or targets, are complex, expensive, time-consuming, and don\u2019t scale.\u00a0</p>\n\n\n\n<p>An automatic sensor calibration solution that simplifies the calibration problem is the <a href=\"https://mainstreetautonomy.com/index.html\">Main Street Autonomy</a> Calibration Anywhere software. Main Street Autonomy is an autonomy software and services company that employs state-of-the-art technology to provide sensor calibration, localization, and mapping solutions to the robotics and autonomous vehicle sectors.&nbsp;</p>\n\n\n\n<p>In this post, you&#8217;ll learn how to use the Calibration Anywhere solution to generate a calibration file that can be integrated into <a href=\"https://developer.nvidia.com/isaac/perceptor\">NVIDIA<strong> </strong>Isaac Perceptor</a> workflows. Isaac Perceptor, built on <a href=\"https://developer.nvidia.com/isaac/ros\">NVIDIA Isaac ROS</a>, is a reference workflow of NVIDIA-accelerated libraries and AI models that helps you quickly build robust autonomous mobile robots (AMRs). This tutorial is for engineers responsible for sensor calibration and those working with perception systems, such as perception engineers.</p>\n\n\n\n<h2 id=\"overview_of_sensor_calibration\"  class=\"wp-block-heading\">Overview of sensor calibration<a href=\"#overview_of_sensor_calibration\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Calibration ensures that different modalities of perception sensors generate coherent sensor data that perceives the world in agreement with each other. Perception sensors include lidar, radar, camera, depth camera, IMU, wheel encoder, and GPS/GNSS, and they capture diverse information like range, reflectivity, image, depth, and motion data.&nbsp;</p>\n\n\n\n<p>When an autonomous forklift approaches a pallet, for example, a 3D lidar identifies the shape, size, and distance to the pallet and load, and stereo cameras running machine learning (ML) workflows identify the fork openings. With proper calibration, the camera-determined position of the fork openings will properly align with the lidar-determined outline of the pallet and load. Without proper calibration, sensor data can be misaligned, leading to inaccurate interpretations, such as incorrect object detection, depth estimation errors, or faulty navigation.</p>\n\n\n\n<p>Traditional sensor calibration is the manual process of determining <em>sensor intrinsics </em>and <em>sensor extrinsics</em>. <em>Sensor intrinsics </em>involve corrections to sensor data for individual sensors, like lens distortion and focal length for cameras. <em>Sensor extrinsics </em>involve positions and orientations relative to each other in a shared coordinate system, which often will be reference points that relate<s>s</s> to the kinematic frame and is used for motion planning and control.&nbsp;</p>\n\n\n\n<p>The process for calibrating two cameras together is relatively straightforward, requires a printed target called a checkerboard, and can take an hour for an engineer to complete. Calibrating more cameras, or cameras versus lidar, cameras versus IMU, or lidar versus IMU, are all incrementally more difficult and require additional targets and engineering effort.</p>\n\n\n\n<p>Main Street Autonomy\u2019s Calibration Anywhere software is an automatic sensor calibration solution that works with any number, combination, and layout of perception sensors in any unstructured environment. No checkerboards or targets are required, and the calibration can be performed almost anywhere with no setup or environmental changes. The calibration process can take less than 10 minutes to complete. No engineers or technicians are required. The solution generates sensor intrinsics, extrinsics, and time offsets for all perception sensors in one pass.</p>\n\n\n\n<h2 id=\"tutorial_prerequisites&nbsp;\"  class=\"wp-block-heading\"><strong>Tutorial prerequisites</strong>&nbsp;<a href=\"#tutorial_prerequisites&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>For the fastest turn-around time for the first calibration, an ideal configuration is outlined below.</p>\n\n\n\n<p>Environment includes:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Nearby textured static structure. Nothing special is required. Examples include an office environment or loading dock or parking lot. Calibrating cameras that are pointed at the ocean is complicated.</li>\n\n\n\n<li>Enough lighting to make observations.</li>\n\n\n\n<li>Low enough humidity (minimal fog, rain, snow) to allow for observations.</li>\n\n\n\n<li>Third-party movers such as people, moving vehicles, or other moving robots that don\u2019t approach the sensors or represent the majority of observations.</li>\n</ul>\n\n\n\n<p>Sensor system includes:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>One of the following:\n<ul class=\"wp-block-list\">\n<li>A 3D lidar</li>\n\n\n\n<li>A 2D lidar</li>\n\n\n\n<li>A stereo camera with known baseline</li>\n</ul>\n</li>\n\n\n\n<li>An IMU</li>\n\n\n\n<li>Sensor system layout:\n<ul class=\"wp-block-list\">\n<li>If a 3D lidar is present, camera field of view (FOV) should overlap at least 50% with FOV of lidar. Depth cameras should be able to see parts of the world that 3D lidar can see. Overlap is not required, but objects that the lidar sees should be visible to the depth cameras once the robot has moved around.</li>\n\n\n\n<li>All sensors are rigidly connected during the calibration.</li>\n</ul>\n</li>\n\n\n\n<li>Sensor data stored in ROS1 or ROS2 bag with standard topics and messages, with accurate timestamps on all camera and depth images, individual lidar and radar points, IMU and GPS measurements, and wheel encoder ticks or speeds.</li>\n\n\n\n<li>Sensor data captured while:\n<ul class=\"wp-block-list\">\n<li>Sensors are moved manually, through teleoperation or autonomously in a manner that doesn\u2019t cause excessive wheel slip or motion blur.</li>\n\n\n\n<li>Sensors are moved in two figure-eight movements, where the individual circles don\u2019t overlap and the diameter of the circles is &gt;1 m.</li>\n\n\n\n<li>Sensors approach textured static structure within 1 m, where the structure fills most of the FOV of each camera.</li>\n\n\n\n<li>Recording length is relatively short. Aim for 60 seconds of data collect, and no more than 5 minutes.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>Sensor systems that don\u2019t meet these requirements can still be calibrated but will just have a longer turn-around time. Sensor data in non-ROS formats require transformation and will have a longer turn-around time. Alternate movement procedures are possible for large or motion-constrained robots. <a href=\"https://mainstreetautonomy.com/demo/\">Contact MSA for more information</a>.</p>\n\n\n\n<h2 id=\"evaluation_procedure\"  class=\"wp-block-heading\">Evaluation procedure<a href=\"#evaluation_procedure\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The process of evaluating Calibration Anywhere is straightforward and involves five steps, outlined below and covered in detail in the following sections.</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li><a href=\"https://mainstreetautonomy.com/demo\">Connect with MSA</a> and describe your system.</li>\n\n\n\n<li>Capture sensor data while the sensors move.</li>\n\n\n\n<li>Upload the sensor data to the <a href=\"https://upload.mainstreetautonomy.com/\">MSA Data Portal</a>.</li>\n\n\n\n<li>Receive a calibration package with URDF output compatible with NVIDIA Isaac Perceptor.</li>\n\n\n\n<li>Import the URDF into the Isaac Perceptor workflow.</li>\n</ol>\n\n\n\n<p>Once MSA has configured Calibration Anywhere for your system, you can use the calibration-as-a-service solution. This involves uploading sensor data and downloading a calibration. You can also deploy Calibration Anywhere in a Docker container and run locally without sending data.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/LJOyHRnkW90?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\">Video 1. Watch a demonstration of the Calibration Anywhere process</figcaption></figure>\n\n\n\n<h2 id=\"step_1_connect_with_msa_and_describe_your_system\"  class=\"wp-block-heading\"><strong>Step 1: Connect with MSA and describe your system</strong><a href=\"#step_1_connect_with_msa_and_describe_your_system\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Visit the <a href=\"https://mainstreetautonomy.com/demo\">MSA Demo page </a>and fill out the form. MSA will contact you with any additional questions and send you credentials for using the MSA Data Portal.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"641\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page-625x641.png\" alt=\"Screenshot showing form to try MSA demo service.\n\" class=\"wp-image-90575\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page-625x641.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page-292x300.png 292w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page-112x115.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page-768x788.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page-645x662.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page-88x90.png 88w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page-362x372.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page-107x110.png 107w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page-1024x1051.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-demo-page.png 1256w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Main Street Autonomy demo page&nbsp;</em></figcaption></figure>\n\n\n\n<h2 id=\"step_2_capture_sensor_data_while_the_sensors_move\"  class=\"wp-block-heading\">Step 2: Capture sensor data while the sensors move<a href=\"#step_2_capture_sensor_data_while_the_sensors_move\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Move the sensor system and capture sensor data as previously described. Multiple ROS bags are fine, but do ensure continuous recording.</p>\n\n\n\n<p>To ensure data quality, which is crucial for calibration success, check the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Data does not contain gaps or drops. Check that compute, network, and disk buffers are not overrun and that data isn\u2019t being lost during the bagging process.</li>\n\n\n\n<li>Topics and messages are present. Check that topics are present for all the sensors you have on the system</li>\n\n\n\n<li>Time stamps are included and are accurate. Per-point time stamps are required for 3D lidar and time stamps for all other sensor data.</li>\n</ul>\n\n\n\n<h2 id=\"step_3_upload_the_sensor_data_to_the_msa_data_portal\"  class=\"wp-block-heading\"><strong>Step 3: Upload the sensor data to the MSA Data Portal</strong><a href=\"#step_3_upload_the_sensor_data_to_the_msa_data_portal\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Visit the <a href=\"https://upload.mainstreetautonomy.com\">MSA upload page</a> and authenticate with your MSA-provided credentials. Click the Manage Robots button and create a Platform and an Instance. A Platform is a specific arrangement of sensors, which might be something like DeliveryBotGen5. An Instance is a specific robot belonging to the Platform, which might be something like 12 or Mocha, if you use names.&nbsp;</p>\n\n\n\n<p>From the Dashboard page, enter a label for your sensor data, select the Robot Instance the data was collected from, and upload your sensor data. Data sent to MSA is protected as Confidential Information under MSA\u2019s<a href=\"https://mainstreetautonomy.com/privacy\"> Privacy Policy</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"487\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2-625x487.png\" alt=\"Screenshot of MSA dashboard showing log list.\" class=\"wp-image-90578\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2-625x487.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2-300x234.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2-148x115.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2-768x598.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2-1536x1196.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2-645x502.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2-385x300.png 385w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2-116x90.png 116w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2-362x282.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2-141x110.png 141w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2-1024x798.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-data-portal-2.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. MSA Data Portal</em></figcaption></figure></div>\n\n\n<h2 id=\"step_4_receive_a_calibration_package_with_isaac_perceptor-compatible_urdf_output\"  class=\"wp-block-heading\">Step 4: Receive a calibration package with Isaac Perceptor-compatible URDF output<a href=\"#step_4_receive_a_calibration_package_with_isaac_perceptor-compatible_urdf_output\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>MSA will use the Calibration Anywhere solution to calibrate the sensors used to capture the sensor data. This process can take a few days or longer for complicated setups. When complete, the calibration will be available for download from the Data Portal, as shown in Figure 1. A notification email will be sent to the user who uploaded the data.</p>\n\n\n\n<p>The calibration output includes the following:</p>\n\n\n\n<p> NVIDIA Isaac Perceptor compatible URDF: <code>extrinsics.urdf</code></p>\n\n\n\n<p> Sensor extrinsics: <code>extrinsics.yaml</code></p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Includes position [x,y,z] and quaternion [x,y,z,w] transforms between the reference point and the 6DoF pose of cameras, 3D lidars, imaging radars, and IMUs, the 3DoF pose of 2D lidars, and the 3D position of GPS/GNSS units.</li>\n</ul>\n\n\n\n<p> Sensor extrinsics: <code>wheels_cal.yaml</code></p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Includes axle track estimate (in meters).</li>\n\n\n\n<li>Includes corrective gain factors for left and right drive wheel speed (or meters-per-tick).&nbsp;</li>\n</ul>\n\n\n\n<p> Sensor intrinsics: <code>&lt;sensor_name&gt;.intrinsics.yaml</code></p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Includes OpenCV-compatible intrinsics for each imaging sensor: a model that includes a projection matrix and a distortion model.\n<ul class=\"wp-block-list\">\n<li>Supporting fisheye, equidistant, ftheta3, rational polynomial, and plumbob models.</li>\n</ul>\n</li>\n\n\n\n<li>Includes readout time for rolling shutter cameras.</li>\n</ul>\n\n\n\n<p> Ground detection: <code>ground.yaml</code></p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Includes ground relative to sensors.</li>\n</ul>\n\n\n\n<p> Timestamp corrections: <code>time_offsets.yaml</code></p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Includes time offsets calculated from a time stamp correction model for cameras, lidars, radars, IMUs, wheel encoders, and GPS/GNSS units.</li>\n</ul>\n\n\n\n<h2 id=\"step_5_import_the_urdf_into_isaac_perceptor_workflow\"  class=\"wp-block-heading\">Step 5: Import the URDF into Isaac Perceptor workflow<a href=\"#step_5_import_the_urdf_into_isaac_perceptor_workflow\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Copy the <code>extrinsics.urdf</code> file to <code>/etc/nova/calibration/isaac_calibration.urdf</code>.&nbsp;</p>\n\n\n\n<p>This is the default URDF path used by Isaac Perceptor. Figure 3 shows the workflow.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1863\" height=\"897\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1.png\" alt=\"Block diagram of end-to-end work flow of MSA\u2019s external calibration for Isaac Perceptor. \n\" class=\"wp-image-90587\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1.png 1863w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1-300x144.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1-625x301.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1-179x86.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1-768x370.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1-1536x740.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1-645x311.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1-500x241.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1-160x77.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1-362x174.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1-228x110.png 228w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/modular-3d-surround-1-1024x493.png 1024w\" sizes=\"(max-width: 1863px) 100vw, 1863px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3<strong>. </strong>NVIDIA Isaac Perceptor workflow with MSA Calibration Anywhere sensor calibration</em>. <em>Image credit: MSA</em></figcaption></figure>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Calibrating sensors using MSA Calibration Anywhere software and integrating the results with <a href=\"https://developer.nvidia.com/isaac/perceptor\">NVIDIA<strong> </strong>Isaac Perceptor</a> workflows requires careful attention to sensor setup and data collection. Ensuring that the sensor system meets the prerequisites described above is important for a fast and successful calibration.</p>\n\n\n\n<p>By following this tutorial and leveraging the resources mentioned, you\u2019ll be well-prepared to execute precise sensor calibration for your robotics or autonomous system project.</p>\n\n\n\n<p><a href=\"https://mainstreetautonomy.com/demo/\">Contact MSA for more information</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Multimodal sensor calibration is critical for achieving sensor fusion for robotics, autonomous vehicles, mapping, and other perception-driven applications. Traditional calibration methods, which rely on structured environments with checkerboards or targets, are complex, expensive, time-consuming, and don\u2019t scale.\u00a0 An automatic sensor calibration solution that simplifies the calibration problem is the Main Street Autonomy Calibration Anywhere software. &hellip; <a href=\"https://developer.nvidia.com/blog/how-to-calibrate-sensors-with-msa-calibration-anywhere-for-nvidia-isaac-perceptor/\">Continued</a></p>\n", "protected": false}, "author": 1862, "featured_media": 90668, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1505932", "discourse_permalink": "https://forums.developer.nvidia.com/t/how-to-calibrate-sensors-with-msa-calibration-anywhere-for-nvidia-isaac-perceptor/310721", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [63], "tags": [453, 1305], "coauthors": [3492, 4122], "class_list": ["post-90569", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-robotics", "tag-featured", "tag-isaac-sim"], "acf": {"post_industry": ["Manufacturing"], "post_products": ["Isaac Sim"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/msa-calibration-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nyN", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Robotics", "link": "https://developer.nvidia.com/blog/category/robotics/", "id": 63}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90569"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1862"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90569"}], "version-history": [{"count": 13, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90569/revisions"}], "predecessor-version": [{"id": 90700, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90569/revisions/90700"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90668"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90569"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90569"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90569"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90569"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90487, "date": "2024-10-22T00:00:00", "date_gmt": "2024-10-22T07:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90487"}, "modified": "2024-10-31T11:40:24", "modified_gmt": "2024-10-31T18:40:24", "slug": "a-beginners-guide-to-simulating-and-testing-robots-with-ros-2-and-nvidia-isaac-sim", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/a-beginners-guide-to-simulating-and-testing-robots-with-ros-2-and-nvidia-isaac-sim/", "title": {"rendered": "A Beginner\u2019s Guide to Simulating and Testing Robots with ROS 2 and NVIDIA Isaac Sim"}, "content": {"rendered": "\n<p><a href=\"https://www.youtube.com/watch?v=AYSfcgVv9-U\">Physical AI-powered robots</a> need to autonomously sense, plan, and perform complex tasks in the physical world. These include transporting and manipulating objects safely and efficiently in dynamic and unpredictable environments.&nbsp;&nbsp;</p>\n\n\n\n<p>Robot simulation enables developers to train, simulate, and validate these advanced systems through virtual robot learning and testing. It all happens in physics-based digital representations of environments, such as warehouses and factories, prior to deployment.&nbsp;</p>\n\n\n\n<p>In this post, we\u2019ll show you how to simulate and validate your robot stack by leveraging your <a href=\"https://github.com/ros2\">ROS 2</a> packages with <a href=\"https://developer.nvidia.com/isaac/sim\">NVIDIA Isaac Sim</a>, a reference application built on the <a href=\"https://developer.nvidia.com/omniverse\">NVIDIA Omniverse</a> platform. We\u2019ll also discuss use cases that Isaac Sim can unlock for AI-enabled robots.\u00a0</p>\n\n\n\n<p>Isaac Sim is built on <a href=\"https://developer.nvidia.com/usd\">Universal Scene Description (OpenUSD)</a>, the foundational framework for simulations in Isaac Sim. As a developer, you can efficiently design, import, build, and share robot models and virtual training environments with ease using Isaac Sim. OpenUSD is also instrumental in streamlining the connection between a robot\u2019s brain and its virtual world through the <a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/features/external_communication/ext_omni_isaac_ros_bridge.html\">ROS 2</a> interface, full-featured Python scripting, and versatile plug-ins for importing robot and environment models.&nbsp;</p>\n\n\n\n<h2 id=\"isaac_sim_with_ros_2_workflow\"  class=\"wp-block-heading\"><strong>Isaac Sim with ROS 2 workflow</strong><a href=\"#isaac_sim_with_ros_2_workflow\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The Isaac Sim to ROS 2 workflow is similar to workflows executed with other robot simulators such as <a href=\"https://gazebosim.org/home\">Gazebo</a>. At a high level, it starts with bringing your robot model into a prebuilt Isaac Sim environment. The next step entails adding sensors to the robot, followed by connecting up the relevant components to the ROS 2 action graph and simulating the robot by controlling it through your ROS 2 packages.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"2182\" height=\"878\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1.png\" alt=\"Diagram showing the workflow for Isaac Sim to ROS, from importing the robot model to adding the robot to a simulated environment, setting up sensors, and simulating the scene.\n\" class=\"wp-image-90563\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1.png 2182w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1-300x121.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1-625x251.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1-179x72.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1-768x309.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1-1536x618.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1-2048x824.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1-645x260.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1-500x201.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1-160x64.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1-362x146.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1-273x110.png 273w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-to-ros-workflow-1-1024x412.png 1024w\" sizes=\"(max-width: 2182px) 100vw, 2182px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Isaac Sim to ROS workflow</em></em></figcaption></figure>\n\n\n\n<h2 id=\"urdf_a_common_starting_point_for_simulation_in_isaac_sim\"  class=\"wp-block-heading\"><strong>URDF: A common starting point for simulation in Isaac Sim</strong><a href=\"#urdf_a_common_starting_point_for_simulation_in_isaac_sim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The ROS 2 workflow in Isaac Sim usually starts with importing the robot model through the <a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/features/environment_setup/ext_omni_isaac_urdf.html#isaac-sim-urdf-importer\">URDF importer.</a> URDF is a widely accepted format for working with robot models in simulation tools.&nbsp;</p>\n\n\n\n<p>Additionally, you can also use the built-in wizard to introduce additional files, data, and environments from third-party tools and services. By answering a few simple questions, you can find the relevant steps to bring in the right asset such as robot models, tools, and sensors into your simulation scene.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"698\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-import-wizard-2-625x698.png\" alt=\"Screenshot of the import wizard panel in Isaac Sim, with key steps numbered to import the correct asset.\n\" class=\"wp-image-90524\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-import-wizard-2-625x698.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-import-wizard-2-268x300.png 268w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-import-wizard-2-103x115.png 103w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-import-wizard-2-768x858.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-import-wizard-2-645x721.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-import-wizard-2-81x90.png 81w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-import-wizard-2-362x405.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-import-wizard-2-98x110.png 98w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-import-wizard-2.png 876w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. The import wizard in Isaac Sim</em></em></figcaption></figure>\n\n\n\n<h2 id=\"prepopulated_scenes_and_simready_assets\"  class=\"wp-block-heading\"><strong>Prepopulated scenes and SimReady assets</strong><a href=\"#prepopulated_scenes_and_simready_assets\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Similar to any robot simulation, you\u2019ll need a scene in which to simulate your robot actions. Isaac Sim offers many prebuilt 3D scenes, from simple office environments to large complex environments such as a warehouse. Additionally, you can also bring more complex 3D scenes from other tools through <a href=\"https://docs.omniverse.nvidia.com/connect/latest/index.html\">USD Connections</a>.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1100\" height=\"620\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene.png\" alt=\"A digital twin of a warehouse with shelves of boxes.\n\" class=\"wp-image-90525\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene.png 1100w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene-768x433.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene-645x364.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/isaac-sim-prepopulated-warehouse-scene-1024x577.png 1024w\" sizes=\"(max-width: 1100px) 100vw, 1100px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. A sample prepopulated warehouse scene in Isaac Sim</em></em></figcaption></figure>\n\n\n\n<p>In addition to the 3D scenes, you can also leverage more than a thousand <a href=\"https://developer.nvidia.com/omniverse/simready-assets\">SimReady assets</a>, which are physically accurate 3D objects that encompass accurate physical properties, behavior, and connected data streams to represent the real world in simulated digital worlds.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1430\" height=\"804\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim.jpg\" alt=\"Examples of SimReady assets in a grid layout.\n\" class=\"wp-image-90526\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim.jpg 1430w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim-300x169.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim-625x351.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim-179x101.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim-768x432.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim-645x363.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim-960x540.jpg 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim-500x281.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim-160x90.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim-362x204.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim-196x110.jpg 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simready-assets-isaac-sim-1024x576.jpg 1024w\" sizes=\"(max-width: 1430px) 100vw, 1430px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. SimReady assets available in Isaac Sim</em></em></figcaption></figure>\n\n\n\n<h2 id=\"adding_sensors&nbsp;&nbsp;\"  class=\"wp-block-heading\"><strong>Adding sensors&nbsp;&nbsp;</strong><a href=\"#adding_sensors&nbsp;&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Sensors enable robots to perceive their environment and take necessary action. Robots such as humanoids, manipulators, and AMRs have multiple on-board sensors comprising stereo cameras, 2D and 3D lidar, and radar. In addition, robots also have physical sensors such as contact and inertial sensors.&nbsp;</p>\n\n\n\n<p>Isaac Sim includes many <a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/features/environment_setup/assets/usd_assets_sensors.html\">third-party sensors</a> from manufacturers such as Intel, Orbbec, Stereolabs, HESAI, SICK, SLAMTEC, and more. You can also build your own custom sensor for simulation.&nbsp;</p>\n\n\n\n<p>Leveraging <a href=\"https://www.nvidia.com/en-us/design-visualization/technologies/rtx/\">NVIDIA RTX technology</a>, you can generate photorealistic images from physically accurate simulation. These rendered images can be then used for training AI models and for software-in-loop.</p>\n\n\n\n<h2 id=\"connecting_to_ros_2&nbsp;\"  class=\"wp-block-heading\"><strong>Connecting to ROS 2&nbsp;</strong><a href=\"#connecting_to_ros_2&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Isaac Sim connects to ROS 2 through the <a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/features/external_communication/ext_omni_isaac_ros_bridge.html\">ROS 2 Bridge extension</a>. This extension consists of various <a href=\"https://docs.omniverse.nvidia.com/extensions/latest/ext_omnigraph.html\">OmniGraph (OG)</a> nodes designed for ROS developers. OG nodes are not the same as ROS nodes. OG nodes provide an encapsulated piece of functionality that can be used in a connected graph structure to perform complex tasks in simulation.</p>\n\n\n\n<p>For example, to publish the time to ROS we will use two main OG nodes. The first one will read the simulation time and is the Isaac Read Simulation Time Node. The output from this node will be the input to the ROS 2 clock Publish Clockr OG node.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"463\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/publish-clock-time-isaac-sim-to-ros-2-625x463.png\" alt=\"Screenshot showing the read simulation time node connected to the time stamp publisher in the ROS 2 Publish Clock OG node.\n\" class=\"wp-image-90528\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/publish-clock-time-isaac-sim-to-ros-2-625x463.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/publish-clock-time-isaac-sim-to-ros-2-300x222.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/publish-clock-time-isaac-sim-to-ros-2-155x115.png 155w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/publish-clock-time-isaac-sim-to-ros-2-768x570.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/publish-clock-time-isaac-sim-to-ros-2-645x478.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/publish-clock-time-isaac-sim-to-ros-2-405x300.png 405w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/publish-clock-time-isaac-sim-to-ros-2-121x90.png 121w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/publish-clock-time-isaac-sim-to-ros-2-362x268.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/publish-clock-time-isaac-sim-to-ros-2-148x110.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/publish-clock-time-isaac-sim-to-ros-2.png 770w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Publish clock time from Isaac Sim to ROS 2&nbsp;</em></figcaption></figure>\n\n\n\n<p>The ROS 2 Bridge provides access to a variety of OG nodes useful for robotics tasks. These OG nodes can be used to publish data from a simulated camera or lidar, publish the transform tree of a robot and subscribe to velocity messages. Their parameters like queue size, topic name, context and QoS can also be modified. They can be connected to build a ROS 2 Action Graph which enables complex tasks like navigation and manipulation with popular ROS 2 packages.&nbsp;</p>\n\n\n\n<p>Enabling the ROS 2 Bridge gives access to <a href=\"https://github.com/ros2/rclpy\">rclpy</a>, the ROS 2 client library for Python. This makes it possible to write your custom ROS 2 code containing nodes, services and actions which can directly access and modify data from the scene and the simulated robot when scripting in Python.</p>\n\n\n\n<p>ROS 2 custom message support is enabled by sourcing your workspace prior to running Isaac Sim. Custom Python or C++ OG nodes can also be written for task specific requirements (for example, publish contact sensor state on a custom message topic).</p>\n\n\n\n<h2 id=\"moving_to_perception_ai-enabled_robots\"  class=\"wp-block-heading\"><strong>Moving to perception AI-enabled robots</strong><a href=\"#moving_to_perception_ai-enabled_robots\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Previous sections have detailed the Isaac Sim to ROS 2 workflow that should mirror your existing workflows. In the following sections, we\u2019ll take a look at some of the features of Isaac Sim for AI-enabled robots that will use perception and cognition models.&nbsp;</p>\n\n\n\n<h3 id=\"scaling_from_simple_to_complex_simulations\"  class=\"wp-block-heading\"><strong>Scaling from simple to complex simulations</strong><a href=\"#scaling_from_simple_to_complex_simulations\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To achieve autonomy at scale, you\u2019ll need to simulate the robots in complex and diverse environments. The OpenUSD foundation of Isaac Sim makes it highly extensible and scalable when it comes to robotics workflows. You can quickly scale your simulation from single robots in a work cell to multiple robots in a complex environment such as a factory or warehouse by modeling all the key elements of the real factory. The video below illustrates an example of the complex simulation you can run within Isaac Sim.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/l5M4sqaRd6w?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;start=1&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. The future of heavy industries starts as a digital twin</em></figcaption></figure>\n\n\n\n<h3 id=\"synthetic_data_generation_for_model_training\"  class=\"wp-block-heading\"><strong>Synthetic data generation for model training</strong><a href=\"#synthetic_data_generation_for_model_training\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Training perception AI models that power these robots requires a lot of data. In most instances, real-world data is extremely difficult to obtain. In addition, the real-world data may not be diverse enough to capture the multitude of scenarios and edge cases.&nbsp;</p>\n\n\n\n<p>To overcome this data gap, sensors in Isaac Sim can be used to generate synthetic data. One of the key features is domain randomization, where you can change many parameters in a simulation scene, including location, lighting, color, texture, background, and many more to generate a diverse set of training data. The additional benefit of using synthetic data is that you can iterate quickly to improve the model KPIs.&nbsp;</p>\n\n\n\n<p><a href=\"https://github.com/NVlabs/FoundationPose\">FoundationPose,</a> a unified foundation model for object pose and tracking, has been trained purely on synthetic data and can be deployed without any fine-tuning. Scaling up from a robot to larger scenes, <a href=\"https://blogs.nvidia.com/blog/ai-city-challenge-omniverse-cvpr/\">synthetic data was used to develop AI models</a> that are being used to enhance operational efficiency in retail and warehouse environments.&nbsp;</p>\n\n\n\n<h3 id=\"multi-agent_software-in-loop_testing\"  class=\"wp-block-heading\"><strong>Multi-agent software-in-loop testing</strong><a href=\"#multi-agent_software-in-loop_testing\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Facilities and warehouses usually inhabit multiple types of robots such as industrial arms, AMRs, and even humanoids that need to perform complex tasks autonomously. Isaac Sim can be used to test and validate the behavior and performance of the entire fleet of robots running their own perception stack across a multitude of scenarios that would otherwise be difficult to cover in the real world.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1858\" height=\"512\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots.png\" alt=\"A workflow diagram showing how Isaac Sim can be used to scale SIL testing.\n\" class=\"wp-image-90533\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots.png 1858w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots-300x83.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots-625x172.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots-179x49.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots-768x212.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots-1536x423.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots-645x178.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots-500x138.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots-160x44.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots-362x100.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots-399x110.png 399w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/workflow-isaac-sim-software-in-loop-multiple-robots-1024x282.png 1024w\" sizes=\"(max-width: 1858px) 100vw, 1858px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 6. Isaac Sim can be used to scale software-in-loop testing for multiple robots</em></em></figcaption></figure>\n\n\n\n<h2 id=\"more_workflows_and_use_cases\"  class=\"wp-block-heading\"><strong>More workflows and use cases</strong><a href=\"#more_workflows_and_use_cases\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Isaac Sim can be extended to additional workflows and use cases by providing you with the flexibility to create custom robotics simulators and extensions tailored to your specific needs. This versatility supports advanced robot learning techniques and scalable training, making it adaptable for various use cases.</p>\n\n\n\n<h3 id=\"robot_learning&nbsp;\"  class=\"wp-block-heading\"><strong>Robot learning&nbsp;</strong><a href=\"#robot_learning&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Isaac Sim has been further extended to robot learning required to ensure that the robot can perform its tasks in a repeatable and efficient way. For example, <a href=\"https://isaac-sim.github.io/IsaacLab/\">Isaac Lab</a>, a lightweight, open-source, reference framework built on Isaac Sim for photorealistic and fast simulations. Isaac Lab uses methods such as reinforcement and imitation learning, and provides developers a way to scale their robot policy training on multi-GPU and multi-node systems.&nbsp;</p>\n\n\n\n<h3 id=\"custom_simulators_and_extensions\"  class=\"wp-block-heading\"><strong>Custom simulators and extensions</strong><a href=\"#custom_simulators_and_extensions\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Isaac Sim can also be used to build your own custom robotics simulators or extensions for your use cases. <a href=\"https://foxglove.dev/\">Foxglove</a>, a member of the <a href=\"https://www.nvidia.com/en-us/startups/\">NVIDIA Inception Program</a> for startups, builds visualization tools for robotics developers that allows them to gain actionable insights that promote more effective collaboration, verification, and faster iteration.&nbsp;</p>\n\n\n\n<p>The Foxglove extension uses a <a href=\"https://github.com/foxglove/ws-protocol\">WebSocket Protocol</a> to seamlessly <a href=\"https://foxglove.dev/blog/realtime-isaac-sim-data-visualization-using-foxglove\">link any Isaac Sim project to a Foxglove visualization interface</a>. It automatically detects all cameras, IMUs, and articulations in the simulation stage, making their data available in Foxglove, along with the complete Transform Tree. To prevent excessive data flow, sensors are only queried when toggled on by the user in Foxglove.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1428\" height=\"874\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/foxglove-custom-extension-simulation.png\" alt=\"Screenshot of the Foxglove custom extension user interface featuring a quadruped robot.\n\" class=\"wp-image-90535\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/foxglove-custom-extension-simulation.png 1428w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/foxglove-custom-extension-simulation-300x184.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/foxglove-custom-extension-simulation-625x383.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/foxglove-custom-extension-simulation-179x110.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/foxglove-custom-extension-simulation-768x470.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/foxglove-custom-extension-simulation-645x395.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/foxglove-custom-extension-simulation-490x300.png 490w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/foxglove-custom-extension-simulation-147x90.png 147w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/foxglove-custom-extension-simulation-362x222.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/foxglove-custom-extension-simulation-180x110.png 180w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/foxglove-custom-extension-simulation-1024x627.png 1024w\" sizes=\"(max-width: 1428px) 100vw, 1428px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. The Foxglove custom extension that helps visualize and debug simulation scenarios</em></figcaption></figure>\n\n\n\n<h2 id=\"get_started_with_robotic_simulation&nbsp;\"  class=\"wp-block-heading\"><strong>Get started with robotic simulation&nbsp;</strong><a href=\"#get_started_with_robotic_simulation&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This post has explained how to connect your existing ROS workflows to Isaac Sim for testing and validation. Furthermore, we also explored some of the features and workflows enabled by Isaac Sim for synthetic data generation and software-in-loop testing for robots powered perception AI.&nbsp;</p>\n\n\n\n<p>To get started with NVIDIA Isaac Sim, <a href=\"https://www.nvidia.com/en-us/omniverse/download/\">download the standard license</a> for free. To get started with ROS workflows on Isaac Sim, check out these resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/ros_ros2_tutorials.html#isaac-sim-short-ros-2-reference-architecture\">Isaac Sim ROS 2 Reference Architecture</a></li>\n\n\n\n<li><a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/ros_ros2_tutorials.html\">Introductory Tutorials on Isaac Sim and ROS/ROS 2</a></li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/use-cases/robotics-simulation/\">Robotics Simulation Use Case</a></li>\n\n\n\n<li><a href=\"https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-OV-03+V1\">Introduction to Robotic Simulations in Isaac Sim Self-Paced Course</a></li>\n</ul>\n\n\n\n<p>Join the robotics community on the <a href=\"https://forums.developer.nvidia.com/c/omniverse/simulation/69\">NVIDIA Developer forums</a>, <a href=\"https://discord.gg/w9VvuYdq\">Discord server</a>, and <a href=\"https://www.youtube.com/@NVIDIAOmniverse\">YouTube</a> channels. Stay up to date on <a href=\"https://www.linkedin.com/showcase/nvidiarobotics\">LinkedIn</a>, <a href=\"https://www.instagram.com/nvidiarobotics/\">Instagram</a>, <a href=\"https://x.com/NVIDIARobotics\">X</a>, and <a href=\"https://www.facebook.com/NVIDIARobotics\">Facebook</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Physical AI-powered robots need to autonomously sense, plan, and perform complex tasks in the physical world. These include transporting and manipulating objects safely and efficiently in dynamic and unpredictable environments.&nbsp;&nbsp; Robot simulation enables developers to train, simulate, and validate these advanced systems through virtual robot learning and testing. It all happens in physics-based digital representations &hellip; <a href=\"https://developer.nvidia.com/blog/a-beginners-guide-to-simulating-and-testing-robots-with-ros-2-and-nvidia-isaac-sim/\">Continued</a></p>\n", "protected": false}, "author": 891, "featured_media": 90562, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1505931", "discourse_permalink": "https://forums.developer.nvidia.com/t/a-beginner-s-guide-to-simulating-and-testing-robots-with-ros-2-and-nvidia-isaac-sim/310720", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [63, 503], "tags": [453, 3700, 1410], "coauthors": [1529, 1072], "class_list": ["post-90487", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-robotics", "category-simulation-modeling-design", "tag-featured", "tag-openusd", "tag-ros"], "acf": {"post_industry": ["General"], "post_products": ["Isaac Sim", "Omniverse"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/robot-arm-code-ros-sim.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nxt", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Robotics", "link": "https://developer.nvidia.com/blog/category/robotics/", "id": 63}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90487"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/891"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90487"}], "version-history": [{"count": 21, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90487/revisions"}], "predecessor-version": [{"id": 90768, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90487/revisions/90768"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90562"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90487"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90487"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90487"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90487"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90636, "date": "2024-10-21T12:15:35", "date_gmt": "2024-10-21T19:15:35", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90636"}, "modified": "2024-10-31T11:54:06", "modified_gmt": "2024-10-31T18:54:06", "slug": "ibms-new-granite-3-0-generative-ai-models-are-small-yet-highly-accurate-and-efficient", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/ibms-new-granite-3-0-generative-ai-models-are-small-yet-highly-accurate-and-efficient/", "title": {"rendered": "IBM\u2019s New Granite 3.0 Generative AI Models Are Small, Yet Highly Accurate and Efficient"}, "content": {"rendered": "\n<p>Today, IBM released the third generation of IBM Granite, a collection of open language models and complementary tools. Prior generations of Granite focused on domain-specific use cases; the latest IBM Granite models meet or exceed the performance of leading similarly sized open models across both academic and enterprise benchmarks.&nbsp;</p>\n\n\n\n<p>The developer-friendly Granite 3.0 generative AI models are designed for function calling, supporting tool-based use cases. They were developed as workhorse enterprise models capable of serving as the primary building block of sophisticated workflows across use cases including text generation, agentic AI, classification, tool calling, summarization, entity extraction, customer service chatbots, and more.\u00a0</p>\n\n\n\n<p><strong>Introducing IBM&#8217;s Granite Generation 3 family</strong></p>\n\n\n\n<p>IBM developed the Granite series, available as an <a href=\"http://ai.nvidia.com\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NIM microservice</a>, for enterprise use, prioritizing industry-leading trust, safety and cost efficiency without compromising performance.&nbsp;</p>\n\n\n\n<p>In its entirety, the Granite 3.0 release comprises of</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Dense, text-only LLMs: <strong>Granite 3.0 8B, Granite 3.0 2B</strong></li>\n\n\n\n<li>Mixture of Experts (MoE) LLMs: <strong>Granite 3.0 3B-A800M, Granite 3.0 1B-A400M</strong></li>\n\n\n\n<li>LLM-based input-output guardrail models: <strong>Granite Guardian 8B, Granite Guardian 2B</strong></li>\n</ul>\n\n\n\n<p>Core components of Granite\u2019s architecture are: <a href=\"https://arxiv.org/pdf/2305.13245\" target=\"_blank\" rel=\"noreferrer noopener\">Group-query attention (GQA) </a>and <a href=\"https://arxiv.org/pdf/2104.09864v4\" target=\"_blank\" rel=\"noreferrer noopener\">Rotary Position Encodings (RoPE)</a> for positional information, multilayer perceptron (MLP) with <a href=\"https://arxiv.org/pdf/2002.05202v1\" target=\"_blank\" rel=\"noreferrer noopener\">SwiGLU </a>activation, <a href=\"https://arxiv.org/abs/1910.07467\">RMSNorm</a>, and shared input/output embeddings.&nbsp;</p>\n\n\n\n<p><strong>Optimized performance with speculative decoding</strong></p>\n\n\n\n<p>Trained on over 12 trillion tokens of carefully curated enterprise data, the new 8B and 2B models demonstrate significant improvements over their predecessors in both performance and speed.&nbsp;</p>\n\n\n\n<p>Speculative decoding is an optimization technique for accelerating model inference speed, helping LLMs generate text faster while using the same (or less) compute resources, and allowing more users to utilize a model at the same time. For example, in a recent IBM Research breakthrough, speculative decoding was used to cut the latency of<a href=\"https://www.ibm.com/granite/docs/models/code/\" target=\"_blank\" rel=\"noreferrer noopener\"> Granite Code 20B</a> in half while quadrupling its throughput.</p>\n\n\n\n<p>In standard<a href=\"https://research.ibm.com/blog/AI-inference-explained\" target=\"_blank\" rel=\"noreferrer noopener\"> inferencing</a>, LLMs process each previous token they\u2019ve generated thus far, then generate one token at a time. In speculative decoding, LLMs also evaluate<em> </em>several prospective tokens that might come <em>after </em>the token they\u2019re about to generate\u2014if these \u201cspeculated\u201d tokens are verified as sufficiently accurate, one pass can produce two or more tokens for the computational \u201cprice\u201d of one.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Benchmark Metric</strong></td><td><strong>Mistral 7B</strong></td><td><strong>Llama-3.1 8B</strong></td><td><strong>Granite-3.0 8B</strong></td></tr><tr><td>IFEval 0-shot</td><td>49.93</td><td>50.37</td><td><strong>52.27</strong></td></tr><tr><td>MT-Bench</td><td>7.62</td><td>8.21</td><td><strong>8.22</strong></td></tr><tr><td>AGI-Eval 5-shot</td><td>37.15</td><td><strong>41.07</strong></td><td>40.52</td></tr><tr><td>MMLU 5-shot</td><td>62.01</td><td><strong>68.27</strong></td><td>65.82</td></tr><tr><td>MMLU-Pro 5-shot</td><td>30.34</td><td><strong>37.97</strong></td><td>34.45</td></tr><tr><td>OBQA 0-shot</td><td><strong>47.40</strong></td><td>43.00</td><td>46.60</td></tr><tr><td>SIQA 0-shot</td><td>59.64</td><td>65.01</td><td><strong>71.21</strong></td></tr><tr><td>Hellaswag 10-shot</td><td><strong>84.61</strong></td><td>80.12</td><td>82.61</td></tr><tr><td>WinoGrande 5-shot</td><td><strong>78.85</strong></td><td>78.37</td><td>77.51</td></tr><tr><td>TruthfulQA 0-shot</td><td>59.68</td><td>54.07</td><td><strong>60.32</strong></td></tr><tr><td>BoolQ 5-shot</td><td>87.34</td><td>87.25</td><td><strong>88.65</strong></td></tr><tr><td>SQuAD 2.0 0-shot</td><td>18.66</td><td>21.49</td><td><strong>21.58</strong></td></tr><tr><td>ARC-C 25-shot</td><td>63.65</td><td>60.67</td><td><strong>64.16</strong></td></tr><tr><td>GPQA 0-shot</td><td>30.45</td><td>32.13</td><td><strong>33.81</strong></td></tr><tr><td>BBH 3-shot</td><td>46.73</td><td>50.81</td><td><strong>51.55</strong></td></tr><tr><td>HumanEvalSynthesis pass@1</td><td>34.76</td><td>63.41</td><td><strong>64.63</strong></td></tr><tr><td>HumanEvalExplain pass@1</td><td>21.65</td><td>45.88</td><td><strong>57.16</strong></td></tr><tr><td>HumanEvalFix pass@1</td><td>53.05</td><td><strong>68.90</strong></td><td>65.85</td></tr><tr><td>MBPP pass@1</td><td>38.60</td><td><strong>52.20</strong></td><td>49.60</td></tr><tr><td>GSM8k 5-shot, cot</td><td>37.68</td><td>65.04</td><td><strong>68.99</strong></td></tr><tr><td>MATH 4-shot</td><td>13.10</td><td><strong>34.46</strong></td><td>30.94</td></tr><tr><td>PAWS-X (7 langs) 0-shot</td><td>56.57</td><td>64.68</td><td><strong>64.94</strong></td></tr><tr><td>MGSM (6 langs) 5-shot</td><td>35.27</td><td>43.00</td><td><strong>48.20</strong></td></tr><tr><td>Average All</td><td>45.86</td><td>52.87</td><td><strong>54.33</strong></td></tr><tr><td>Open LLM Leaderboard 1</td><td>65.54</td><td>68.58</td><td><strong>69.04</strong></td></tr><tr><td>Open LLM Leaderboard 2</td><td>34.61</td><td>37.28</td><td><strong>37.56</strong></td></tr><tr><td>LiveBench</td><td>22.40</td><td><strong>27.60</strong></td><td>26.20</td></tr><tr><td>MixEval</td><td>73.55</td><td>73.35</td><td><strong>76.5</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Accuracy performance of IBM Granite-3.0 8B Instruct model across popular benchmarks compared to other foundational LLMs. </em></figcaption></figure>\n\n\n\n<p>Granite 3.0 8B Instruct kept pace with Mistral and Llama models on<a href=\"https://arxiv.org/abs/2407.11005\" target=\"_blank\" rel=\"noreferrer noopener\"> RAGBench</a>, a benchmarking dataset consisting of 100,000 retrieval augmented generation (RAG) tasks drawn from industry corpora such as user manuals.</p>\n\n\n\n<p><strong>IBM Granite\u2019s first MoE models</strong></p>\n\n\n\n<p>IBM Granite Generation 3 also includes Granite&#8217;s first mixture of experts (MoE) models, Granite-3B-A800M-Instruct and Granite-1B-A400-Instruct. Trained on over 10 trillion tokens of data, the Granite MoE models are ideal for deployment in on-device applications or situations requiring extremely low latency.</p>\n\n\n\n<p>In this architecture, the MLP layers used by the Dense models are replaced with MoE layers. Core components of Granite MoE architecture are: Fine-grained experts, <a href=\"https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/\" target=\"_blank\" rel=\"noreferrer noopener\">Dropless Token Routing</a> that ensures not a single input token is dropped by the MoE router regardless of the load imbalance among experts, and <a href=\"https://arxiv.org/abs/2408.15664\" target=\"_blank\" rel=\"noreferrer noopener\">Load Balancing Loss</a> as a strategy to maintain balanced distribution of expert load.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Benchmark Metric</strong></td><td><strong>Llama-3.2</strong></td><td><strong>SmolLM</strong></td><td><strong>Granite-3.0</strong></td></tr><tr><td>Active parameters</td><td>1B</td><td>1.7B</td><td>800M</td></tr><tr><td>Total parameters</td><td>1B</td><td>1.7B</td><td>3B</td></tr><tr><td><em>Instruction Following</em></td><td>&nbsp;</td><td>&nbsp;</td><td><strong>&nbsp;</strong></td></tr><tr><td>IFEval 0-shot</td><td>41.68</td><td>9.20</td><td><strong>42.49</strong></td></tr><tr><td>MT-Bench</td><td>5.78</td><td>4.82</td><td><strong>7.02</strong></td></tr><tr><td><em>Human Exams</em></td><td>&nbsp;</td><td>&nbsp;</td><td><strong>&nbsp;</strong></td></tr><tr><td>AGI-Eval 5-shot</td><td>19.63</td><td>19.50</td><td><strong>25.70</strong></td></tr><tr><td>MMLU 5-shot</td><td>45.40</td><td>28.47</td><td><strong>50.16</strong></td></tr><tr><td>MMLU-Pro 5-shot</td><td>19.52</td><td>11.13</td><td><strong>20.51</strong></td></tr><tr><td><em>Commonsense</em></td><td>&nbsp;</td><td>&nbsp;</td><td><strong>&nbsp;</strong></td></tr><tr><td>OBQA 0-shot</td><td>34.60</td><td>39.40</td><td><strong>40.80</strong></td></tr><tr><td>SIQA 0-shot</td><td>35.50</td><td>34.26</td><td><strong>59.95</strong></td></tr><tr><td>Hellaswag 10-shot</td><td>59.74</td><td>62.61</td><td><strong>71.86</strong></td></tr><tr><td>WinoGrande 5-shot</td><td>61.01</td><td>58.17</td><td><strong>67.01</strong></td></tr><tr><td>TruthfulQA 0-shot</td><td>43.83</td><td>39.73</td><td><strong>48.00</strong></td></tr><tr><td><em>Reading Comprehension</em></td><td>&nbsp;</td><td>&nbsp;</td><td><strong>&nbsp;</strong></td></tr><tr><td>BoolQ 5-shot</td><td>66.73</td><td>69.97</td><td><strong>78.65</strong></td></tr><tr><td>SQuAD 2.0 0-shot</td><td>16.50</td><td><strong>19.80</strong></td><td>6.71</td></tr><tr><td><em>Reasoning</em></td><td>&nbsp;</td><td>&nbsp;</td><td><strong>&nbsp;</strong></td></tr><tr><td>ARC-C 25-shot</td><td>41.38</td><td>45.56</td><td><strong>50.94</strong></td></tr><tr><td>GPQA 0-shot</td><td>25.67</td><td>25.42</td><td><strong>26.85</strong></td></tr><tr><td>BBH 3-shot</td><td>33.54</td><td>30.69</td><td><strong>37.70</strong></td></tr><tr><td><em>Code</em></td><td>&nbsp;</td><td>&nbsp;</td><td><strong>&nbsp;</strong></td></tr><tr><td>HumanEvalSynthesis pass@1</td><td>35.98</td><td>18.90</td><td><strong>39.63</strong></td></tr><tr><td>HumanEvalExplain pass@1</td><td>21.49</td><td>6.25</td><td><strong>40.85</strong></td></tr><tr><td>HumanEvalFix pass@1</td><td><strong>36.62</strong></td><td>3.05</td><td>35.98</td></tr><tr><td>MBPP</td><td><strong>37.00</strong></td><td>25.20</td><td>27.40</td></tr><tr><td><em>Math</em></td><td>&nbsp;</td><td>&nbsp;</td><td><strong>&nbsp;</strong></td></tr><tr><td>GSM8k 5-shot,cot</td><td>26.16</td><td>0.61</td><td><strong>47.54</strong></td></tr><tr><td>MATH 4-shot</td><td>17.62</td><td>0.14</td><td><strong>19.86</strong></td></tr><tr><td><em>Multilingual</em></td><td>&nbsp;</td><td>&nbsp;</td><td><strong>&nbsp;</strong></td></tr><tr><td>PAWS-X (7 langs) 0-shot</td><td>34.44</td><td>17.86</td><td><strong>50.23</strong></td></tr><tr><td>MGSM (6 langs) 5-shot</td><td>23.80</td><td>0.07</td><td><strong>28.87</strong></td></tr><tr><td>Average All</td><td>34.07</td><td>24.82</td><td><strong>40.20</strong></td></tr><tr><td><em>Open Leaderboards</em></td><td>&nbsp;</td><td>&nbsp;</td><td><strong>&nbsp;</strong></td></tr><tr><td>Open LLM Leaderboard 1</td><td>47.36</td><td>39.87</td><td><strong>55.83</strong></td></tr><tr><td>Open LLM Leaderboard 2</td><td>26.50</td><td>18.30</td><td><strong>27.79</strong></td></tr><tr><td>LiveBench</td><td>11.60</td><td>3.40</td><td><strong>16.8</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Accuracy performance of IBM Granite-3.0 MoE 3B model compared to other foundational LLMs.</em></figcaption></figure>\n\n\n\n<p><strong>Granite Guardian: leading safety guardrails</strong></p>\n\n\n\n<p>The new Guardian 3.0 8B and Granite Guardian 3.0 2B are variants of their respective correspondingly sized base pre-trained Granite models,<a href=\"https://www.ibm.com/topics/fine-tuning\" target=\"_blank\" rel=\"noreferrer noopener\"> fine-tuned</a> to evaluate and classify model inputs and outputs into various categories of risk and harm dimensions, including jailbreaking, bias, violence, profanity, sexual content, and unethical behavior. </p>\n\n\n\n<p>The Granite Guardian 3.0 models also cover a range of RAG-specific concerns, evaluating for qualities like groundedness (measuring the degree to which an output is supported by the retrieved documents), context relevance (gauging whether the documents retrieved were germane to the input prompt) and answer relevance.\u00a0</p>\n\n\n\n<p>The model family is developer-friendly, offered under the Apache 2.0 license and accompanied by new developer recipes available in IBM\u2019s Granite Community on GitHub.&nbsp;</p>\n\n\n\n<p><strong>Deploy Granite models anywhere with NVIDIA NIM</strong></p>\n\n\n\n<p>NVIDIA has partnered with IBM to offer the Granite family of models through NVIDIA NIM &#8211; a set of easy-to-use microservices designed for secure, reliable deployment of high performance AI model inferencing across clouds, data centers and workstations.</p>\n\n\n\n<p>NIM uses inference optimization engines, industry-standard APIs, and prebuilt containers to provide high-throughput AI inference that scales with demand.</p>\n\n\n\n<p>NVIDIA NIM delivers best-in-class throughput, enabling enterprises to generate more tokens, faster. For generative AI applications, token processing is the key performance metric, and increased token throughput directly translates to higher revenue for enterprises and better user experience.</p>\n\n\n\n<p><strong>Get started</strong></p>\n\n\n\n<p>Experience the <a href=\"https://build.nvidia.com/nim?q=granite\" target=\"_blank\" rel=\"noreferrer noopener\">Granite models</a> with free NVIDIA cloud credits. You can start testing the model at scale and build a proof of concept (POC) by connecting your application to the NVIDIA-hosted API endpoint running on a fully accelerated stack.&nbsp;</p>\n\n\n\n<p>Visit the <a href=\"https://www.ibm.com/granite/docs/\" target=\"_blank\" rel=\"noreferrer noopener\">documentation page</a> to download the models and deploy on any NVIDIA GPU-accelerated workstation, data center, or cloud platform.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Today, IBM released the third generation of IBM Granite, a collection of open language models and complementary tools. Prior generations of Granite focused on domain-specific use cases; the latest IBM Granite models meet or exceed the performance of leading similarly sized open models across both academic and enterprise benchmarks.&nbsp; The developer-friendly Granite 3.0 generative AI &hellip; <a href=\"https://developer.nvidia.com/blog/ibms-new-granite-3-0-generative-ai-models-are-small-yet-highly-accurate-and-efficient/\">Continued</a></p>\n", "protected": false}, "author": 2386, "featured_media": 90657, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1505538", "discourse_permalink": "https://forums.developer.nvidia.com/t/ibm-s-new-granite-3-0-generative-ai-models-are-small-yet-highly-accurate-and-efficient/310661", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110, 1903], "tags": [3266, 2932, 1958, 3613, 3270], "coauthors": [4124, 610], "class_list": ["post-90636", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "category-features", "tag-chatbot", "tag-large-language-models", "tag-news", "tag-retrieval-augmented-generation-rag", "tag-generative-ai-text"], "acf": {"post_industry": ["Academia / Education", "Cloud Services"], "post_products": ["NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/IBM-Granite-Models-NVIDIA-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nzS", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90636"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2386"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90636"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90636/revisions"}], "predecessor-version": [{"id": 90665, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90636/revisions/90665"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90657"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90636"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90636"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90636"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90636"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90546, "date": "2024-10-21T09:00:00", "date_gmt": "2024-10-21T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90546"}, "modified": "2024-10-31T09:21:26", "modified_gmt": "2024-10-31T16:21:26", "slug": "ai-accurately-forecasts-extreme-weather-up-to-23-days-ahead", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/ai-accurately-forecasts-extreme-weather-up-to-23-days-ahead/", "title": {"rendered": "AI Accurately Forecasts Extreme Weather Up to 23 Days Ahead"}, "content": {"rendered": "\n<p>New research from the University of Washington is refining AI weather models using deep learning for more accurate predictions and longer-term forecasts. The study, published in <a href=\"https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2024GL110651\"><em>Geophysical Research Letters</em></a>, shows how adjusting initial atmospheric data enables advanced AI models to extend current forecast limits. As extreme weather becomes increasingly more severe and frequent due to climate change, giving governments, businesses, the public, and emergency responders more time to prepare for natural disasters such as floods, heatwaves, or hurricanes could help reduce loss of life and property.&nbsp;</p>\n\n\n\n<p>\u201cIf a perfect weather model is given slightly imperfect initial conditions, the error compounds over time and results in an inaccurate forecast,\u201d said lead author Trent Vonich, a PhD candidate at the University of Washington. \u201cThis is especially true when modeling a chaotic system such as the Earth\u2019s atmosphere. There has been great focus recently on making better models, while somewhat ignoring the fact that a perfect model is only half the problem. Machine learning models help us address this because they are fully differentiable end-to-end, allowing us to capture nonlinear interactions between inputs and outputs\u2014something legacy techniques cannot do.\u201d\u00a0</p>\n\n\n\n<p>While state-of-the-art AI weather forecasting systems, such as Google\u2019s <a href=\"https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/\">GraphCast</a> and Huawei\u2019s<strong> </strong><a href=\"https://www.huaweicloud.com/intl/en-us/about/takeacloudleap2024/ai-weather-prediction.html\">Pangu-Weather</a>, reliably predict upcoming weather up to 10 days ahead, they&#8217;re limited by the accuracy of the initial data fed into their system.&nbsp;</p>\n\n\n\n<p>These models were trained on the massive ERA5 reanalysis dataset containing petabytes of information. The dataset captures hourly temperature, wind speed, humidity, air pressure, precipitation, and cloud cover across a global grid of 37 pressure levels. It includes historical weather conditions, dating back to 1979, and near-real-time data.\u00a0</p>\n\n\n\n<p>The researchers focused on refining the initial atmospheric variables leading up to the June 2021 Pacific Northwest Heat Wave to improve the accuracy of this extreme event. They applied nonlinear optimization using the GPU-accelerated JAX framework to optimize the data.</p>\n\n\n\n<p>According to Vonich, it takes only 20 minutes to perform 100 initial conditions updates on an <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 Tensor Core GPU</a>.</p>\n\n\n\n<figure class=\"wp-block-image size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"545\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-625x545.jpg\" alt=\"4 diagrams showing GraphCast forecasting.\" class=\"wp-image-90547\" style=\"width:840px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-625x545.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-300x262.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-132x115.jpg 132w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-768x670.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-1536x1340.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-2048x1787.jpg 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-645x563.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-344x300.jpg 344w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-103x90.jpg 103w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-362x316.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-126x110.jpg 126w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/weather-forecast-GrahpCast-1024x894.jpg 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Comparison of atmospheric patterns and temperature anomalies on June 30, 2021, for ERA5 reanalysis and GraphCast 10-day forecasts using control, globally optimized, and regionally optimized initial conditions from 20 June 2021</em></figcaption></figure>\n\n\n\n<p>The researchers tested their framework\u2019s accuracy using atmospheric data captured during the 2021 Pacific Northwest heatwave, which was excluded from the original training dataset. The optimized data reduced 10-day forecast errors by 90%, successfully predicting the intensity and timing of the heatwave. It also more than doubled the prediction window, improving unoptimized forecasts up to 23 days in advance.&nbsp;</p>\n\n\n\n<p>\u201cThis research may show that more accurate weather observations and measurements may be just as important as developing better models,\u201d Vonich said. \u201cIf this technique can be used to identify systematic biases in the initial conditions, it could have an immediate impact on improving operational forecasts. Plus, more lead time enables greater preparation for communities. Aviation, shipping, and countless other industries rely on accurate weather forecasts, too. Improvements can translate to an economic benefit for them as well.&#8221;</p>\n\n\n\n<p>Read the full news story on <a href=\"https://eos.org/research-spotlights/machine-learning-could-improve-extreme-weather-warnings\">Eos</a>.</p>\n\n\n\n<p>Catch up on the study <a href=\"https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2024GL110651#pane-pcw-figures\"><em>Predictability Limit of the 2021 Pacific Northwest Heatwave From Deep-Learning Sensitivity Analysis</em></a>.</p>\n\n\n\n<p></p>\n", "protected": false}, "excerpt": {"rendered": "<p>New research from the University of Washington is refining AI weather models using deep learning for more accurate predictions and longer-term forecasts. The study, published in Geophysical Research Letters, shows how adjusting initial atmospheric data enables advanced AI models to extend current forecast limits. As extreme weather becomes increasingly more severe and frequent due to &hellip; <a href=\"https://developer.nvidia.com/blog/ai-accurately-forecasts-extreme-weather-up-to-23-days-ahead/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 90548, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1505418", "discourse_permalink": "https://forums.developer.nvidia.com/t/ai-accurately-forecasts-extreme-weather-up-to-23-days-ahead/310642", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 2758, 503, 1903], "tags": [3941, 1913, 453, 1877], "coauthors": [2315], "class_list": ["post-90546", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-edge-computing", "category-simulation-modeling-design", "category-features", "tag-ai-impact", "tag-climate-weather-ocean-modeling", "tag-featured", "tag-research"], "acf": {"post_industry": ["General"], "post_products": ["A100"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Weather-Prediction-Hurricane.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-nyq", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "AI Platforms / Deployment", "link": "https://developer.nvidia.com/blog/category/deployment/", "id": 4150}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90546"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90546"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90546/revisions"}], "predecessor-version": [{"id": 90599, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90546/revisions/90599"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90548"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90546"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90546"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90546"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90546"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90392, "date": "2024-10-17T11:28:20", "date_gmt": "2024-10-17T18:28:20", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90392"}, "modified": "2024-10-30T12:00:27", "modified_gmt": "2024-10-30T19:00:27", "slug": "ai-medical-imagery-model-offers-fast-cost-efficient-expert-analysis", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/ai-medical-imagery-model-offers-fast-cost-efficient-expert-analysis/", "title": {"rendered": "AI Medical Imagery Model Offers Fast, Cost-Efficient Expert Analysis\u00a0"}, "content": {"rendered": "\n<p>Researchers at UCLA have developed a new AI model that can expertly analyze 3D medical images of diseases in a fraction of the time it would otherwise take a human clinical specialist.&nbsp;</p>\n\n\n\n<p>The deep-learning framework, named SLIViT (<strong>SL</strong>ice <strong>I</strong>ntegration by <strong>Vi</strong>sion <strong>T</strong>ransformer), analyzes images from different imagery modalities, including retinal scans, ultrasound videos, CTs, MRIs, and others, identifying potential disease-risk biomarkers.</p>\n\n\n\n<p>Dr. Eran Halperin, a computational medicine expert and professor at UCLA who led the study, said the model is highly accurate across a wide variety of diseases, outperforming many existing, disease-specific foundation models. It uses a novel pre-training and fine-tuning method that relies on large, accessible public data sets. As a result, Halperin believes that the model can be deployed\u2014at relatively low costs\u2014to identify different disease biomarkers, democratizing expert-level medical imaging analysis.</p>\n\n\n\n<p>The researchers used <a href=\"https://www.nvidia.com/en-us/data-center/tesla-t4/\">NVIDIA T4 GPUs</a> and<a href=\"https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf\"> NVIDIA V100 Tensor Core GPUs</a>, along with<a href=\"https://developer.nvidia.com/cuda-toolkit\"> NVIDIA CUDA</a>, to conduct their research.</p>\n\n\n\n<p>Currently, medical imagery experts are often overwhelmed. Patients often wait weeks to get their X-rays, MRIs, or CT scans evaluated before they can begin treatment.</p>\n\n\n\n<p>One of the potential advantages of SLIViT is how it can expertly analyze patient data at scale, and how its expertise can be upgraded. For example, once new medical imaging techniques are developed, the model can be fine-tuned with that new data, which can be pushed out and used in future analyses.</p>\n\n\n\n<p>Halperin noted that the model is also easily deployable. Especially in places where medical imagery experts are scarce, in the future the model could potentially make a material difference in patient outcomes.</p>\n\n\n\n<p>Before SLIViT, Dr. Halperin said, it was practically infeasible to evaluate large numbers of scans at the level of a human clinical expert. With SLIViT, large-scale, accurate analysis is realistic.</p>\n\n\n\n<p>\u201cThe model can make a dramatic impact on identifying disease biomarkers, without the need for large amounts of manually annotated images,\u201d Halperin said. \u201cThese disease biomarkers can help us understand the disease trajectory of patients. In the future, it may be possible to use these insights to tailor treatment to patients based on the biomarkers found through SLIVIT, and hopefully make a dramatic improvement in patients\u2019 lives.\u201d&nbsp;</p>\n\n\n\n<p>According to Dr. Oren Avram, lead author of a paper the UCLA researchers published in<a href=\"https://www.nature.com/articles/s41551-024-01257-9\"> Nature Biomedical Engineering</a>, the study revealed two surprising\u2014yet related\u2014results.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"600\" height=\"196\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/SLIViT-Optical-Imagery.gif\" alt=\"Two, side-by-side 3D video imagery showing two cross sections of a human retina observed via an Optical Coherence Tomography Imaging scan.\" class=\"wp-image-90542\" style=\"width:612px;height:auto\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. 3D optical coherence tomography GIF of a human retina</em></figcaption></figure></div>\n\n\n<p>First, while the model was largely pre-trained on datasets of 2D scans, it accurately identifies disease biomarkers in 3D scans of human organs. Typically, a model designed to analyze 3D images is trained on 3D datasets. But 3D medical data is far more expensive to acquire and thus far less abundant and accessible than 2D medical data.&nbsp;</p>\n\n\n\n<p>The UCLA researchers found that by pre-training their model on 2D scans\u2014which are far more accessible\u2014and fine-tuning it on a relatively small amount of 3D scans, the model outperformed specialized models trained only on 3D scans.</p>\n\n\n\n<p>The second unanticipated outcome was how good the model was at transfer learning. It learned to identify different disease biomarkers by fine-tuning on datasets consisting of imagery from very different modalities and organs.&nbsp;</p>\n\n\n\n<p>\u201cWe trained the model on 2D retinal scans\u2014so images of your eye\u2014but then fine-tuned the model on an MRI of a liver, which seemingly have no connection, because they\u2019re two totally different organs and imaging technologies,\u201d Avram said. \u201cBut we learned that between the retina and the liver, and between an OCT and MRI, some basic features are shared, and these can be used to help the model with downstream learnings even though the imagery domains are totally different.\u201d&nbsp;</p>\n\n\n\n<p>Read additional <a href=\"https://www.uclahealth.org/news/release/new-ai-model-efficiently-reaches-clinical-expert-level\">news</a> from UCLA about SLIViT.</p>\n\n\n\n<p>Check out the SLIViT paper, <em><a href=\"https://www.nature.com/articles/s41551-024-01257-9\">Accurate prediction of disease-risk factors from volumetric medical scans by a deep vision model pre-trained with 2D scans</a></em>.</p>\n\n\n\n<p>Access the model on <a href=\"https://github.com/cozygene/SLIViT\">GitHub</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Researchers at UCLA have developed a new AI model that can expertly analyze 3D medical images of diseases in a fraction of the time it would otherwise take a human clinical specialist.&nbsp; The deep-learning framework, named SLIViT (SLice Integration by Vision Transformer), analyzes images from different imagery modalities, including retinal scans, ultrasound videos, CTs, MRIs, &hellip; <a href=\"https://developer.nvidia.com/blog/ai-medical-imagery-model-offers-fast-cost-efficient-expert-analysis/\">Continued</a></p>\n", "protected": false}, "author": 2156, "featured_media": 90399, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1503802", "discourse_permalink": "https://forums.developer.nvidia.com/t/ai-medical-imagery-model-offers-fast-cost-efficient-expert-analysis/310214", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [503, 1903], "tags": [3941, 453, 90, 1877, 795], "coauthors": [3876], "class_list": ["post-90392", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-simulation-modeling-design", "category-features", "tag-ai-impact", "tag-featured", "tag-medical-imaging", "tag-research", "tag-tesla-t4"], "acf": {"post_industry": ["Healthcare & Life Sciences"], "post_products": ["CUDA", "V100"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/3D-Medical-AI-e1729025409453.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nvW", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90392"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2156"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90392"}], "version-history": [{"count": 17, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90392/revisions"}], "predecessor-version": [{"id": 90545, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90392/revisions/90545"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90399"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90392"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90392"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90392"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90392"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90313, "date": "2024-10-16T11:00:00", "date_gmt": "2024-10-16T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90313"}, "modified": "2024-10-30T12:00:28", "modified_gmt": "2024-10-30T19:00:28", "slug": "simulating-quantum-dynamics-systems-with-nvidia-gpus", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/simulating-quantum-dynamics-systems-with-nvidia-gpus/", "title": {"rendered": "Simulating Quantum Dynamics Systems with NVIDIA GPUs"}, "content": {"rendered": "\n<p>Quantum dynamics describe how objects obeying the laws of quantum mechanics interact with their surroundings, ultimately enabling predictions about how matter behaves. Accurate quantum dynamics simulations inform the development of new materials, solar cells, batteries, sensors, and many other cutting-edge technologies. They\u2019re also a critical tool in designing and building useful <a href=\"https://www.nvidia.com/en-us/glossary/quantum-computing/\">quantum computers</a>, including the design of novel types of qubits, improving gate fidelities, and performing device calibration.</p>\n\n\n\n<p>In practice, simulating quantum systems is extremely challenging. The standard steps of a dynamics simulation include preparing a quantum state, evolving it in time, and then measuring some property of the system such as the system\u2019s average energy or transition probabilities between its energy levels.\u00a0 In practice, this means solving differential equations governed by the Schrodinger Equation or the Lindblad Master Equation. Many-body quantum systems are represented by exponentially large Hilbert Spaces which make exact solutions intractable for conventional simulation methods.\u00a0\u00a0</p>\n\n\n\n<p>To overcome this, clever approximations and numerical methods are used instead. The challenge is finding approximate methods that are computationally efficient while retaining a high degree of accuracy. Techniques like tensor networks can efficiently compute the dynamics of large-scale quantum systems, but struggle with highly entangled systems. New tools are needed to extend the reach of simulation techniques and explore more interesting and relevant systems.</p>\n\n\n\n<p>Researchers Jens Eisert and Steven Thomson from the Free University of Berlin used NVIDIA GPUs to develop and test a powerful new method for simulating quantum dynamics. Their article <a href=\"https://www.nature.com/articles/s41567-024-02549-2\">Unravelling Quantum Dynamics Using Flow Equations</a>, recently featured in the journal Nature Physics, provides a powerful new GPU-accelerated method to simulate these systems.&nbsp;</p>\n\n\n\n<h2 id=\"streamlined_dynamics_simulations\"  class=\"wp-block-heading\">Streamlined dynamics simulations<a href=\"#streamlined_dynamics_simulations\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Jens and Steven tackled the challenge of simulating quantum systems using the method of <em>flow equations</em>. Instead of taking a single quantum state and evolving it in time, the flow equations method diagonalizes the Hamiltonian matrix <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=H&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"H\" class=\"latex\" /><em> </em>describing the quantum system. This is accomplished by applying a large number of infinitesimally small unitary transformations (<img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=U%5E%7B%5Cdagger%7DHU&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"U^{&#92;dagger}HU\" class=\"latex\" />, where <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=U&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"U\" class=\"latex\" /> is a unitary matrix) to the initial <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=H&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"H\" class=\"latex\" />.&nbsp;</p>\n\n\n\n<p>The full unitary transform is a time-ordered integral over the dummy flow time variable <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=l&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"l\" class=\"latex\" />. A time-ordered integral ensures that each step corresponds to the evolution of the Hamiltonian chronologically as <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=l&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"l\" class=\"latex\" /> goes from 0 to infinity. It turns out that this numerical task can be efficiently parallelized using GPUs, offering a tractable approach to simulating a system&#8217;s dynamics.</p>\n\n\n\n<p>The primary advantage of flow equations is that the simulation is not limited by the degree of entanglement, but by the desired accuracy of the numerical procedure. This means that the error is a mathematical truncation that tends to be far less restrictive than the so-called \u2018entanglement barrier\u2019, and can be systematically improved when higher accuracy is required.</p>\n\n\n\n<p>The second advantage is that a two or three dimensional system can easily be \u201cunfolded\u201d into a one-dimensional representation and solved with flow equations (Figure 1).&nbsp; The ability to simulate multidimensional systems is crucial for real-world quantum applications, which generally require consideration of more than one dimension.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1992\" height=\"858\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem.png\" alt=\"On the left: a 2D lattice. On the right: a 1D chain. Image shows that a 2D system can be formulated as a 1D problem.\" class=\"wp-image-90319\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem.png 1992w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem-300x129.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem-625x269.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem-179x77.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem-768x331.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem-1536x662.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem-645x278.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem-500x215.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem-362x156.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem-255x110.png 255w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/2d-system-formulated-as-1d-problem-1024x441.png 1024w\" sizes=\"(max-width: 1992px) 100vw, 1992px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Example of a two dimensional quantum lattice system \u201cunfolded\u201d as a one dimensional chain with nonlocal interactions.</em> Image adapted from <a href=\"https://www.nature.com/articles/s41567-024-02549-2\">Unravelling Quantum Dynamics Using Flow Equations</a></em></figcaption></figure></div>\n\n\n<p>Unfortunately, flow equations are not a panacea for simulating quantum dynamics. They struggle to converge when the initial Hamiltonian has multiple states with nearly identical energies, a common occurrence for some of the most interesting cases. This led Jens and Steven to propose the innovative idea of using so-called <em>scrambling transforms</em>. Using these to \u2018scramble\u2019 the initial Hamiltonian with an additional transformation helps remove degeneracies which would otherwise impede the diagonalization procedure (Figure 2).&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1659\" height=\"908\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach.png\" alt=\"Two different approaches to solving a Rubik's Cube puzzle, the Standard Approach and the Scrambling Transforms Approach. The Scrambling Transforms Approach improves convergence of flow equations.\n\" class=\"wp-image-90322\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach.png 1659w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach-300x164.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach-625x342.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach-179x98.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach-768x420.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach-1536x841.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach-645x353.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach-500x274.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach-160x88.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach-362x198.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach-201x110.png 201w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/scrambling-initial-hamiltonian-approach-1024x560.png 1024w\" sizes=\"(max-width: 1659px) 100vw, 1659px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. Scrambling the initial Hamiltonian can improve the convergence towards the final solution. </em>Image adapted from <a href=\"https://www.nature.com/articles/s41567-024-02549-2\">Unravelling Quantum Dynamics Using Flow Equations</a></em></figcaption></figure>\n\n\n\n<h2 id=\"large-scale_gpu-enabled_dynamics_simulations\"  class=\"wp-block-heading\">Large-scale GPU-enabled dynamics simulations<a href=\"#large-scale_gpu-enabled_dynamics_simulations\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Studies using the flow equation technique have been largely analytical, leveraging pen and paper to find clever ways of avoiding unwieldy calculations. In 2023, Steven and his colleague Marco Schir\u00f2 published foundational work for turning this promising technique into a powerful and more reliable numerical method, which can leverage the strengths of NVIDIA GPUs. For details, see <a href=\"https://scipost.org/SciPostPhys.14.5.125\">Local integrals of Motion in Quasiperiodic Many-Body Localized Systems</a>.</p>\n\n\n\n<p>The method is well suited for parallelisation, as the many underlying matrix and tensor multiplications can be efficiently split into many smaller operations. A single NVIDIA GPU (such as the NVIDIA RTX A5000 used by Steven) runs operations on tens of thousands of cores, providing a huge speedup compared to even the best multicore CPUs.</p>\n\n\n\n<p>The gap between CPU and GPU calculations grows quickly, even when only considering relatively small systems and modest GPU resources (Figure 3). Performing 24 particle simulations, which required over 2 hours to run on CPU, could be completed in under 15 minutes on a single NVIDIA GTX 1660Ti GPU. Even higher speedups are expected using more powerful data-center grade GPUs like the <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100 Tensor Core</a>, which alleviates the memory bottleneck.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1266\" height=\"553\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/graph-flow-equation-computations-acceleration-gpus.png\" alt=\"Graph showing that flow equation computations are greatly accelerated with GPUs.\n\" class=\"wp-image-90324\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/graph-flow-equation-computations-acceleration-gpus.png 1266w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/graph-flow-equation-computations-acceleration-gpus-300x131.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/graph-flow-equation-computations-acceleration-gpus-625x273.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/graph-flow-equation-computations-acceleration-gpus-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/graph-flow-equation-computations-acceleration-gpus-768x335.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/graph-flow-equation-computations-acceleration-gpus-645x282.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/graph-flow-equation-computations-acceleration-gpus-500x218.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/graph-flow-equation-computations-acceleration-gpus-160x70.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/graph-flow-equation-computations-acceleration-gpus-362x158.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/graph-flow-equation-computations-acceleration-gpus-252x110.png 252w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/graph-flow-equation-computations-acceleration-gpus-1024x447.png 1024w\" sizes=\"(max-width: 1266px) 100vw, 1266px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. GPUs provide a significant speedup (more than 8x for L = 24 particles) </em></em><em><em>over CPUs for flow equation simulations. </em>Image credit: Steven J. Thomson and Marco Schiro</em></figcaption></figure>\n\n\n\n<p>The speedup provided by GPUs enables the flow-equation technique to be employed for larger scale 2D systems, unlocking a new frontier for numerical simulations of quantum matter.</p>\n\n\n\n<p>According to Steven Thomson,<em> </em>\u201cGPUs were absolutely essential to the success of this work, and our numerical technique was developed specifically to make use of their strengths. Without them, our simulations would have taken tens or hundreds of times longer to run. This would have not only taken unreasonably long, but would also have come with a huge environmental cost due to the energy required to run our simulations for such a long time.\u201d</p>\n\n\n\n<h2 id=\"a_new_dimension_for_quantum_dynamics\"  class=\"wp-block-heading\">A new dimension for quantum dynamics<a href=\"#a_new_dimension_for_quantum_dynamics\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Future work will explore flow equation simulations of larger 2D and 3D systems, leveraging multi-node GPU systems to further push the boundaries of quantum dynamics simulations. By building on the foundation laid by Jens and Steven, researchers will be able to simulate a wider variety of quantum systems than ever before, complementing the strengths and weaknesses of existing methods such as tensor networks.</p>\n\n\n\n<h2 id=\"get_started_accelerating_your_research\"  class=\"wp-block-heading\">Get started accelerating your research<a href=\"#get_started_accelerating_your_research\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This groundbreaking work was possible in part thanks to the <a href=\"https://www.nvidia.com/en-us/industries/higher-education-research/academic-grant-program/\">NVIDIA Academic Grant Program</a>, which grants researchers free access to NVIDIA compute resources to further their work.&nbsp;Researchers focused on generative AI and large language models (LLMs), simulation and modeling (including quantum computing), data science, graphics and vision, and edge AI are encouraged to <a href=\"https://www.nvidia.com/en-us/industries/higher-education-research/academic-grant-program/\">apply</a>.&nbsp;</p>\n\n\n\n<p>To learn more about NVIDIA initiatives related to quantum computing and simulation including tools like <a href=\"https://developer.nvidia.com/cuda-q\">CUDA-Q</a> for developing large-scale quantum applications, visit <a href=\"https://www.nvidia.com/en-us/solutions/quantum-computing/\">NVIDIA Quantum</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Quantum dynamics describe how objects obeying the laws of quantum mechanics interact with their surroundings, ultimately enabling predictions about how matter behaves. Accurate quantum dynamics simulations inform the development of new materials, solar cells, batteries, sensors, and many other cutting-edge technologies. They\u2019re also a critical tool in designing and building useful quantum computers, including the &hellip; <a href=\"https://developer.nvidia.com/blog/simulating-quantum-dynamics-systems-with-nvidia-gpus/\">Continued</a></p>\n", "protected": false}, "author": 2367, "featured_media": 90317, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1503059", "discourse_permalink": "https://forums.developer.nvidia.com/t/simulating-quantum-dynamics-systems-with-nvidia-gpus/310098", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [503], "tags": [3634, 453, 2735], "coauthors": [4103, 4104, 3645, 3359], "class_list": ["post-90313", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-simulation-modeling-design", "tag-cuda-q", "tag-featured", "tag-quantum-computing"], "acf": {"post_industry": ["HPC / Scientific Computing"], "post_products": ["CUDA-Q"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/point-field-waves.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nuF", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90313"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2367"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90313"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90313/revisions"}], "predecessor-version": [{"id": 90382, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90313/revisions/90382"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90317"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90313"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90313"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90313"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90313"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90100, "date": "2024-10-16T09:50:10", "date_gmt": "2024-10-16T16:50:10", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90100"}, "modified": "2024-10-30T11:55:08", "modified_gmt": "2024-10-30T18:55:08", "slug": "maximizing-energy-and-power-efficiency-in-applications-with-nvidia-gpus", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/maximizing-energy-and-power-efficiency-in-applications-with-nvidia-gpus/", "title": {"rendered": "Maximizing Energy and Power Efficiency in Applications with NVIDIA GPUs"}, "content": {"rendered": "\n<p>As the demand for high-performance computing (HPC) and AI applications grows, so does the importance of energy efficiency. NVIDIA Principal Developer Technology Engineer, Alan Gray, shares insights on optimizing energy and power efficiency for various applications running on the latest NVIDIA technologies, including <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100 Tensor Core GPUs</a> and <a href=\"https://www.nvidia.com/en-us/data-center/dgx-platform/\">NVIDIA DGX A100</a> systems.</p>\n\n\n\n<p>Traditionally, the focus has been on maximizing performance by reducing the time to solution. However, rising energy costs and the environmental impact of data centers are pushing developers to consider energy consumption as a critical factor. Energy usage, defined as the product of power and time, can be optimized by carefully tuning GPU settings and application-level configurations.</p>\n\n\n\n<p>This session is perfect for HPC and AI developers, data center operators, and GPU programmers looking to optimize energy efficiency in conjunction with performance. It\u2019s also valuable for researchers using applications like GROMACS or AI inference models, as well as IT teams focused on reducing energy costs and environmental impact.</p>\n\n\n\n<script src=\"https://api-prod.nvidia.com/search/nvidia-search-library.js\"></script>\n \n\n<div id=\"nvidia-event-details-widget\"></div>\n<style>\n.nvidia-search-widget .cleanslate , .nvidia-search-widget .player-overlay {\ndisplay:none;\n}\n</style>\n \n\n<script>\n \n NvidiaSearchLibrary.EventSessionDetailsWidget.mount({\n          site: 'https://www.nvidia.com',\n          language: 'en-us',\n          sessionId: 'gtc24-s62419',\n          jwtToken: '',\n \u2002\u2002\u2002\u2002voltronApiUrl:  'https://api-prod.nvidia.com/services/nod/api/v1/',\n          apiUrl: 'https://api-prod.nvidia.com/search/graphql',\n           onLogin: () => { },\n          onLogout: () => { },\n       \n          onSeeAllSessions: (speakerName) => {\n            window.location.href =  'https://www.nvidia.com/en-us/on-demand/search/?q=\"' + speakerName+'\"';\n          },\n          searchApiUrl: 'https://api-prod.nvidia.com/search/graphql',\n          searchToken: '',\n          uiConfId: '50468382',\n          showSessionRating: false,\n          anonToken: '',\n        });\n \n</script>\n\n\n\n<p>Follow along with a&nbsp;<a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Energy-Efficiency-GTC.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">PDF of the session</a> as Gray dives into several key topics focused on optimizing energy and power efficiency for HPC and AI applications running on NVIDIA GPUs, including:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Introduction to energy optimization</strong>: key considerations for balancing performance and energy efficiency in HPC and AI applications.</li>\n\n\n\n<li><strong>GPU clock frequency tuning</strong>: exploring how adjusting clock frequency affects power consumption, runtime, and overall energy savings.</li>\n\n\n\n<li><strong>Application benchmarks</strong>: insights from energy optimization in workloads such as GROMACS, Quantum Espresso, and TensorRT-LLM.</li>\n\n\n\n<li><strong>Non-GPU power impact</strong>: addressing energy consumption from CPUs, memory, and cooling systems, and optimizing with techniques like Direct Liquid Cooling (DLC).</li>\n\n\n\n<li><strong>Energy efficiency on NVIDIA H100 and DGX A100</strong>: analysis of energy-saving potential on these platforms and how non-GPU components affect total power consumption.</li>\n\n\n\n<li><strong>Application-level optimizations</strong>: various application-level techniques to optimize for performance and energy efficiency.</li>\n\n\n\n<li><strong>Holistic data center energy strategies</strong>: a comprehensive approach to minimizing energy usage through hardware and software optimizations in data centers.</li>\n</ul>\n\n\n\n<p>Watch the advanced talk on&nbsp;<a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s62419/\" target=\"_blank\" rel=\"noreferrer noopener\">Energy and Power Efficiency for Applications on the Latest NVIDIA Technology</a>, explore more videos on NVIDIA On-Demand, and gain valuable skills and insights from industry experts by joining the&nbsp;<a href=\"https://developer.nvidia.com/developer-program\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Developer Program</a>.</p>\n\n\n\n<p><em>This content was partially crafted with the assistance of generative AI and LLMs. It underwent careful review and was edited by the NVIDIA Technical Blog team to ensure precision, accuracy, and quality.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>As the demand for high-performance computing (HPC) and AI applications grows, so does the importance of energy efficiency. NVIDIA Principal Developer Technology Engineer, Alan Gray, shares insights on optimizing energy and power efficiency for various applications running on the latest NVIDIA technologies, including NVIDIA H100 Tensor Core GPUs and NVIDIA DGX A100 systems. Traditionally, the &hellip; <a href=\"https://developer.nvidia.com/blog/maximizing-energy-and-power-efficiency-in-applications-with-nvidia-gpus/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 72817, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1503034", "discourse_permalink": "https://forums.developer.nvidia.com/t/maximizing-energy-and-power-efficiency-in-applications-with-nvidia-gpus/310094", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 4146, 2758, 1903], "tags": [453, 1049, 2780, 3986], "coauthors": [2315], "class_list": ["post-90100", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-development", "category-edge-computing", "category-features", "tag-featured", "tag-gromacs", "tag-hopper", "tag-nvidia-on-demand"], "acf": {"post_industry": ["HPC / Scientific Computing"], "post_products": ["DGX", "H100"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Video"], "post_collections": ["GTC March 2024"]}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-grace-hopper.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nre", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90100"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90100"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90100/revisions"}], "predecessor-version": [{"id": 90442, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90100/revisions/90442"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/72817"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90100"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90100"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90100"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90100"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90198, "date": "2024-10-16T09:30:00", "date_gmt": "2024-10-16T16:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90198"}, "modified": "2024-10-30T11:57:03", "modified_gmt": "2024-10-30T18:57:03", "slug": "scale-high-performance-ai-inference-with-google-kubernetes-engine-and-nvidia-nim", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/scale-high-performance-ai-inference-with-google-kubernetes-engine-and-nvidia-nim/", "title": {"rendered": "Scale High-Performance AI Inference with Google Kubernetes Engine and NVIDIA NIM"}, "content": {"rendered": "\n<p>The rapid evolution of AI models has driven the need for more efficient and scalable inferencing solutions. As organizations strive to harness the power of AI, they face challenges in deploying, managing, and scaling AI inference workloads. <a href=\"http://ai.nvidia.com\">NVIDIA NIM</a> and <a href=\"https://cloud.google.com/kubernetes-engine/\">Google Kubernetes Engine (GKE)</a> together offer a powerful solution to address these challenges. NVIDIA has collaborated with Google Cloud to bring NVIDIA NIM on GKE to accelerate AI inference, providing secure, reliable, and high-performance inferencing at scale with simplified deployment available on Google Cloud Marketplace.</p>\n\n\n\n<p>NVIDIA NIM, part of the <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> software platform available on Google Cloud Marketplace, is a set of easy-to-use microservices designed for secure, reliable deployment of high-performance AI model inferencing. NIM is now integrated with GKE, a managed Kubernetes service that is used to deploy and operate containerized applications at scale using Google Cloud infrastructure.\u00a0</p>\n\n\n\n<p>This post explains how NIM on GKE streamlines the deployment and management of AI inference workloads. This powerful and flexible solution for AI model inferencing leverages the robust capabilities of GKE and the NVIDIA full stack AI platform on Google Cloud.</p>\n\n\n\n<h2 id=\"easy_deployment_of_performance-optimized_inference&nbsp;\"  class=\"wp-block-heading\">Easy deployment of performance-optimized inference&nbsp;<a href=\"#easy_deployment_of_performance-optimized_inference&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The integration of NVIDIA NIM and GKE provides several key benefits for organizations looking to accelerate AI inference:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Simplified deployment</strong>: The one-click deployment feature of NVIDIA NIM on GKE through Google Cloud Marketplace makes it easy to set up and manage AI inference workloads, reducing the time and effort required for deployment.</li>\n\n\n\n<li><strong>Flexible model support</strong>: Support for a wide range of AI models, including open-source models, NVIDIA AI foundation models, and custom models, ensures that organizations can use the best models for their specific applications.</li>\n\n\n\n<li><strong>Efficient performance</strong>: Built on industry-standard technologies like <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a>, <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a>, and PyTorch, the platform delivers high-performance AI inference, enabling organizations to process large volumes of data quickly and efficiently.</li>\n\n\n\n<li><strong>Accelerated computing</strong>: Access to a range of NVIDIA GPU instances on Google Cloud\u2014including the NVIDIA H100, A100, and L4\u2014provide a range of accelerated compute options to cover a variety of workloads for a broad set of cost and performance needs.</li>\n\n\n\n<li><strong>Seamless integration</strong>: Compatibility with standard APIs and minimal coding requirements enable easy integration of existing AI applications, reducing the need for extensive rework or redevelopment.</li>\n\n\n\n<li><strong>Enterprise-grade features</strong>: Security, reliability, and scalability features ensure that AI inference workloads are protected and can handle varying levels of demand without compromising performance.</li>\n\n\n\n<li><strong>Streamlined procurement</strong>: Google Cloud Marketplace availability simplifies the acquisition and deployment process, enabling organizations to quickly access and deploy the platform as needed.</li>\n</ul>\n\n\n\n<h2 id=\"get_started_with_nvidia_nim_on_gke\"  class=\"wp-block-heading\">Get started with NVIDIA NIM on GKE<a href=\"#get_started_with_nvidia_nim_on_gke\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To get started leveraging NIM on GKE, follow the steps detailed in this section.</p>\n\n\n\n<p><strong>Step 1</strong>: Access NVIDIA NIM on <a href=\"https://console.cloud.google.com/marketplace/product/nvidia/nvidia-nim\">GKE in the Google Cloud console</a> and initiate the deployment process. Click the Launch button, and the Deployment details page appears.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"643\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-listing-gke-google-cloud-console-625x643.png\" alt=\"Screenshot of the NVIDIA NIM marketplace listing on Google Kubernetes Engine (GKE) in the Google Cloud console.\n\" class=\"wp-image-90207\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-listing-gke-google-cloud-console-625x643.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-listing-gke-google-cloud-console-292x300.png 292w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-listing-gke-google-cloud-console-112x115.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-listing-gke-google-cloud-console-768x790.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-listing-gke-google-cloud-console-645x663.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-listing-gke-google-cloud-console-88x90.png 88w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-listing-gke-google-cloud-console-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-listing-gke-google-cloud-console-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-listing-gke-google-cloud-console-362x372.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-listing-gke-google-cloud-console-107x110.png 107w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-listing-gke-google-cloud-console.png 916w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Initiate the deployment process with the NVIDIA NIM listing on GKE in the Google Cloud console</em></figcaption></figure>\n\n\n\n<p><strong>Step 2</strong>: Configure the platform to meet specific AI inference requirements, including selecting the desired AI models and setting up deployment parameters. Provide details including a deployment name. You can use an existing Service Account or create a new one.<br></p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"620\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-625x620.png\" alt=\"Screenshot of the new NVIDIA NIM deployment page showing deployment name, service account name and ID, and other details.\n\" class=\"wp-image-90210\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-625x620.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-300x297.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-116x115.png 116w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-768x762.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-645x640.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-303x300.png 303w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-91x90.png 91w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-150x150.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-362x359.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke-111x110.png 111w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-nim-deployment-page-gke.png 948w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Create a new deployment service account or use an existing account for NIM on GKE</em></figcaption></figure>\n\n\n\n<p>Next, select the appropriate GPU in a specific region corresponding to the instance type from the drop-down menu.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"459\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-gpu-instance-625x459.png\" alt=\"Screenshot of drop-down menu with NVIDIA GPU instance and region selected/highlighted.\n\" class=\"wp-image-90212\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-gpu-instance-625x459.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-gpu-instance-300x220.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-gpu-instance-157x115.png 157w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-gpu-instance-768x564.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-gpu-instance-645x473.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-gpu-instance-409x300.png 409w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-gpu-instance-123x90.png 123w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-gpu-instance-362x266.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-gpu-instance-150x110.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-gpu-instance.png 812w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Select the NVIDIA GPU instance and region</em></figcaption></figure>\n\n\n\n<p><strong>Step 3</strong>: Select your NIM from the drop-down menu.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"520\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-nim-model-625x520.png\" alt=\"Drop-down menu to select the correct NIM model name.\n\" class=\"wp-image-90213\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-nim-model-625x520.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-nim-model-300x250.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-nim-model-138x115.png 138w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-nim-model-768x639.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-nim-model-645x537.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-nim-model-360x300.png 360w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-nim-model-108x90.png 108w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-nim-model-362x301.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-nim-model-132x110.png 132w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/menu-nvidia-nim-model.png 824w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Select your NIM model</em></figcaption></figure>\n\n\n\n<p><strong>Step 4:</strong> Read and accept the EULA, then click Deploy. The deployment takes approximately 15-20 minutes, depending on the NIM and cluster parameters that you choose.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"704\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/deployment-gke-nim-1-625x704.png\" alt=\"Screenshot showing required configuration, governing terms and conditions, and a blue Deploy button. \n\" class=\"wp-image-90214\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/deployment-gke-nim-1-625x704.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/deployment-gke-nim-1-266x300.png 266w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/deployment-gke-nim-1-102x115.png 102w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/deployment-gke-nim-1-768x866.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/deployment-gke-nim-1-645x727.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/deployment-gke-nim-1-80x90.png 80w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/deployment-gke-nim-1-362x408.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/deployment-gke-nim-1-98x110.png 98w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/deployment-gke-nim-1.png 834w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Deployment takes approximately 15-20 minutes</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"297\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/status-nim-deployment-625x297.png\" alt=\"Screenshot of view status of your NIM deployment.\n\" class=\"wp-image-90216\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/status-nim-deployment-625x297.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/status-nim-deployment-300x142.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/status-nim-deployment-179x85.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/status-nim-deployment-768x365.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/status-nim-deployment-645x306.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/status-nim-deployment-500x237.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/status-nim-deployment-160x76.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/status-nim-deployment-362x172.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/status-nim-deployment-232x110.png 232w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/status-nim-deployment-1024x486.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/status-nim-deployment.png 1154w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Status of NIM deployment</em></figcaption></figure>\n\n\n\n<p><strong>Step 5:</strong> Get credentials for the GKE cluster that was created. Navigate to the <a href=\"https://console.cloud.google.com/projectselector2/kubernetes/list/overview?supportedpurview=project\">Google Cloud console</a> to find the new cluster. Then select the Option menu \u2192 Connect to grab credentials for it.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\ngcloud container clusters get-credentials $CLUSTER --region $REGION --project $PROJECT\n</pre></div>\n\n\n<p>After the cluster is running, run the inference by setting up port forwarding to the NIM container:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nkubectl -n nim port-forward service/my-nim-nim-llm 8000:8000 &amp;\n</pre></div>\n\n\n<p>Next, run an inference request against the NIM endpoint using the following curl commands:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\ncurl -X GET &#039;http://localhost:8000/v1/health/ready&#039;\n\ncurl -X GET &#039;http://localhost:8000/v1/models&#039;\n\ncurl -X &#039;POST&#039; \\\n    &#039;http://localhost:8000/v1/chat/completions&#039; \\\n    -H &#039;accept: application/json&#039; \\\n    -H &#039;Content-Type: application/json&#039; \\\n    -d &#039;{\n  &quot;messages&quot;: &#x5B;\n    {\n      &quot;content&quot;: &quot;You are a polite and respectful chatbot helping people plan a vacation.&quot;,\n      &quot;role&quot;: &quot;system&quot;\n    },\n    {\n      &quot;content&quot;: &quot;What should I do for a 4 day vacation in Spain?&quot;,\n      &quot;role&quot;: &quot;user&quot;\n    }\n  ],\n  &quot;model&quot;: &quot;meta/llama-3.1-8b-instruct&quot;,\n  &quot;max_tokens&quot;: 4096,\n  &quot;top_p&quot;: 1,\n  &quot;n&quot;: 1,\n  &quot;stream&quot;: true,\n  &quot;stop&quot;: &quot;\\n&quot;,\n  &quot;frequency_penalty&quot;: 0.0\n}&#039;\n</pre></div>\n\n\n<p>For reranking models, use the following call:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n# rerank-qa\ncurl -X &#039;POST&#039; \\\n  &#039;http://localhost:8000/v1/ranking&#039; \\\n  -H &#039;accept: application/json&#039; \\\n  -H &#039;Content-Type: application/json&#039; \\\n  -d &#039;{\n&quot;query&quot;: {&quot;text&quot;: &quot;which way should i go?&quot;},\n&quot;model&quot;: &quot;nvidia/nv-rerankqa-mistral-4b-v3&quot;,\n&quot;passages&quot;: &#x5B;\n{\n&quot;text&quot;: &quot;two roads diverged in a yellow wood, and sorry i could not travel both and be one traveler, long i stood and looked down one as far as i could to where it bent in the undergrowth;&quot;\n},\n{\n&quot;text&quot;: &quot;then took the other, as just as fair, and having perhaps the better claim because it was grassy and wanted wear, though as for that the passing there had worn them really about the same,&quot;\n},\n{\n&quot;text&quot;: &quot;and both that morning equally lay in leaves no step had trodden black. oh, i marked the first for another day! yet knowing how way leads on to way i doubted if i should ever come back.&quot;\n}\n]\n}&#039;\n</pre></div>\n\n\n<p>For embedding models, use the following call:<span style=\"font-size: 11pt;font-family: Arial, sans-serif;background-color: transparent;vertical-align: baseline\"></span></p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n# embed\ncurl -X &quot;POST&quot; \\\n  &quot;http://localhost:8000/v1/embeddings&quot; \\\n  -H &#039;accept: application/json&#039; \\\n  -H &#039;Content-Type: application/json&#039; \\\n  -d &#039;{\n&quot;input&quot;: &#x5B;&quot;Hello world&quot;],\n&quot;model&quot;: &quot;nvidia/nv-embedqa-e5-v5&quot;,\n&quot;input_type&quot;: &quot;query&quot;\n}&#039;\n</pre></div>\n\n\n<p>Ensure that you have the correct URL and the model mentioned under the model parameter. </p>\n\n\n\n<p>You can also load test and get performance metrics such as throughput and latency of the deployed model using the <a href=\"https://docs.nvidia.com/nim/benchmarking/llm/latest/overview.html#executive-summary\">NVIDIA GenAI-Perf</a> tool.</p>\n\n\n\n<p>Integrate existing AI applications and models with NVIDIA NIM on GKE, leveraging standard APIs and compatibility features to ensure seamless operation. Scale AI inference workloads as needed, using the platform&#8217;s scalability features to handle varying levels of demand and optimize resource usage.</p>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\">Summary<a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai/#referrer=ai-subdomain\">NVIDIA NIM</a> on GKE is a powerful solution for accelerating AI inference, offering ease of use, broad model support, robust foundations, seamless compatibility, and enterprise-grade security, reliability, and scalability. Enterprises can now enhance their AI capabilities, streamline deployment processes, and achieve high-performance AI inferencing at scale. NVIDIA NIM on GKE provides the tools and infrastructure needed to drive innovation and deliver impactful AI solutions. Find <a href=\"https://console.cloud.google.com/marketplace/product/nvidia/nvidia-nim\">NVIDIA NIM on Google Cloud Marketplace</a>.\u00a0</p>\n\n\n\n<p>To learn more, see <a href=\"https://cloud.google.com/blog/products/containers-kubernetes/nvidia-nims-are-available-on-gke\">Efficiently Serve Optimized AI models with NVIDIA NIM Microservices on GKE</a>.<span id=\"docs-internal-guid-410aff4c-7fff-72b9-3ada-c7e7b17ea431\"></span></p>\n", "protected": false}, "excerpt": {"rendered": "<p>The rapid evolution of AI models has driven the need for more efficient and scalable inferencing solutions. As organizations strive to harness the power of AI, they face challenges in deploying, managing, and scaling AI inference workloads. NVIDIA NIM and Google Kubernetes Engine (GKE) together offer a powerful solution to address these challenges. NVIDIA has &hellip; <a href=\"https://developer.nvidia.com/blog/scale-high-performance-ai-inference-with-google-kubernetes-engine-and-nvidia-nim/\">Continued</a></p>\n", "protected": false}, "author": 1363, "featured_media": 90203, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1503025", "discourse_permalink": "https://forums.developer.nvidia.com/t/scale-high-performance-ai-inference-with-google-kubernetes-engine-and-nvidia-nim/310091", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 4146, 4147, 1205], "tags": [296, 453, 572, 3739], "coauthors": [2764, 1302, 3987, 4111], "class_list": ["post-90198", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-data-science", "category-development", "category-models", "category-networking-communications", "tag-ai-inference-microservices", "tag-featured", "tag-kubernetes", "tag-nim"], "acf": {"post_industry": ["General"], "post_products": ["NIM"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-google-cloud-logos.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nsO", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90198"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1363"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90198"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90198/revisions"}], "predecessor-version": [{"id": 90438, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90198/revisions/90438"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90203"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90198"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90198"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90198"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90198"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89987, "date": "2024-10-16T09:00:00", "date_gmt": "2024-10-16T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89987"}, "modified": "2024-10-30T11:57:31", "modified_gmt": "2024-10-30T18:57:31", "slug": "treating-brain-disease-with-brain-machine-interactive-neuromodulation-and-nvidia-jetson", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/treating-brain-disease-with-brain-machine-interactive-neuromodulation-and-nvidia-jetson/", "title": {"rendered": "Treating Brain Disease with Brain-Machine Interactive Neuromodulation and NVIDIA Jetson"}, "content": {"rendered": "\n<p>Neuromodulation is a technique that enhances or restores brain function by directly intervening in neural activity. It is commonly used to treat conditions like Parkinson&#8217;s disease, epilepsy, and depression. The shift from open-loop to closed-loop neuromodulation strategies enables on-demand modulation, improving therapeutic effects while reducing side effects. This could lead to significant advancements in precision and personalized electronic medicine.</p>\n\n\n\n<p>Closed-loop neuromodulation strategies face challenges in real-time neural decoding and encoding. Machine learning algorithms and neural networks are used to interpret complex neural activity linked to various pathological states. However, we need precise interventions to restore neural functions affected by disease. Mobile systems facilitate research on patients under conditions of free-moving and chronic treatment.</p>\n\n\n\n<h2 id=\"brain-machine_interactive_neuromodulation_research_tool\"  class=\"wp-block-heading\">Brain-Machine Interactive Neuromodulation Research Tool<a href=\"#brain-machine_interactive_neuromodulation_research_tool\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To address these challenges, the researchers developed the Brain-Machine Interactive Neuromodulation Research Tool (BMINT). This tool senses neural activity, processes data using machine learning algorithms and neural networks, and delivers real-time electrical stimulation. It enables bidirectional information transfer between the brain and the tool, employing edge AI computing for efficient real-time signal processing.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"904\" height=\"526\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-workflow.png\" alt=\"The research tool consists of three modules: recording, computing, and stimulation. The recording and stimulation modules are connected to the Jetson computing module through SPI and UART, respectively. Machine learning algorithms and neural networks are integrated into the research tool through the intelligent computing framework.\" class=\"wp-image-90080\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-workflow.png 904w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-workflow-300x175.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-workflow-625x364.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-workflow-179x104.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-workflow-768x447.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-workflow-645x375.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-workflow-500x291.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-workflow-155x90.png 155w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-workflow-362x211.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-workflow-189x110.png 189w\" sizes=\"(max-width: 904px) 100vw, 904px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. BMINT workflow</em></figcaption></figure></div>\n\n\n<p>The BMINT consists of three main hardware modules:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Recording:</strong> Includes eight channels for recording neurophysiological signals with a 24-bit amplitude resolution and a 2000 Hz sampling frequency. The researchers used an <a href=\"https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit\">NVIDIA Jetson Nano developer kit</a> as a computing module because of edge AI computing capability and compact form factor, low power consumption.</li>\n\n\n\n<li><strong>Computing:</strong>&nbsp; Has various I/O ports to deliver control commands to interface with other neuromodulation devices, i.e., transcranial magnetic stimulation (TMS) and ultrasonic stimulation.</li>\n\n\n\n<li><strong>Stimulation:</strong> Delivers 2-channel constant current electrical stimulation, with pulse parameters (amplitude, frequency and pulse width) adjusted in real time.</li>\n</ul>\n\n\n\n<p>The researchers choose the NVIDIA Jetson platform as the computing module because of public pretrained AI model libraries available, pretrained AI models (for example, NVIDIA NGC, TorchVision, and TensorFlow Hub), and various optimization and acceleration tools (for example, CUDA, cuDNN, and NVIDIA TensorRT) for custom-trained AI models.</p>\n\n\n\n<p>The data is saved and streamed to Jetson through its serial peripheral interface (SPI) functionality. After data is fed into Jetson, the GPU-accelerated algorithm, such as SVM, CNN, and RNN, is applied to process the signal in real time. Finally, the model uses UART to drive the stimulation module.</p>\n\n\n\n<h2 id=\"results\"  class=\"wp-block-heading\">Results<a href=\"#results\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The Jetson Nano in the BMINT increased the computation efficiency by about 14.77x<strong> </strong>compared to using the CPU alone. Considering the cycles of important beta or gamma neural oscillations related to brain diseases, that is, about 50 ms or 15 ms, the ideal system time delay should be stable and below these cycles so that cycle-by-cycle phase modulation could be possible.&nbsp;</p>\n\n\n\n<p>The BMINT research tool achieved the lowest system time delay of 2.829 \u00b1 0.057 ms from the onset of the input pulse to the output stimulus, while maintaining a stable, real-time performance.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"904\" height=\"498\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-intelligent-computing-framework.png\" alt=\"The diagram shows that the intelligent computing framework integrates the AI ecosystem and supports the data streaming of hardware. The data from the recording module are sent to a real-time processing pipeline. The data are passed through processing algorithms, including preprocessing, neural decoding and stimulation encoding, to generate stimulation signals with specific parameters. The stimuli control commands are sent to the stimulation module and delivered to the brain.\" class=\"wp-image-90081\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-intelligent-computing-framework.png 904w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-intelligent-computing-framework-300x165.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-intelligent-computing-framework-625x344.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-intelligent-computing-framework-179x99.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-intelligent-computing-framework-768x423.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-intelligent-computing-framework-645x355.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-intelligent-computing-framework-500x275.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-intelligent-computing-framework-160x88.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-intelligent-computing-framework-362x199.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-intelligent-computing-framework-200x110.png 200w\" sizes=\"(max-width: 904px) 100vw, 904px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. BMINT intelligent computing framework</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"864\" height=\"636\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-closed-loop-algorithm-performance.png\" alt=\"One diagram shows its performance mostly achieved over 90% in AUC, accuracy, sensitivity, specificity across 10 patients. The other diagram shows it has a short stimulation latency in a couple of milliseconds.\" class=\"wp-image-90082\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-closed-loop-algorithm-performance.png 864w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-closed-loop-algorithm-performance-300x221.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-closed-loop-algorithm-performance-625x460.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-closed-loop-algorithm-performance-156x115.png 156w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-closed-loop-algorithm-performance-768x565.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-closed-loop-algorithm-performance-645x475.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-closed-loop-algorithm-performance-408x300.png 408w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-closed-loop-algorithm-performance-122x90.png 122w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-closed-loop-algorithm-performance-362x266.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/bmint-closed-loop-algorithm-performance-149x110.png 149w\" sizes=\"(max-width: 864px) 100vw, 864px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Algorithm performance and the complete closed-loop neuromodulation process using BMINT</em></figcaption></figure></div>\n\n\n<p>In the simulated online demonstration, the BMINT successfully showcased the complete process of applying the ML model for real-time, closed-loop neuromodulation in epilepsy.&nbsp;</p>\n\n\n\n<p>The sensitivity of the model was 96.16 % (patient 10, Fig. 3a). In the 500s online process, there are six false positive detections and the false positive rate was about 1.42 % (6 samples/423 samples).&nbsp;</p>\n\n\n\n<p>In epilepsy treatment with deep brain stimulation, the algorithms are generally tuned to achieve better sensitivity so that the seizure can be suppressed successfully. Optimal performance with both high sensitivity and specificity is highly desired to avoid unnecessary stimulation and deliver effective stimulation to suppress seizures.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\"><strong>Conclusion</strong><a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The BMINT research tool enabled intelligent closed-loop neuromodulation and has capabilities of neural sensing of local field potentials; ECoG, EEG, or EMGs; computing with mainstream ML algorithms; and delivering electrical stimulation pulse by pulse in real time.&nbsp;</p>\n\n\n\n<p>The research tool achieved a system time delay of less than 3 ms, computing capabilities in feasible computation cost, efficient deployment of machine learning algorithms, and neural network acceleration process.</p>\n\n\n\n<p>For more information, see the <a href=\"https://doi.org/10.1016/j.heliyon.2024.e32609\">Brain-Machine Interactive Neuromodulation Research Tool with Edge AI Computing</a> paper and the <a href=\"https://github.com/gaosiy/research-tool-seizure-detection\">/gaosiy/research-tool-seizure-detection</a> GitHub repo.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Neuromodulation is a technique that enhances or restores brain function by directly intervening in neural activity. It is commonly used to treat conditions like Parkinson&#8217;s disease, epilepsy, and depression. The shift from open-loop to closed-loop neuromodulation strategies enables on-demand modulation, improving therapeutic effects while reducing side effects. This could lead to significant advancements in precision &hellip; <a href=\"https://developer.nvidia.com/blog/treating-brain-disease-with-brain-machine-interactive-neuromodulation-and-nvidia-jetson/\">Continued</a></p>\n", "protected": false}, "author": 2357, "featured_media": 90074, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1503004", "discourse_permalink": "https://forums.developer.nvidia.com/t/treating-brain-disease-with-brain-machine-interactive-neuromodulation-and-nvidia-jetson/310088", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2758, 63], "tags": [3941, 296, 453, 1957], "coauthors": [4092, 4093, 4094], "class_list": ["post-89987", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-edge-computing", "category-robotics", "tag-ai-impact", "tag-ai-inference-microservices", "tag-featured", "tag-neuroscience"], "acf": {"post_industry": ["Healthcare & Life Sciences"], "post_products": ["Jetson"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/brain-machine-interactive-neuromodulation-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-npp", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Robotics", "link": "https://developer.nvidia.com/blog/category/robotics/", "id": 63}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89987"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2357"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89987"}], "version-history": [{"count": 3, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89987/revisions"}], "predecessor-version": [{"id": 90279, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89987/revisions/90279"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90074"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89987"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89987"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89987"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89987"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89970, "date": "2024-10-16T09:00:00", "date_gmt": "2024-10-16T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89970"}, "modified": "2024-10-29T14:00:38", "modified_gmt": "2024-10-29T21:00:38", "slug": "simplify-ai-application-development-with-nvidia-cloud-native-stack", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/simplify-ai-application-development-with-nvidia-cloud-native-stack/", "title": {"rendered": "Simplify AI Application Development with NVIDIA Cloud Native Stack"}, "content": {"rendered": "\n<p>In the rapidly evolving landscape of AI and data science, the demand for scalable, efficient, and flexible infrastructure has never been higher. Traditional infrastructure can often struggle to meet the demands of modern AI workloads, leading to bottlenecks in development and deployment processes. As organizations strive to deploy AI models and data-intensive applications at scale, cloud-native technologies have emerged as a game-changer.&nbsp;</p>\n\n\n\n<p>To help organizations with their AI application development process, NVIDIA has developed and validated <a href=\"https://github.com/NVIDIA/cloud-native-stack\">NVIDIA Cloud Native Stack</a> (CNS), an open-source reference architecture used by NVIDIA to test and certify all supported AI software.</p>\n\n\n\n<p>CNS enables you to run and test containerized GPU-accelerated applications orchestrated by Kubernetes, with easy access to features such as Multi-Instance GPU (MIG) and GPUDirect RDMA on platforms that support these features. CNS is for development and testing purposes, but applications developed on CNS can subsequently be run in production on enterprise Kubernetes-based platforms.</p>\n\n\n\n<p>This post explores the following key areas:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>The components and benefits of CNS</li>\n\n\n\n<li>How KServe on CNS enhances AI model evaluation and deployment</li>\n\n\n\n<li>Implementing NVIDIA NIM with these solutions in your AI infrastructure</li>\n</ul>\n\n\n\n<h2 id=\"cns_overview\"  class=\"wp-block-heading\">CNS overview<a href=\"#cns_overview\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>CNS provides a reference architecture that includes various versioned software components tested together to ensure optimal operation, including the following:\u00a0</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Kubernetes</li>\n\n\n\n<li>Helm</li>\n\n\n\n<li>Containerd</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html\">NVIDIA GPU Operator</a>&nbsp;</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/networking/display/kubernetes2441/index.html\">NVIDIA Network Operator</a></li>\n</ul>\n\n\n\n<p>NVIDIA GPU Operator simplifies the ability to run AI workloads on cloud-native technologies, providing an easy way to experience the latest NVIDIA features: </p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://www.nvidia.com/en-us/technologies/multi-instance-gpu/\">Multi-Instance GPU (MIG)</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/gpudirect\">GPUDirect RDMA</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/gpudirect\">GPUDirect Storage</a></li>\n\n\n\n<li>GPU monitoring capabilities</li>\n</ul>\n\n\n\n<p>CNS also includes optional add-on tools:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>microK8s</li>\n\n\n\n<li>Storage</li>\n\n\n\n<li>LoadBalancer</li>\n\n\n\n<li>Monitoring&nbsp;</li>\n\n\n\n<li><a href=\"https://kserve.github.io/website/master/get_started/#install-the-kserve-quickstart-environment\">KServe</a></li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"307\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cloud-native-stack-architecture-625x307.png\" alt=\"Diagram shows storage, LoadBalancer, monitoring, MicroK8s, and KServe as the top layer. Next is MIG, GPU Monitoring, and GPUDirect RDMA over GPU Driver Container, NVIDIA Container Toolkit, Device Plug-in, and Network Driver Container. Last four layers are GPU Operator and Network Operator over Kubernetes, Container Engine, and Linux Distribution.\" class=\"wp-image-89975\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cloud-native-stack-architecture-625x307.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cloud-native-stack-architecture-300x148.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cloud-native-stack-architecture-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cloud-native-stack-architecture-768x378.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cloud-native-stack-architecture-645x317.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cloud-native-stack-architecture-500x246.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cloud-native-stack-architecture-160x79.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cloud-native-stack-architecture-362x178.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cloud-native-stack-architecture-224x110.png 224w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cloud-native-stack-architecture-1024x504.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-cloud-native-stack-architecture.png 1478w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. CNS components</em></figcaption></figure></div>\n\n\n<p>CNS abstracts away much of the complexity involved in setting up and maintaining these environments, enabling you to focus on prototyping and testing AI applications, rather than assembling and managing the underlying software infrastructure.&nbsp;</p>\n\n\n\n<p>Applications developed on CNS are assured to be compatible with deployments based on <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, providing a smooth transition from development to production. In addition, Kubernetes platforms that adhere to the versions of components as defined in this stack are assured to run NVIDIA AI software in a supported manner.</p>\n\n\n\n<p>CNS can be deployed on bare metal, cloud, or VM-based environments. It is available in <a href=\"https://github.com/NVIDIA/cloud-native-stack/tree/master/install-guides\">Install Guides</a> for manual installations and <a href=\"https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks\">Ansible Playbooks</a> for automated installations. For more information, see <a href=\"https://github.com/NVIDIA/cloud-native-stack?tab=readme-ov-file#getting-started\">Getting Started</a>.&nbsp;</p>\n\n\n\n<p>Add-on tools are disabled by default. For more information about enabling add-on tools, see <a href=\"https://github.com/NVIDIA/cloud-native-stack/blob/master/playbooks/readme.md#installation\">NVIDIA Cloud Native Stack Installation</a>.&nbsp; &nbsp;</p>\n\n\n\n<p>Interested in a pre-provisioned CNS environment? <a href=\"https://www.nvidia.com/en-us/data-center/launchpad/\">NVIDIA LaunchPad</a> provides pre-provisioned environments to get you started quickly.</p>\n\n\n\n<h2 id=\"enhancing_ai_model_evaluation\"  class=\"wp-block-heading\">Enhancing AI model evaluation<a href=\"#enhancing_ai_model_evaluation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://github.com/kserve/kserve\">KServe</a> is a powerful tool that enables organizations to serve machine learning models efficiently in a cloud-native environment. By using the scalability, resilience, and flexibility of Kubernetes, KServe simplifies the prototyping and development of sophisticated AI models and applications.</p>\n\n\n\n<p>NNS with KServe enables the deployment of Kubernetes clusters that can handle the complex workflows associated with AI model training and inference.&nbsp;</p>\n\n\n\n<h2 id=\"deploying_nvidia_nim_with_kserve&nbsp;\"  class=\"wp-block-heading\">Deploying NVIDIA NIM with KServe&nbsp;<a href=\"#deploying_nvidia_nim_with_kserve&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Deploying <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a> on CNS with KServe not only simplifies the development process but also ensures that your AI workflows are scalable, resilient, and easy to manage. By using Kubernetes and KServe, you can seamlessly integrate NVIDIA NIM with other microservices, creating a robust and efficient AI application development platform. For more information, see <a href=\"https://blogs.nvidia.com/blog/kserve-nim-inference/\">KServe Providers Dish Up NIMble Inference in Clouds and Data Centers</a>.\u00a0</p>\n\n\n\n<p>Follow the instructions to install the <a href=\"https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks#enable-kserve-on-cns\">CNS with KServe.</a> After KServe is deployed on the cluster, we recommend enabling the storage and monitoring option to monitor the performance of the deployed model and scale the services as needed.&nbsp;</p>\n\n\n\n<p>Then, follow the steps for <a href=\"https://github.com/NVIDIA/cloud-native-stack/tree/master/playbooks#example-deploying-nim-on-top-of-kserve\">Deploying NIM on Kserve</a>. For more information about different ways to deploy NIM, see NIM-Deploy, which contains examples of how NIM can be deployed using KServe and Helm charts as well.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>CNS is a reference architecture that is intended for development and testing purposes. It represents a significant advancement in the deployment and management of generative AI and data science workloads because the software stack from CNS has been fully tested to work seamlessly together.&nbsp;</p>\n\n\n\n<p>CNS, combined with KServe, provides a robust solution for simplifying AI model and application development. With this validated reference architecture, you can overcome the complexities of infrastructure management and focus on driving innovation in your AI initiatives. The flexibility to run on bare metal, cloud, or VM-based environments; scalability; and ease of use make CNS an ideal choice for organizations of all sizes.</p>\n\n\n\n<p>Whether you&#8217;re deploying NIM microservices, using KServe for model serving, or integrating advanced GPU features, CNS provides the tools and capabilities needed to accelerate AI innovation and provides a path to bring powerful solutions to production with greater efficiency and ease.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In the rapidly evolving landscape of AI and data science, the demand for scalable, efficient, and flexible infrastructure has never been higher. Traditional infrastructure can often struggle to meet the demands of modern AI workloads, leading to bottlenecks in development and deployment processes. As organizations strive to deploy AI models and data-intensive applications at scale, &hellip; <a href=\"https://developer.nvidia.com/blog/simplify-ai-application-development-with-nvidia-cloud-native-stack/\">Continued</a></p>\n", "protected": false}, "author": 808, "featured_media": 89972, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1503005", "discourse_permalink": "https://forums.developer.nvidia.com/t/simplify-ai-application-development-with-nvidia-cloud-native-stack/310089", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 852, 3110, 1205], "tags": [572], "coauthors": [1352, 3227], "class_list": ["post-89970", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-data-center-cloud", "category-generative-ai", "category-networking-communications", "tag-kubernetes"], "acf": {"post_industry": ["General"], "post_products": ["AI Enterprise", "NIM"], "post_learning_levels": ["General Interest"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cloud-native-stack-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-np8", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89970"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/808"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89970"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89970/revisions"}], "predecessor-version": [{"id": 90361, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89970/revisions/90361"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89972"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89970"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89970"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89970"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89970"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90299, "date": "2024-10-15T13:28:17", "date_gmt": "2024-10-15T20:28:17", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90299"}, "modified": "2024-10-17T11:18:56", "modified_gmt": "2024-10-17T18:18:56", "slug": "future-proof-your-networking-stack-with-nvidia-doca-ofed", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/future-proof-your-networking-stack-with-nvidia-doca-ofed/", "title": {"rendered": "Future-Proof Your Networking Stack with NVIDIA DOCA-OFED"}, "content": {"rendered": "\n<p>The <a href=\"https://developer.nvidia.com/networking/doca\">NVIDIA DOCA software platform</a> unlocks the potential of the <a href=\"https://www.nvidia.com/en-us/networking/products/data-processing-unit/\">NVIDIA BlueField networking platform</a> and provides all needed host drivers for NVIDIA BlueField and ConnectX devices. Optimized for peak performance, DOCA equips users to meet the demands of increasingly complex workloads. Its modular structure offers the flexibility needed to adapt to emerging technologies and higher data throughputs.\u00a0</p>\n\n\n\n<p>With a recent networking software release, NVIDIA transitioned from the MLNX_OFED suite of drivers and tools for InfiniBand and Ethernet solutions, to <a href=\"https://docs.nvidia.com/doca/sdk/nvidia+mlnx_ofed+to+doca-ofed+transition+guide/index.html\">DOCA-OFED</a>, marking the end of standalone MLNX_OFED releases. This shift reflects the move toward a more unified, scalable, and programmable networking stack integrated within the NVIDIA DOCA framework. The final version of MLNX_EN, a lightweight subset of MLNX_OFED, will also be released, with future features accessible only through DOCA.&nbsp;</p>\n\n\n\n<p>DOCA-OFED will provide the same robust driver functionality as MLNX_OFED, supported by the open-source community, with enhanced programmability through DOCA.&nbsp;</p>\n\n\n\n<p>Customers and partners are strongly encouraged to transition to DOCA to benefit from a modern, feature-rich networking stack built for the future of accelerated computing and AI workloads. DOCA is the exclusive software platform for new features and updates going forward, ensuring access to the latest networking advancements. By transitioning to DOCA, you can future-proof your infrastructure by accessing NVIDIA\u2019s latest innovations in accelerated networking through a continually evolving software stack.&nbsp;</p>\n\n\n\n<p>This post introduces DOCA-OFED and explains how it replaces MLNX_OFED, and why you should make the switch today.</p>\n\n\n\n<h2 id=\"introducing_doca-ofed\"  class=\"wp-block-heading\">Introducing DOCA-OFED<a href=\"#introducing_doca-ofed\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA has introduced DOCA-OFED as part of the DOCA-Host package. DOCA-Host is a unified software package for host servers, incorporating all the essential components of DOCA and MLNX_OFED. DOCA-Host has four installation profiles, which are designed to work seamlessly with the latest NVIDIA hardware innovations, including BlueField Data Processing Units (DPUs) and SuperNICs, and ConnectX Network Interface Cards (NICs).\u00a0</p>\n\n\n\n<p>The DOCA-OFED package is the new replacement for MNLX_OFED, including the same components, drivers, and tools.</p>\n\n\n\n<h3 id=\"key_features_of_doca-ofed\"  class=\"wp-block-heading\">Key features of DOCA-OFED<a href=\"#key_features_of_doca-ofed\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Unified software package</strong>: DOCA-OFED is part of the DOCA-Host package, which simplifies the installation and configuration process. This unified approach means you have a \u201cone-stop shop\u201d to manage your entire network stack on all your host servers, reducing complexity and potential errors.</li>\n\n\n\n<li><strong>Broad compatibility</strong>: Whether you are using BlueField DPUs or SuperNICs, ConnectX, or both, DOCA-OFED is compatible with all these devices. This broad compatibility provides flexibility in deployment and ensures you can leverage the latest hardware innovations from NVIDIA.</li>\n\n\n\n<li><strong>Comprehensive toolset</strong>: DOCA-OFED includes all the drivers, user-space libraries, and utilities that you are familiar with from MLNX_OFED, ensuring you have everything you need to manage your network effectively.</li>\n\n\n\n<li><strong>Standard Linux installation</strong>: DOCA-Host is available as a public repo, supporting installation and management with standard Linux package managers.</li>\n</ul>\n\n\n\n<h2 id=\"why_transition_to_doca-ofed\"  class=\"wp-block-heading\">Why transition to DOCA-OFED?<a href=\"#why_transition_to_doca-ofed\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Transitioning to DOCA-OFED is not just an upgrade. It&#8217;s a step toward ensuring that your network infrastructure is ready to handle upcoming demands and can leverage future hardware and software advancements from NVIDIA.</p>\n\n\n\n<p>&nbsp;Here\u2019s why you should consider making the switch:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Simplified management</strong>: Managing your network stack becomes more straightforward with DOCA-OFED as part of the DOCA-Host package. This unified approach means a single package on all of your hosts that is easier to install and configure, reducing the potential for errors and simplifying ongoing management tasks.</li>\n\n\n\n<li><strong>Future-ready</strong>: DOCA-OFED is designed with future-proofing in mind. By adopting this new solution, you ensure your network infrastructure is ready to handle upcoming demands and can leverage future hardware and software advancements from NVIDIA.</li>\n\n\n\n<li><strong>Ongoing innovation: </strong>MLNX_OFED will no longer be available as a stand-alone product starting in 2025 and will be supported only with long-term support, with all new devices from NVIDIA being supported only with DOCA-OFED. All new features will only be included in DOCA-OFED.</li>\n</ul>\n\n\n\n<h2 id=\"get_started_with_doca-ofed\"  class=\"wp-block-heading\">Get started with DOCA-OFED<a href=\"#get_started_with_doca-ofed\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Making the switch to DOCA-OFED is straightforward. Follow these steps to get started:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li><strong>Access DOCA-Host online or download the DOCA-Host package</strong>: The <a href=\"https://developer.nvidia.com/doca-downloads\">DOCA-Host download package</a> includes everything you need to get started with DOCA-OFED.&nbsp;</li>\n\n\n\n<li><strong>Uninstall MLNX_OFED: </strong>Remove any previous installation of MLNX_OFED.</li>\n\n\n\n<li><strong>Install DOCA-OFED</strong>: Follow the installation instructions included in the DOCA-Host package. The installation process is streamlined and designed to be user-friendly, ensuring you can get up and running quickly. Whether you have BlueField DPUs, BlueField SuperNICs, ConnectX, or a combination of the three, the installation process is designed to accommodate all configurations.</li>\n\n\n\n<li><strong>Test and validate</strong>: Once you have completed the installation and configuration, it is essential to test and validate your new setup. This step ensures everything is working correctly and that you are achieving the expected performance improvements.</li>\n\n\n\n<li><strong>Develop your applications</strong>: DOCA-OFED fully supports the applications and pipelines you have already developed based on MLNX_OFED. With DOCA-OFED you can best utilize your DPU and NIC devices and get the best out of your network.&nbsp;</li>\n</ol>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>DOCA enables the most optimized AI fabric for AI clouds and factories, network management, and optimization. By transitioning from MLNX_OFED to DOCA-OFED, you are not only upgrading your network stack, but also ensuring your network infrastructure is ready to handle upcoming demands and can leverage future hardware and software advancements from NVIDIA. With simplified, unified management and a comprehensive toolset, DOCA-OFED is the ideal choice for modern data centers and AI fabrics. DOCA-OFED is the one software stack to manage your NVIDIA networking devices.&nbsp;</p>\n\n\n\n<p>Embrace the future of data center networking with DOCA-OFED. <a href=\"https://developer.nvidia.com/doca-downloads\">Download the DOCA-Host package</a> to start the transition and experience the next level of high-performance networking.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The NVIDIA DOCA software platform unlocks the potential of the NVIDIA BlueField networking platform and provides all needed host drivers for NVIDIA BlueField and ConnectX devices. Optimized for peak performance, DOCA equips users to meet the demands of increasingly complex workloads. Its modular structure offers the flexibility needed to adapt to emerging technologies and higher &hellip; <a href=\"https://developer.nvidia.com/blog/future-proof-your-networking-stack-with-nvidia-doca-ofed/\">Continued</a></p>\n", "protected": false}, "author": 1610, "featured_media": 90302, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1502331", "discourse_permalink": "https://forums.developer.nvidia.com/t/future-proof-your-networking-stack-with-nvidia-doca-ofed/309961", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 852, 1205], "tags": [1466, 1461, 453], "coauthors": [3141], "class_list": ["post-90299", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-cybersecurity", "category-data-center-cloud", "category-networking-communications", "tag-bluefield", "tag-doca", "tag-featured"], "acf": {"post_industry": ["General"], "post_products": ["BlueField DPU", "DOCA"], "post_learning_levels": ["Beginner Technical", "Intermediate Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-doca.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nur", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Networking / Communications", "link": "https://developer.nvidia.com/blog/category/networking-communications/", "id": 1205}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90299"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1610"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90299"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90299/revisions"}], "predecessor-version": [{"id": 90386, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90299/revisions/90386"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90302"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90299"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90299"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90299"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90299"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89677, "date": "2024-10-15T11:00:00", "date_gmt": "2024-10-15T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89677"}, "modified": "2024-10-18T13:10:29", "modified_gmt": "2024-10-18T20:10:29", "slug": "train-highly-accurate-llms-with-the-zyda-2-open-5t-token-dataset-processed-with-nvidia-nemo-curator", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/train-highly-accurate-llms-with-the-zyda-2-open-5t-token-dataset-processed-with-nvidia-nemo-curator/", "title": {"rendered": "Train Highly Accurate LLMs with the Zyda-2 Open 5T-Token Dataset Processed with NVIDIA NeMo Curator"}, "content": {"rendered": "\n<p>Open-source datasets have significantly democratized access to high-quality data, lowering the barriers of entry for developers and researchers to train cutting-edge <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI models</a>. By providing free access to diverse, high-quality, and well-curated datasets, open-source datasets enable the open-source community to train models at or close to the frontier, facilitating the rapid advancement of AI.</p>\n\n\n\n<p><a href=\"https://www.zyphra.com/\">Zyphra</a> makes AI systems more accessible, exploring the frontiers of performance through cutting-edge architectures, and advancing the study and understanding of powerful models.&nbsp;</p>\n\n\n\n<p>To achieve its vision, the Zyphra team has been collaborating closely with the <a href=\"https://developer.nvidia.com/blog/scale-and-curate-high-quality-datasets-for-llm-training-with-nemo-curator/\">NVIDIA NeMo Curator</a> team to create Zyda-2, an open, high-quality, pretraining dataset comprising an impressive 5T tokens in English, 5x the size of <a href=\"https://www.zyphra.com/post/zyda\">Zyda-1</a>. This dataset encompasses a vast range of topics and domains and ensures a high level of diversity and quality, which are critical for training robust and competitive models such as <a href=\"https://huggingface.co/Zyphra/Zamba2-2.7B\">Zamba</a>.&nbsp;</p>\n\n\n\n<h2 id=\"train_highly_accurate_llms_with_zyda-2&nbsp;\"  class=\"wp-block-heading\">Train highly accurate LLMs with Zyda-2&nbsp;<a href=\"#train_highly_accurate_llms_with_zyda-2&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Zyda-2 is ideal for general high-quality language model pretraining that is especially focused on language proficiency, as opposed to code and math that require additional specialized datasets. This is because Zyda-2 possesses the strengths of the top existing datasets while improving on their weaknesses.</p>\n\n\n\n<p>Figure 1 shows the Zyda-2 outperforms existing state-of-the-art open-source language modeling datasets in aggregate evaluation scores. The Zyphra team performed this ablation study using the Zamba2-2.7B parameter model and the aggregate score is a mean of MMLU, Hellaswag, Piqa, Winogrande, Arc-Easy, and Arc-Challenge.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"745\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda-2-performance-training-1024x745.png\" alt=\"Bar graph shows that Zamba2-2.7B annealed using Zyda-2 outperforms other popular datasets in aggregate evaluation scores. The aggregate score is a mean of MMLU, Hellaswag, Piqa, Winogrande, Arc-Easy, and Arc-Challenge.\" class=\"wp-image-89687\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda-2-performance-training-1024x745.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda-2-performance-training-300x218.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda-2-performance-training-625x455.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda-2-performance-training-158x115.png 158w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda-2-performance-training-768x559.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda-2-performance-training-645x469.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda-2-performance-training-412x300.png 412w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda-2-performance-training-124x90.png 124w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda-2-performance-training-362x263.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda-2-performance-training-151x110.png 151w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda-2-performance-training.png 1462w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Performance of Zyda-2 compared to other open-source language modeling datasets</em></figcaption></figure></div>\n\n\n<p>Zyphra\u2019s training methodology is to focus on maximizing model quality and efficiency at a given memory and latency budget, both for on-device and cloud deployments. The Zyphra team has also trained Zamba2-7B, a 7B-parameter hybrid model on an early version of the Zyda-2 dataset that has outperformed other frontier models on the leaderboard, testifying to the strength of this dataset at scale.&nbsp;</p>\n\n\n\n<p>Access the <a href=\"https://build.nvidia.com/zyphra/zamba2-7b-instruct\" target=\"_blank\" rel=\"noreferrer noopener\">Zamba2-7B</a> packaged as an <a href=\"http://ai.nvidia.com\">NVIDIA NIM microservice</a> for easy deployment on any GPU-accelerated system or through industry-standard APIs.</p>\n\n\n\n<h2 id=\"building_blocks_of_zyda-2\"  class=\"wp-block-heading\">Building blocks of Zyda-2<a href=\"#building_blocks_of_zyda-2\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Zyda-2 combines the existing sources of open high-quality tokens such as DCLM, FineWeb-edu, Dolma, and Zyda-1. It performs robust filtering and cross-deduplication to improve the performance of each dataset alone. Zyda-2 combines the best elements of these datasets with many high-quality educational samples for logical reasoning and factual knowledge, while its other Zyda-1 component provides more diversity and variety and excels at more linguistic and writing tasks.&nbsp;</p>\n\n\n\n<p>In short, while each component dataset has its own strengths and weaknesses, the combined Zyda-2 dataset can fill these gaps. The total training budget to obtain a given model quality is reduced compared to the naive combination of these datasets through the use of deduplication and aggressive filtering.&nbsp;</p>\n\n\n\n<p>Here\u2019s how Zyphra has used NVIDIA NeMo Curator to build its data processing pipelines and improve the quality of the data.&nbsp;</p>\n\n\n\n<h2 id=\"nemo_curator\u2019s_role_in_creating_the_dataset\"  class=\"wp-block-heading\">NeMo Curator\u2019s role In creating the dataset<a href=\"#nemo_curator\u2019s_role_in_creating_the_dataset\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NeMo Curator is a GPU-accelerated data curation library that improves generative AI model performance by processing large-scale, high-quality datasets for pretraining and customization.</p>\n\n\n\n<p>Yury Tokpanov, dataset lead at Zyphra said, \u201cNeMo Curator played a crucial role in bringing the dataset to market faster. By using GPUs to accelerate the data processing pipelines, our team reduced the total cost of ownership (TCO) by 2x and processed the data 10x faster (from 3 weeks to 2 days). Because of the quality improvement in data, It was worth it for us to stop training, do processing with NeMo Curator, and train a model on the processed dataset.\u201d</p>\n\n\n\n<p>To accelerate workflows on GPUs, NeMo Curator uses RAPIDS libraries such as cuDF, cuML, and cuGraph and scales to 100+ TB of data. High-quality data is crucial for improving the accuracy of generative AI models. To continually enhance data quality, NeMo Curator supports several techniques such as exact, fuzzy,&nbsp; and semantic deduplication, <a href=\"https://huggingface.co/collections/nvidia/nemo-curator-classifier-models-66b25154213dafdcb8bde900\">classifier models</a>, and <a href=\"https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/nemotron_340B_synthetic_datagen\">synthetic data generation</a>.&nbsp;</p>\n\n\n\n<p>With NeMo Curator, Zyphra has been able to streamline the process of data pre-processing, cleaning, and organization, ultimately leading to a dataset that is well-suited for developing advanced language models.&nbsp;</p>\n\n\n\n<p>NeMo Curator\u2019s features, including deduplication and quality classification, were essential for distilling the raw component datasets of Zyda-2 into the highest-quality subset to be used for training. The <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/gpudeduplication.html\">fuzzy deduplication</a> techniques based on LSH minhashing included in NeMo Curator helped Zyphra\u2019s team find and remove 13% of data from the DCLM dataset as duplicates found in other datasets.</p>\n\n\n\n<p>The <a href=\"https://huggingface.co/nvidia/quality-classifier-deberta\">quality classifier model</a> was also used to assess the Dolma-CC and Zyda-1 component data subsets, marking 25% and 17% of them respectively as high quality. Zyda&#8217;s team has found that including only the high-quality subset in the final dataset improved performance.&nbsp;</p>\n\n\n\n<p>Figure 2 represents the improvement in accuracy when trained on high-quality subsets of the original datasets. The chart shows training on 50B tokens of the full Zyda and Dolma datasets compared to training only those documents labeled \u2018high\u2019 in the quality classifier from NeMo Curator.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"690\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/from-scratch-training-transformer-1024x690.png\" alt=\"Bar graph shows significant improvements through filtering. The aggregate score is a mean of MMLU, Hellaswag, Piqa, Winogrande, Arc-Easy, and Arc-Challenge.\" class=\"wp-image-89686\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/from-scratch-training-transformer-1024x690.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/from-scratch-training-transformer-300x202.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/from-scratch-training-transformer-625x421.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/from-scratch-training-transformer-171x115.png 171w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/from-scratch-training-transformer-768x517.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/from-scratch-training-transformer-645x434.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/from-scratch-training-transformer-445x300.png 445w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/from-scratch-training-transformer-134x90.png 134w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/from-scratch-training-transformer-362x244.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/from-scratch-training-transformer-163x110.png 163w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/from-scratch-training-transformer.png 1532w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Effect of from-scratch training for a 1.4B transformer</em></figcaption></figure></div>\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Download the <a href=\"https://huggingface.co/datasets/Zyphra/Zyda2\">Zyda-2 dataset</a> directly from Hugging Face and train higher-accuracy models. It comes with an ODC-By license which enables you to train on or build off of Zyda-2 subject to the license agreements and terms of use of the original data sources.</p>\n\n\n\n<p>For more information, see the Zyda-2 tutorial on the <a href=\"https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/zyda2-tutorial\">/NVIDIA/NeMo-Curator</a> GitHub repo. You can also try the <a href=\"https://build.nvidia.com/zyphra/zamba2-7b-instruct\">Zamba2-7B NIM microservice</a> for free directly from the NVIDIA API Catalog.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Open-source datasets have significantly democratized access to high-quality data, lowering the barriers of entry for developers and researchers to train cutting-edge generative AI models. By providing free access to diverse, high-quality, and well-curated datasets, open-source datasets enable the open-source community to train models at or close to the frontier, facilitating the rapid advancement of AI. &hellip; <a href=\"https://developer.nvidia.com/blog/train-highly-accurate-llms-with-the-zyda-2-open-5t-token-dataset-processed-with-nvidia-nemo-curator/\">Continued</a></p>\n", "protected": false}, "author": 1921, "featured_media": 90177, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1502292", "discourse_permalink": "https://forums.developer.nvidia.com/t/train-highly-accurate-llms-with-the-zyda-2-open-5t-token-dataset-processed-with-nvidia-nemo-curator/309950", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 3110], "tags": [3273, 453, 2932], "coauthors": [3612, 4059, 4060, 4061, 4062], "class_list": ["post-89677", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-generative-ai", "tag-accelerated-data-analytics", "tag-featured", "tag-large-language-models"], "acf": {"post_industry": ["General"], "post_products": ["NeMo", "NeMo Curator"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/zyda2-open-token-dataset-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nkp", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89677"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1921"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89677"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89677/revisions"}], "predecessor-version": [{"id": 90609, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89677/revisions/90609"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90177"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89677"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89677"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89677"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89677"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90242, "date": "2024-10-15T09:35:00", "date_gmt": "2024-10-15T16:35:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90242"}, "modified": "2024-10-17T11:18:58", "modified_gmt": "2024-10-17T18:18:58", "slug": "supermicro-launches-nvidia-bluefield-powered-jbof-to-optimize-ai-storage", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/supermicro-launches-nvidia-bluefield-powered-jbof-to-optimize-ai-storage/", "title": {"rendered": "Supermicro Launches NVIDIA BlueField-Powered JBOF to Optimize AI Storage"}, "content": {"rendered": "\n<p>The growth of AI is driving exponential growth in computing power and a doubling of networking speeds every few years. Less well-known is that it\u2019s also putting new demands on storage.&nbsp;</p>\n\n\n\n<p>Training new models typically requires high-bandwidth networked access to petabytes of data, while inference with the latest types of retrieval augmented generation (RAG) requires low-latency access to hundreds of terabytes of storage.\u00a0New models also run training, indexing, and retrieval on rich image and video data. Many of the new AI inference indices and search tools rely on vector databases and must also preserve extensive metadata about all embedded content.\u00a0\u00a0</p>\n\n\n\n<p>Traditional file storage often doesn\u2019t work well for these new workloads, and traditional storage hardware is not the most efficient or performant way to support AI. Often, object storage is the best architecture for large amounts of data.\u00a0New storage solutions are needed that offer better performance and efficiency, relative to both price and energy consumption.\u00a0\u00a0</p>\n\n\n\n<h2 id=\"enter_the_dpu_for_storage\"  class=\"wp-block-heading\">Enter the DPU for storage<a href=\"#enter_the_dpu_for_storage\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.supermicro.com/en/\">Supermicro</a> has long offered many JBOF (just a bunch of flash) solutions deployed as direct-attached storage or networked file or object storage. Now they are launching a new JBOF that is powered by the <a href=\"https://www.nvidia.com/en-us/networking/products/data-processing-unit/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA BlueField data processing unit (DPU)</a>.\u00a0</p>\n\n\n\n<p>Instead of using a traditional storage server design with separate CPU, memory, and networking cards, the new JBOF uses the DPU to run the software, connect to the network, support remote management, and accelerate critical networking, storage, and security functions in customized silicon.&nbsp;</p>\n\n\n\n<p>The DPU card replaces the CPU, network cards, DRAM modules, CPU PCIe switch, crypto accelerators, BMC, and remote management port. External PCIe switches may still be required to connect to SSDs.&nbsp;</p>\n\n\n\n<p>BlueField is the NVIDIA market-leading DPU and it is optimized to offload and accelerate networking, storage, security, and management features. Because the DPU unifies networking, CPU, memory controllers, PCIe switch, and traffic accelerators on one chip, data can move directly between SSDs and the high-speed network ports without needing to be processed by an external CPU or cross a separate PCIe bus multiple times. This enables low-latency storage access and makes the system more efficient in terms of price, performance, and power efficiency.&nbsp;&nbsp;</p>\n\n\n\n<p>The BlueField DPU storage controller card supports up to 400 Gb/s of network traffic and can also accelerate the NVMe over Fabrics (NVMe-oF) storage protocol and other RDMA-based storage traffic. It also acts as the PCIe root complex to manage the SSDs and uses its Arm cores to run the storage software allowing the JBOF to present as block, file, or object storage and deploy as part of a scale-out storage solution.&nbsp;</p>\n\n\n\n<p>BlueField also provides security offloads and remote management functionality\u2014including its own baseboard management controller (BMC) and separate management ports\u2014typically required for large-scale data center deployments and cloud operations.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"295\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler-1024x295.png\" alt=\"A comparison of two JBOF designs shows the traditional JBOF with a separate CPU, network card, memory, and PCIe switch. The simplified JBOF shows one DPU card replacing those multiple components while providing similar or often better functionality.\u00a0\" class=\"wp-image-90236\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler-1024x295.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler-300x86.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler-625x180.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler-179x52.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler-768x221.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler-1536x443.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler-645x186.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler-500x144.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler-160x46.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler-362x104.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler-382x110.png 382w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/jbof-design-simpler.png 1600w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The DPU card replaces the CPU, network card, PCIe switch, memory, and BMC, resulting in a simpler and more efficient design</em></figcaption></figure></div>\n\n\n<h2 id=\"new_supermicro_jbof\"  class=\"wp-block-heading\">New Supermicro JBOF<a href=\"#new_supermicro_jbof\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Supermicro\u2019s new 2RU JBOF showcases a flexible design, supporting either 36 E3.S SSDs or 24 U.2 SSDs, with a raw capacity of up to 1.44 PB, later supporting up to 2 PB with newer 60-TB SSDs. It holds two controller canisters, each capable of supporting up to two BlueField-3 DPUs and one NVIDIA GPU. </p>\n\n\n\n<p>The JBOF can be deployed with two canisters for active-active or active-passive high availability within each JBOF chassis or with just one canister for maximum efficiency in cloud storage situations where redundancy and failover are handled by software across many JBOFs.\u00a0</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"443\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd-1024x443.png\" alt=\"The diagram shows two front-view renderings of the new Supermicro JBOF show it can support different form factors of single- or dual-ported SSDs for storage. Two rear views of the JBOF show that it can support one or two slide-in controller canisters, each containing up to two BlueField-3 DPU cards and one NVIDIA GPU card.\" class=\"wp-image-90235\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd-1024x443.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd-300x130.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd-625x270.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd-179x77.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd-768x332.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd-1536x664.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd-645x279.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd-500x216.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd-362x157.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd-254x110.png 254w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-single-dual-ssd.png 1600w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. The new Supermicro JBOF can use 36 E3.S or 24 U.2 NVMe SSDs, either single-port or dual-port</em></figcaption></figure></div>\n\n\n<p>Figure 2 shows that each controller canister houses up to two NVIDIA BlueField-3 DPUs and one NVIDIA GPU card. Customers can deploy two canisters per JBOF for high availability or one canister for higher cost efficiency and reduced power consumption.</p>\n\n\n\n<p>The dense design is ideal for both scale-up and scale-out file and object storage, while the high networking throughput\u2014up to 800Gb/s per JBOF\u2014supports the needs of AI training and HPC workloads.</p>\n\n\n\n<p>Testing of the Supermicro JBOF showed that under a storage workload with one BlueField DPU installed, it saturates the 400-Gb/s network connections. The BlueField-based JBOF also shows 13% lower latency for a small-block (4 KB) random read workload, clocking in at 86 <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;mu\" class=\"latex\" />s for the new JBOF versus 100 <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Cmu&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;mu\" class=\"latex\" />s for a traditional X86-based JBOF.</p>\n\n\n\n<p>The use of a single DPU card to replace the CPU, memory, networking card, and BMC results in a significant power savings of up to 50% for the non-SSD subsystem or 10-15% power savings for the entire JBOF (including the SSDs). In a large, scale-out storage deployment, this can mean kilowatts of power savings across the data center.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1020\" height=\"824\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/high-performance-distributed-solution.png\" alt=\"Two diagrams show the traditional storage host and target connection with the new host-target connection. In both, an NVIDIA ConnectX card provides high-speed networking to the storage host (initiator). One diagram shows the storage target JBOF using a BlueField DPU as the controller instead of the traditional CPU-based design, resulting in higher performance and efficiency compared to traditional JBOF designs.\" class=\"wp-image-90234\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/high-performance-distributed-solution.png 1020w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/high-performance-distributed-solution-300x242.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/high-performance-distributed-solution-625x505.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/high-performance-distributed-solution-142x115.png 142w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/high-performance-distributed-solution-768x620.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/high-performance-distributed-solution-645x521.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/high-performance-distributed-solution-371x300.png 371w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/high-performance-distributed-solution-111x90.png 111w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/high-performance-distributed-solution-362x292.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/high-performance-distributed-solution-136x110.png 136w\" sizes=\"(max-width: 1020px) 100vw, 1020px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. Storage solutions can use NVIDIA ConnectX network cards in each host (initiator) and the NVIDIA BlueField DPU as the controller in the JBOF storage target, such as the new Supermicro JBOF.</em></em></figcaption></figure></div>\n\n\n<h2 id=\"solution_partners\"  class=\"wp-block-heading\">Solution partners<a href=\"#solution_partners\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This JBOF is ideal for cloud deployments and object storage. As such, Supermicro is teaming up with multiple partners including Cloudian, Hammerspace, Kioxia, and Micron.&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<p>Cloudian delivers scale-out, high-performance object storage optimized for fast access to large content, including video, audio, photos, and vector databases. The Cloudian software runs on&nbsp;both the storage clients and on the BlueField DPU in the Supermicro JBOF, taking advantage of the faster performance and higher efficiency in the JBOF.&nbsp;</p>\n\n\n\n<p>Hammerspace supports a scale-out parallel file system for optimum shared access to billions of files and petabytes of data. Many AI and HPC workloads require file storage, and the Hammerspace solution provides a global namespace, parallel access for higher performance, and data orchestration services to automate the flow of data. The server side of the Hammerspace software runs on the BlueField DPU within the Supermicro JBOF.&nbsp;&nbsp;</p>\n\n\n\n<p>Kioxia and Micron deliver innovative SSDs supporting different capacities, performance levels, and cost points, available in both the U.2 and E3.S form factors. Supermicro includes the optimum SSD type for each customer\u2019s storage workload.&nbsp;&nbsp;</p>\n\n\n\n<p>Supermicro is working with additional software, SSD, and systems partners to integrate this new JBOF into additional scalable storage solutions.&nbsp;&nbsp;</p>\n\n\n\n<h2 id=\"see_it_at_the_ocp_global_summit\"  class=\"wp-block-heading\">See it at the OCP Global Summit<a href=\"#see_it_at_the_ocp_global_summit\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The new Supermicro BlueField-powered JBOF will be on display in the Supermicro booth (B21) at the 2024 OCP Global Summit, October 15-17 in San Jose, California. In addition, Supermicro and NVIDIA are presenting a session on DPU-powered AI storage on Wednesday, October 16 at 1:10 p.m. (Concourse Level, 210DH).</p>\n\n\n\n<p>For the complete list of NVIDIA-related sessions, see <a href=\"https://www.nvidia.com/en-us/events/ocp-summit/\">NVIDIA at OCP Summit 2024</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The growth of AI is driving exponential growth in computing power and a doubling of networking speeds every few years. Less well-known is that it\u2019s also putting new demands on storage.&nbsp; Training new models typically requires high-bandwidth networked access to petabytes of data, while inference with the latest types of retrieval augmented generation (RAG) requires &hellip; <a href=\"https://developer.nvidia.com/blog/supermicro-launches-nvidia-bluefield-powered-jbof-to-optimize-ai-storage/\">Continued</a></p>\n", "protected": false}, "author": 2365, "featured_media": 90237, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1502249", "discourse_permalink": "https://forums.developer.nvidia.com/t/supermicro-launches-nvidia-bluefield-powered-jbof-to-optimize-ai-storage/309944", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 1205], "tags": [493, 453], "coauthors": [4101], "class_list": ["post-90242", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-networking-communications", "tag-storage", "tag-featured"], "acf": {"post_industry": ["Cloud Services"], "post_products": ["BlueField DPU"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supermicro-jbof-bluefield-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ntw", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Networking / Communications", "link": "https://developer.nvidia.com/blog/category/networking-communications/", "id": 1205}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90242"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2365"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90242"}], "version-history": [{"count": 15, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90242/revisions"}], "predecessor-version": [{"id": 90332, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90242/revisions/90332"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90237"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90242"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90242"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90242"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90242"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90182, "date": "2024-10-15T09:30:00", "date_gmt": "2024-10-15T16:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90182"}, "modified": "2024-10-19T08:44:27", "modified_gmt": "2024-10-19T15:44:27", "slug": "nvidia-contributes-nvidia-gb200-nvl72-designs-to-open-compute-project", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-contributes-nvidia-gb200-nvl72-designs-to-open-compute-project/", "title": {"rendered": "NVIDIA Contributes NVIDIA GB200 NVL72 Designs to Open Compute Project"}, "content": {"rendered": "\n<p>During the 2024 <a href=\"https://www.opencompute.org/blog/open-compute-project-foundation-expands-its-open-systems-for-ai-initiative\">OCP Global Summit</a>, NVIDIA announced that it has contributed the <a href=\"https://www.nvidia.com/en-us/data-center/gb200-nvl72/\">NVIDIA GB200 NVL72</a> rack and compute and switch tray liquid cooled designs to the <a href=\"https://www.opencompute.org/\">Open Compute Project (OCP)</a>.&nbsp;</p>\n\n\n\n<p>This post provides details about this contribution and explains how it increases the utility of current design standards to meet the high compute density demands of modern data centers. It also explores how the ecosystem is building on top of the GB200 designs, reducing cost and implementation time for new AI data centers.</p>\n\n\n\n<h2 id=\"nvidia_open-source_initiatives\"  class=\"wp-block-heading\">NVIDIA open-source initiatives<a href=\"#nvidia_open-source_initiatives\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA has a rich history of open-source initiatives. NVIDIA engineers have released over 900 software projects on GitHub and have open-sourced essential components of the AI software stack. The <a href=\"https://github.com/triton-inference-server/server\">NVIDIA Triton Inference Server</a>, for example, is now integrated into all major cloud service providers to serve AI models in production. Additionally, NVIDIA engineers are actively involved in numerous open-source foundations and standards bodies, including the Linux Foundation, the Python Software Foundation, and the PyTorch Foundation.</p>\n\n\n\n<p>This commitment to openness extends to the Open Compute Project, where NVIDIA has consistently made design contributions across multiple generations of hardware products. Notable contributions include the <a href=\"https://www.opencompute.org/blog/nvidia-contributes-h100-tensor-core-based-hgx-baseboard-physical-spec-to-ocp-oai-project#:~:text=NVIDIA's%20HGX%20contribution%20satisfies%20all,while%20consuming%20similar%20active%20power.\">NVIDIA HGX H100 baseboard</a>, which has become the de facto baseboard standard for AI servers, and the <a href=\"https://www.nvidia.com/en-us/networking/ethernet-adapters/\">NVIDIA ConnectX-7</a> adapter, which now serves as the foundation design of the OCP Network Interface Card (NIC) 3.0.&nbsp;</p>\n\n\n\n<p>NVIDIA is also a founding and governance board member of the OCP <a href=\"https://www.opencompute.org/wiki/Networking/SAI\">SAI</a> (Switch Abstraction Interface) project and is the second top contributor to the <a href=\"https://www.opencompute.org/projects/sonic\">SONiC </a>(Software for Open Networking in the Cloud) project.&nbsp;</p>\n\n\n\n<h2 id=\"meeting_data_center_compute_demands\"  class=\"wp-block-heading\">Meeting data center compute demands<a href=\"#meeting_data_center_compute_demands\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The compute power required to train autoregressive transformer models has exploded, growing by a staggering 20,000x over the last 5 years. Meta\u2019s Llama 3.1 405B model, launched earlier this year, required 38 billion petaflops of accelerated compute to train, 50x more than the Llama 2 70B model launched only a year earlier. Training and serving these large models cannot be managed on a single GPU; rather, they must be parallelized across massive GPU clusters.&nbsp;</p>\n\n\n\n<p>Parallelism comes in various forms\u2014tensor parallel, pipeline parallel, and expert parallel, each of which provides unique benefits in terms of throughput and user interactivity. Often, these methods are combined to create optimal training and inference deployment strategies to meet user experience requirements and data center budget objectives. For a deeper dive into parallelism techniques for large models, see <a href=\"https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/\">Demystifying Inference Deployments for Trillion Parameter LLMs</a>.&nbsp;</p>\n\n\n\n<h2 id=\"importance_of_multi-gpu_interconnect&nbsp;\"  class=\"wp-block-heading\">Importance of multi-GPU interconnect&nbsp;<a href=\"#importance_of_multi-gpu_interconnect&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>One common challenge that arises with model parallelism is the high volume of GPU-to-GPU communication. Tensor parallel GPU communication patterns highlight just how interconnected these GPUs are. For example, with AllReduce, every GPU has to send the results of its calculation to every other GPU at every layer of the neural network before the final model output is determined. Any latency during these communications can lead to significant inefficiencies, with GPUs left idle, waiting for the communication protocols to complete. This reduces overall system efficiency and increases the total cost of ownership (TCO).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"853\" height=\"480\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-to-gpu-communication-tensor-parallel-deployments.gif\" alt=\"High-level animation showing the GPU-to-GPU communication needed in Tensor Parallel deployments. \n\" class=\"wp-image-90185\"/><figcaption class=\"wp-element-caption\"><em><em>Figure 1. GPU collective communication in a tensor parallel deployment&nbsp;&nbsp;</em></em></figcaption></figure>\n\n\n\n<p>To combat these communication bottlenecks, data centers and cloud service providers leverage NVIDIA <a href=\"https://www.nvidia.com/en-us/data-center/nvlink/\">NVSwitch and NVLink</a> interconnect technology. NVSwitch and NVLink are specifically engineered to speed up communication between GPUs, reducing GPU idle time and increasing throughput.&nbsp;</p>\n\n\n\n<p>Prior to the introduction of the NVIDIA GB200 NVL72, the maximum number of GPUs that could be connected in a single NVLink domain was limited to eight on an HGX H200 baseboard, with a communication speed of 900 GB/s per GPU. The introduction of the GB200 NVL72 design dramatically expanded these capabilities: the NVLink domain can now support up to 72 <a href=\"https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/\">NVIDIA Blackwell</a> GPUs, with a communication speed of 1.8 TB/s per GPU, 36x faster than state-of-the-art 400 Gbps Ethernet standards.&nbsp;</p>\n\n\n\n<p>This leap in NVLink domain size and speed can accelerate training and inference of trillion-parameter models, such as GPT-MoE-1.8T, by up to 4x and 30x, respectively.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1119\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain.png\" alt=\"Diagram showing the NVIDIA GB200 NVL72 NVLink domain, where every GPU can communicate with every other GPU at 1.8 TB/s.\n\" class=\"wp-image-90187\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain-300x168.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain-625x350.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain-179x100.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain-768x430.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain-1536x860.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain-645x361.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain-500x280.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain-362x203.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain-197x110.png 197w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-nvlink-domain-1024x573.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA GB200 NVL72 NVLink domain delivering an aggregate AllReduce bandwidth of 260 TB/s<br></em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1109\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray.png\" alt=\"Photo composite of the backplane of a GB200 NVL72 rack without the compute and switch trays with zoom-ins on the cable cartridge and copper cables. Bottom right shows top view photo of an NVLink switch tray. \n\" class=\"wp-image-90188\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray-300x166.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray-625x347.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray-179x99.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray-768x426.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray-1536x852.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray-645x358.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray-500x277.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray-362x201.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray-198x110.png 198w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-nv72-rack-nvlink-switch-tray-1024x568.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. The NVIDIA GB200 NVL72 rack, with four NVLink cartridges housing over 5,000 energy efficient coaxial copper cables, enables each GPU to communicate with every other GPU 36x faster than state-of-the-art Ethernet standards&nbsp;</em></em></figcaption></figure>\n\n\n\n<h2 id=\"accelerating_infrastructure_innovations_and_contributions\"  class=\"wp-block-heading\">Accelerating infrastructure innovations and contributions<a href=\"#accelerating_infrastructure_innovations_and_contributions\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Supporting the weight, mating force, and cooling requirements of such a large GPU NVLink domain in a single rack necessitates careful electrical and mechanical modifications to the rack architecture, along with the compute and switch chassis that house the GPUs and NVSwitch chips.&nbsp;</p>\n\n\n\n<p>NVIDIA has worked closely with partners to build on top of existing design principles to increase their utility and support the high compute density and <a href=\"https://www.nvidia.com/en-us/glossary/energy-efficiency/\">energy efficiency</a> of the GB200 NVL72. The rack, tray, and internal component designs were derived from the NVIDIA MGX architecture. Today, we are excited to open and contribute these designs with OCP to establish a modular and reusable high-compute density infrastructure for AI.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"936\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights.png\" alt=\"CAD designs from the NVIDIA GB200 NVL72 OCP submission.\n\" class=\"wp-image-90189\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights-300x140.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights-625x293.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights-768x360.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights-1536x719.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights-645x302.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights-500x234.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights-160x75.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights-362x170.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights-235x110.png 235w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-gb200-ocp-submission-highlights-1024x479.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. NVIDIA GB200 NVL72 OCP submission highlights</em></em></figcaption></figure>\n\n\n\n<p>Design modification highlights that NVIDIA is making available include the following.&nbsp;</p>\n\n\n\n<h3 id=\"rack_reinforcements&nbsp;\"  class=\"wp-block-heading\">Rack reinforcements&nbsp;<a href=\"#rack_reinforcements&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To efficiently house 18 compute trays, nine switch trays, and four NVLink cartridges supporting over 5,000 copper cables in a single rack, NVIDIA implemented several key modifications to the existing rack designs, including:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Adaptations to support 19\u201d EIA gear in a 1 RU form factor within the rack to double the space available for IO cabling and improve tray density.&nbsp;&nbsp;</li>\n\n\n\n<li>Adding over 100 lbs of steel reinforcements, significantly increasing the rack&#8217;s strength and stability to withstand the 6,000 lbs of mating force generated between its components and frame.&nbsp;</li>\n\n\n\n<li>Incorporating rear rack extensions to safeguard cable bracings and manifold fittings, ensuring the longevity and proper functioning of these elements.&nbsp;</li>\n\n\n\n<li>Introducing blind mate slide rails and latching features to facilitate NVLink installations, liquid cooling system integration, and easier maintenance procedures using blind mate connectors. This rack redesign optimizes space utilization, enhances structural integrity, and improves overall system reliability and maintainability.</li>\n</ul>\n\n\n\n<h3 id=\"high_capacity_bus_bar&nbsp;\"  class=\"wp-block-heading\">High capacity bus bar&nbsp;<a href=\"#high_capacity_bus_bar&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To accommodate the high compute density and increased power requirements of the rack, we developed a new design specification for an enhanced high capacity bus bar. This upgraded bus bar maintains the same width as the existing ORV3 but features a deeper profile, significantly increasing its ampacity. The new design supports a substantially higher 1,400 amp current flow, offering a 2x increase in amperage compared to current standards. This enhancement ensures that the bus bar can effectively handle the elevated power demands of modern high-performance computing environments, without requiring additional horizontal space within the rack.</p>\n\n\n\n<h3 id=\"nvlink_cartridges&nbsp;\"  class=\"wp-block-heading\">NVLink cartridges&nbsp;<a href=\"#nvlink_cartridges&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To enable high-speed communication between all 72 NVIDIA Blackwell GPUs in the NVLink domain, we implemented a novel design featuring four NVLink cartridges mounted vertically at the rear of the rack. These cartridges accommodate over 5,000 active copper cables, delivering an impressive aggregate All-to-All bandwidth of 130 TB/s and 260 TB/s AllReduce bandwidth.&nbsp;</p>\n\n\n\n<p>This design ensures that each GPU can communicate with every other GPU in the domain at 1.8TB/s, significantly enhancing overall system performance. As part of our submission, we are providing detailed information on the volumetrics and precise mounting locations for these NVLink cartridges, contributing to future implementations and improvements in high-performance computing infrastructure.</p>\n\n\n\n<h3 id=\"liquid_cooling_manifolds_and_floating_blind_mates\"  class=\"wp-block-heading\">Liquid cooling manifolds and floating blind mates<a href=\"#liquid_cooling_manifolds_and_floating_blind_mates\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To efficiently manage the 120 KW cooling capacity required for the rack, we&#8217;ve implemented direct liquid cooling techniques. Building upon existing designs, we&#8217;ve introduced two key innovations. First, we developed an enhanced Blind Mate Liquid Cooling Manifold design, capable of delivering efficient cooling.&nbsp;</p>\n\n\n\n<p>Second, we created a novel Floating Blind Mate Tray connection, which effectively distributes coolant to both compute and switch trays significantly improving the ability of the liquid quick disconnects to align and reliably mate in the rack. By leveraging these enhanced liquid cooling solutions, we are able to address the high thermal management demands of modern high-performance computing environments, ensuring optimal performance and longevity of the rack components.</p>\n\n\n\n<h3 id=\"compute_and_switch_tray_mechanical_form_factors\"  class=\"wp-block-heading\">Compute and switch tray mechanical form factors<a href=\"#compute_and_switch_tray_mechanical_form_factors\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To accommodate the high compute density of the rack we introduced 1RU liquid-cooled compute and switch tray form factors. We also developed a new, denser DC-SCM (Data Center Secure Control Module) design that&#8217;s 10% smaller than the current standard. In addition, we implemented a narrower bus bar connector to maximize available rear panel space. These modifications optimize space utilization while maintaining performance.&nbsp;</p>\n\n\n\n<p>Additionally, we created a modular bay design for the compute tray that flexibly adapts to diverse user I/O requirements. These enhancements collectively support 1 RU liquid-cooled form factors for both compute and switch trays, significantly increasing the rack&#8217;s computational density and networking capabilities while ensuring efficient cooling and adaptability to various user needs.</p>\n\n\n\n<h2 id=\"new_joint_nvidia_gb200_nvl72_reference_architecture\"  class=\"wp-block-heading\">New joint NVIDIA GB200 NVL72 reference architecture<a href=\"#new_joint_nvidia_gb200_nvl72_reference_architecture\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>At OCP, NVIDIA also announced a new joint GB200 NVL72 reference architecture with <a href=\"https://www.vertiv.com/en-emea/about/news-and-insights/news-releases/vertiv-codevelops-with-nvidia-complete-power-and-cooling-blueprint-for--nvidia-gb200-nvl72-platform/\" target=\"_blank\" rel=\"noreferrer noopener\">Vertiv</a>, a leader in power and cooling technologies and expert in designing, building and servicing high compute density data centers. This new reference architecture will significantly reduce implementation time for CSPs and data centers deploying the <a href=\"https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/\">NVIDIA Blackwell</a> platform.</p>\n\n\n\n<p>The new reference architecture eliminates the need for data centers to develop their own power, cooling, and spacing designs for GB200 NVL72 from scratch. By leveraging Vertiv\u2019s expertise in space saving power management and energy efficient cooling technologies, data centers can deploy 7MW GB200 NVL72 clusters globally reducing implementation time by up to 50%, all while reducing power space footprint and increasing cooling energy efficiency.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"779\" height=\"387\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/floor-plan-liquid-cooled-data-center-vertiv-design-architecture-1.png\" alt=\"Floor plan of a liquid cooled data center showing the locations of the power management, thermal management, racks and distribution systems. \n\" class=\"wp-image-90292\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/floor-plan-liquid-cooled-data-center-vertiv-design-architecture-1.png 779w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/floor-plan-liquid-cooled-data-center-vertiv-design-architecture-1-300x149.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/floor-plan-liquid-cooled-data-center-vertiv-design-architecture-1-625x310.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/floor-plan-liquid-cooled-data-center-vertiv-design-architecture-1-179x89.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/floor-plan-liquid-cooled-data-center-vertiv-design-architecture-1-768x382.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/floor-plan-liquid-cooled-data-center-vertiv-design-architecture-1-645x320.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/floor-plan-liquid-cooled-data-center-vertiv-design-architecture-1-500x248.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/floor-plan-liquid-cooled-data-center-vertiv-design-architecture-1-160x79.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/floor-plan-liquid-cooled-data-center-vertiv-design-architecture-1-362x180.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/floor-plan-liquid-cooled-data-center-vertiv-design-architecture-1-221x110.png 221w\" sizes=\"(max-width: 779px) 100vw, 779px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 5. Floor plan of a 7MW  liquid cooled data center showing Vertiv\u2019s reference design</em></em></figcaption></figure>\n\n\n\n<p>Vertiv is a great example of how data center infrastructure (DCI) providers are building on top of the NVIDIA Blackwell platform. Over 40 DCIs are now building and innovating on top of the Blackwell platform including Amphenol, Auras, Astron, AVC, Beehe, Bellwether, BizLink, Boyd, Chenbro, Chengfwa, CoolIT, Cooler Master, CPC, Danfoss, Delta, Envicool, Flex, Foxconn, Interplex, JPC,&nbsp; Kingslide, Lead Wealth, LiteOn, LOTES, Luxshare, Megmeet, Molex, Motivair, Nidec, Nvent, Ourack, Parker, Pinda, QCT, Refas, Readore, Repon, Rittal, Sanyo Denki, Schneider Electronic, Simula, Staubli, SUNON, Taicheng, TE Connectivity, and Yuans Technology.</p>\n\n\n\n<p>Additionally, system partners such as <a href=\"https://www.asrockrack.com/general/news.fr.asp?id=252\">AsRock Rack</a>, <a href=\"https://press.asus.com/news/press-releases/asus-ai-solutions-2024-ocp-summit/\" target=\"_blank\" rel=\"noreferrer noopener\">ASUS</a>, <a href=\"https://www.dell.com/en-us/dt/corporate/newsroom/announcements/detailpage.press-releases~usa~2024~10~dell-servers-storage-at-ocp.htm#/filter-on/Country:en-us\">Dell Technologies</a>, <a href=\"https://www.gigabyte.com/us/Press/News/2222\">GIGABYTE</a>, Hewlett Packard Enterprise, <a href=\"https://ir.tdsynnex.com/news/press-release-details/2024/Hyve-Solutions-Announces-New-Orion-Product-Line-Featuring-NVIDIA-HGX-Platform-at-2024-OCP-Global-Summit/default.aspx\">Hyve</a>, Inventec, <a href=\"https://www.msi.com/news/detail/MSI-Showcases-Innovation-at-2024-OCP-Global-Summit--Highlighting-DC-MHS--CXL-Memory-Expansion--and-MGX-enabled-AI-Servers-144801\">MSI</a>, Pegatron, <a href=\"https://www.qct.io/Press-Releases/index/PR/Server/QCT-Showcases-NVIDIA-Accelerated-Computing-and-Networking-Technologies-at-the-2024-Open-Compute-Project-Global-Summit/1/0\">QCT</a>, <a href=\"https://www.supermicro.com/en/pressreleases/supermicro-adds-new-petascale-jbof-all-flash-storage-solution-integrating-nvidia\">Supermicro</a> and <a href=\"https://www.wiwynn.com/news/wiwynn-launches-state-of-the-art-ai-data-center-and-cooling-solutions-at-ocp-global-summit-2024\">Wiwynn</a> are creating a wide range of servers based on Blackwell products.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The NVIDIA GB200 NVL72 design represents a significant milestone in the evolution of modern high compute density data centers. By addressing the pressing challenges of training and serving growing AI models and high GPU-to-GPU communication, this contribution accelerates the adoption of energy-efficient high compute density platforms in the data center while reinforcing the importance of collaboration within the open ecosystem. We\u2019re excited to see how the OCP community will leverage and build on top of the GB200 NVL72 design contributions.&nbsp;</p>\n\n\n\n<p>To learn more about the NVIDIA GB200 NVL72 OCP contribution, see the <a href=\"https://www.opencompute.org/documents/mgx-accelerated-computing-rack-and-trays-specification-101024-pdf\">OCP specification documentation</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>During the 2024 OCP Global Summit, NVIDIA announced that it has contributed the NVIDIA GB200 NVL72 rack and compute and switch tray liquid cooled designs to the Open Compute Project (OCP).&nbsp; This post provides details about this contribution and explains how it increases the utility of current design standards to meet the high compute density &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-contributes-nvidia-gb200-nvl72-designs-to-open-compute-project/\">Continued</a></p>\n", "protected": false}, "author": 2008, "featured_media": 90296, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1502242", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-contributes-nvidia-gb200-nvl72-designs-to-open-compute-project/309943", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1205], "tags": [453, 3280], "coauthors": [3708], "class_list": ["post-90182", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "category-networking-communications", "tag-featured", "tag-open-source-software"], "acf": {"post_industry": ["Cloud Services"], "post_products": ["GB200"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ocp-nvidia-gb200.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nsy", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90182"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2008"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90182"}], "version-history": [{"count": 20, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90182/revisions"}], "predecessor-version": [{"id": 90641, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90182/revisions/90641"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90296"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90182"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90182"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90182"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90182"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90176, "date": "2024-10-15T09:30:00", "date_gmt": "2024-10-15T16:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90176"}, "modified": "2024-11-01T07:27:00", "modified_gmt": "2024-11-01T14:27:00", "slug": "powering-next-generation-ai-networking-with-nvidia-supernics", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/powering-next-generation-ai-networking-with-nvidia-supernics/", "title": {"rendered": "Powering Next-Generation AI Networking with NVIDIA SuperNICs"}, "content": {"rendered": "\n<p>In the era of generative AI, accelerated networking is essential to build high-performance computing fabrics for massively distributed AI workloads. NVIDIA continues to lead in this space, offering state-of-the-art Ethernet and InfiniBand solutions that maximize the performance and efficiency of AI factories and cloud data centers.</p>\n\n\n\n<p>At the core of these solutions are NVIDIA SuperNICs\u2014a new class of network accelerators optimized to power hyperscale AI workloads. These SuperNICs are integral components of NVIDIA&#8217;s Spectrum-X Ethernet and Quantum-X800 InfiniBand networking platforms, designed to deliver unprecedented scalability and performance.&nbsp;</p>\n\n\n\n<p>The latest addition in the NVIDIA SuperNIC portfolio, ConnectX-8 SuperNICs, join BlueField-3 SuperNICs in driving the next wave of innovations for accelerated, massive-scale AI computing fabrics. With a total data throughput of 800 Gb/s, ConnectX-8 SuperNICs deliver the speed, robustness, and scalability necessary to power trillion-parameter AI models, seamlessly integrating with NVIDIA switches for optimal performance.<br><br>This post explores the unique attributes of the NVIDIA SuperNICs and their pivotal role in advancing modern AI infrastructure.</p>\n\n\n\n<h2 id=\"leveraging_roce_for_ai_workloads\"  class=\"wp-block-heading\"><strong>Leveraging RoCE for AI workloads</strong><a href=\"#leveraging_roce_for_ai_workloads\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>For AI model training, it is critical to move immense datasets at high speed between GPUs across the data center to reduce training time and achieve faster time-to-market for AI solutions.</p>\n\n\n\n<p>NVIDIA SuperNICs, featuring best-in-class, in-hardware RoCE acceleration and GPUDirect RDMA at speeds up to 800 Gb/s, address these challenges by enabling direct data movement between GPUs while bypassing the CPU.&nbsp;</p>\n\n\n\n<p>This direct communication pathway minimizes CPU overhead and reduces latency, resulting in faster, more efficient data transfer between GPU memory. In practical terms, this capability enables greater parallelism, scaling AI workloads across many nodes without the bottlenecks typically introduced by traditional CPU-based data transfers.</p>\n\n\n\n<h2 id=\"enhancing_ai_performance_with_spectrum-x_roce_adaptive_routing\"  class=\"wp-block-heading\">Enhancing AI performance with Spectrum-X RoCE adaptive routing<a href=\"#enhancing_ai_performance_with_spectrum-x_roce_adaptive_routing\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>One of the key capabilities for boosting AI network performance within Spectrum-X is direct data placement (DDP) support featured by NVIDIA SuperNICs.&nbsp;</p>\n\n\n\n<p>As generative AI workloads scale across thousands of nodes, conventional IP routing protocols, such as equal-cost multipath (ECMP), struggle to handle the large, sustained data flows\u2014referred to as <em>elephant flows</em>\u2014that AI models generate. These flows can overwhelm network resources and lead to congestion, reducing overall network performance.</p>\n\n\n\n<p>Spectrum-X RoCE adaptive routing dynamically adjusts how traffic is distributed across available network paths, ensuring that high-bandwidth flows are optimally routed to prevent network congestion. This approach uses the capabilities of the NVIDIA Spectrum-4 Ethernet switch, which evenly sprays packets across multiple paths to balance the load, avoiding bottlenecks caused by traditional static routing mechanisms.</p>\n\n\n\n<p>However, with packet spraying, the challenge of out-of-order packet delivery arises.&nbsp;</p>\n\n\n\n<p>NVIDIA SuperNICs resolve this by placing packets directly in order into the buffer as they arrive at the receiving end, ensuring that data is received in the correct sequence. This tight coordination between NVIDIA switches and SuperNICs enables&nbsp;efficient, high-speed AI workload communication, ensuring that large-scale AI models can continue processing data without interruption or degradation in performance.</p>\n\n\n\n<h2 id=\"addressing_congestion_in_ai_networks\"  class=\"wp-block-heading\">Addressing congestion in AI networks<a href=\"#addressing_congestion_in_ai_networks\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>AI workloads are highly susceptible to congestion due to their bursty nature. The frequent, short-lived traffic spikes generated by AI model training\u2014particularly during collective operations where multiple GPUs synchronize and share data\u2014require advanced congestion management to maintain network performance. Traditional congestion control methods, such as TCP-based flow control, are insufficient for AI\u2019s unique traffic patterns.</p>\n\n\n\n<p>To address this, Spectrum-X employs advanced congestion control mechanisms that are tightly integrated with the Spectrum-4 switch\u2019s real-time telemetry capabilities. This integration enables the SuperNIC to proactively adjust data transmission rates based on current network utilization, preventing congestion before it becomes problematic.&nbsp;</p>\n\n\n\n<p>By using in-band, high-frequency telemetry data, the SuperNIC can react with microsecond precision, ensuring that network bandwidth is optimized and latency is minimized, even under high-traffic conditions.</p>\n\n\n\n<h2 id=\"accelerating_ai_networks_with_enhanced_programmable_io\"  class=\"wp-block-heading\">Accelerating AI networks with enhanced programmable I/O<a href=\"#accelerating_ai_networks_with_enhanced_programmable_io\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>As AI workloads grow more complex, network infrastructure must evolve not only in speed but also in adaptability to support diverse communication patterns across thousands of nodes.&nbsp;</p>\n\n\n\n<p>NVIDIA SuperNICs are at the forefront of this innovation, offering enhanced programmable I/O capabilities that are crucial for modern AI data center environments. These SuperNICs feature an accelerated packet processing pipeline capable of operating at line speed with up to 800 Gb/s of throughput.&nbsp;</p>\n\n\n\n<p>By offloading packet processing tasks from the CPU to the SuperNIC, this pipeline significantly reduces network latency and improves overall system efficiency. The programmable nature of the pipeline, powered by the NVIDIA DOCA software framework, provides network professionals with the flexibility to build and optimize networks at massive scale.</p>\n\n\n\n<p>NVIDIA SuperNICs feature a data path accelerator (DPA) that enhances their programmability. The DPA is a highly parallel I/O processor equipped with 16 hyperthreaded cores, specifically designed to handle I/O-intensive workloads. It can be easily programmed through DOCA for a variety of low-code applications, such as device emulation, congestion control, and traffic management. This programmability enables organizations to tailor their network infrastructure to the specific needs of their AI workloads, ensuring that data flows efficiently across the network while maintaining peak performance.</p>\n\n\n\n<h2 id=\"securing_network_connectivity_for_ai\"  class=\"wp-block-heading\">Securing network connectivity for AI<a href=\"#securing_network_connectivity_for_ai\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Securing AI models is essential for protecting sensitive data and intellectual property from potential breaches and adversarial attacks. As your organizations build AI factories and cloud data centers, you need effective security solutions to address vulnerabilities that could undermine model performance and trustworthiness, ultimately preserving competitive advantage and user privacy.</p>\n\n\n\n<p>Traditional network encryption methods often struggle to scale beyond 100 Gb/s, leaving critical data at risk. In contrast, NVIDIA SuperNICs offer accelerated networking with in-line crypto acceleration at speeds of up to 800 Gb/s, ensuring that data remains encrypted in transit while achieving peak AI performance.&nbsp;</p>\n\n\n\n<p>With hardware-accelerated support for IPsec and scalable PSP crypto operations, NVIDIA SuperNICs provide a proven solution for securing AI network environments.\u00a0</p>\n\n\n\n<p>Developed by Google and contributed to the open-source community, PSP employs a stateless design from the ground up, making it ideal to support the requirements of hyperscale data center environments. This architecture enables each request to be processed independently, enhancing scalability and resilience in managing cryptographic operations across distributed systems.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In the dynamic landscape of generative AI, NVIDIA SuperNICs are setting the stage for a transformative era in networking, serving as an integral part of the NVIDIA Spectrum-X and Quantum-X800 networking platforms.&nbsp;</p>\n\n\n\n<p>With their unparalleled capabilities\u2014from ultra-fast data throughput and intelligent congestion management to robust security features and programmable I/O\u2014these network accelerators are revolutionizing how AI workloads are delivered. By seamlessly integrating cutting-edge technologies with unmatched performance, NVIDIA SuperNICs empower organizations to unleash the full potential of their AI initiatives, driving innovation at unprecedented scales.</p>\n\n\n\n<p>For more information about NVIDIA SuperNICs, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://resources.nvidia.com/en-us-accelerated-networking-resource-library/next-generation-netw\">Next-Generation Networking for the Next Wave of AI</a></li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/networking/products/ethernet/supernic/\">NVIDIA Ethernet SuperNIC</a></li>\n\n\n\n<li><a href=\"https://resources.nvidia.com/en-us-accelerated-networking-resource-library/connectx-datasheet-c\">NVIDIA ConnectX-8 SuperNIC</a> (datasheet)</li>\n\n\n\n<li><a href=\"https://blogs.nvidia.com/blog/what-is-a-supernic/\">What is a SuperNIC?</a></li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>In the era of generative AI, accelerated networking is essential to build high-performance computing fabrics for massively distributed AI workloads. NVIDIA continues to lead in this space, offering state-of-the-art Ethernet and InfiniBand solutions that maximize the performance and efficiency of AI factories and cloud data centers. At the core of these solutions are NVIDIA SuperNICs\u2014a &hellip; <a href=\"https://developer.nvidia.com/blog/powering-next-generation-ai-networking-with-nvidia-supernics/\">Continued</a></p>\n", "protected": false}, "author": 746, "featured_media": 90217, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1502241", "discourse_permalink": "https://forums.developer.nvidia.com/t/powering-next-generation-ai-networking-with-nvidia-supernics/309942", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1205], "tags": [1634, 453, 658, 4100], "coauthors": [1227], "class_list": ["post-90176", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "category-networking-communications", "tag-ethernet", "tag-featured", "tag-infiniband", "tag-supernics"], "acf": {"post_industry": ["Cloud Services"], "post_products": ["BlueField DPU", "ConnectX", "Spectrum Ethernet"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/supernics-connectx-spectrum-x-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nss", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90176"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/746"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90176"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90176/revisions"}], "predecessor-version": [{"id": 91368, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90176/revisions/91368"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90217"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90176"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90176"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90176"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90176"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90307, "date": "2024-10-15T06:00:00", "date_gmt": "2024-10-15T13:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90307"}, "modified": "2024-10-28T14:53:55", "modified_gmt": "2024-10-28T21:53:55", "slug": "datastax-announces-new-ai-development-platform-built-with-nvidia-ai", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/datastax-announces-new-ai-development-platform-built-with-nvidia-ai/", "title": {"rendered": "DataStax Announces New AI Development Platform, Built with NVIDIA AI"}, "content": {"rendered": "\n<p>As enterprises increasingly adopt AI technologies, they face a complex challenge of efficiently developing, securing, and continuously improving AI applications to leverage their data assets. They need a unified, end-to-end solution that simplifies AI development, enhances security, and enables continuous optimization, allowing organizations to harness the full potential of their data for AI-driven innovation.&nbsp;</p>\n\n\n\n<p>This is why DataStax worked with NVIDIA to create the <a href=\"https://www.datastax.com/press-release/announcing-datastax-ai-platform-built-with-nvidia-ai?utm_medium=referral&amp;utm_source=nvidia-blog&amp;utm_campaign=dapbna-launch\">DataStax AI Platform</a>, now integrated with <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo</a> and <a href=\"https://www.nvidia.com/en-us/ai/#referrer=ai-subdomain\">NIM</a>, part of <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> software. The Platform provides a unified stack, making it easier for enterprises to build AI applications that leverage their data and the tools necessary to continuously tune and improve application performance and relevancy and achieve <a href=\"https://www.datastax.com/blog/datastax-nvidia-collaborate-to-deliver-genai-applications-with-fast-embeddings?utm_medium=partner&amp;utm_source=nvidia&amp;utm_campaign=ai-platform&amp;utm_content=partner-announcement\">19x better performance throughput</a>. The platform builds on existing integrations with DataStax and the NVIDIA AI Enterprise platform announced earlier this year.</p>\n\n\n\n<p>In this blog post, we\u2019ll investigate multiple points in the generative AI application lifecycle and share how the DataStax AI Platform built with NVIDIA simplifies the process \u2014 from creating the initial application using <a href=\"https://www.nvidia.com/en-us/ai-data-science/ai-workflows/\">NVIDIA NIM Agent Blueprints</a> and Langflow, to enhancing <a href=\"https://www.nvidia.com/en-us/glossary/data-science/large-language-models/\">LLM</a> responses with NVIDIA NeMo Guardrails, to further improving application performance and relevancy with customer data and fine-tuning.&nbsp;</p>\n\n\n\n<h2 id=\"getting_started_quickly_with_nim_agent_blueprints_and_langflow&nbsp;\"  class=\"wp-block-heading\">Getting started quickly with NIM Agent Blueprints and Langflow&nbsp;<a href=\"#getting_started_quickly_with_nim_agent_blueprints_and_langflow&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA <a href=\"https://blogs.nvidia.com/blog/nim-agent-blueprints/\">NIM Agent Blueprints</a> provide reference architectures for specific AI use cases, significantly lowering the entry barrier for AI application development. The integration of these blueprints with <a href=\"https://www.datastax.com/products/langflow\">Langflow</a> creates a powerful synergy that addresses key challenges in the AI development lifecycle and can reduce development time by up to 60%.</p>\n\n\n\n<p>Consider the <a href=\"https://build.nvidia.com/nvidia/multimodal-pdf-data-extraction-for-enterprise-rag\">multimodal PDF data extraction NIM Agent Blueprint</a>, which coordinates various NIM microservices including NeMo Retriever for ingestion, embedding, and reranking and for optimally running the LLM. This blueprint tackles one of the most complex aspects of building retrieval-augmented generation (RAG) applications: document ingestion and processing. By simplifying these intricate workflows, developers can focus on innovation rather than technical hurdles.</p>\n\n\n\n<p>Langflow\u2019s visual development interface makes it easy to represent a NIM Agent Blueprint as an executable flow. This allows for rapid prototyping and experimentation, enabling developers to:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Visually construct AI workflows using key NeMo Retriever embedding, ingestion, and LLM NIM components</li>\n\n\n\n<li>Mix and match NVIDIA and Langflow components</li>\n\n\n\n<li>Easily incorporate custom documents and models</li>\n\n\n\n<li>Leverage DataStax Astra DB for vector storage</li>\n\n\n\n<li>Expose flows as API endpoints for seamless deployment</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1314\" height=\"1442\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ingestion2.png\" alt=\"Langflow interface showing NVIDIA NeMo Retriever and DataStax Astra DB components.\" class=\"wp-image-90354\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ingestion2.png 1314w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ingestion2-273x300.png 273w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ingestion2-625x686.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ingestion2-105x115.png 105w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ingestion2-768x843.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ingestion2-645x708.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ingestion2-82x90.png 82w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ingestion2-362x397.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ingestion2-100x110.png 100w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ingestion2-1024x1124.png 1024w\" sizes=\"(max-width: 1314px) 100vw, 1314px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Document ingestion using NeMo Retriever components in Langflow</em></figcaption></figure></div>\n\n\n<p>This combination not only streamlines the development process, but also bridges the gap between prototype and production. It also encourages team collaboration, enabling multiple users, even with a less technical background, to understand, test, and adjust the application. By making advanced AI capabilities more accessible, it fosters innovation and opens up new possibilities for AI applications across various industries.</p>\n\n\n\n<h2 id=\"enhancing_ai_security_and_control_with_nemo_guardrails\"  class=\"wp-block-heading\">Enhancing AI security and control with NeMo Guardrails<a href=\"#enhancing_ai_security_and_control_with_nemo_guardrails\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Building on the rapid development enabled by NIM Agent Blueprints in Langflow, enhancing AI applications with advanced security features becomes remarkably straightforward. Langflow&#8217;s component-based approach, which already enabled quick implementation of the PDF extraction blueprint, now facilitates seamless integration of <a href=\"https://docs.nvidia.com/nemo/guardrails/introduction.html\">NeMo Guardrails</a>.</p>\n\n\n\n<p>NeMo Guardrails offers crucial features for responsible AI deployment such as:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Jailbreak and hallucination protection&nbsp;</li>\n\n\n\n<li>Topic boundary setting</li>\n\n\n\n<li>Custom policy enforcement</li>\n</ul>\n\n\n\n<p>The power of this integration lies in its simplicity. Just as developers could swiftly create the initial application using Langflow&#8217;s visual interface, they can now drag and drop NeMo Guardrails components to enhance security. This approach enables rapid experimentation and iteration, allowing developers to:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Easily add content moderation to existing flows</li>\n\n\n\n<li>Quickly configure thresholds and test various safety rules</li>\n\n\n\n<li>Seamlessly integrate advanced security techniques by adding more guardrails with minimal code changes</li>\n</ol>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"2112\" height=\"1270\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails.png\" alt=\"Langflow interface showing NeMo Guardrails and NVIDIA NIM components.\" class=\"wp-image-90349\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails.png 2112w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails-300x180.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails-625x376.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails-179x108.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails-768x462.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails-1536x924.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails-2048x1232.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails-645x388.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails-500x300.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails-150x90.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails-362x218.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails-183x110.png 183w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/guardrails-1024x616.png 1024w\" sizes=\"(max-width: 2112px) 100vw, 2112px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Adding NeMo Guardrails to the NIM Agent Blueprint in Langflow</em></figcaption></figure></div>\n\n\n<p>By leveraging Langflow&#8217;s pre-built integration with NeMo Guardrails, developers can focus on fine-tuning AI behavior rather than grappling with complex security implementations. This integration not only reduces development time, but also promotes the adoption of robust safety measures in AI applications, positioning organizations at the forefront of responsible AI innovation.</p>\n\n\n\n<h2 id=\"evolving_ai_through_continual_improvement\"  class=\"wp-block-heading\">Evolving AI through continual improvement<a href=\"#evolving_ai_through_continual_improvement\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In the rapidly advancing field of AI, static models \u2014 even LLMs \u2014 quickly become outdated. The integration of NVIDIA NeMo fine-tuning tools, <a href=\"https://www.datastax.com/products/datastax-astra\">Astra DB\u2019s</a> search/retrieval tunability, and Langflow creates a powerful ecosystem for continuous AI evolution, ensuring that applications achieve higher relevance and performance with each iteration.</p>\n\n\n\n<p>This integrated approach uses three key components for model training and fine-tuning:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li><strong>NeMo Curator</strong>: Refines and prepares operational and customer interaction data from Astra DB and other sources, creating optimal datasets for fine-tuning.</li>\n\n\n\n<li><strong>NeMo Customizer</strong>: Utilizes these curated datasets to fine-tune LLMs, SLMs, or embedding models, tailoring them to specific organizational needs.</li>\n\n\n\n<li><strong>NeMo Evaluator:</strong>&nbsp; Rigorously assesses the fine-tuned models across various metrics, ensuring performance improvements before deployment.</li>\n</ol>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"960\" height=\"540\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nemo-llm-mm-stack.png\" alt=\"Overview of NVIDIA NeMo microservices used for model training and fine-tuning.\" class=\"wp-image-90350\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nemo-llm-mm-stack.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nemo-llm-mm-stack-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nemo-llm-mm-stack-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nemo-llm-mm-stack-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nemo-llm-mm-stack-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nemo-llm-mm-stack-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nemo-llm-mm-stack-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nemo-llm-mm-stack-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nemo-llm-mm-stack-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nemo-llm-mm-stack-196x110.png 196w\" sizes=\"(max-width: 960px) 100vw, 960px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. NVIDIA NeMo overview including model training and fine-tuning</em></figcaption></figure></div>\n\n\n<p>By modeling this fine-tuning pipeline visually in Langflow, organizations can create a seamless, iterative process of AI improvement. This approach offers several strategic advantages:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Data-driven optimization</strong>: Leveraging real-world interaction data from Astra DB ensures that model improvements are based on actual usage patterns and customer needs.</li>\n\n\n\n<li><strong>Agile model evolution</strong>: The visual pipeline in Langflow allows for quick adjustments to the fine-tuning process, enabling rapid experimentation and optimization.</li>\n\n\n\n<li><strong>Customized AI solutions</strong>: Fine-tuning based on organization-specific data leads to AI models that are uniquely tailored to particular industry needs or use cases.</li>\n\n\n\n<li><strong>Continuous performance enhancement</strong>: Regular evaluation and fine-tuning ensure that AI applications consistently improve in relevance and effectiveness over time.</li>\n</ul>\n\n\n\n<p>This integrated ecosystem transforms AI development from a point-in-time deployment to a continuous improvement cycle, enabling organizations to maintain cutting-edge AI capabilities that evolve with their business needs.</p>\n\n\n\n<p>The DataStax AI Platform built with NVIDIA unifies advanced AI tools included with NVIDIA AI Enterprise, DataStax\u2019s robust data management, search flexibility, and Langflow\u2019s intuitive visual interface, creating a comprehensive ecosystem for enterprise AI development. This integration enables organizations to rapidly prototype, securely deploy, and continuously optimize AI applications, transforming complex data into actionable intelligence while significantly reducing time-to-value.</p>\n\n\n\n<p>To learn more, check out this <a href=\"https://dtsx.io/dapbna-demo-video\">video</a> and <a href=\"https://www.datastax.com/products/ai-platform-built-with-nvidia-ai/contact-us?utm_medium=referral&amp;utm_source=nvidia-blog&amp;utm_campaign=dapbna-launch\">sign up</a> for the DataStax AI Platform built with NVIDIA.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>As enterprises increasingly adopt AI technologies, they face a complex challenge of efficiently developing, securing, and continuously improving AI applications to leverage their data assets. They need a unified, end-to-end solution that simplifies AI development, enhances security, and enables continuous optimization, allowing organizations to harness the full potential of their data for AI-driven innovation.&nbsp; This &hellip; <a href=\"https://developer.nvidia.com/blog/datastax-announces-new-ai-development-platform-built-with-nvidia-ai/\">Continued</a></p>\n", "protected": false}, "author": 1264, "featured_media": 90366, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1502129", "discourse_permalink": "https://forums.developer.nvidia.com/t/datastax-announces-new-ai-development-platform-built-with-nvidia-ai/309916", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [296, 3562, 453, 2932, 1958], "coauthors": [2598], "class_list": ["post-90307", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-ai-inference-microservices", "tag-ai-platform", "tag-featured", "tag-large-language-models", "tag-news"], "acf": {"post_industry": ["General"], "post_products": ["AI Enterprise", "NeMo", "NeMo Curator", "NeMo Microservices", "NeMo Retriever", "NIM"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/DataStax-AI-Platform-Built-by-NVIDIA.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nuz", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90307"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1264"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90307"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90307/revisions"}], "predecessor-version": [{"id": 90618, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90307/revisions/90618"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90366"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90307"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90307"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90307"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90307"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89981, "date": "2024-10-14T12:30:00", "date_gmt": "2024-10-14T19:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89981"}, "modified": "2024-10-17T11:19:00", "modified_gmt": "2024-10-17T18:19:00", "slug": "advancing-surgical-robotics-with-ai-driven-simulation-and-digital-twin-technology", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/advancing-surgical-robotics-with-ai-driven-simulation-and-digital-twin-technology/", "title": {"rendered": "Advancing Surgical Robotics with AI-Driven Simulation and Digital Twin Technology"}, "content": {"rendered": "\n<p>The integration of robotic surgical assistants (RSAs) in operating rooms offers substantial advantages for both surgeons and patient outcomes. Currently operated through teleoperation by trained surgeons at a console, these surgical robot platforms provide augmented dexterity that has the potential to streamline surgical workflows and alleviate surgeon workloads. Exploring visual behavior cloning for next-generation surgical assistants could further enhance the capabilities and efficiency of robotic-assisted surgeries.</p>\n\n\n\n<p>This post introduces two template frameworks for robotic surgical assistance: <a href=\"https://orbit-surgical.github.io/sufia/\">Surgical First Interactive Autonomy Assistant (SuFIA)</a> and <a href=\"https://orbit-surgical.github.io/sufia-bc/\">Surgical First Interactive Autonomy Assistant &#8211; Behavior Cloning (SuFIA-BC)</a>. SuFIA uses natural language guidance and large language models (LLMs) for high-level planning and control of surgical robots, while SuFIA-BC enhances the dexterity and precision of robotic surgical assistants through behavior cloning (BC) techniques. These frameworks explore the recent advances in both LLMs and BC techniques and tune them to excel to the unique challenges of surgical scenes.&nbsp;</p>\n\n\n\n<p>This research aims to accelerate the development of surgical robotic assistants, with the eventual goal of alleviating surgeon fatigue, enhancing patient safety, and democratizing access to high-quality healthcare. SuFIA and SuFIA-BC advance this field by demonstrating their capabilities across various surgical subtasks in simulated and physical settings. Moreover, the photorealistic assets introduced in this work enable the broader research community to explore surgical robotics\u2014a field that has traditionally faced significant barriers to entry due to limited data accessibility, the high costs of expert demonstrations, and the expensive hardware required.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/jCvZYf0saRo?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. State-of-the-art approaches in behavior cloning for augmented dexterity in robotic surgical assistants (RSAs)</em></figcaption></figure>\n\n\n\n<p>This research enhances the <a href=\"https://blogs.nvidia.com/blog/orbit-surgical-robotics-research-icra/\">ORBIT-Surgical framework</a> to create a photorealistic training environment for surgical robots, featuring anatomically accurate models and high-fidelity rendering using <a href=\"https://www.nvidia.com/en-us/omniverse/\">NVIDIA Omniverse</a>. <a href=\"https://orbit-surgical.github.io/\">ORBIT-Surgical</a> is an open-simulation framework for learning surgical augmented dexterity. It\u2019s based on <a href=\"https://isaac-sim.github.io/IsaacLab/\">NVIDIA Isaac Lab</a>, a modular framework for robot learning built on <a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/index.html\">NVIDIA Isaac Sim</a>, which provides support for various libraries for reinforcement learning and imitation learning.</p>\n\n\n\n<h2 id=\"surgical_digital_twins\"  class=\"wp-block-heading\"><strong>Surgical digital twins</strong><a href=\"#surgical_digital_twins\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Figure 1 shows a surgical <a href=\"https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/\">digital twin</a> workflow that illustrates the full pipeline for creating photorealistic anatomical models, from raw CT volume data to final <a href=\"https://developer.nvidia.com/usd\">Universal Scene Description (OpenUSD)</a> in Omniverse. The process includes organ segmentation, mesh conversion, mesh cleaning and refinement, photorealistic texturing, and culminating in the assembly of all textured organs into a unified OpenUSD file.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"815\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow.png\" alt=\"Four-part image showing (left to right) CT Volume, Organ Segmentation, Organ Meshes, and OpenUSD.\" class=\"wp-image-90000\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow-300x122.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow-625x255.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow-179x73.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow-768x313.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow-1536x626.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow-645x263.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow-500x204.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow-160x65.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow-362x148.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow-270x110.png 270w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-digital-twin-workflow-1024x417.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Surgical digital twin workflow</em></figcaption></figure></div>\n\n\n<p>The resulting digital twin simulator generates high-quality synthetic data crucial for training and evaluating behavior cloning models in complex surgical tasks. The study investigates various visual observation modalities, including RGB images from single and multicamera setups and point cloud representations derived from single camera depth data.&nbsp;</p>\n\n\n\n<h2 id=\"policy_learning_and_expert_demonstrations_with_teleoperation\"  class=\"wp-block-heading\">Policy learning and expert demonstrations with teleoperation<a href=\"#policy_learning_and_expert_demonstrations_with_teleoperation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The experimental framework includes five fundamental surgical subtasks designed for evaluation: tissue retraction, needle lift, needle handover, suture pad threading, and block transfer. To learn more and view task videos, see <a href=\"https://orbit-surgical.github.io/sufia-bc/\">SuFIA-BC: Generating High Quality Demonstration Data for Visuomotor Policy Learning in Surgical Subtasks</a>.&nbsp;</p>\n\n\n\n<p>Results indicate that while simpler tasks yield comparable performance across models, complex tasks reveal significant differences in encoder effectiveness. Point cloud-based models generally excel in spatially defined tasks such as needle lift and needle handover, while RGB-based models perform better where color cues are necessary for semantic understanding.</p>\n\n\n\n<p>The number of expert demonstrations were varied to determine the trained models\u2019 sample efficiency. In this experiment, the models demonstrated varying success rates based on the number of training demonstrations, highlighting common failure modes when fewer demonstrations were used. These findings emphasize the importance of architectures with greater sample efficiency and underline the importance of the introduced framework where data collection is significantly more accessible than real-world data. Furthermore, generalization capabilities were assessed using different needle instances, with multicamera RGB models showing better adaptability compared to point cloud-based models.</p>\n\n\n\n<p>Robustness to changes in camera perspectives was evaluated, revealing that point cloud models exhibited superior resilience to viewpoint changes compared to RGB-based models, highlighting their potential for practical deployment in surgical settings.</p>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\">Summary<a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Explore this groundbreaking technology by accessing the open-source assets linked in this article. Visit <a href=\"https://github.com/orbit-surgical/orbit-surgical/releases/tag/v0.1.0\">ORBIT-Surgical</a> on GitHub to access video demonstrations used for training policies, along with photorealistic human organ models. By leveraging these resources, you can advance surgical robotics research, experiment with different learning approaches, and develop innovative solutions for complex surgical procedures. We encourage the community to build upon this foundation, share insights, and collaborate to enhance robotic-assisted surgery.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The integration of robotic surgical assistants (RSAs) in operating rooms offers substantial advantages for both surgeons and patient outcomes. Currently operated through teleoperation by trained surgeons at a console, these surgical robot platforms provide augmented dexterity that has the potential to streamline surgical workflows and alleviate surgeon workloads. Exploring visual behavior cloning for next-generation surgical &hellip; <a href=\"https://developer.nvidia.com/blog/advancing-surgical-robotics-with-ai-driven-simulation-and-digital-twin-technology/\">Continued</a></p>\n", "protected": false}, "author": 2352, "featured_media": 90447, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1501442", "discourse_permalink": "https://forums.developer.nvidia.com/t/advancing-surgical-robotics-with-ai-driven-simulation-and-digital-twin-technology/309828", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [97, 63, 503], "tags": [3941, 453, 2375, 2932, 1409, 3700], "coauthors": [4086, 4087, 3605], "class_list": ["post-89981", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-virtual-reality", "category-robotics", "category-simulation-modeling-design", "tag-ai-impact", "tag-featured", "tag-industrial-digitalization-digital-twin", "tag-large-language-models", "tag-omniverse", "tag-openusd"], "acf": {"post_industry": "", "post_products": "", "post_learning_levels": "", "post_content_types": "", "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tissue-retraction.png", "jetpack_shortlink": "https://wp.me/pcCQAL-npj", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Robotics", "link": "https://developer.nvidia.com/blog/category/robotics/", "id": 63}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89981"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2352"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89981"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89981/revisions"}], "predecessor-version": [{"id": 90445, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89981/revisions/90445"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90447"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89981"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89981"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89981"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89981"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87861, "date": "2024-10-14T11:39:40", "date_gmt": "2024-10-14T18:39:40", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87861"}, "modified": "2024-10-17T11:19:01", "modified_gmt": "2024-10-17T18:19:01", "slug": "learning-fluid-flow-with-ai-enabled-virtual-wind-tunnels", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/learning-fluid-flow-with-ai-enabled-virtual-wind-tunnels/", "title": {"rendered": "Learning Fluid Flow with AI-Enabled Virtual Wind Tunnels"}, "content": {"rendered": "\n<p>There\u2019s never enough time to do everything, even in engineering education. Employers want engineers capable of wielding simulation tools to expedite iterative research, design, and development. Some instructors try to address this by teaching for weeks or months, on derivations of numerical methods, approaches to discretization, the intricacies of turbulence models, and more.&nbsp;</p>\n\n\n\n<p>Unfortunately, focusing on fundamentals leaves little or no time to help develop higher-level skills and intuition with simulation-driven engineering, which is what employers want and need.</p>\n\n\n\n<p>Others have tried to address this through an applied approach. Instructing and mentoring students as they work through software tutorials and more complex projects, which initially appears to be a solution.&nbsp;</p>\n\n\n\n<p>It\u2019s a trap. Instead of learning the fundamentals, we ask students to dive into software packages with complex interfaces and infinite settings, designed and developed to suit any and all special use cases. This gives students some familiarity with navigating simulation projects and software, but sacrifices understanding of the numerical method fundamentals. This approach also fails to deliver the skills employers are seeking.</p>\n\n\n\n<p>The following is a brief overview of the approach which I developed: an automated computational fluid dynamics (CFD) workflow that is more intuitive and powerful for engineering students.</p>\n\n\n\n<p>This simulation tool uses a numerical solver around arbitrary objects to generate the dataset for training an AI model using <a href=\"https://developer.nvidia.com/modulus\">NVIDIA Modulus</a> as the underlying framework. The AI surrogate model is used for exploring design variants providing students with an easy-to-use experimental platform for developing an intuitive understanding and developing analysis skills.\u00a0</p>\n\n\n\n<h2 id=\"challenges_with_engineering_tools_for_teaching\"  class=\"wp-block-heading\">Challenges with engineering tools for teaching<a href=\"#challenges_with_engineering_tools_for_teaching\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>As an educator, I teach engineers to be objective and reflective. Reflecting on the use of simulation tools from a mechanical engineering perspective, finite element analysis and computational fluid dynamics are tools created to improve on and expedite design and analysis work.\u00a0</p>\n\n\n\n<p>The importance of simulation tools has not been lost on the field, as educators have devoted entire courses to learning the intricacies and applications of both. Unfortunately, elevating education on simulation tools to standalone courses has exacerbated the problem.</p>\n\n\n\n<p>Engineering simulation tools should not have standalone courses in undergraduate education. They should be embedded within all courses, ever-present. Providing students with challenges of varying complexity and from different perspectives to develop knowledge and familiarity. Providing instructors with the time needed to help students develop the higher-level skills and intuition sought by future employers.</p>\n\n\n\n<p>Reframing the way simulation tools are taught and incorporated in undergraduate engineering education would have a significant impact. Students would learn the simulation tools at the onset of their programs. It would be so fundamental to this reframed model that it would be easy to fall for the same original trap: devote an entire course to teaching about simulation tools.</p>\n\n\n\n<p>Given what appears to be the start of an infinite loop that leads to the same logical solution, what can be done? Engineers must ask for more from simulation software and technology.</p>\n\n\n\n<p>Fast. Accurate. Effortless. These are the characteristics that engineers should expect from their tools. These also are the characteristics of tools that enable instructors to weave their use throughout engineering education curriculums and help create future engineers.</p>\n\n\n\n<h2 id=\"ai_models_as_teaching_tools&nbsp;\"  class=\"wp-block-heading\">AI models as teaching tools&nbsp;<a href=\"#ai_models_as_teaching_tools&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Rather than try to address all possible fluid flow problems, I set a goal to develop a simulation tool for our students that embodied these characteristics.\u00a0</p>\n\n\n\n<p>Historically, most fluid\u2013 and aero-related senior design projects at MSOE have involved external flows around ground or air vehicles. To address these use cases, I built an automated CFD workflow that simulates the flow around arbitrary objects but also does so for a group of geometric variants. The generated dataset is used as the training and validation sets for a MeshGraphNet (MGN), which is then used to infer surface stresses and forces on hundreds of additional geometric variants.</p>\n\n\n\n<p>Using NVIDIA Modulus as the physics-ML training platform, I explored a few approaches. While an initial version of the automated workflow used physics-Iinformed neural networks (PINNs), the reduced memory usage and speedups provided by using an MGN were perfect fits for the problem.\u00a0</p>\n\n\n\n<p>Just as the Ahmed Body AeroGraphNet example uses the mesh surface, surface pressure, surface shear stress, Reynolds number, and important geometric parameters as inputs into the model, the virtual wind tunnel uses the same, with programmatically modified geometric parameters that work with any arbitrary 3D model.\u00a0</p>\n\n\n\n<p>As more novel architectures emerge in the future, I will use Modulus to seamlessly improve the surrogate models.</p>\n\n\n\n<h2 id=\"removing_barriers_for_students\"  class=\"wp-block-heading\">Removing barriers for students<a href=\"#removing_barriers_for_students\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>I used MSOE\u2019s compute cluster <a href=\"https://msoe.dev/\">Rosie</a>, and its student portal <a href=\"https://openondemand.org/\">Open OnDemand</a>, to create a simple web submission page for students to launch simulations. Open OnDemand enables more traditional command-line interface (CLI) submissions but also provides a platform for improved interfaces and interactivity.</p>\n\n\n\n<p>Students upload a 3D model, specify a Reynolds number,&nbsp; and specify whether the vehicle is air\u2013 or ground-based and if they want AI-enabled feedback. After launching the simulations, the students are emailed when the job is complete.&nbsp;</p>\n\n\n\n<p>The email contains drag, lift, and lateral forces, center of pressure information, automatically generated streamline plots and videos, and a file containing surface stress data in case they want to make additional plots. Using inferred results, they also get images and videos of where they can modify their design to achieve changes in aerodynamic forces and centers of pressure.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"409\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/msoe-virtual-wind-tunnel-form-625x409.png\" alt=\"Form includes fields for selecting a 3D model, setting a Reynolds number, choosing a ground or air vehicle, and requesting traditional or AI simulation.\" class=\"wp-image-87867\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/msoe-virtual-wind-tunnel-form-625x409.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/msoe-virtual-wind-tunnel-form-300x196.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/msoe-virtual-wind-tunnel-form-176x115.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/msoe-virtual-wind-tunnel-form-458x300.png 458w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/msoe-virtual-wind-tunnel-form-137x90.png 137w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/msoe-virtual-wind-tunnel-form-362x237.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/msoe-virtual-wind-tunnel-form-168x110.png 168w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/msoe-virtual-wind-tunnel-form.png 637w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Submission form for HPC and AI engineering simulations</em></figcaption></figure></div>\n\n\n<p>While a simple webform and email may seem archaic compared to generative aerodynamic suggestions, it reframes the interaction. Students don\u2019t need training. They don\u2019t need high-performance computing (HPC) or CLI knowledge. They perform a simple calculation of the Reynolds number for their 3D model and flow. Not completely effortless, but it\u2019s getting closer.</p>\n\n\n\n<h2 id=\"ai-enabled_end-to-end_simulation_pipeline\"  class=\"wp-block-heading\">AI-enabled, end-to-end simulation pipeline<a href=\"#ai-enabled_end-to-end_simulation_pipeline\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Instead of creating a programmatic solution to modify arbitrary, triangulated meshes while guaranteeing valid geometries, I chose a different route.\u00a0</p>\n\n\n\n<p>Student-supplied models are used as the source for generating a volumetric or voxelated copy. This volumetric copy is then modified by adding and removing material over its surface to create hundreds or thousands of modified variants. The volumetric variants are then remeshed and used during training, validation, and inference.</p>\n\n\n\n<p>The original geometry and a training and validation set are used as the models for input into a traditional CFD solver, <a href=\"https://openfoam.org/\">OpenFOAM</a>. Generated object meshes are typically in the range of 15\u201320M total cells with 70\u201390K surface cells and solvers complete 500 iterations.&nbsp;</p>\n\n\n\n<p>The output from OpenFOAM includes the surface stresses, pressure, and shear, for the original model and all of the variants. This dataset is then used as input into the Modulus training pipeline using MGN along with relevant geometric modification parameters.</p>\n\n\n\n<p>Instead of comparing inferred results from trained MGN to OpenFOAM results when creating feedback images and video, inference is completed for the training, validation, and test sets. The inferred results are then compared to each other to generate image and video feedback to be emailed to students.&nbsp;</p>\n\n\n\n<p>While each parallelized OpenFOAM solution can take over an hour to complete, training on a <a href=\"https://www.nvidia.com/en-in/data-center/dgx-1/\">DGX-1 system</a> takes approximately four hours and inference on a loaded mesh takes less than one second.&nbsp;</p>\n\n\n\n<p>I designed the virtual wind tunnel to use parallel jobs of parallel-CPU OpenFOAM runs to generate the dataset and then perform training and inference on four NVIDIA V100 GPUs. Rosie was recently upgraded to include NVIDIA H100 GPUs, which I look forward to using for additional acceleration of the wind tunnel.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"600\" height=\"338\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/WindTunnel_Modification_CoP.gif\" alt=\"GIF shows six screenshots of rotating visuals for geometric modifications and how their changes impacted drag, lift, and lateral forces.\" class=\"wp-image-89433\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Comparison of inferred Modulus results quickly performed on an array of geometric modifications</em></figcaption></figure></div>\n\n\n<h2 id=\"accessible_cfd_simulations\"  class=\"wp-block-heading\">Accessible CFD simulations<a href=\"#accessible_cfd_simulations\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To see how students would use the solution, \u00a0I made the initial OpenFOAM-only version of the virtual wind tunnel available to students last academic year. After students increased their use of the toolchain, I made available the automatic generation of variants, training, and inference.</p>\n\n\n\n<p>The overall endeavor appears to have paid off. More CFD analyses and wind tunnel experiments were carried out last academic year than senior design groups have completed in almost a decade, combined. This enabled students to focus on using the available system as an easy-to-use experimental platform, rather than secondary challenges that they had to overcome. I am advising three additional aero-focused design projects this year and look forward to seeing what they can design and develop.</p>\n\n\n\n<p>Recent advances in AI software and technology have made leaps forward in engineering tools and simulations almost tangible. There\u2019s no better time to be reimagining how we interact with the engineering simulation tools. We need the next generation of tools to be designed for novices, for students, and even for our youngest learners, starting in elementary school.&nbsp;</p>\n\n\n\n<p>Instead of focusing on computational fluid dynamics, we can teach iterative design of winglet shapes, biomimicry-inspired features in an underbody venturi, intuition, and high-level analysis skills, and the name and feel of the wind.</p>\n\n\n\n<h2 id=\"get_started&nbsp;\"  class=\"wp-block-heading\">Get started&nbsp;<a href=\"#get_started&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Educators around the world can get started with their AI journey with the free Deep Learning for Science and Engineering Teaching Kit by joining the <a href=\"https://developer.nvidia.com/teaching-kits\">NVIDIA DLI Teaching Kit Program</a>. The entire lecture part of the course is also openly available through <a href=\"https://www.nvidia.com/en-us/on-demand/deep-learning-for-science-and-engineering/\">NVIDIA On-Demand </a>for educators and students everywhere.</p>\n\n\n\n<p>For researchers interested in exploring AI, the <a href=\"https://github.com/nVIDIA/modulus\">/NVIDIA/modulus</a> GitHub repo has excellent resources for getting started.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>There\u2019s never enough time to do everything, even in engineering education. Employers want engineers capable of wielding simulation tools to expedite iterative research, design, and development. Some instructors try to address this by teaching for weeks or months, on derivations of numerical methods, approaches to discretization, the intricacies of turbulence models, and more.&nbsp; Unfortunately, focusing &hellip; <a href=\"https://developer.nvidia.com/blog/learning-fluid-flow-with-ai-enabled-virtual-wind-tunnels/\">Continued</a></p>\n", "protected": false}, "author": 2266, "featured_media": 89431, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1501423", "discourse_permalink": "https://forums.developer.nvidia.com/t/learning-fluid-flow-with-ai-enabled-virtual-wind-tunnels/309823", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 503], "tags": [1916, 453, 2375, 3281], "coauthors": [3995], "class_list": ["post-87861", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-simulation-modeling-design", "tag-computational-fluid-dynamics", "tag-featured", "tag-industrial-digitalization-digital-twin", "tag-physics-ml"], "acf": {"post_industry": ["Academia / Education"], "post_products": ["Modulus"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Best practice"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fluid-flow-ai-wind-tunnels-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mR7", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87861"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2266"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87861"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87861/revisions"}], "predecessor-version": [{"id": 90298, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87861/revisions/90298"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89431"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87861"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87861"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87861"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87861"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]