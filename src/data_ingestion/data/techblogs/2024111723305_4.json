[{"id": 90119, "date": "2024-10-14T08:54:39", "date_gmt": "2024-10-14T15:54:39", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90119"}, "modified": "2024-10-21T09:29:21", "modified_gmt": "2024-10-21T16:29:21", "slug": "ai-research-revs-up-ev-charging-for-large-scale-optimization-speed-and-savings", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/ai-research-revs-up-ev-charging-for-large-scale-optimization-speed-and-savings/", "title": {"rendered": "AI Research Revs Up EV Charging for Large-Scale Optimization, Speed, and Savings"}, "content": {"rendered": "\n<p>Electric vehicle (EV) charging is getting a jolt with an innovative new AI algorithm that boosts efficiency, reduces cost, and keeps the grid from short-circuiting under pressure. Developed by a team of researchers from the Royal Military College of Canada (RMC), the <a href=\"https://www.mdpi.com/2079-9292/13/9/1783#\">real-time smart solution</a> optimizes charging schedules for large parking lots, balancing quick charging with energy availability. By making charging faster, cheaper, and more available, the AI-powered algorithm could help pave the way for more widespread adoption of EVs\u2014a cleaner option for reducing emissions and meeting climate goals.&nbsp;</p>\n\n\n\n<p>\u201cOptimizing the charging schedule of EVs in a smart parking lot has huge impacts not only on the consumers, who end up paying a lower price, but also on the environment when electricity usage is maximized during periods when it is plentiful,\u201d said Vincent Roberge, study lead author and a professor in the Department of Electrical and Computer Engineering at RMC.</p>\n\n\n\n<p>While the popularity of EVs is growing, one major downfall is the availability of vehicle charging stations. Anticipating and managing the technology\u2019s reliance on the power grid is key to keeping drivers happy and energy infrastructure stable. This is especially important in large parking lots, where hundreds of drivers need to charge their vehicles simultaneously.&nbsp;</p>\n\n\n\n<p>To address this, the researchers developed an AI-powered algorithm that optimizes charging schedules based on vehicle arrival and departure times, charging time, energy demand, electricity cost based on time of day, and charging rate limits. Using this data, and accounting for all cars in the parking lot, it calculates different combinations of charging schedules, picking the best option based on minimizing cost while avoiding overloading the power grid.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"469\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Electric-Car-Charging-Lot-625x469.jpg\" alt=\"An image of an EV plugged in and charging in a parking lot.\" class=\"wp-image-90135\" style=\"object-fit:cover\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. An electric car charging in a parking lot</em></figcaption></figure></div>\n\n\n<p>The researchers simulated different EV parking lot sizes to test the algorithm&#8217;s performance. They started with a small 20-EV parking lot and then scaled up the model to parking lots with 40 to 500 vehicles.\u00a0</p>\n\n\n\n<p>The team developed the algorithm using two <a href=\"https://www.nvidia.com/en-us/design-visualization/rtx-a6000/\">NVIDIA RTX A6000 GPUs</a> awarded through the <a href=\"https://www.nvidia.com/en-us/industries/higher-education-research/academic-grant-program/\">NVIDIA Academic Grant Program</a>. It uses a particle swarm optimization (PSO) algorithm, boosted by NVIDIA CUDA-accelerated GPU parallel processing for automated, real-time updates as vehicles enter or exit the lot.\u00a0According to Roberge, the researchers used PSO, an AI technique from the field of computational intelligence, to compute optimized schedules for EV recharging.\u00a0</p>\n\n\n\n<p>\u201cThe PSO works by independently improving a large number of possible solutions. These can be evaluated in parallel on the GPU, greatly reducing the time needed to perform the optimization,\u201d Roberge said.</p>\n\n\n\n<p>The model runs on multicore CPUs and GPUs, achieving real-time performance with an <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4070-family/\">NVIDIA GeForce RTX 4070 Ti GPU</a>. CUDA-accelerated GPUs greatly boost the scheduling process, delivering a speedup of up to 247.6x, optimizing charging for a 500-EV parking lot in under 30 seconds.</p>\n\n\n\n<p>By scheduling EV charging during off-peak hours, the model can help reduce strain on the electrical grid and cut reliance on fossil-fuel power plants, lowering emissions. Optimized charging schedules could also ease the need for costly infrastructure upgrades, improve grid stability, and maximize charging capacity by reducing peak power demand and avoiding periods of high-cost energy use.</p>\n\n\n\n<p>The researchers are exploring additional applications of CUDA and GPUs for large-scale smart grid optimization. They&#8217;re working on reconfiguring the power distribution network to accommodate renewable energy resources.\u00a0</p>\n\n\n\n<p>\u201cThis reconfiguration will ensure that the distribution network is always operating in an optimized state no matter the demand in energy or variation in the renewable energy production,\u201d Roberge said.</p>\n\n\n\n<p>Read the research <a href=\"https://www.mdpi.com/2079-9292/13/9/1783#\"><em>Parallel Algorithm on Multicore Processor and Graphics Processing Unit for the Optimization of Electric Vehicle Recharge Scheduling</em></a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Electric vehicle (EV) charging is getting a jolt with an innovative new AI algorithm that boosts efficiency, reduces cost, and keeps the grid from short-circuiting under pressure. Developed by a team of researchers from the Royal Military College of Canada (RMC), the real-time smart solution optimizes charging schedules for large parking lots, balancing quick charging &hellip; <a href=\"https://developer.nvidia.com/blog/ai-research-revs-up-ev-charging-for-large-scale-optimization-speed-and-savings/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 90121, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1501347", "discourse_permalink": "https://forums.developer.nvidia.com/t/ai-research-revs-up-ev-charging-for-large-scale-optimization-speed-and-savings/309812", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 503, 1903], "tags": [3941, 1913, 1937, 453, 4125, 1877], "coauthors": [2315], "class_list": ["post-90119", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-data-science", "category-simulation-modeling-design", "category-features", "tag-ai-impact", "tag-climate-weather-ocean-modeling", "tag-oil-and-gas", "tag-featured", "tag-nvidia-academic-grant-program", "tag-research"], "acf": {"post_industry": ["Academia / Education", "Energy", "Smart Cities / Spaces"], "post_products": ["RTX GPU"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Electric-Vehicle-AI-Research.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nrx", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90119"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90119"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90119/revisions"}], "predecessor-version": [{"id": 90278, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90119/revisions/90278"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90121"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90119"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90119"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90119"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90119"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89758, "date": "2024-10-11T11:36:14", "date_gmt": "2024-10-11T18:36:14", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89758"}, "modified": "2024-10-17T11:19:02", "modified_gmt": "2024-10-17T18:19:02", "slug": "transforming-cfd-simulations-with-ml-using-nvidia-modulus", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/transforming-cfd-simulations-with-ml-using-nvidia-modulus/", "title": {"rendered": "Transforming CFD\u00a0Simulations with ML Using NVIDIA Modulus"}, "content": {"rendered": "\n<p>Simulations play a critical role in advancing science and engineering, especially in the vast field of fluid dynamics. However, high-fidelity fluid simulations require extensive computational resources, often constraining practical applications. Accurately simulating complex flows can take weeks of computational effort, slowing down advancements in critical fields such as aerospace and environmental engineering.&nbsp;</p>\n\n\n\n<p>Machine learning (ML) is revolutionizing computational fluid dynamics (CFD) by addressing these challenges. ML algorithms enable researchers to use large-scale datasets and create models that mimic the real-world behavior of complex flow problems with a markedly reduced computational cost.</p>\n\n\n\n<p>One promising ML approach in fluid dynamics involves <a href=\"https://github.com/NVIDIA/modulus/tree/main/modulus/models/fno\">Fourier neural operators</a> (FNOs), which can learn resolution-invariant solution operators. FNOs have opened the possibility of training models for complex flows on low-resolution data that can be dynamically integrated into high-fidelity numerical simulations, which decreases computational costs for many applications.&nbsp;</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/modulus\">NVIDIA Modulus</a> offers an easy way to leverage these advantages of FNOs with its open-source framework designed for building, training, and fine-tuning FNOs and other cutting-edge ML models. With optimized implementations of numerous state-of-the-art ML algorithms, it serves as an exceptionally versatile tool for a variety of applications.</p>\n\n\n\n<p>A major focus in the engineering community today is the augmentation of traditional numerical simulations with AI surrogates to combine their respective strengths. To advance this goal, the research team at the <a href=\"https://www.tum.de/en/\">Technical University of Munich</a> (TUM), led by Professor Dr. Nikolaus A. Adams, is pioneering innovative methods by integrating ML models into established simulation workflows.&nbsp;</p>\n\n\n\n<p>These hybrid approaches aim to enhance both the accuracy and efficiency of numerical simulations by leveraging the predictive power of AI while maintaining the physical accuracy of traditional numerical methods.&nbsp;</p>\n\n\n\n<p>\u201cFor investigating integrated machine-learning simulation workflows, our team has been developing a fully differentiable and highly versatile lattice Boltzmann solver that premises on the algorithmic similarities between simulations and ML models,\u201d said Dr. Adams, head of AER at TU Munich.</p>\n\n\n\n<p>\u201cBy dynamically integrating ML algorithms, specifically FNOs, into our LBM framework, we achieve performance enhancements with orders-of-magnitude speedups over traditional CFD methods. This hybrid approach is revolutionizing the field, enabling us to solve complex fluid dynamical problems for new applications in record time.\u201d</p>\n\n\n\n<h2 id=\"using_machine_learning_within_numerical_methods\"  class=\"wp-block-heading\">Using machine learning within numerical methods<a href=\"#using_machine_learning_within_numerical_methods\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The TUM team is developing a hybrid simulation environment based on the lattice Boltzmann method (LBM) that integrates machine learning dynamically into the numerical simulation workflow. The LBM excels at computing multiphase and multicomponent flows in complex geometries, such as porous media flows, enabling the team to investigate complex flows with high numerical efficiency.\u00a0</p>\n\n\n\n<p>By implementing LBM based on the machine-learning library PyTorch, the research team capitalizes on efficient tensor computing and out-of-the-box GPU hardware acceleration, creating the fast and easy-to-use <a href=\"https://www.mep.tum.de/mep/scicohub/torchlbm/\">TorchLBM</a> solver. It provides a modular implementation of state-of-the-art numerical building blocks for LBM simulations of complex single- and multiphase flows, such as the interaction of a droplet with a circular obstacle.&nbsp;</p>\n\n\n\n<p>To further increase the computational efficiency of TorchLBM, the research team developed hybrid approaches that integrated the predictive power of FNOs into the simulation workflow. These hybrid approaches use the fact that the underlying numerical methods require calculating the output of discretized operators, which can be segmented into sequentially connected units, similar to the architecture of many ML models (Figure 1).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"751\" height=\"356\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/lbm-steps-algorithm.jpg\" alt=\"Diagram shows the visualization of one full LBM cycle, including the moments, equilibrium, collision, streaming, and boundary condition calculations, finished with the time update to the next LBM step.\" class=\"wp-image-89765\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/lbm-steps-algorithm.jpg 751w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/lbm-steps-algorithm-300x142.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/lbm-steps-algorithm-625x296.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/lbm-steps-algorithm-179x85.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/lbm-steps-algorithm-645x306.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/lbm-steps-algorithm-500x237.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/lbm-steps-algorithm-160x76.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/lbm-steps-algorithm-362x172.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/lbm-steps-algorithm-232x110.jpg 232w\" sizes=\"(max-width: 751px) 100vw, 751px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Different steps of the computation in an LBM algorithm</em></figcaption></figure></div>\n\n\n<p>Central to the hybrid implementation was the use of NVIDIA Modulus. The platform provides a user-friendly SciML toolkit with novel architectures and utilities tailor-made for CFD use cases and can be integrated seamlessly with existing machine learning workflows, allowing for rapid development and deployment of the hybrid models. NVIDIA Modulus enabled the team to use the FNO model out of the box, facilitating and accelerating their research going forward.\u00a0</p>\n\n\n\n<p>By extending the TorchLBM solver with the NVIDIA Modulus FNO model, the team created a hybrid machine-learning simulation workflow, focusing on combining the physical accuracy of the LBM with the computational efficiency of FNOs.</p>\n\n\n\n<h2 id=\"ai-augmented_differentiable_lbm_solver\"  class=\"wp-block-heading\">AI-augmented differentiable LBM solver<a href=\"#ai-augmented_differentiable_lbm_solver\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Aiming to develop AI surrogate models that use the FNOs&#8217; independence from the simulation resolution, the researchers showcase their approach for two complex flow problems:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Dynamic evolution of the K\u00e1rm\u00e1n Vortex Street</li>\n\n\n\n<li>Steady-state flow field through porous media</li>\n</ul>\n\n\n\n<p>For each case, separate FNOs for the density and velocity field were trained using an NVIDIA RTX A6000 GPU.&nbsp;</p>\n\n\n\n<p>In the K\u00e1rm\u00e1n Vortex Street case, the team simulated the flow in 2D. The data was generated for various Reynolds numbers using a purely numerical <a href=\"https://www.mep.tum.de/mep/scicohub/torchlbm/\">TorchLBM</a> simulation. Results from advancing the simulation with only FNOs showed some instability after several recursive uses due to hyperparameter tuning challenges.</p>\n\n\n\n<p>However, in hybrid simulations, where the simulation was advanced alternately with an FNO and the LBM, the results were stable and benefited from the drastically reduced computational costs, achieving significant speedups. By optimizing the FNOs prediction timestep, computational costs were halved compared to traditional methods.</p>\n\n\n\n<p>A visual comparison between flows simulated solely by FNOs and those using hybrid simulations demonstrates these improvements for long-time predictions (Figure 2). <em>&nbsp;</em>The pipeline combines an AI model with the LBM method.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdArEUvtHQ-BlPgEzrWLy1UMeT3l36QfhuYPKVzDZTmuy28NPe1sAQfyTm7UdLziWBHhmsp7otWjIH5jHwlKocUucOscrzzM0yIpGuqjAYCvmqcdNgOfOQTK9d_yGszRCfZ1kP9VQ3_xb8xxakDOptYSJY?key=693YyzgWgIh6dU3av3P9eg\" alt=\"This image shows the visual comparison of a pure FNO long-term prediction and the hybrid simulation of the K\u00e1rm\u00e1n Vortex Street. Towards the end of the simulation, the pure FNO prediction develops instabilities in the flow field while the hybrid simulation remains stable.\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Predictions from an AI model and a hybrid simulation pipeline</em></figcaption></figure></div>\n\n\n<p>In the second case, the team simulated the flow through arbitrarily distributed obstacles with periodic boundary conditions. The goal was to use FNOs to predict the steady-state flow field, initialize the simulation using the FNO prediction, and subsequently use the LBM for further iterating toward the steady-state solution. The FNO predictions facilitated efficient initialization of simulations, leading to reductions in time-to-solution by up to 50%, even for high-resolution simulations.\u00a0</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"486\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-low-resolution-1024x486.png\" alt=\"Diagram shows a 50% reduced time-to-solution for the low-resolution porous media flow (32x32x32). Initialization with the FNO-predicted flow field allows the drastic reduction of the computation cost.\" class=\"wp-image-89766\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-low-resolution-1024x486.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-low-resolution-300x142.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-low-resolution-625x297.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-low-resolution-179x85.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-low-resolution-768x365.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-low-resolution-645x306.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-low-resolution-500x237.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-low-resolution-160x76.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-low-resolution-362x172.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-low-resolution-232x110.png 232w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-low-resolution.png 1238w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Efficiency gain of the hybrid model compared to the traditional numerical simulation for a low-resolution simulation</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"483\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-high-resolution-1024x483.png\" alt=\"Diagram shows a 50% reduced time-to-solution for the high-resolution porous media flow (128x128x128). Also for zero-shot super-resolution, the hybrid workflow drastically reduces the computation cost.\" class=\"wp-image-89767\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-high-resolution-1024x483.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-high-resolution-300x142.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-high-resolution-625x295.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-high-resolution-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-high-resolution-768x362.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-high-resolution-645x304.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-high-resolution-500x236.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-high-resolution-160x76.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-high-resolution-362x171.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-high-resolution-233x110.png 233w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/efficiency-gain-high-resolution.png 1250w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Efficiency gain of the hybrid model compared to the traditional numerical simulation for a super-resolution simulation</em></figcaption></figure></div>\n\n\n<p>The TUM team\u2019s approach demonstrates how hybrid simulations that integrate ML models into traditional numerical methods can maintain stability, reduce computational time, and improve overall efficiency in fluid dynamics research.&nbsp;</p>\n\n\n\n<p>The successful application of FNOs to these test cases highlights the potential for machine learning to revolutionize fluid dynamical simulations and other complex engineering problems.&nbsp;</p>\n\n\n\n<h2 id=\"a_new_era_of_cfd_research\"  class=\"wp-block-heading\">A new era of CFD research<a href=\"#a_new_era_of_cfd_research\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>TUM&#8217;s pioneering work in integrating machine learning dynamically into a fluid dynamical solver sets a new benchmark in the field. Their innovative use of NVIDIA Modulus and the development of hybrid models demonstrate the immense potential of machine learning in transforming fluid dynamics research.&nbsp;</p>\n\n\n\n<p>Moving forward, the team aims to refine these hybrid models, scale up their simulations through multi-GPU setups, and integrate their workflows into<a href=\"https://docs.omniverse.nvidia.com/extensions/latest/ext_modulus.html\"> NVIDIA Omniverse</a>, further enhancing their proposed approach and exploring new applications.&nbsp;&nbsp;</p>\n\n\n\n<p>The advancements made by the TUM team unveil new possibilities for applications across various industries. As more researchers and engineers adopt similar methodologies, you can anticipate a revolution in fluid dynamical applications across sectors, leading to more efficient designs, improved performance, and accelerated innovation.&nbsp;</p>\n\n\n\n<p>NVIDIA supports the ML research community by providing an enterprise-grade platform with easy-to-use GPU-optimized utilities and models so that advanced AI tools and knowledge are accessible to all.</p>\n\n\n\n<p>NVIDIA Modulus plays a crucial role in this transformation. For more information, see the NVIDIA Deep Learning Institute course, <a href=\"https://courses.nvidia.com/courses/course-v1:DLI+S-OV-04+V1/\">Introduction to Physics-Informed Machine Learning with Modulus</a>.\u00a0Download the latest <a href=\"https://docs.nvidia.com/deeplearning/modulus/getting-started/index.html\">Modulus container or pip wheels</a> for use in personal environments. </p>\n\n\n\n<p>For more information about how to customize and contribute to the framework, see the <a href=\"https://github.com/NVIDIA/modulus\">/NVIDIA/modulus</a> GitHub repo.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Simulations play a critical role in advancing science and engineering, especially in the vast field of fluid dynamics. However, high-fidelity fluid simulations require extensive computational resources, often constraining practical applications. Accurately simulating complex flows can take weeks of computational effort, slowing down advancements in critical fields such as aerospace and environmental engineering.&nbsp; Machine learning (ML) &hellip; <a href=\"https://developer.nvidia.com/blog/transforming-cfd-simulations-with-ml-using-nvidia-modulus/\">Continued</a></p>\n", "protected": false}, "author": 2336, "featured_media": 89772, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1500138", "discourse_permalink": "https://forums.developer.nvidia.com/t/transforming-cfd-simulations-with-ml-using-nvidia-modulus/309536", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 503], "tags": [1916, 453], "coauthors": [4070, 4071, 4072, 4073, 4074, 4069], "class_list": ["post-89758", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-simulation-modeling-design", "tag-computational-fluid-dynamics", "tag-featured"], "acf": {"post_industry": ["General"], "post_products": ["Modulus"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cfd-simulations-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nlI", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89758"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2336"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89758"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89758/revisions"}], "predecessor-version": [{"id": 90219, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89758/revisions/90219"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89772"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89758"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89758"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89758"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89758"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90153, "date": "2024-10-10T09:54:41", "date_gmt": "2024-10-10T16:54:41", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90153"}, "modified": "2024-11-11T20:33:29", "modified_gmt": "2024-11-12T04:33:29", "slug": "advanced-rag-techniques-for-telco-o-ran-specifications-using-nvidia-nim-microservices", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/advanced-rag-techniques-for-telco-o-ran-specifications-using-nvidia-nim-microservices/", "title": {"rendered": "Advanced RAG Techniques for Telco O-RAN Specifications Using NVIDIA NIM Microservices"}, "content": {"rendered": "\n<p>Mobile communication standards play a crucial role in the telecommunications ecosystem by harmonizing technology protocols to facilitate interoperability between networks and devices from different vendors. As these standards evolve, telecommunications companies face the ongoing challenge of managing complexity and volume.</p>\n\n\n\n<p>By leveraging <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI</a>, telecommunications companies can automate the interpretation and application of technical standards. This reduces the time and effort required to navigate through, analyze, and implement rules and protocols from large volumes of specifications. To demonstrate the power of generative AI in processing standards documents, we have developed a chatbot demo for the O-RAN (Open Radio Access Network) standards.</p>\n\n\n\n<p>O-RAN provides a set of specifications that aims to promote interoperability, openness, and innovation in the radio access network (RAN) component of telecommunications networks by using open interfaces and modular hardware and software.</p>\n\n\n\n<p>This post details our approach, which uses <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a> microservices and <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation (RAG)</a> to efficiently generate responses to complex queries involving large volumes of technical specifications and workflows. This demonstrates the potential of generative AI to transform industry practices and effectively manage complex standards.</p>\n\n\n\n<h2 id=\"o-ran_chatbot_rag_architecture\"  class=\"wp-block-heading\">O-RAN Chatbot RAG architecture<a href=\"#o-ran_chatbot_rag_architecture\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To deploy the O-RAN chatbot, we used NIM microservices designed for cloud-native, end-to-end RAG applications. Specifically, we used the <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo Retriever</a> text embedding NIM, <a href=\"https://build.nvidia.com/nvidia/nv-embedqa-mistral-7b-v2\">NV-Embed-QA-Mistral-7B-v2</a>, to convert passages from O-RAN documentation and user queries into vector representations. Additionally, we implemented a relevance-based NeMo Retriever text reranking NIM to reorder retrieved passages for improved semantic sorting.</p>\n\n\n\n<p>To manage data flow and ensure seamless interaction between components, we integrated various chatbot elements using the LangChain framework. We chose a GPU-accelerated FAISS vector database to store embeddings and employed NIM microservices for <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models (LLMs)</a> to generate answers. We implemented a front-end using Streamlit, enabling users to interact directly with the chatbot. Additionally, we deployed <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo Guardrails</a> to ensure the answers provided were both relevant and factual and further enhance the user experience. Figure 1 illustrates the architecture. To download the code for reference, visit the <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/tree/main/community/oran-chatbot-multimodal\">NVIDIA/GenerativeAIExamples</a> GitHub repo.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"628\" height=\"324\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/o-ran-chatbot-rag-architecture.png\" alt=\"Architecture diagram for an O-RAN chatbot using icons representing a user, multiple NVIDIA NIM microservices, FAISS vector database, O-RAN documents, and a chain server.\n\" class=\"wp-image-90159\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/o-ran-chatbot-rag-architecture.png 628w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/o-ran-chatbot-rag-architecture-300x155.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/o-ran-chatbot-rag-architecture-625x322.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/o-ran-chatbot-rag-architecture-179x92.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/o-ran-chatbot-rag-architecture-500x258.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/o-ran-chatbot-rag-architecture-160x83.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/o-ran-chatbot-rag-architecture-362x187.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/o-ran-chatbot-rag-architecture-213x110.png 213w\" sizes=\"(max-width: 628px) 100vw, 628px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Overview of O-RAN chatbot RAG components</em></em></figcaption></figure>\n\n\n\n<h2 id=\"naive_rag_challenges\"  class=\"wp-block-heading\">Naive RAG challenges<a href=\"#naive_rag_challenges\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Once we set up the basic RAG architecture without enhancements (Naive RAG), we noticed several issues with the responses. First, the answers provided were often too verbose, and the chatbot&#8217;s tone did not align with the intended context. We were able to improve these aspects through appropriate prompt tuning.</p>\n\n\n\n<p>Second, we observed that the basic RAG pipeline was unable to retrieve some relevant documents, leading to inaccurate or misleading responses. Additionally, the pipeline struggled to accurately answer the most complex questions, often resulting in partially correct answers or hallucinations.</p>\n\n\n\n<p>While prompt tuning successfully addressed tone and verbosity issues, a different approach was needed to handle challenges with retrieval and response accuracy. To tackle these, we first experimented with advanced retrieval strategies and then evaluated different language models. These efforts were aimed at optimizing the bot\u2019s overall quality, which will be detailed in the following sections.</p>\n\n\n\n<h2 id=\"optimized_retrieval_strategy\"  class=\"wp-block-heading\">Optimized retrieval strategy<a href=\"#optimized_retrieval_strategy\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To address the issue of retrieval accuracy, we closely examined queries where the retrieved content was incomplete. We discovered that the problem often arose because relevant parts of the answer were spread across different documents, preventing the retrieval system from accessing all necessary contexts. To tackle this challenge, we explored enhancements to our basic RAG by experimenting with two advanced retrieval methods, Advanced RAG and HyDE, which could potentially improve performance.</p>\n\n\n\n<h3 id=\"advanced_rag&nbsp;\"  class=\"wp-block-heading\">Advanced RAG&nbsp;<a href=\"#advanced_rag&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The first enhancement we tried was implementing a query transformation technique, known as <a href=\"https://arxiv.org/pdf/2404.01037\">Advanced RAG</a>, which uses an LLM to generate multiple subqueries from the initial query. This approach aimed to improve retrieval accuracy by expanding the search space and refining the relevance of the retrieved documents. Figure 2 illustrates the structure of Advanced RAG.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"760\" height=\"202\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/advanced-rag-workflow.png\" alt=\"Architecture diagram showing an advanced approach to document retrieval-augmented generation with a query input (left), generated embeddings, top retrieved and reranked documents, and answer output (right) using NVIDIA NIM microservices.\n\" class=\"wp-image-90161\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/advanced-rag-workflow.png 760w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/advanced-rag-workflow-300x80.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/advanced-rag-workflow-625x166.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/advanced-rag-workflow-179x48.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/advanced-rag-workflow-645x171.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/advanced-rag-workflow-500x133.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/advanced-rag-workflow-160x43.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/advanced-rag-workflow-362x96.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/advanced-rag-workflow-414x110.png 414w\" sizes=\"(max-width: 760px) 100vw, 760px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. Advanced RAG workflow</em></em></figcaption></figure>\n\n\n\n<h3 id=\"hyde_rag\"  class=\"wp-block-heading\">HyDE RAG<a href=\"#hyde_rag\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Next, we explored another method called HyDE (Hypothetical Document Embeddings) RAG. <a href=\"https://arxiv.org/pdf/2212.10496\">HyDE</a> enhances retrieval by considering potential answers, allowing the system to find more contextually relevant documents. This technique had previously outperformed many dense retrievers and demonstrated performance comparable to fine-tuned retrievers across various tasks. Figure 3 provides an overview of how we implemented HyDE RAG and its integration into the retrieval process.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"721\" height=\"205\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hyde-rag-workflow.png\" alt=\"Workflow diagram showing a HyDE RAG workflow with a query input (left), generated embeddings and reranked documents, and final answer output (right) using NVIDIA NIM microservices.\n\" class=\"wp-image-90162\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hyde-rag-workflow.png 721w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hyde-rag-workflow-300x85.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hyde-rag-workflow-625x178.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hyde-rag-workflow-179x51.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hyde-rag-workflow-645x183.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hyde-rag-workflow-500x142.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hyde-rag-workflow-160x45.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hyde-rag-workflow-362x103.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/hyde-rag-workflow-387x110.png 387w\" sizes=\"(max-width: 721px) 100vw, 721px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. HyDE RAG workflow</em></em></figcaption></figure>\n\n\n\n<h3 id=\"retriever_strategy_evaluation\"  class=\"wp-block-heading\">Retriever strategy evaluation<a href=\"#retriever_strategy_evaluation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>After implementing the Advanced RAG and HyDE RAG techniques, we proceeded to evaluate their performance compared to the basic Naive RAG. Our evaluation combined the insights of human expertise with the efficiency and consistency of automated methods, leveraging the strengths of both approaches.</p>\n\n\n\n<p>For the human evaluation, we engaged O-RAN engineers to create 20 questions that covered various aspects of the latest standard release. We then generated answers using all three RAG methodologies: Naive RAG, Advanced RAG, and HyDE RAG. The experts assessed the quality of each response by rating it on a scale of 1 to 5, considering both the overall quality and the relevance of the answer to the question.</p>\n\n\n\n<p>For the automated evaluation, we used RAGAs, an open-source framework that employs a state-of-the-art LLM to act as a judge, automating the evaluation process. Figure 4 illustrates our evaluation methodology, showing how both human and automated assessments were integrated to provide a comprehensive comparison of the RAG techniques.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"850\" height=\"225\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/retriever-strategy-evaluation.png\" alt=\"Architecture diagram showing workflow to compare three retriever strategies: Naive RAG, HyDe RAG and Advanced RAG with Evaluation Dataset as input (left), human evaluation and LLM-as-a-Judge, and best retriever strategy as output (right).\n\" class=\"wp-image-90163\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/retriever-strategy-evaluation.png 850w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/retriever-strategy-evaluation-300x79.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/retriever-strategy-evaluation-625x165.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/retriever-strategy-evaluation-179x47.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/retriever-strategy-evaluation-768x203.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/retriever-strategy-evaluation-645x171.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/retriever-strategy-evaluation-500x132.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/retriever-strategy-evaluation-160x42.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/retriever-strategy-evaluation-362x96.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/retriever-strategy-evaluation-416x110.png 416w\" sizes=\"(max-width: 850px) 100vw, 850px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. Retriever strategy evaluation</em></em></figcaption></figure>\n\n\n\n<p>Figure 5 displays the results of these evaluations, clearly showing that using the enhanced RAG techniques led to a noticeable improvement in the quality of the responses. Both human and automated evaluations consistently found that the Advanced RAG method performed better than both the Naive RAG and HyDE RAG methods.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"786\" height=\"305\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/human-ragas-evaluation-results-rag.png\" alt=\"Side-by-side images. On the left: Bar chart showing retrieval results for Naive RAG (left), Advanced RAG (center), and HyDE RAG (right). On the right: Bar chart showing context precision (green) and context recall (orange) results for Naive RAG (left), Advanced RAG (center), and HyDE RAG (right).\" class=\"wp-image-90166\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/human-ragas-evaluation-results-rag.png 786w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/human-ragas-evaluation-results-rag-300x116.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/human-ragas-evaluation-results-rag-625x243.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/human-ragas-evaluation-results-rag-179x69.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/human-ragas-evaluation-results-rag-768x298.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/human-ragas-evaluation-results-rag-645x250.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/human-ragas-evaluation-results-rag-500x194.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/human-ragas-evaluation-results-rag-160x62.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/human-ragas-evaluation-results-rag-362x140.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/human-ragas-evaluation-results-rag-283x110.png 283w\" sizes=\"(max-width: 786px) 100vw, 786px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 5. Human evaluation results (left) and RAGAs LLM-as-a-Judge evaluation results (right) for different RAG strategies</em></em></figcaption></figure></div>\n\n\n<h2 id=\"selection_of_nvidia_llm_nim\"  class=\"wp-block-heading\">Selection of NVIDIA LLM NIM<a href=\"#selection_of_nvidia_llm_nim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>After identifying the best retriever strategy, we aimed to further improve answer accuracy by evaluating different LLM NIM microservices. As shown in the workflow below, we experimented with various models to determine the most accurate one. Using the Advanced RAG pipeline, we generated answers with different LLM NIM microservices and assessed their performance. To do this, we employed the RAGAs framework using LLM-as-a-Judge to calculate two key metrics: faithfulness and answer relevancy.</p>\n\n\n\n<p>Given the large number of NIM microservices we needed to compare, we opted to prioritize automated evaluation over human evaluation, which would have been time-consuming and required significant engineering resources. Figure 6 illustrates our LLM NIM evaluation process.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1077\" height=\"258\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-llm-nim-evaluation-process.png\" alt=\"Workflow diagram showing evaluation of six NVIDIA LLM NIM microservices using GPT-4 as a judge with input evaluation dataset (left), Advanced RAG, and output for the most accurate LLM (right).\n\" class=\"wp-image-90168\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-llm-nim-evaluation-process.png 1077w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-llm-nim-evaluation-process-300x72.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-llm-nim-evaluation-process-625x150.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-llm-nim-evaluation-process-179x43.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-llm-nim-evaluation-process-768x184.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-llm-nim-evaluation-process-645x155.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-llm-nim-evaluation-process-500x120.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-llm-nim-evaluation-process-160x38.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-llm-nim-evaluation-process-362x87.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-llm-nim-evaluation-process-459x110.png 459w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-llm-nim-evaluation-process-1024x245.png 1024w\" sizes=\"(max-width: 1077px) 100vw, 1077px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 6. NVIDIA LLM NIM evaluation process</em></em></figcaption></figure>\n\n\n\n<p>Based on the results in Figure 7, we noticed that all the LLMs performed at par, showing little performance difference between them. This suggested that retrieval optimization was the key factor. Once refined, it enabled all the open-source LLMs to achieve comparable performance.\u00a0</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1091\" height=\"528\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ragas-llm-model-evaulation.png\" alt=\"Bar chart showing results for RAGAs LLM model evaluation in faithfulness (green) and answer relevancy (orange) for six LLMs based on Mixtral and Llama 3 evaluated. \n\" class=\"wp-image-90170\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ragas-llm-model-evaulation.png 1091w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ragas-llm-model-evaulation-300x145.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ragas-llm-model-evaulation-625x302.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ragas-llm-model-evaulation-179x87.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ragas-llm-model-evaulation-768x372.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ragas-llm-model-evaulation-645x312.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ragas-llm-model-evaulation-500x242.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ragas-llm-model-evaulation-160x77.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ragas-llm-model-evaulation-362x175.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ragas-llm-model-evaulation-227x110.png 227w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/ragas-llm-model-evaulation-1024x496.png 1024w\" sizes=\"(max-width: 1091px) 100vw, 1091px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 7.\u00a0 RAGAs LLM-as-a-Judge evaluation results for different LLM NIM microservices</em></em></figcaption></figure>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>We demonstrated the value of building advanced RAG pipelines to create an expert chatbot capable of understanding O-RAN technical specifications by utilizing NVIDIA LLM NIM microservices and NeMo Retriever embedding and reranking NIM microservices. By leveraging open-source LLMs enhanced with advanced retrieval techniques, we significantly improved the accuracy of responses to complex technical questions.</p>\n\n\n\n<p>Our evaluation framework showed that the Advanced RAG method consistently outperformed other methodologies in both retrieval accuracy and overall response quality. The success of the O-RAN chatbot highlights that integrating the NVIDIA end-to-end platform for developing custom generative AI enables telecommunications companies to improve their efficiency in processing technical standards, thereby maintaining a competitive edge in the rapidly evolving telecommunications industry.</p>\n\n\n\n<p>To learn more, visit the <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/tree/main/community/oran-chatbot-multimodal\">NVIDIA/GenerativeAIExamples</a> GitHub repo.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Mobile communication standards play a crucial role in the telecommunications ecosystem by harmonizing technology protocols to facilitate interoperability between networks and devices from different vendors. As these standards evolve, telecommunications companies face the ongoing challenge of managing complexity and volume. By leveraging generative AI, telecommunications companies can automate the interpretation and application of technical standards. &hellip; <a href=\"https://developer.nvidia.com/blog/advanced-rag-techniques-for-telco-o-ran-specifications-using-nvidia-nim-microservices/\">Continued</a></p>\n", "protected": false}, "author": 2103, "featured_media": 90154, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1499338", "discourse_permalink": "https://forums.developer.nvidia.com/t/advanced-rag-techniques-for-telco-o-ran-specifications-using-nvidia-nim-microservices/309390", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [453, 2932, 3739, 4142, 3613, 549], "coauthors": [3825, 4097, 4013, 4098, 4099, 3358, 3932], "class_list": ["post-90153", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-large-language-models", "tag-nim", "tag-recommenders-personalization", "tag-retrieval-augmented-generation-rag", "tag-telecommunications"], "acf": {"post_industry": ["Telecommunications"], "post_products": ["NeMo", "NeMo Retriever", "NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/skyscrapers-overlay.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ns5", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90153"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2103"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90153"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90153/revisions"}], "predecessor-version": [{"id": 90174, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90153/revisions/90174"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90154"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90153"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90153"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90153"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90153"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90087, "date": "2024-10-09T12:00:00", "date_gmt": "2024-10-09T19:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90087"}, "modified": "2024-11-05T18:26:22", "modified_gmt": "2024-11-06T02:26:22", "slug": "nvidia-grace-cpu-delivers-world-class-data-center-performance-and-breakthrough-energy-efficiency", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-grace-cpu-delivers-world-class-data-center-performance-and-breakthrough-energy-efficiency/", "title": {"rendered": "NVIDIA Grace CPU Delivers World-Class Data Center Performance and Breakthrough Energy Efficiency"}, "content": {"rendered": "\n<p>NVIDIA designed the <a href=\"https://www.nvidia.com/en-us/data-center/grace-cpu-superchip/\">NVIDIA Grace CPU</a> to be a new kind of high-performance, data center CPU\u2014one built to deliver breakthrough <a href=\"https://www.nvidia.com/en-us/glossary/energy-efficiency/\">energy efficiency</a> and optimized for performance at data center scale.&nbsp;</p>\n\n\n\n<p>Accelerated computing is enabling giant leaps in performance and energy efficiency compared to traditional CPU computing. To deliver these speedups, full-stack innovation at data center scale is required, spanning chips, systems, software, and algorithms. Choosing the right architecture for the right workload with the best energy-efficient performance is critical to maximizing the performance and minimizing the footprint of your data center.</p>\n\n\n\n<p>As workloads are increasingly accelerated, there remain use cases that today primarily run on traditional CPUs\u2014particularly code that is sparse and \u201cbranchy\u201d serialized tasks such as graph analytics. At the same time, data centers are increasingly power-constrained, limiting the growth of their capabilities. This means that all workloads that can be accelerated should be accelerated. Those that cannot be accelerated must be run on the most efficient compute possible, and the CPU must be optimized for those workloads.&nbsp;</p>\n\n\n\n<p>The new, energy-efficient Grace CPU requires outstanding single-thread performance, as well as enough cores to run many applications simultaneously. Those cores each require significant memory bandwidth to ensure high CPU core utilization and the ability to communicate with each other quickly and efficiently.&nbsp;</p>\n\n\n\n<h2 id=\"designed_for_greater_energy_efficiency_with_no_performance_compromise&nbsp;\"  class=\"wp-block-heading\">Designed for greater energy efficiency with no performance compromise&nbsp;<a href=\"#designed_for_greater_energy_efficiency_with_no_performance_compromise&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-gb/data-center/grace-cpu/\">NVIDIA Grace architecture</a> is designed for an accelerated computing world in which GPUs and coherently coupled CPU-GPU architectures accelerate the data center. Such architectures require a CPU with outstanding single-thread performance, a fast fabric, exceptional energy efficiency, and high memory bandwidth.&nbsp;</p>\n\n\n\n<p>The NVIDIA Grace CPU<strong> </strong>combines 72 high-performance and energy-efficient Arm Neoverse V2 cores cores, connected with the NVIDIA Scalable Coherency Fabric (SCF). The NVIDIA SCF is a high-bandwidth, on-chip fabric that provides a total of 3.2 TB/s of bisection bandwidth\u2014double that of traditional CPUs. A high-bandwidth on-chip fabric is needed to deliver maximum system-level performance by maintaining the data flow among CPU cores, cache, memory, and system input and output. Traditional CPUs with a chiplet architecture are less energy efficient and have area and communication overhead that provides less predictable performance.</p>\n\n\n\n<p>Grace is the <a href=\"https://developer.nvidia.com/blog/nvidia-grace-cpu-superchip-architecture-in-depth/\">first data center CPU to use high-speed LPDDR5X memory</a> with server-class reliability through mechanisms like error-correcting code (ECC). By using this more efficient memory type and a wide memory subsystem, Grace delivers up to 500 GB/s of memory bandwidth while consuming just one-fifth the energy of traditional DDR memory at similar cost.&nbsp;</p>\n\n\n\n<p>These numerous innovations mean that the <a href=\"https://www.nvidia.com/en-us/data-center/grace-cpu-superchip/\">NVIDIA Grace CPU Superchip</a> delivers outstanding performance, memory bandwidth, and data-movement capabilities with breakthrough performance per watt. At the data center level, this translates into a generational leap in performance and outstanding total cost of ownership (TCO). Grace architecture delivers these benefits in a data center grade, general-purpose CPU, which means that it provides versatility and performance across a broad range of foundational data center workloads such as microservices, data analytics, graph analytics, and simulation.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"852\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison.png\" alt=\"The chart compares the NVIDIA Grace CPU Superchip with the Intel Xeon 8480+ and AMD EPYC 9654 2S servers across a range of application based workloads with NVIDIA Grace leading by up to 2x.\n\" class=\"wp-image-90095\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison-300x128.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison-625x266.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison-768x327.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison-1536x655.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison-645x275.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison-500x213.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison-362x154.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison-258x110.png 258w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-performance-comparison-1024x436.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. NVIDIA Grace CPU Superchip performance compared against x86 2S servers</em></em></figcaption></figure>\n\n\n\n<p>Figure 1 compares the raw performance per server between NVIDIA Grace architecture and leading x86 servers and shows that it delivers leading server level performance against the best of the x86 competition.&nbsp;</p>\n\n\n\n<p>The exceptional memory bandwidth and fabric performance of the Grace architecture make it strong across several types of popular applications, including:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Microservices:</strong> Small, independent services that help data centers scale easily and manage individual services without affecting the entire application. The Google protocol buffers workload tested measures how quickly data can be serialized and parsed to be exchanged between microservices.\u00a0</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/glossary/high-performance-computing/\"><strong>High-performance computing (HPC)</strong></a><strong> and data analytics:</strong> Workloads such as weather forecasting and Hi-Bench K-means Spark are highly sensitive to achievable memory bandwidth. The leading memory bandwidth and fast NVIDIA designed fabric enables Grace to deliver best-in-class performance on these benchmarks.</li>\n\n\n\n<li><strong>Graph analytics:</strong> Commonly used as part of optimization algorithms, fraud detection and social network analyses in financial services, healthcare, and marketing and operations in many industries. In the GapBS Breadth First Search benchmark, Grace\u2014which has twice the fabric bandwidth of traditional x86 CPUs\u2014stands out compared to the competition. Control flows expand to all available CPU cores and then back down to a single CPU core, benefiting from fast communication between CPU cores.</li>\n</ul>\n\n\n\n<p>On workloads such as compression that scale well with cores, Grace can perform similarly to higher core count products, with the high-performance cores and high-bandwidth NVIDIA SCF.</p>\n\n\n\n<p>Figure 2 shows the energy efficiency of those servers. With its low-power and high-bandwidth memory, Grace delivers 2x more performance for the same power envelope as the competition across the full spectrum of workloads.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"852\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison.png\" alt=\"The chart compares the NVIDIA Grace CPU Superchip with the Intel Xeon 8480+ and AMD EPYC 9654 2S server energy efficiency across a range of application based workloads with NVIDIA Grace leading by up to 3x.\n\" class=\"wp-image-90096\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison-300x128.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison-625x266.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison-768x327.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison-1536x655.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison-645x275.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison-500x213.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison-362x154.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison-258x110.png 258w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-energy-efficiency-comparison-1024x436.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. NVIDIA Grace CPU Superchip energy efficiency compared to x86 2S servers</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">NVIDIA Grace Superchip 480GB of LPDDR5X, AMD EPYC 9654 768 GB of DDR5, and Intel Xeon Platinum 8480+ with 1TB DDR5. OS: Ubuntu 22.04 Compilers: GCC 12.3 unless noted below. Power for energy efficiency includes CPU + memory measured power.</p>\n\n\n\n<p class=\"has-small-font-size\">Compression: Snappy (Commit af720f9a3b2c831f173b6074961737516f2d3a46 | N instances in parallel) Microservices: Google Protobufs (Commit 7cd0b6fbf1643943560d8a9fe553fd206190b27f | N instances in parallel) Seismic Data Proc: SPECFEM3D four_material_simple_model; HPC SDK 24.3 CFD: OpenFOAM Motorbike | Large v2212 MD: CP2K RPA 2023.1 Weather: WRF CONUS12km x86: ICC 2024.01; Climate: NEMO Gyre_Pisces v4.2.0 Weather: ICON QUBICC 80 km resolution Data Analytics: HiBench+K-means Spark (HiBench 7.1.1, Hadoop 3.3.3, Spark 3.3.0; Grace: NVHPC 24.5, x86: Intel 2021.4) Graph Analytics: The Gap Benchmarks Suite BFS arXiv:1508.03619 [cs.DC], 2015.</p>\n\n\n\n<h2 id=\"nvidia_grace_delivers_consistent_performance\"  class=\"wp-block-heading\">NVIDIA Grace delivers consistent performance<a href=\"#nvidia_grace_delivers_consistent_performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Beyond its exceptional performance and energy efficiency, the Grace CPU is designed to sustain consistent performance levels with deterministic performance. Grace can maintain maximum frequency even when all cores are active and deliver high levels of performance even when power is reduced.</p>\n\n\n\n<p>The NVIDIA SCF removes data movement bottlenecks. By combining a high-bandwidth fabric and a wide LPDDR5X memory interface, the Grace CPU achieves over 90% STREAM efficiency (a measure of delivered memory bandwidth relative to peak-rated bandwidth) even when all cores are active. In contrast, competitive systems will reach just over 80% max efficiency, dropping to around 70% when all cores are active (Figure 3).&nbsp;</p>\n\n\n\n<p>The Grace CPU enables use of the optimal number of CPU cores while ensuring each core can fully utilize the available memory bandwidth. As a result, Grace delivers leading performance in memory bandwidth-bound workloads including weather forecasting or data analytics (Figure 1).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"928\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad.png\" alt=\"A chart shows the increase in memory bandwidth per core that sustains close to the max throughout the use of the full CPU.\n\" class=\"wp-image-90097\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad-300x139.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad-625x290.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad-179x83.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad-768x357.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad-1536x713.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad-645x299.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad-500x232.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad-160x74.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad-362x168.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad-237x110.png 237w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-sream-triad-1024x475.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. NVIDIA Grace CPU delivers a flat STREAM Triad bandwidth curve\u00a0</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">Competitive comparison from <a href=\"https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/tuning-guides/58002_amd-epyc-9004-tg-hpc.pdf\">High Performance Tuning Guide for AMD EPYC 9004 Series Processors</a> using STREAM Triad results on a system with 2x 9654 and 1 DPC (DIMM Per Channel), with DDR5-4800 Dual-Rank DIMMs.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"865\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores.png\" alt=\"A chart compares Grace against x86 for STREAM efficiency, with Grace delivering over 90% efficiency at max bandwidth and with all cores active. \n\" class=\"wp-image-90098\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores-300x130.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores-625x270.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores-179x77.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores-768x332.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores-1536x665.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores-645x279.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores-500x216.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores-362x157.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores-254x110.png 254w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu-stream-efficiency-all-cores-1024x443.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. NVIDIA Grace CPU Superchip achieves over 90% STREAM efficiency at both max bandwidth and with all cores active</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">NVIDIA Grace CPU Superchip 480GB of LPDDR5X. OS: Ubuntu 22.04 Compilers: GCC 12.3.</p>\n\n\n\n<p class=\"has-small-font-size\">Competitive comparison from <a href=\"https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/tuning-guides/58002_amd-epyc-9004-tg-hpc.pdf\">High Performance Tuning Guide for AMD EPYC 9004 Series Processors</a> using STREAM Triad results on a system with 2x 9654 and 1 DPC (DIMM Per Channel), with DDR5-4800 Dual-Rank DIMMs.</p>\n\n\n\n<h2 id=\"unmatched_levels_of_data_center_performance\"  class=\"wp-block-heading\">Unmatched levels of data center performance<a href=\"#unmatched_levels_of_data_center_performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In today\u2019s post-Moore\u2019s Law era, traditional CPU methods of meeting the insatiable demand for computing performance require disproportionate increases in cost and energy. Data centers are becoming constrained in power delivery, limiting the growth of their capabilities. To meet these challenges and bolster <a href=\"https://www.nvidia.com/en-us/data-center/sustainable-computing/\">sustainable computing</a> goals, modern data centers must accelerate all workloads. Workloads that cannot be accelerated must use the most energy-efficient computing available.&nbsp;</p>\n\n\n\n<p>NVIDIA Grace meets these challenges with the ability to deliver twice the performance in the same power, opening new opportunities for optimizing your data center. Data center operators have the option of doubling the performance in the same power envelope, or maintaining a consistent level of performance in only half the energy. This opens the potential to use those power savings for acceleration with GPUs in a limited power budget.&nbsp;</p>\n\n\n\n<p>NVIDIA Grace is built using Arm standards. This means that any work done transitioning to other Arm data center class architectures will run on Grace, and any work done on NVIDIA Grace will work on the rest of the Arm data center ecosystem. Transitioning to NVIDIA Grace also enables tightly coupled CPU and GPU architectures with products such as the <a href=\"https://www.nvidia.com/en-us/data-center/gb200-nvl72/\">NVIDIA GB200 Grace Blackwell Superchip</a> in the <a href=\"https://developer.nvidia.com/blog/nvidia-gb200-nvl72-delivers-trillion-parameter-llm-training-and-real-time-inference/\">NVIDIA GB200 NVL72</a>. With Grace, data centers can standardize on a single CPU architecture that also works across the entire Arm ecosystem.</p>\n\n\n\n<p>Ready to get started? Try the <a href=\"https://www.nvidia.com/en-us/launchpad/grace-cpu-superchip/\">free hands-on NVIDIA Grace CPU lab</a> through NVIDIA LaunchPad.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA designed the NVIDIA Grace CPU to be a new kind of high-performance, data center CPU\u2014one built to deliver breakthrough energy efficiency and optimized for performance at data center scale.&nbsp; Accelerated computing is enabling giant leaps in performance and energy efficiency compared to traditional CPU computing. To deliver these speedups, full-stack innovation at data center &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-grace-cpu-delivers-world-class-data-center-performance-and-breakthrough-energy-efficiency/\">Continued</a></p>\n", "protected": false}, "author": 795, "featured_media": 90089, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1498913", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-grace-cpu-delivers-world-class-data-center-performance-and-breakthrough-energy-efficiency/309284", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 503, 1903], "tags": [1913, 453, 3099, 4159], "coauthors": [1327, 2732], "class_list": ["post-90087", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "category-simulation-modeling-design", "category-features", "tag-climate-weather-ocean-modeling", "tag-featured", "tag-grace-hopper-superchip", "tag-inference-performance"], "acf": {"post_industry": ["Energy", "Hardware / Semiconductor", "HPC / Scientific Computing"], "post_products": ["GB200", "Grace CPU"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-grace-cpu.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nr1", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90087"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/795"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90087"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90087/revisions"}], "predecessor-version": [{"id": 90099, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90087/revisions/90099"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90089"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90087"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90087"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90087"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90087"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90127, "date": "2024-10-09T09:53:54", "date_gmt": "2024-10-09T16:53:54", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90127"}, "modified": "2024-10-17T11:19:05", "modified_gmt": "2024-10-17T18:19:05", "slug": "just-released-updated-math-libraries-in-cuda-toolkit-12-6-2", "status": "publish", "type": "post", "link": "https://nvda.ws/4dFvX9L", "title": {"rendered": "Just Released: Updated Math Libraries in CUDA Toolkit 12.6.2"}, "content": {"rendered": "\n<p>CUDA Toolkit 12.6.2 improves performance and provides new features in cuBLAS, cuSOLVER, and cuFFT LTO libraries.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>CUDA Toolkit 12.6.2 improves performance and provides new features in cuBLAS, cuSOLVER, and cuFFT LTO libraries.</p>\n", "protected": false}, "author": 338, "featured_media": 89661, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1498875", "discourse_permalink": "https://forums.developer.nvidia.com/t/just-released-updated-math-libraries-in-cuda-toolkit-12-6-2/309273", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/4dFvX9L", "_links_to_target": "_blank"}, "categories": [503], "tags": [1932, 453, 1958], "coauthors": [2968], "class_list": ["post-90127", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-simulation-modeling-design", "tag-development-tools-and-libraries", "tag-featured", "tag-news"], "acf": {"post_industry": ["HPC / Scientific Computing"], "post_products": ["cuBLAS", "CUDA", "cuFFT", "cuSOLVER"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuda-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nrF", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90127"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/338"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90127"}], "version-history": [{"count": 1, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90127/revisions"}], "predecessor-version": [{"id": 90128, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90127/revisions/90128"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89661"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90127"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90127"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90127"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90127"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89747, "date": "2024-10-09T09:00:00", "date_gmt": "2024-10-09T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89747"}, "modified": "2024-10-17T11:19:06", "modified_gmt": "2024-10-17T18:19:06", "slug": "develop-academic-and-industrial-applications-with-a-new-specialized-math-model", "status": "publish", "type": "post", "link": "https://nvda.ws/3TOxKlS", "title": {"rendered": "Develop Academic and Industrial Applications with a New Specialized Math Model"}, "content": {"rendered": "\n<p>Mathstral, an advanced AI model developed from the ground up, can deliver superior performance for enhanced learning of math, engineering, and science.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Mathstral, an advanced AI model developed from the ground up, can deliver superior performance for enhanced learning of math, engineering, and science.</p>\n", "protected": false}, "author": 492, "featured_media": 89749, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1498854", "discourse_permalink": "https://forums.developer.nvidia.com/t/develop-academic-and-industrial-applications-with-a-new-specialized-math-model/309269", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3TOxKlS", "_links_to_target": "_blank"}, "categories": [3110], "tags": [453, 1066], "coauthors": [610, 3872], "class_list": ["post-89747", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-pre-trained-foundation-models"], "acf": {"post_industry": ["Consumer Internet", "Retail / Consumer Packaged Goods"], "post_products": ["AI Foundation Models"], "post_learning_levels": ["General Interest"], "post_content_types": ["Announcement"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/mathstral-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nlx", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89747"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/492"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89747"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89747/revisions"}], "predecessor-version": [{"id": 89967, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89747/revisions/89967"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89749"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89747"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89747"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89747"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89747"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 90040, "date": "2024-10-09T08:00:00", "date_gmt": "2024-10-09T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=90040"}, "modified": "2024-11-05T18:26:35", "modified_gmt": "2024-11-06T02:26:35", "slug": "boosting-llama-3-1-405b-throughput-by-another-1-5x-on-nvidia-h200-tensor-core-gpus-and-nvlink-switch", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/boosting-llama-3-1-405b-throughput-by-another-1-5x-on-nvidia-h200-tensor-core-gpus-and-nvlink-switch/", "title": {"rendered": "Boosting Llama 3.1 405B Throughput by Another 1.5x on NVIDIA H200 Tensor Core GPUs and NVLink Switch"}, "content": {"rendered": "\n<p>The continued growth of LLMs capability, fueled by increasing parameter counts and support for longer contexts, has led to their usage in a wide variety of applications, each with diverse deployment requirements. For example, a chatbot supports a small number of users at very low latencies for good interactivity. Meanwhile, synthetic data generation requires high throughput to process many items at once. Delivering optimal inference performance across a wide range of use cases with one platform requires optimization across the entire technology stack.&nbsp;</p>\n\n\n\n<p>Cutting-edge LLMs, like Llama 3.1 405B, require multiple GPUs working together for peak performance. To effectively use multiple GPUs for processing inference requests, an <a href=\"https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/\" target=\"_blank\" rel=\"noreferrer noopener\">inference software stack must provide developers with optimized implementations of key parallelism techniques</a>, including tensor, pipeline, and expert parallelism. These parallelism techniques require that GPUs be able to transfer data quickly and efficiently, necessitating a robust GPU-to-GPU interconnect fabric for maximum performance.</p>\n\n\n\n<p>In this post, we explain two of these parallelism techniques and show, on an NVIDIA HGX H200 system with NVLink and NVSwitch, how the right parallelism increases Llama 3.1 405B performance by 1.5x in throughput-sensitive scenarios. We also show how use of pipeline parallelism enabled a 1.2x speedup in the MLPerf Inference v4.1 Llama 2 70B benchmark on HGX H100 compared to our <a href=\"https://developer.nvidia.com/blog/nvidia-blackwell-platform-sets-new-llm-inference-records-in-mlperf-inference-v4-1/\" target=\"_blank\" rel=\"noreferrer noopener\">results published in August</a>. These improvements are possible due to recent software improvements in <a href=\"https://github.com/NVIDIA/TensorRT-LLM\" target=\"_blank\" rel=\"noreferrer noopener\">TensorRT-LLM</a> with NVSwitch.</p>\n\n\n\n<h2 id=\"choosing_parallelism_for_deployment\"  class=\"wp-block-heading\">Choosing parallelism for deployment<a href=\"#choosing_parallelism_for_deployment\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Both tensor parallel (TP) and pipeline parallel (PP) techniques increase compute and memory capacity by splitting models across multiple GPUs, but they differ in how they impact performance. Pipeline parallelism is a low-overhead mechanism for efficiently increasing overall throughput, while tensor parallelism is a higher-overhead mechanism for reducing latency. In some scenarios, TP can also increase throughput proportional to a single GPU. More details on these techniques are in the following sections.</p>\n\n\n\n<p>To illustrate the trade-offs between tensor and pipeline parallelism, we investigate the Llama 2 and Llama 3.1 family of models in two scenarios: minimum latency for peak interactivity, and maximum throughput for peak efficiency. This comparison focuses on total output tokens per second, which is representative of interactivity at small concurrencies (minimum latency) and efficiency at large concurrencies (maximum throughput).</p>\n\n\n\n<figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><tbody><tr><td colspan=\"2\" rowspan=\"2\"><em>Llama 3.1 405B</em><br><em>Output Tokens/second</em><br><em>(higher is better)</em></td><td colspan=\"2\">Parallelism</td></tr><tr><td>Tensor</td><td>Pipeline</td></tr><tr><td rowspan=\"2\"><br>Scenario</td><td>minimum latency</td><td>56</td><td>10</td></tr><tr><td>maximum throughput</td><td>506</td><td>764</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Llama 3.1 405B inference throughput (tokens/second) using tensor and pipeline parallelism, in both minimum latency and maximum throughput scenarios.</em><br><br><em>NVIDIA H200 HGX | Measured on internal TensorRT-LLM based on v0.14a | FP8 PTQ | 2048:128 | Minimum latency: Concurrency 1&nbsp; | Maximum throughput: maximum concurrency fit in memory</em></figcaption></figure>\n\n\n\n<p>In the table above, tensor parallelism is compared to pipeline parallelism with each across eight GPUs on Llama 3.1 405B, the largest and most capable open source LLM available today. In the minimum latency scenario, TP allows for more available GPU compute to generate each token, leading to 5.6x faster performance than pipeline parallelism. However, for maximum throughput, pipeline parallelism can improve maximum system throughput by 1.5x by reducing overhead and leveraging the additional bandwidth available with NVLink Switch.</p>\n\n\n\n<h2 id=\"pipeline_parallelism_delivers_12x_boost_on_mlperf_on_h100\"  class=\"wp-block-heading\">Pipeline parallelism delivers 1.2x boost on MLPerf on H100<a href=\"#pipeline_parallelism_delivers_12x_boost_on_mlperf_on_h100\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The TensorRT-LLM software improvements also benefit smaller models. When the recent pipeline parallelism improvements in TensorRT-LLM were applied to MLPerf Llama 2 70B scenario, throughput on an HGX H100 8-GPU system increased by 21% compared to our MLPerf Inference v4.1 results published in August.</p>\n\n\n\n<figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><tbody><tr><td colspan=\"2\" rowspan=\"2\"><em>MLPerf Inference&nbsp;</em><br><em>Output Tokens/second</em><br><em>(higher is better)</em></td><td colspan=\"2\">Parallelism</td></tr><tr><td>Tensor Parallelism</td><td>Pipeline Parallelism</td></tr><tr><td>Scenario</td><td>Llama 2 70B</td><td><em>24,525&nbsp;</em></td><td><em>29,741&nbsp;</em></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Llama 2 70B inference throughput (tokens/second) using tensor and pipeline</em>.<br><br><em>Results obtained for the available category of Closed Division, on OpenORCAdataset using NVIDIA H100 Tensor Core GPU, official numbers from 4.1-0043 submission used for Tensor Parallelism, Pipeline parallelism based on scripts provided in submission ID- 4.1-0043 and TensorRT-LLM version 0.12.0.</em><br><em>Result not verified by MLCommons Association. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See </em><a href=\"http://www.mlcommons.org/\" target=\"_blank\" rel=\"noreferrer noopener\"><em>www.mlcommons.org</em></a><em> for more information.</em></figcaption></figure>\n\n\n\n<p>Tensor and pipeline parallelism are both valuable techniques. Individually, they are suitable for different use cases, however, developers can combine them in various ways to optimize inference throughput within a given interactivity target. We will dive into how to find this balance in a future blog.</p>\n\n\n\n<h2 id=\"tensor_and_pipeline_parallelism_explained\"  class=\"wp-block-heading\">Tensor and pipeline parallelism explained<a href=\"#tensor_and_pipeline_parallelism_explained\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Tensor parallelism (TP) splits the execution of each model layer across multiple GPUs. Every calculation is distributed across available GPUs, and each GPU performs its own portion of the calculation. Then, every GPU broadcasts its individual results, known as partial sums, to every other GPU using an <a href=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html\" target=\"_blank\" rel=\"noreferrer noopener\">AllReduce</a> operation. This process generates substantial data traffic between the GPUs.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"853\" height=\"480\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/tensor-parallelism-on-dnn.gif\" alt=\"Tensor parallel execution splits the execution of each model layer across multiple GPUs, and then each GPU performs its own portion of the calculation, and results are broadcast to all other GPUs. \" class=\"wp-image-90041\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. Applying tensor parallelism to a neural network.&nbsp;</em></figcaption></figure></div>\n\n\n<p>Pipeline parallelism (PP) operates by splitting groups of model layers \u2013 or stages \u2013&nbsp; across available GPUs. A request will begin on one GPU and will continue execution across subsequent stages on subsequent GPUs. With PP, communication only occurs between adjacent stages, rather than between all GPUs like with TP execution. While communication is less frequent, very high-bandwidth communication between stages is critical to ensure that execution does not stall, degrading performance.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"853\" height=\"480\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-parallelism-on-dnn.gif\" alt=\"Pipeline parallelism (PP) operates by splitting groups of model layers \u2013 or stages \u2013  across available GPUs. A request will begin on one GPU and will continue execution across subsequent stages on subsequent GPUs.\" class=\"wp-image-90042\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Applying pipeline parallelism to a neural network.&nbsp;</em></figcaption></figure></div>\n\n\n<p>For minimum latency use cases, the communication traffic generated during tensor parallel execution does not saturate available interconnect bandwidth. This means that multiple GPUs can work in tandem to generate each token, increasing interactivity. Meanwhile, with pipeline parallel execution, a request can only utilize the GPU compute available within a given stage. This means that compute per token does not increase with additional GPUs with pipeline parallelism.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"522\" height=\"674\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-3.-GPU-to-GPU-bandwidth-with-and-without-NVSwitch.png\" alt=\"On the top, 8 GPUs are connected to each other with a centralized NVSwitch. \nDiagram shows 8 GPUs on the bottom, each with links going to every other GPU. \" class=\"wp-image-90043\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-3.-GPU-to-GPU-bandwidth-with-and-without-NVSwitch.png 522w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-3.-GPU-to-GPU-bandwidth-with-and-without-NVSwitch-232x300.png 232w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-3.-GPU-to-GPU-bandwidth-with-and-without-NVSwitch-89x115.png 89w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-3.-GPU-to-GPU-bandwidth-with-and-without-NVSwitch-70x90.png 70w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-3.-GPU-to-GPU-bandwidth-with-and-without-NVSwitch-362x467.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-3.-GPU-to-GPU-bandwidth-with-and-without-NVSwitch-85x110.png 85w\" sizes=\"(max-width: 522px) 100vw, 522px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. GPU-to-GPU bandwidth with and without NVSwitch.</em></figcaption></figure>\n\n\n\n<p>For scenarios where high throughput is required, the all-to-all communication pattern of TP can become a bottleneck, hindering performance. If link bandwidth is fixed regardless of the number of available connections, then in high-throughput use cases PP can improve throughput somewhat, as communication overhead is reduced, however, execution can still be link limited. With a high-bandwidth interconnect like NVLink with NVSwitch, communication overhead can be minimized, and throughput can scale well with additional GPUs.</p>\n\n\n\n<h2 id=\"nvlink_switch_helps_maximize_high-throughput_performance\"  class=\"wp-block-heading\">NVLink Switch helps maximize high-throughput performance<a href=\"#nvlink_switch_helps_maximize_high-throughput_performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Each NVIDIA Hopper architecture GPU incorporates 18 NVLinks with each providing 50 GB/s of bandwidth per direction, providing a total of 900 GB/s of NVLink bandwidth. Each HGX H100 8-GPU or H200 server features four NVLink Switches. During TP model execution across eight GPUs, each GPU communicates to every other GPU using seven, equal-bandwidth connections. This means that communication across any connection happens at 1/7th of NVLink bandwidth, or about 128 GB/s.&nbsp;</p>\n\n\n\n<p>PP execution, however, only requires connections to the previous and next stages. This means that communication can happen over two higher-bandwidth connections providing bandwidth of 450 GB/s each. This means that with NVLink and NVLink Switch, effective connection bandwidth between stages is 3.5x higher than would be possible without NVLink Switch. This allows PP to have significantly higher performance than TP in maximum throughput scenarios.&nbsp;</p>\n\n\n\n<p>Choosing parallelism is about finding the right balance between compute and capacity for the target scenario. NVLink Switch provides developers with the flexibility to select the optimal parallelism configuration leading to better performance than what is possible with either a single GPU, or across multiple GPUs with tensor parallelism alone.&nbsp;</p>\n\n\n\n<p>When considering production deployments \u2013 for which LLM service operators may seek to maximize throughput within a fixed latency constraint \u2013 the ability to combine both tensor parallelism and pipeline parallelism to achieve desired interactivity while maximizing server throughput for optimal cost is critical. TensorRT-LLM is capable of efficiently combining these techniques. In a future blog post, we will deep dive into picking latency thresholds and GPU configurations to maximize throughput under the desired threshold, and show how NVSwitch improves performance in these online scenarios.</p>\n\n\n\n<h2 id=\"the_nvidia_platform_is_advancing_at_the_speed_of_light\"  class=\"wp-block-heading\">The NVIDIA platform is advancing at the speed of light<a href=\"#the_nvidia_platform_is_advancing_at_the_speed_of_light\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The NVIDIA platform provides developers with a full technology stack to optimize generative AI inference performance. NVIDIA Hopper architecture GPUs \u2013 available from every major cloud and server maker \u2013 connected with the high-bandwidth, NVLink and NVLink Switch AI fabric, and running TensorRT-LLM software provide outstanding performance for the latest LLMs. And, through continuous optimization, we continue to increase performance, lower total cost of ownership, and enable the next wave of AI innovation.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The continued growth of LLMs capability, fueled by increasing parameter counts and support for longer contexts, has led to their usage in a wide variety of applications, each with diverse deployment requirements. For example, a chatbot supports a small number of users at very low latencies for good interactivity. Meanwhile, synthetic data generation requires high &hellip; <a href=\"https://developer.nvidia.com/blog/boosting-llama-3-1-405b-throughput-by-another-1-5x-on-nvidia-h200-tensor-core-gpus-and-nvlink-switch/\">Continued</a></p>\n", "protected": false}, "author": 1481, "featured_media": 90044, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1498824", "discourse_permalink": "https://forums.developer.nvidia.com/t/boosting-llama-3-1-405b-throughput-by-another-1-5x-on-nvidia-h200-tensor-core-gpus-and-nvlink-switch/309256", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1903], "tags": [296, 453, 4159, 3933, 2932], "coauthors": [2940, 2732, 3849, 4089], "class_list": ["post-90040", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "category-features", "tag-ai-inference-microservices", "tag-featured", "tag-inference-performance", "tag-llama", "tag-large-language-models"], "acf": {"post_industry": ["Cloud Services"], "post_products": ["HGX", "Hopper", "NVLink", "TensorRT-LLM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/HGX-H200-product-photo-close-up-copy-2.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nqg", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90040"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1481"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=90040"}], "version-history": [{"count": 13, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90040/revisions"}], "predecessor-version": [{"id": 90140, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/90040/revisions/90140"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90044"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=90040"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=90040"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=90040"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=90040"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87739, "date": "2024-10-08T12:20:54", "date_gmt": "2024-10-08T19:20:54", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87739"}, "modified": "2024-10-17T11:51:42", "modified_gmt": "2024-10-17T18:51:42", "slug": "mistral-nemo-minitron-8b-foundation-model-delivers-unparalleled-accuracy", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/mistral-nemo-minitron-8b-foundation-model-delivers-unparalleled-accuracy/", "title": {"rendered": "Mistral-NeMo-Minitron 8B Model Delivers Unparalleled Accuracy"}, "content": {"rendered": "\n<p><em>This post was originally published August 21, 2024 but has been revised with current data.</em></p>\n\n\n\n<p>Recently, NVIDIA and Mistral AI unveiled <a href=\"https://blogs.nvidia.com/blog/mistral-nvidia-ai-model/\">Mistral NeMo 12B</a>, a leading state-of-the-art large language model (LLM). Mistral NeMo 12B consistently outperforms similarly sized models on a wide range of <a href=\"https://mistral.ai/news/mistral-nemo/\">benchmarks</a>.&nbsp;</p>\n\n\n\n<p>We announced Mistral-NeMo-Minitron 8B, one of the most advanced open-access models in its size class. This model consistently delivers leading accuracy on nine popular benchmarks.&nbsp;The Mistral-NeMo-Minitron 8B base model was obtained by width-pruning the <a href=\"https://huggingface.co/mistralai/Mistral-Nemo-Base-2407\">Mistral NeMo 12B base model</a>, followed by a light retraining process using knowledge distillation. This is a successful recipe that NVIDIA originally proposed in the paper, <a href=\"https://arxiv.org/pdf/2407.14679\">Compact Language Models via Pruning and Knowledge Distillation</a>. It\u2019s been proven time and again with NVIDIA <a href=\"https://github.com/NVlabs/Minitron\">Minitron</a> 8B and 4B, and <a href=\"https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/\">Llama-3.1-Minitron</a> 4B models.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"425\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base-1024x425.png\" alt=\"Diagram shows work moving through models, teacher correction, pruning, distillation, synthetic data generation, and multi-stage alignment.\" class=\"wp-image-90066\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base-1024x425.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base-300x124.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base-625x259.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base-179x74.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base-768x318.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base-1536x637.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base-645x267.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base-500x207.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base-160x66.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base-362x150.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base-265x110.png 265w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/model-pruning-distillation-mistral-nemo-minitron-8b-base.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Model pruning and distillation for Mistral-NeMo-Minitron-8B-Base and -Instruct models</em></figcaption></figure></div>\n\n\n<p>In Figure 1, the Nemotron-4-340B-Instruct and -Reward models were used to generate synthetic data for the alignment.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table class=\"has-fixed-layout\"><tbody><tr><td colspan=\"2\"></td><td>MMLU&nbsp;5-shot</td><td>GMS8k 0-shot</td><td>GPQA 0-shot</td><td>HumanEval0-shot</td><td>MBPP 0-shot</td><td>IFEval</td><td>MTBench (GPT4-Turbo)</td><td>BFCL v2 Live</td></tr><tr><td colspan=\"2\">Mistral-NeMo-Minitron 8B Instruct</td><td><strong>70.4</strong></td><td><strong>87.1</strong></td><td><strong>31.5</strong></td><td>71.3</td><td>72.5</td><td><strong>84.4</strong></td><td><strong>7.86</strong></td><td><strong>67.6</strong></td></tr><tr><td colspan=\"2\">Llama-3.1-8B-Instruct</td><td>69.4</td><td>83.9</td><td>30.4</td><td><strong>72.6</strong></td><td><strong>72.8</strong></td><td>79.7</td><td>7.78</td><td>44.3</td></tr><tr><td colspan=\"2\">Mistral-NeMo-12B-Instruct</td><td>68.4</td><td>79.8</td><td>28.6</td><td>68.3</td><td>66.7</td><td>64.7</td><td>8.10</td><td>47.9</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Accuracy of the Mistral-NeMo-Minitron-8B-Instruct model compared to Llama-3.1-8B-Instruct and the teacher Mistral-NeMo-12B models. Bold numbers represent the best amongst the 8B model class.</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td colspan=\"2\"></td><td class=\"has-text-align-center\" data-align=\"center\">Training tokens</td><td class=\"has-text-align-center\" data-align=\"center\">Wino-Grande 5-shot</td><td class=\"has-text-align-center\" data-align=\"center\">ARC<br>Challenge 25-shot</td><td class=\"has-text-align-center\" data-align=\"center\">MMLU 5-shot</td><td class=\"has-text-align-center\" data-align=\"center\">Hella<br>Swag 10-shot</td><td class=\"has-text-align-center\" data-align=\"center\">GSM8K 5-shot</td><td class=\"has-text-align-center\" data-align=\"center\">TruthfulQA 0-shot</td><td class=\"has-text-align-center\" data-align=\"center\">XLSum en (20%)<br>3-shot</td><td class=\"has-text-align-center\" data-align=\"center\">MBPP<br>0-shot</td><td class=\"has-text-align-center\" data-align=\"center\">Human<br>Eval<br>0-shot</td></tr><tr><td colspan=\"2\">Llama-3.1-8B</td><td class=\"has-text-align-center\" data-align=\"center\">15T</td><td class=\"has-text-align-center\" data-align=\"center\">77.27</td><td class=\"has-text-align-center\" data-align=\"center\">57.94</td><td class=\"has-text-align-center\" data-align=\"center\">65.28</td><td class=\"has-text-align-center\" data-align=\"center\">81.80</td><td class=\"has-text-align-center\" data-align=\"center\">48.60</td><td class=\"has-text-align-center\" data-align=\"center\">45.06</td><td class=\"has-text-align-center\" data-align=\"center\">30.05</td><td class=\"has-text-align-center\" data-align=\"center\">42.27</td><td class=\"has-text-align-center\" data-align=\"center\">24.76</td></tr><tr><td colspan=\"2\">Gemma-7B</td><td class=\"has-text-align-center\" data-align=\"center\">6T</td><td class=\"has-text-align-center\" data-align=\"center\">78</td><td class=\"has-text-align-center\" data-align=\"center\">61</td><td class=\"has-text-align-center\" data-align=\"center\">64</td><td class=\"has-text-align-center\" data-align=\"center\">82</td><td class=\"has-text-align-center\" data-align=\"center\">50</td><td class=\"has-text-align-center\" data-align=\"center\">45</td><td class=\"has-text-align-center\" data-align=\"center\">17</td><td class=\"has-text-align-center\" data-align=\"center\">39</td><td class=\"has-text-align-center\" data-align=\"center\">32</td></tr><tr><td colspan=\"2\">Mistral-NeMo-Minitron-8B</td><td class=\"has-text-align-center\" data-align=\"center\">380B</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>80.35</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>64.42</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>69.51</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>83.03</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>58.45</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>47.56</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>31.94</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>43.77</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>36.22</strong></td></tr><tr><td colspan=\"2\">Mistral-NeMo-12B</td><td class=\"has-text-align-center\" data-align=\"center\">N/A</td><td class=\"has-text-align-center\" data-align=\"center\">82.24</td><td class=\"has-text-align-center\" data-align=\"center\">65.10</td><td class=\"has-text-align-center\" data-align=\"center\">68.99</td><td class=\"has-text-align-center\" data-align=\"center\">85.16</td><td class=\"has-text-align-center\" data-align=\"center\">56.41</td><td class=\"has-text-align-center\" data-align=\"center\">49.79</td><td class=\"has-text-align-center\" data-align=\"center\">33.43</td><td class=\"has-text-align-center\" data-align=\"center\">42.63</td><td class=\"has-text-align-center\" data-align=\"center\">23.78</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 2. Accuracy of the Mistral-NeMo-Minitron-8B-Base model compared to Llama-3.1-8B-Base and the teacher Mistral-NeMo-12B models. Bold numbers represent the best amongst the 8B model class.</em></em></figcaption></figure>\n\n\n\n<h2 id=\"overview_of_model_pruning_and_distillation&nbsp;\"  class=\"wp-block-heading\">Overview of model pruning and distillation&nbsp;<a href=\"#overview_of_model_pruning_and_distillation&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><em>Model pruning</em> is the process of making a model smaller and leaner, either by dropping layers (<em>depth pruning</em>) or dropping neurons and attention heads and embedding channels (<em>width pruning</em>). Pruning is often accompanied by some amount of retraining for accuracy recovery.</p>\n\n\n\n<p><em>Model distillation</em> is a technique used to transfer knowledge from a large, complex model, often called the <em>teacher model</em>, to a smaller, simpler <em>student model</em>. The goal is to create a more efficient model that retains much of the predictive power of the original, larger model while being faster and less resource-intensive to run. Herein, we employ distillation as a light retraining procedure after pruning, on a dataset much smaller than that used in model training from scratch.</p>\n\n\n\n<p><em>Iterative pruning and distillation </em>is an approach where, starting from a single pretrained model, multiple progressively smaller models can be obtained. For example, a 15B model can be pruned and distilled to obtain an 8B model, which in turn serves as a starting point for pruning and distilling a 4B model, and so on.&nbsp;</p>\n\n\n\n<p>The combination of model pruning followed by light retraining through distillation is an effective and cost-efficient approach to train a family of models. For each additional model, just 100-400B tokens are used for retraining\u2014a greater than 40x reduction compared to training from scratch. As such, the compute cost savings to train a family of models (12B, 8B, and 4B) is up to 1.95x compared to training all models from scratch.&nbsp;</p>\n\n\n\n<p>The learning from extensive ablation studies has been summarized into 10 best practices for <a href=\"https://arxiv.org/pdf/2407.14679\">structured weight pruning combined with knowledge distillation</a>. We found that width pruning consistently outperforms depth pruning and, most importantly, pruned and distilled models outperform models trained from scratch in quality.&nbsp;</p>\n\n\n\n<h2 id=\"mistral-nemo-minitron_8b\"  class=\"wp-block-heading\">Mistral-NeMo-Minitron 8B<a href=\"#mistral-nemo-minitron_8b\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Following our best practices, we width-pruned the Mistral NeMo 12B model to obtain an 8B target model. This section details the steps and parameters used to obtain the Mistral-NeMo-Minitron 8B base model, as well as its performance.</p>\n\n\n\n<h3 id=\"teacher_fine-tuning\"  class=\"wp-block-heading\">Teacher fine-tuning<a href=\"#teacher_fine-tuning\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To correct for the distribution shift across the original dataset the model was trained on, we first fine-tuned the unpruned Mistral NeMo 12B model on our dataset using 127B tokens. Experiments showed that, without correcting for the distribution shift, the teacher provides suboptimal guidance on the dataset when being distilled.</p>\n\n\n\n<h3 id=\"width-only_pruning\"  class=\"wp-block-heading\">Width-only pruning<a href=\"#width-only_pruning\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Given our goal of obtaining the strongest 8B model possible, we proceeded with width-only pruning. We pruned both the embedding (hidden) and MLP intermediate dimensions along the width axis to compress Mistral NeMo 12B. Specifically, we computed importance scores for each attention head, embedding channel, and MLP hidden dimension using the activation-based strategy. Following importance estimation, we:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Pruned the MLP intermediate dimension from 14336 to 11520</li>\n\n\n\n<li>Pruned the hidden size from 5120 to 4096</li>\n\n\n\n<li>Retained the attention headcount and number of layers</li>\n</ul>\n\n\n\n<h3 id=\"distillation_parameters\"  class=\"wp-block-heading\">Distillation parameters<a href=\"#distillation_parameters\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>We distilled the model with peak learning rate=1e-4, minimum learning rate=4.5e-7, linear warm up of 60 steps, cosine decay schedule, and a global batch size of 768 using 380B tokens (the same dataset used in teacher fine-tuning).</p>\n\n\n\n<h2 id=\"mistral-nemo-minitron-8b-instruct\"  class=\"wp-block-heading\">Mistral-NeMo-Minitron-8B-Instruct<a href=\"#mistral-nemo-minitron-8b-instruct\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>We applied an advanced alignment technique consisting of two-stage instruction finetuning and two-stage preference optimization, resulting in a state-of-the-art instruct model with excellent performance in instruction following, language reasoning, function calling, and safety benchmarks. </p>\n\n\n\n<p>The alignment data was synthetically generated using the <a href=\"https://build.nvidia.com/nvidia/nemotron-4-340b-instruct\">Nemotron-340B-Instruct</a> model in conjunction with the <a href=\"https://build.nvidia.com/nvidia/nemotron-4-340b-reward\">Nemotron-340B-Reward</a> model. The model alignment was done with <a href=\"https://github.com/NVIDIA/NeMo-Aligner\">NVIDIA NeMo Aligner</a>.</p>\n\n\n\n<h3 id=\"performance_benchmarks\"  class=\"wp-block-heading\">Performance benchmarks<a href=\"#performance_benchmarks\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>We optimized the Mistral-NeMo-Minitron-8B-Base model, the teacher Mistral-NeMo-12B model, and the LLama-3.1-8B model with <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">NVIDIA TensorRT-LLM</a>, an open-source toolkit for optimized LLM inference.&nbsp;</p>\n\n\n\n<p>Figures 2 and 3 show the throughput requests per second of different models in FP8 and BF16 precision on different use cases, represented as input sequence length/output sequence length (ISL/OSL) combinations at batch size 32 on one <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100</a> 80-GB GPU.&nbsp;</p>\n\n\n\n<p>The Llama-3.1-8B model is the fastest, at an average of ~1.4x throughput of Mistral-NeMo-12B, followed by Mistral-NeMo-Minitron-8B-Base at a 1.2x improvement over Mistral-NeMo-12B. This is primarily because the Llama-3.1-8B model has 32 layers compared to Mistral-NeMo-12B with 40 layers.&nbsp;</p>\n\n\n\n<p>Deployment in FP8 also delivers a performance boost of ~1.4x across all three models compared to BF16.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"633\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-bf16-throughput-1024x633.png\" alt=\"Bar chart shows theMistral-NeMo-Minitron-8B shows up to a 25% throughput improvement over the original Mistral-NeMo-12B in BF16.\" class=\"wp-image-89872\" title=\"Chart\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-bf16-throughput-1024x633.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-bf16-throughput-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-bf16-throughput-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-bf16-throughput-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-bf16-throughput-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-bf16-throughput-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-bf16-throughput-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-bf16-throughput-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-bf16-throughput-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-bf16-throughput-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-bf16-throughput.png 1200w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Performance benchmarks for request BF16 throughput at different I/O length combinations</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"633\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-fp8-throughput-1024x633.png\" alt=\"Bar chart shows that the Mistral-NeMo-Minitron-8B shows more than 20% throughput improvements over the original Mistral-NeMo-12B in FP8.\" class=\"wp-image-89873\" title=\"Chart\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-fp8-throughput-1024x633.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-fp8-throughput-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-fp8-throughput-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-fp8-throughput-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-fp8-throughput-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-fp8-throughput-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-fp8-throughput-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-fp8-throughput-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-fp8-throughput-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-fp8-throughput-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/performance-request-fp8-throughput.png 1200w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Performance benchmarks for request FP8 throughput at different I/O length combinations</em></figcaption></figure></div>\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Mistral-NeMo-Minitron-8B provides class-leading accuracy and consistently outperforms recently introduced state-of-the-art models of similar size. Mistral-NeMo-Minitron-8B is our first work on the distillation of the Mistral-NeMo-12B model and provides strong support for our <a href=\"https://arxiv.org/pdf/2407.14679\">structured weight pruning combined with knowledge distillation</a> best practices. </p>\n\n\n\n<p>Mistral-NeMo-Minitron-8B-Instruct also demonstrated our state-of-the-art alignment training recipe. Further work distilling, aligning, and obtaining even smaller and more accurate models is planned. Implementation support for <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/model-optimization/pruning/depth-pruning.html\">depth pruning</a> and <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/model-optimization/distillation/distillation.html\">distillation</a> is available in the <a href=\"https://github.com/NVIDIA/NeMo\">NVIDIA NeMo</a> framework for generative AI training. Example usage is provided as a <a href=\"https://github.com/NVIDIA/NeMo/tree/main/tutorials/llm/llama-3/pruning-distillation\">notebook</a>.</p>\n\n\n\n<p>For more information, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://research.nvidia.com/publication/_llm-pruning-and-distillation-practice-minitron-approach\">LLM Pruning and Distillation in Practice: The Minitron Approach</a></li>\n\n\n\n<li><a href=\"https://arxiv.org/abs/2407.14679\">Compact Language Models via Pruning and Knowledge Distillation</a>&nbsp;</li>\n\n\n\n<li><a href=\"https://github.com/NVlabs/Minitron\">/NVlabs/Minitron</a> GitHub repo</li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/NeMo-Aligner\">/NVIDIA/NeMo-Aligner</a> Github repo&nbsp;</li>\n\n\n\n<li><a href=\"https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base\">Mistral-NeMo-Minitron-8B-Base model</a> on Hugging Face</li>\n\n\n\n<li><a href=\"https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Instruct\">Mistral-NeMo-Minitron 8B-Instruct model</a> on Hugging Face</li>\n\n\n\n<li><a href=\"https://build.nvidia.com/nvidia/mistral-nemo-minitron-8b-8k-instruct\">Mistral-NeMo-Minitron-8B-Instruct model</a> on the NVIDIA API Catalog</li>\n</ul>\n\n\n\n<h3 id=\"acknowledgments\"  class=\"wp-block-heading\">Acknowledgments<a href=\"#acknowledgments\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><em>This work would not have been possible without contributions from many people at NVIDIA. To mention a few of them:</em></p>\n\n\n\n<p><strong><em>Foundation model</em></strong><em>: Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Sanjeev Satheesh, Jupinder Parmar, Pavlo Molchanov, Mostofa Patwary, Daniel Korzekwa, Ashwath Aithal, Mohammad Shoeybi, Bryan Catanzaro, and Jan Kautz</em></p>\n\n\n\n<p><strong><em>Alignment</em></strong><em>: Gerald Shen, Jiaqi Zeng, Ameya Sunil Mahabaleshwarkar, Zijia Chen, Hayley Ross, Brandon Rowlett, Oluwatobi Olabiyi, Shizhe Diao, Yoshi Suhara, Shengyang Sun, Zhilin Wang, Yi Dong, Zihan Liu, Rajarshi Roy, Wei Ping, Makesh Narsimhan Sreedhar, Shaona Ghosh, Somshubra Majumdar, Vahid Noroozi, Aleksander Ficek, Siddhartha Jain, Wasi Uddin Ahmad, Jocelyn Huang, Sean Narenthiran, Igor Gitman, Shubham Toshniwal, Ivan Moshkov, Evelina Bakhturina, Matvei Novikov, Fei Jia, Boris Ginsburg, and Oleksii Kuchaiev</em></p>\n\n\n\n<p><strong><em>TensorRT-LLM</em></strong><em>: Bobby Chen, James Shen, and Chenhan Yu</em></p>\n\n\n\n<p><strong><em>Hugging Face support</em></strong><em>: Ao Tang, Yoshi Suhara, and Greg Heinrich</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>This post was originally published August 21, 2024 but has been revised with current data. Recently, NVIDIA and Mistral AI unveiled Mistral NeMo 12B, a leading state-of-the-art large language model (LLM). Mistral NeMo 12B consistently outperforms similarly sized models on a wide range of benchmarks.&nbsp; We announced Mistral-NeMo-Minitron 8B, one of the most advanced open-access &hellip; <a href=\"https://developer.nvidia.com/blog/mistral-nemo-minitron-8b-foundation-model-delivers-unparalleled-accuracy/\">Continued</a></p>\n", "protected": false}, "author": 657, "featured_media": 87745, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1471768", "discourse_permalink": "https://forums.developer.nvidia.com/t/mistral-nemo-minitron-8b-foundation-model-delivers-unparalleled-accuracy/304116", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 1903], "tags": [3693, 453, 2779, 3650, 2932, 3673], "coauthors": [1014, 3982, 3981, 3983, 1170, 4090, 4091], "class_list": ["post-87739", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "category-features", "tag-ai-foundation-models", "tag-featured", "tag-h100", "tag-llm-techniques", "tag-large-language-models", "tag-tensorrtllm"], "acf": {"post_industry": ["Academia / Education"], "post_products": ["AI Foundation Models", "H100", "NeMo", "TensorRT-LLM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Benchmark", "News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/community-ai-model-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mP9", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87739"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/657"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87739"}], "version-history": [{"count": 41, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87739/revisions"}], "predecessor-version": [{"id": 90085, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87739/revisions/90085"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87745"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87739"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87739"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87739"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87739"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89903, "date": "2024-10-08T11:30:00", "date_gmt": "2024-10-08T18:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89903"}, "modified": "2024-10-17T11:19:08", "modified_gmt": "2024-10-17T18:19:08", "slug": "just-released-nvidia-modulus-v24-09", "status": "publish", "type": "post", "link": "https://docs.nvidia.com/deeplearning/modulus/release-notes/index.html", "title": {"rendered": "Just Released: NVIDIA Modulus v24.09"}, "content": {"rendered": "\n<p>NVIDIA Modulus v24.09 delivers utilities to physics-inform training and validation of any model, plus other enhancements.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA Modulus v24.09 delivers utilities to physics-inform training and validation of any model, plus other enhancements.</p>\n", "protected": false}, "author": 1392, "featured_media": 89905, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1497509", "discourse_permalink": "https://forums.developer.nvidia.com/t/just-released-nvidia-modulus-v24-09/309154", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://docs.nvidia.com/deeplearning/modulus/release-notes/index.html", "_links_to_target": "_blank"}, "categories": [503], "tags": [1913, 453, 2216], "coauthors": [2802], "class_list": ["post-89903", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-simulation-modeling-design", "tag-climate-weather-ocean-modeling", "tag-featured", "tag-modulus"], "acf": {"post_industry": ["Energy", "HPC / Scientific Computing"], "post_products": ["Modulus"], "post_learning_levels": ["General Interest"], "post_content_types": ["Announcement"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/windmills-water.png", "jetpack_shortlink": "https://wp.me/pcCQAL-no3", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89903"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1392"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89903"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89903/revisions"}], "predecessor-version": [{"id": 89914, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89903/revisions/89914"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89905"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89903"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89903"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89903"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89903"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89963, "date": "2024-10-08T08:00:00", "date_gmt": "2024-10-08T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89963"}, "modified": "2024-10-17T11:19:09", "modified_gmt": "2024-10-17T18:19:09", "slug": "nvidia-cuda-x-now-accelerates-the-polars-data-processing-library", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-cuda-x-now-accelerates-the-polars-data-processing-library/", "title": {"rendered": "NVIDIA CUDA-X Now Accelerates the Polars Data Processing Library"}, "content": {"rendered": "\n<p><a href=\"https://pola.rs/\">Polars</a>, one of the fastest-growing data analytics tools, has just crossed 9M monthly downloads. As a modern DataFrame library, it is designed for efficiently processing datasets that fit on a single machine, without the overhead and complexity of distributed computing systems that are required for massive-scale workloads.</p>\n\n\n\n<p>As enterprises grapple with complex data problems\u2014ranging from detecting time-boxed patterns in credit card transactions to managing quickly shifting inventory needs across a global customer base\u2014even higher performance is essential.</p>\n\n\n\n<p>Polars and NVIDIA engineers just released the <a href=\"https://developer.nvidia.com/blog/polars-gpu-engine-powered-by-rapids-cudf-now-available-in-open-beta/\">Polars GPU engine</a> powered by <a href=\"https://developer.nvidia.com/rapids\">RAPIDS cuDF</a> in open beta, bringing accelerated computing to the growing Polars community with zero code change required. This brings even more acceleration to the query execution for Polars, making this speedy data processing software up to 13x faster compared to running on CPUs. It\u2019s like giving rocket fuel to a cheetah to help it sprint even faster.</p>\n\n\n\n<p>\u201cThe collaboration with NVIDIA is a unique opportunity that offers the power of NVIDIA RAPIDS and GPUs to everyone looking to get even more performance from Polars,\u201d said Ritchie Vink, author and CEO of Polars.</p>\n\n\n\n<p>RAPIDS, part of <a href=\"https://www.nvidia.com/en-us/technologies/cuda-x/\">NVIDIA CUDA-X</a>, is an open-source suite of GPU-accelerated libraries designed to improve data science and analytics pipelines. RAPIDS cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and manipulating data.</p>\n\n\n\n<h2 id=\"nvidia_software_accelerates_data_processing_at_every_scale\"  class=\"wp-block-heading\">NVIDIA software accelerates data processing at every scale<a href=\"#nvidia_software_accelerates_data_processing_at_every_scale\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With data science and engineering teams building more and more data processing pipelines to fuel AI applications, it\u2019s critical to choose the right software and infrastructure for the job to keep things running smoothly.</p>\n\n\n\n<p>For workloads well-suited to individual servers, workstations, and laptops, developers frequently use libraries like Polars to accelerate iterations, reduce complexity in development environments, and lower infrastructure costs.&nbsp;</p>\n\n\n\n<p>On these single-machine-sized workloads, quick iteration time is often the top priority, as data scientists often must do exploratory analysis to guide downstream model training or decision-making. Performance bottlenecks from CPU-only computing reduce productivity and can limit the number of test/train cycles that can be completed.&nbsp;&nbsp;</p>\n\n\n\n<p>For large-scale data processing workloads too large for a single machine, organizations turn to frameworks like <a href=\"https://spark.apache.org/\">Apache Spark</a> to help them distribute the work across nodes in the data center. At this scale, cost and power efficiency are often the top priorities, but costs can quickly balloon due to the inefficiencies of using traditional CPU-based computing infrastructure.</p>\n\n\n\n<p>The NVIDIA <a href=\"https://www.nvidia.com/en-us/technologies/cuda-x/\">CUDA-X data processing</a> platform is designed with these needs in mind, optimized for <a href=\"https://blogs.nvidia.com/blog/spark-rapids-energy-efficiency/\">cost- and energy-efficiency for large-scale workloads</a> and <a href=\"https://developer.nvidia.com/blog/rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes/\">performance for single-machine-sized workloads</a>.</p>\n\n\n\n<p>Medium-scale workloads where productivity and performance are critical can see performance gains on both <a href=\"https://github.com/pola-rs/polars\">Polars</a>, as well as 50x faster performance on the <a href=\"https://pandas.pydata.org/\">pandas</a> library using NVIDIA GPU-enabled systems instead of CPUs, based on industry-standard benchmarks.&nbsp;</p>\n\n\n\n<p>With the RAPIDS Accelerator for Apache Spark, workflows where cost and energy efficiency are critical can see <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s62257/\">cost savings of up to 80%</a> and <a href=\"https://developer.nvidia.com/blog/nvidia-gh200-superchip-delivers-breakthrough-energy-efficiency-and-node-consolidation-for-apache-spark/\">energy savings of up to 12x</a>.</p>\n\n\n\n<h2 id=\"get_started_today\"  class=\"wp-block-heading\">Get started today<a href=\"#get_started_today\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The world is creating more data than ever before and accelerated computing makes it possible to operationalize it efficiently. Whether you\u2019re running on a workstation or scaling out in the data center, NVIDIA-accelerated data processing software can improve productivity and reduce costs.</p>\n\n\n\n<p>For more information about how you can accelerate your data analytics workflows with zero code change, see the <a href=\"https://developer.nvidia.com/rapids\">NVIDIA RAPIDS page</a><strong>.</strong></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Polars, one of the fastest-growing data analytics tools, has just crossed 9M monthly downloads. As a modern DataFrame library, it is designed for efficiently processing datasets that fit on a single machine, without the overhead and complexity of distributed computing systems that are required for massive-scale workloads. As enterprises grapple with complex data problems\u2014ranging from &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-cuda-x-now-accelerates-the-polars-data-processing-library/\">Continued</a></p>\n", "protected": false}, "author": 988, "featured_media": 89966, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1496664", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-cuda-x-now-accelerates-the-polars-data-processing-library/309006", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [453, 4085], "coauthors": [1793], "class_list": ["post-89963", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "tag-featured", "tag-polars"], "acf": {"post_industry": ["HPC / Scientific Computing"], "post_products": ["cuDF", "RAPIDS"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/inference-data-analytics-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-np1", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89963"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/988"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89963"}], "version-history": [{"count": 1, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89963/revisions"}], "predecessor-version": [{"id": 89965, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89963/revisions/89965"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89966"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89963"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89963"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89963"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89963"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89885, "date": "2024-10-08T08:00:00", "date_gmt": "2024-10-08T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89885"}, "modified": "2024-10-17T11:19:09", "modified_gmt": "2024-10-17T18:19:09", "slug": "accelerate-large-linear-programming-problems-with-nvidia-cuopt", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerate-large-linear-programming-problems-with-nvidia-cuopt/", "title": {"rendered": "Accelerate Large Linear Programming Problems with NVIDIA cuOpt"}, "content": {"rendered": "\n<p>The evolution of linear programming (LP) solvers has been marked by significant milestones over the past century, from <a href=\"https://en.wikipedia.org/wiki/Simplex_algorithm\">Simplex</a> to the <a href=\"https://en.wikipedia.org/wiki/Interior-point_method\">interior point method (IPM)</a>. The introduction of <a href=\"https://arxiv.org/pdf/2106.04756\">primal-dual linear programming (PDLP)</a> has brought another significant advancement.&nbsp;</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai-data-science/products/cuopt/\">NVIDIA cuOpt</a> has now implemented PDLP with GPU acceleration. Using cutting-edge algorithms, NVIDIA hardware, dedicated CUDA features, and NVIDIA GPU libraries, the cuOpt LP solver achieves over 5,000x faster performance compared to CPU-based solvers.&nbsp;</p>\n\n\n\n<p>This post examines the key components of LP solver algorithms, GPU acceleration in LP, and cuOpt performance on <a href=\"https://plato.asu.edu/bench.html\">Mittelmann&#8217;s benchmark</a> and <a href=\"https://en.wikipedia.org/wiki/Minimum-cost_flow_problem\">Min Cost Flow problem</a> instances.</p>\n\n\n\n<h2 id=\"harnessing_cutting-edge_innovations_for_large-scale_lp\"  class=\"wp-block-heading\">Harnessing cutting-edge innovations for large-scale LP<a href=\"#harnessing_cutting-edge_innovations_for_large-scale_lp\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>LP is a method that involves optimizing a linear objective function, subject to a set of linear constraints.&nbsp;</p>\n\n\n\n<p>Consider this scenario: A farmer must decide which vegetables to grow and in what quantities to maximize profit, given limitations on land, seeds, and fertilizer. The goal is to determine the optimal revenue while respecting all constraints, as quickly as possible.</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/blog/building-an-ai-agent-for-supply-chain-optimization-with-nvidia-nim-and-cuopt/\">NVIDIA developed an LLM agent</a> example that helps model the problem and solve it using an LP solver. LP is an essential tool for optimization and has applications in resource allocation, production planning, supply chain, and, as a backbone for <a href=\"https://en.wikipedia.org/wiki/Integer_programming\">mixed-integer programming</a> (MIP) solvers. Solving mathematical problems with millions of variables and constraints in seconds is challenging, if not impossible, in some cases.&nbsp;</p>\n\n\n\n<p>There are three requirements to solve LP problems efficiently on GPUs:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Efficient and massively parallel algorithms</li>\n\n\n\n<li>NVIDIA GPU libraries and CUDA features</li>\n\n\n\n<li>Cutting-edge NVIDIA GPUs</li>\n</ul>\n\n\n\n<h3 id=\"efficient_and_massively_parallel_algorithms\"  class=\"wp-block-heading\">Efficient and massively parallel algorithms<a href=\"#efficient_and_massively_parallel_algorithms\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://en.wikipedia.org/wiki/Simplex_algorithm\">Simplex</a>, introduced by Dantzig in 1947, remains a core component of most LP and MIP solvers. It works by following the edges of the feasible region to find the optimum.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"534\" height=\"405\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/simplex-method.gif\" alt=\"Visual animation shows how the Simplex method works.\" class=\"wp-image-90026\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. Simplex method </em><br><em>(Source: </em><a href=\"https://www.youtube.com/watch?v=C0TTxV0n9OA\">Visually Explained &#8211; What is Linear Programming (LP)?</a><em>)</em></figcaption></figure></div>\n\n\n<p>The next major advancement came with the <a href=\"https://en.wikipedia.org/wiki/Interior-point_method\">interior point method (IPM)</a>, discovered by I. I. Dikin in 1967. IPM, which moves through the interior of the polytope towards the optimum, is now considered state-of-the-art for solving large-scale LPs on CPUs. However, both techniques face limitations in massive parallelization.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"568\" height=\"377\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/interior-point-method.gif\" alt=\"Visual animation shows how the IPM method works.\" class=\"wp-image-90025\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Interior Point Method </em><br><em>(Source: </em><a href=\"https://www.youtube.com/watch?v=C0TTxV0n9OA\">Visually Explained &#8211; What is Linear Programming (LP)?</a><em>)</em></figcaption></figure></div>\n\n\n<p>In 2021, a new groundbreaking technique to solve large LPs was introduced by the Google Research team: <a href=\"https://arxiv.org/pdf/2106.04756\">PDLP</a>. It is a first-order method (FOM) that uses the derivative of the problem to iteratively optimize the objective and minimize constraint violation.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"640\" height=\"480\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gradient-descent.gif\" alt=\"Visual animation shows how the gradient descent works.\" class=\"wp-image-90024\"/><figcaption class=\"wp-element-caption\"><em>Figure 3. Gradient descent</em></figcaption></figure></div>\n\n\n<p>PDLP enhances <a href=\"https://hal.science/hal-00490826/file/pd_alg_final.pdf\">primal-dual hybrid gradient (PDHG)</a> algorithm by introducing tools to improve convergence, including a presolver, diagonal preconditioning, adaptive restarting, and dynamic primal-dual step size selection. Presolving and preconditioning make the input problem simpler and improves numerical stability while restarting and dynamic step size computation enables the solver to adapt itself during optimization.</p>\n\n\n\n<p>A key advantage of FOM over previous methods is its ease of massive parallelization, making it well-suited for GPU implementation.</p>\n\n\n\n<p>PDLP employs two highly parallelizable computational patterns: Map operations and sparse matrix-vector multiplications (SpMV). This approach enables PDLP to efficiently handle millions of variables and constraints in parallel, making it extremely effective on GPUs.</p>\n\n\n\n<p>Map is extensively used in PDLP to perform additions, subtractions, and so on for all the variables and constraints that can span millions of elements. It is extremely parallel and efficient on GPUs.</p>\n\n\n\n<p>SpMV corresponds to multiplying a sparse matrix (containing many zeros) and a vector. While this matrix size can reach tens of billions, it contains far fewer useful values.&nbsp;For instance, in a vegetable planting problem, a constraint such as, &#8220;I can&#8217;t plant more than 3.5 kg of potatoes&#8221; would contain only one useful value among millions of variables.&nbsp;</p>\n\n\n\n<p>SpMV algorithms have been extensively optimized for GPUs, making them orders of magnitude faster than CPU implementations.</p>\n\n\n\n<h3 id=\"nvidia_gpu_libraries_and_cuda_features\"  class=\"wp-block-heading\">NVIDIA GPU libraries and CUDA features<a href=\"#nvidia_gpu_libraries_and_cuda_features\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To have the best performance, our GPU PDLP implementation uses cutting-edge CUDA features and the following NVIDIA libraries:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>cuSparse</li>\n\n\n\n<li>Thrust</li>\n\n\n\n<li>RMM</li>\n</ul>\n\n\n\n<p><a href=\"https://docs.nvidia.com/cuda/cusparse/\">cuSparse</a> is the NVIDIA GPU-accelerated library for sparse linear algebra. It efficiently performs SpMVs, a challenging task on GPUs. cuSparse employs unique algorithms designed to fully leverage the NVIDIA massively parallel architecture.</p>\n\n\n\n<p>Thrust is part of the <a href=\"https://nvidia.github.io/cccl/cpp.html\">NVIDIA CUDA Core Compute Libraries</a> (CCCL) and provides high-level C++ parallel algorithms. It simplifies the expression of complex algorithms using patterns and iterators for GPU execution. I used Thrust for map operations and the restart process, which entails sorting values by key. This is a task that can be demanding on the GPU but is efficiently optimized by Thrust.</p>\n\n\n\n<p><a href=\"https://github.com/rapidsai/rmm\">RMM</a> is the fast and flexible NVIDIA memory management system that enables the safe and efficient handling of GPU memory through the use of a memory pool.</p>\n\n\n\n<p>Finally, I took advantage of advanced CUDA features. One of the most significant challenges in parallelizing PDLP on GPUs is the restart procedure, which is inherently iterative and not suited for parallel execution. To address this, I used <a href=\"https://developer.nvidia.com/blog/cooperative-groups/\">CUDA Cooperative Groups</a>, which enable you to define GPU algorithms at various levels, with the largest being the grid that encompasses all workers. By implementing a cooperative kernel launch and using grid synchronization, you&nbsp;can efficiently and elegantly express the iterative restart procedure on the GPU.</p>\n\n\n\n<h3 id=\"cutting-edge_nvidia_gpus&nbsp;\"  class=\"wp-block-heading\">Cutting-edge NVIDIA GPUs&nbsp;<a href=\"#cutting-edge_nvidia_gpus&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>GPUs achieve fast computation by using thousands of threads to solve many problems in parallel. However, before processing, the GPU must first transfer the data from the main memory to its worker threads.&nbsp;</p>\n\n\n\n<p><em>Memory bandwidth</em> refers to the amount of data that can be transferred per second. While CPUs can usually handle hundreds of GB/s, the latest GPU, <a href=\"https://www.nvidia.com/en-us/data-center/hgx/\">NVIDIA HGX B100</a>, has a bandwidth of eight TB/s, two orders of magnitude larger.</p>\n\n\n\n<p>The performance of this PDLP implementation scales directly with increased memory bandwidth due to its heavy reliance on memory-intensive computational patterns like Map and SpMV. With future NVIDIA GPU bandwidth increases, PDLP will automatically become faster, unlike other CPU-based LP solvers.</p>\n\n\n\n<h2 id=\"cuopt_outperforms_state-of-the-art_cpu_lp_solvers_on_mittelmann\u2019s_benchmark\"  class=\"wp-block-heading\">cuOpt outperforms state-of-the-art CPU LP solvers on Mittelmann\u2019s benchmark<a href=\"#cuopt_outperforms_state-of-the-art_cpu_lp_solvers_on_mittelmann\u2019s_benchmark\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The industry standard to benchmark the speed of LP solvers is <a href=\"https://plato.asu.edu/ftp/lpfeas.html\">Mittelmann\u2019s benchmark</a>. The objective is to determine the optimal value of the LP function while adhering to the constraints in the shortest time possible. The benchmark problems represent various scenarios and contain between hundreds of thousands to tens of millions of values.</p>\n\n\n\n<p>For the comparison, I ran a state-of-the-art CPU LP solver and compared it to this GPU LP solver. I used the same threshold of 10<sup>-4</sup> and disabled crossover. For more information, see the <a href=\"#potential-for-pdlp-refinement\">Potential for PDLP refinement</a> section later in this post.&nbsp;</p>\n\n\n\n<p>Both solvers operated under <code>float64</code> precision.&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>For the CPU LP solver, I used a recommended CPU setup: AMD EPYC 7313P servers with 16 cores and 256 GB of DDR4 memory.</li>\n\n\n\n<li>For the cuOpt LP solver, I used an NVIDIA H100 SXM Tensor Core GPU to benefit from the high bandwidth and ran without presolve.&nbsp;</li>\n</ul>\n\n\n\n<p>I considered the full solve time without I/O, including scaling for both solvers and presolving for the CPU LP solver. Only instances that have converged for both solvers with a correct objective value are showcased in Figure 4. cuOpt is faster on 60% of the instances and more than 10x faster in 20% of the instances. The biggest speed-up is 5000x on one instance of a large multi-commodity flow optimization problem.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"498\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman-1024x498.png\" alt=\"Bar chart shows that cuOpt is faster on 60% of the instances and more than 10x faster in 20% of the instances. The biggest speed-up is 5000x on one instance of a large multi-commodity flow optimization problem.\" class=\"wp-image-89906\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman-1024x498.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman-300x146.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman-625x304.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman-179x87.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman-768x374.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman-1536x747.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman-645x314.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman-500x243.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman-362x176.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman-226x110.png 226w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mittelman.png 1636w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. cuOpt acceleration compared to CPU LP on Mittelmann\u2019s benchmark</em></figcaption></figure></div>\n\n\n<p>I also compared cuOpt against a state-of-the-art CPU PDLP implementation using the same setup and conditions. cuOpt is consistently faster and between 10x to 3000x faster.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"498\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2-1024x498.png\" alt=\"Bar chart shows that cuOpt is consistently faster and between 10x to 3000x faster.\" class=\"wp-image-90262\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2-1024x498.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2-300x146.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2-625x304.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2-179x87.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2-768x374.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2-1536x747.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2-645x314.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2-500x243.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2-362x176.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2-226x110.png 226w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-pdlp-mittelman-fixed-2.png 1636w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. cuOpt acceleration compared to a CPU PDLP implementation on Mittelmann\u2019s benchmark</em></figcaption></figure></div>\n\n\n<p>The multi-commodity flow problem (MCF) involves finding the most efficient way to route multiple different types of goods through a network from various starting points to their respective destinations, ensuring that the network&#8217;s capacity constraints are not exceeded. One way to solve an MCF problem is to convert it to an LP. On a set of large MCF instances, PDLP is consistently faster, between 10x and 300x.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"494\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf-1024x494.png\" alt=\"Bar chart shows that cuOpt is consistently faster and between 10x to 300x faster.\" class=\"wp-image-89910\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf-1024x494.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf-300x145.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf-625x302.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf-179x86.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf-768x371.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf-1536x742.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf-645x311.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf-500x241.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf-160x77.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf-362x175.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf-228x110.png 228w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-acceleration-vs-cpu-lp-mcf.png 1636w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. cuOpt acceleration compared to the CPU LP solver on a set of MCF instances&nbsp;</em></figcaption></figure></div>\n\n\n<h2 id=\"potential-for-pdlp-refinement\"  class=\"wp-block-heading\" >Potential for PDLP refinement<a href=\"#potential-for-pdlp-refinement\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The NVIDIA cuOpt LP solver delivers incredible performance, but there\u2019s potential for future enhancements:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Handling higher accuracy</li>\n\n\n\n<li>Requiring high bandwidth</li>\n\n\n\n<li>Convergence issues on some problems</li>\n\n\n\n<li>Limited benefit for small LPs</li>\n</ul>\n\n\n\n<h3 id=\"handling_higher_accuracy\"  class=\"wp-block-heading\">Handling higher accuracy<a href=\"#handling_higher_accuracy\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To decide whether you\u2019ve solved an LP, you measure two things:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Optimality gap:</strong> Measures how far you are from finding the optimum of the objective function.</li>\n\n\n\n<li><strong>Feasibility:</strong> Measures how far you are from respecting the constraints.&nbsp;</li>\n</ul>\n\n\n\n<p>An LP is considered solved when both quantities are zero. Reaching an exact value of zero can be challenging and often unnecessary, so LP solvers use a threshold that enables faster convergence while maintaining accuracy. Both quantities are now only required to be below this threshold, which is relative to the magnitude of the values of the problem.</p>\n\n\n\n<p>Most LP solvers use a threshold, especially for large problems that are extremely challenging to solve. The industry standard so far was to use 10<sup>-8.</sup> While PDLP can solve problems using 10<sup>-8</sup>, it is then usually significantly slower. This can be an issue if you require high accuracy. In practice, many find 10<sup>-4</sup> accurate enough and sometimes even lower. This heavily benefits PDLP while not being a big differentiator for other LP-solving algorithms.</p>\n\n\n\n<h3 id=\"requiring_high_bandwidth\"  class=\"wp-block-heading\">Requiring high bandwidth<a href=\"#requiring_high_bandwidth\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>PDLP\u2019s performance scales linearly with memory bandwidth, making it more efficient on new GPU architectures. It requires a recent server-grade GPU to reproduce the results shown in the performance analysis section.</p>\n\n\n\n<h3 id=\"convergence_issues_on_some_problems\"  class=\"wp-block-heading\">Convergence issues on some problems<a href=\"#convergence_issues_on_some_problems\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>While PDLP can solve most LPs quickly, it sometimes needs a significant number of steps to converge, resulting in higher runtimes. On Mittelmann\u2019s benchmark, cuOpt LP Solver times out after one hour on 8 of the 49 public instances, due to a slow convergence rate.&nbsp;</p>\n\n\n\n<h3 id=\"limited_benefit_for_small_lps\"  class=\"wp-block-heading\">Limited benefit for small LPs<a href=\"#limited_benefit_for_small_lps\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Small LPs benefit less from the GPU\u2019s high bandwidth, which doesn\u2019t enable PDLP to scale as well compared to CPU solvers. The cuOpt LP solver offers a batch mode for this scenario where you can provide and solve hundreds of small LPs in parallel.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The cuOpt LP solver uses CUDA programming, NVIDIA GPU libraries, and cutting-edge NVIDIA GPUs to solve LPs, potentially orders of magnitude faster than CPU and scaling to over a billion coefficients. As a result, it&#8217;s particularly beneficial for tackling large-scale problems, where its advantages become even more prominent.</p>\n\n\n\n<p>Some use cases will still work better with traditional Simplex or IPM and I expect the future solvers to be a combination of GPU and CPU techniques. </p>\n\n\n\n<p>Sign up to be notified when you can <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/contact-sales/\">try the cuOpt LP</a>. Try NVIDIA <a href=\"https://build.nvidia.com/nvidia/nvidia-cuopt\">cuOpt Vehicle Routing Problem (VRP</a>) today with <a href=\"https://build.nvidia.com/search?term=llm\">NVIDIA-hosted NIM</a> microservices for the latest AI models for free on the <a href=\"https://build.nvidia.com/explore/discover\">NVIDIA API Catalog</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The evolution of linear programming (LP) solvers has been marked by significant milestones over the past century, from Simplex to the interior point method (IPM). The introduction of primal-dual linear programming (PDLP) has brought another significant advancement.&nbsp; NVIDIA cuOpt has now implemented PDLP with GPU acceleration. Using cutting-edge algorithms, NVIDIA hardware, dedicated CUDA features, and &hellip; <a href=\"https://developer.nvidia.com/blog/accelerate-large-linear-programming-problems-with-nvidia-cuopt/\">Continued</a></p>\n", "protected": false}, "author": 2351, "featured_media": 90020, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1497415", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerate-large-linear-programming-problems-with-nvidia-cuopt/309128", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [453, 4088], "coauthors": [4084], "class_list": ["post-89885", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "tag-featured", "tag-linear-programming"], "acf": {"post_industry": ["General"], "post_products": ["cuOpt"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-linear-programming-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nnL", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89885"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2351"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89885"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89885/revisions"}], "predecessor-version": [{"id": 90357, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89885/revisions/90357"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/90020"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89885"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89885"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89885"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89885"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87127, "date": "2024-10-08T08:00:00", "date_gmt": "2024-10-08T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87127"}, "modified": "2024-10-22T13:34:00", "modified_gmt": "2024-10-22T20:34:00", "slug": "rapidly-triage-container-security-with-the-vulnerability-analysis-nvidia-nim-agent-blueprint", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/rapidly-triage-container-security-with-the-vulnerability-analysis-nvidia-nim-agent-blueprint/", "title": {"rendered": "Rapidly Triage Container Security with the Vulnerability Analysis NVIDIA NIM Agent Blueprint"}, "content": {"rendered": "\n<p>Addressing software security issues is becoming more challenging as the number of vulnerabilities reported in the <a href=\"https://www.cve.org/\">CVE database</a> continues to grow at an accelerated pace. Assessing a single container for vulnerabilities requires the collection, comprehension, and synthesis of hundreds of pieces of information. With over 200K vulnerabilities reported at the end of 2023, the traditional approach to scanning and patching has become unmanageable.&nbsp;</p>\n\n\n\n<p>Enterprises are increasingly adopting <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI</a> to drive innovation across domains. Vulnerability detection&nbsp;and resolution will become a top generative AI use case in software delivery, according to the <a href=\"https://www.idc.com/getdoc.jsp?containerId=US52023824&amp;pageType=PRINTFRIENDLY\">IDC</a>.</p>\n\n\n\n<p>Generative AI can improve vulnerability defense while reducing the burden on security teams. Organizations have already begun to explore its use for automation, but scaling it at an enterprise level requires a complex AI system.&nbsp;</p>\n\n\n\n<p>Video 1 shows how NVIDIA uses generative AI and <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation</a> (RAG) to accelerate vulnerability analysis in software containers at enterprise scale, and dramatically reduce the time to assess and mitigate CVEs from hours or days to mere seconds.\u00a0</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/CEi_I7FMRV4?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Supercharge Software Delivery with Event-Driven RAG</em></figcaption></figure>\n\n\n\n<p>This AI agent example is enabling NVIDIA cybersecurity and systems integrator partners to build solutions that can connect large language models (LLMs) to data to drive greater efficiencies for software development.</p>\n\n\n\n<h2 id=\"key_takeaways\"  class=\"wp-block-heading\">Key takeaways<a href=\"#key_takeaways\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Using <a href=\"https://ai.nvidia.com/\">NVIDIA NIM</a> and the <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/morpheus/\">NVIDIA Morpheus cybersecurity AI SDK</a>, this event-driven RAG example can dramatically decrease CVE analysis and remediation from days to just seconds.&nbsp;</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/introduction-to-llm-agents/#what_is_an_ai_agent\">LLM agents</a> can expedite investigations and cut through the noise of an increasing number of known CVEs to highlight urgent security risks.</li>\n\n\n\n<li>In this example, we implement multiple LLM agents to automate vulnerability management, verification, and VEX justification, all triggered by the results of upstream vulnerability scans.</li>\n\n\n\n<li>Built on NVIDIA Morpheus, this NIM Agent Blueprint uses asynchronous and parallel GPU processing for scalable, fast analysis of multiple CVEs simultaneously. This architecture enables real-time insights into container and vulnerability information, streamlining the validation process and addressing potential security threats.</li>\n</ul>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\">Summary<a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Try the blueprint for free at <a href=\"https://build.nvidia.com/nvidia/vulnerability-analysis-for-container-security\">build.nvidia.com</a>. Learn more and get notified of the upcoming release of a downloadable <a href=\"https://www.nvidia.com/en-us/solutions/ai/cybersecurity/newsletter/\">vulnerability analysis NIM Agent blueprint.</a></p>\n\n\n\n<p>For more information about how to implement generative AI for CVE analysis at an enterprise scale, see <a href=\"https://developer.nvidia.com/blog/applying-generative-ai-for-cve-analysis-at-an-enterprise-scale/\">Applying Generative AI for CVE Analysis at an Enterprise Scale</a>.</p>\n\n\n\n<p>For more information, see the following resources:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://developer.nvidia.com/morpheus-cybersecurity\">NVIDIA Morpheus SDK</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/morpheus/getting_started.html\">Morpheus on NGC</a><a href=\"https://docs.nvidia.com/morpheus/getting_started.html\"></a></li>\n\n\n\n<li><a href=\"https://github.com/nv-morpheus/Morpheus\">/nv-morpheus</a> GitHub repo</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>Addressing software security issues is becoming more challenging as the number of vulnerabilities reported in the CVE database continues to grow at an accelerated pace. Assessing a single container for vulnerabilities requires the collection, comprehension, and synthesis of hundreds of pieces of information. With over 200K vulnerabilities reported at the end of 2023, the traditional &hellip; <a href=\"https://developer.nvidia.com/blog/rapidly-triage-container-security-with-the-vulnerability-analysis-nvidia-nim-agent-blueprint/\">Continued</a></p>\n", "protected": false}, "author": 2033, "featured_media": 89937, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1497414", "discourse_permalink": "https://forums.developer.nvidia.com/t/rapidly-triage-container-security-with-the-vulnerability-analysis-nvidia-nim-agent-blueprint/309127", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 3110], "tags": [3843, 453, 4134, 3613, 1511], "coauthors": [3738], "class_list": ["post-87127", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-cybersecurity", "category-generative-ai", "tag-cve", "tag-featured", "tag-nim-agent-blueprint", "tag-retrieval-augmented-generation-rag", "tag-security-ai"], "acf": {"post_industry": ["General"], "post_products": ["Morpheus", "NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/vulnerability-nim-agent-blueprint-featured.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-mFh", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Cybersecurity", "link": "https://developer.nvidia.com/blog/category/cybersecurity/", "id": 1464}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87127"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2033"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87127"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87127/revisions"}], "predecessor-version": [{"id": 89954, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87127/revisions/89954"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89937"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87127"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87127"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87127"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87127"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89920, "date": "2024-10-08T07:00:00", "date_gmt": "2024-10-08T14:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89920"}, "modified": "2024-11-11T20:34:20", "modified_gmt": "2024-11-12T04:34:20", "slug": "bringing-ai-ran-to-a-telco-near-you", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/bringing-ai-ran-to-a-telco-near-you/", "title": {"rendered": "Bringing AI-RAN to a Telco Near You"}, "content": {"rendered": "\n<p>Inferencing for generative AI and AI agents will drive the need for AI compute infrastructure to be distributed from edge to central clouds. <a href=\"https://www.idc.com/getdoc.jsp?containerId=prUS52600524#:~:text=NEEDHAM%2C%20Mass.%2C%20September%2017,services%20to%20business%20and%20consumer\">IDC predicts</a> that &#8220;Business AI (consumer excluded) will contribute $19.9 trillion to the global economy and account for 3.5% of GDP by 2030.&#8221;</p>\n\n\n\n<p>5G networks must also evolve to serve this new incoming AI traffic. At the same time, there is an opportunity for telcos to become the local AI compute infrastructure for hosting enterprise AI workloads, independent of network connectivity while meeting their data privacy and sovereignty requirements. This is where an accelerated computing infrastructure shines &#8211; with the ability to accelerate both Radio signal processing and AI workloads. And most importantly, the same compute infrastructure can be used to process AI and radio access network (RAN) services. This combination has been called <a href=\"https://ai-ran.org/news/industry-leaders-in-ai-and-wireless-form-ai-ran-alliance/\">AI-RAN by the telecoms industry</a>.&nbsp;</p>\n\n\n\n<p>NVIDIA is introducing Aerial RAN Computer-1, the world\u2019s first AI-RAN deployment platform, that can serve AI and RAN workloads concurrently, on a common accelerated infrastructure.&nbsp;</p>\n\n\n\n<p>Following the launch of the <a href=\"https://www.t-mobile.com/news/business/t-mobile-launches-ai-ran-innovation-center-with-nvidia\">AI-RAN Innovation Center by T-Mobile</a>, the Aerial RAN Computer-1 turns AI-RAN into reality with a deployable platform that telcos can adopt globally. It can be used in small, medium, or large configurations for deployment at cell sites, distributed or centralized sites, effectively turning the network into a multi-purpose infrastructure that serves voice, video, data, and AI traffic.&nbsp;</p>\n\n\n\n<p>This is a transformative solution that reimagines wireless networks for AI, with AI. It is also a huge opportunity for telcos to fuel the AI flywheel, leveraging their distributed network infrastructure, low latency, guaranteed quality of service, massive scale, and ability to preserve data privacy, security, and localization &#8211; all key requirements for AI inferencing and agentic AI applications.</p>\n\n\n\n<h2 id=\"ai-ran_ai_aerial_and_aerial_ran_computer-1\"  class=\"wp-block-heading\">AI-RAN, AI Aerial, and Aerial RAN Computer-1<a href=\"#ai-ran_ai_aerial_and_aerial_ran_computer-1\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>AI-RAN is the technology framework to build multipurpose networks that are also AI-native. As telcos embrace AI-RAN, and move from the traditional single-purpose ASIC-based computing networks for RAN to new multi-purpose accelerated computing-based networks serving RAN and AI together, telcos can now participate in the new AI economy and can leverage AI to improve the efficiency of their networks.&nbsp;</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/industries/telecommunications/ai-ran/\">NVIDIA AI Aerial</a> includes three computer systems to design, simulate, train, and deploy AI-RAN-based 5G and 6G wireless networks. Aerial RAN Computer-1 is the base foundation of NVIDIA AI Aerial and provides a commercial-grade deployment platform for AI-RAN.</p>\n\n\n\n<p>Aerial RAN Computer-1 (Figure 1) offers a common scalable hardware foundation to run RAN and AI workloads including &#8211; software-defined 5G, Private 5G RAN from NVIDIA or other RAN software providers, containerized network functions, AI microservices from NVIDIA or partners or host internal and third-party generative AI applications. Aerial RAN Computer-1 is modular by design, enabling it to scale from D-RAN to C-RAN architectures covering rural to dense urban use cases.</p>\n\n\n\n<p>NVIDIA CUDA-X Libraries are central to accelerated computing, providing speed, accuracy, and reliability in addition to improved efficiency. That means more work is done in the same power envelope. Most importantly, domain-specific libraries, including telecom-specific adaptations, are key to making Aerial RAN Computer-1 suited for telecom deployments.&nbsp;</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/networking/doca\">NVIDIA DOCA</a> offers a suite of tools and libraries that can significantly boost the performance enhancements for telco workloads, including RDMA, PTP/timing synchronization, and Ethernet-based fronthaul (eCPRI), as well as AI workloads that are crucial for modern network infrastructure.</p>\n\n\n\n<p>Collectively, the full stack enables scalable hardware, common software, and an open architecture to deliver a high-performance AI-RAN together with ecosystem partners.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"946\" height=\"503\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-aerial-ran-computer-1-stack-1.png\" alt=\"Stack diagram shows components in the AI-RAN Orchestrator, containerized functions, accelerated libraries, the CUDA and DOCA operating systems, accelerated compute with GB200 NVL-2, and networking with Spectrum-X.\" class=\"wp-image-90266\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-aerial-ran-computer-1-stack-1.png 946w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-aerial-ran-computer-1-stack-1-300x160.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-aerial-ran-computer-1-stack-1-625x332.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-aerial-ran-computer-1-stack-1-179x95.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-aerial-ran-computer-1-stack-1-768x408.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-aerial-ran-computer-1-stack-1-645x343.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-aerial-ran-computer-1-stack-1-500x266.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-aerial-ran-computer-1-stack-1-160x85.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-aerial-ran-computer-1-stack-1-362x192.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-aerial-ran-computer-1-stack-1-207x110.png 207w\" sizes=\"(max-width: 946px) 100vw, 946px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA Aerial RAN Computer-1, as a part of the NVIDIA AI Aerial platform</em></figcaption></figure></div>\n\n\n<h2 id=\"benefits_of_aerial_ran_computer-1\"  class=\"wp-block-heading\">Benefits of Aerial RAN Computer-1<a href=\"#benefits_of_aerial_ran_computer-1\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With Aerial RAN Computer-1, wireless networks can turn into a massively distributed grid of AI and RAN data centers, unleashing new monetization avenues for telcos while paving the way for 6G with a software upgrade.</p>\n\n\n\n<p>Benefits of Aerial RAN Computer-1 for telecom service providers include the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Monetize with AI and generative AI applications, AI inferencing at the edge, or with GPU-as-a-Service.</li>\n\n\n\n<li>Increase utilization of infrastructure by 2-3x compared to single-purpose base stations that are typically only 30% utilized today. Use the same infrastructure to host internal generative AI workloads and other containerized network functions such as UPF and RIC.</li>\n\n\n\n<li>Improve radio network performance through site-specific AI learning, with up to 2x gains possible in spectral efficiency. This means direct cost savings per Mhz of the acquired spectrum.</li>\n\n\n\n<li>Deliver high-performance RAN and AI experiences for next-gen applications that intertwine AI into every interaction. Aerial RAN Computer-1 can service up to 170 Gb/s throughput in RAN-only mode and 25K tokens/sec in AI-only mode, or a combination of both with superior performance compared to traditional networks.</li>\n</ul>\n\n\n\n<h2 id=\"building_blocks_of_aerial_ran_computer-1\"  class=\"wp-block-heading\">Building blocks of Aerial RAN Computer-1<a href=\"#building_blocks_of_aerial_ran_computer-1\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The key hardware components of Aerial RAN Computer-1 include the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>NVIDIA GB200 NVL2</li>\n\n\n\n<li>NVIDIA Blackwell GPU</li>\n\n\n\n<li>NVIDIA Grace CPU</li>\n\n\n\n<li>NVLink2 C2C</li>\n\n\n\n<li>Fifth-generation NVIDIA NVLink</li>\n\n\n\n<li>Key-value caching</li>\n\n\n\n<li>MGX reference architecture</li>\n\n\n\n<li>Real-time mainstream LLM inference</li>\n</ul>\n\n\n\n<h3 id=\"nvidia_gb200_nvl2&nbsp;\"  class=\"wp-block-heading\">NVIDIA GB200 NVL2&nbsp;<a href=\"#nvidia_gb200_nvl2&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The NVIDIA GB200 NVL2 platform (Figure 2) used in Aerial RAN Computer-1 revolutionizes data center and edge computing, offering unmatched performance for mainstream large language models (LLMs), vRAN, vector database searches, and data processing.&nbsp;</p>\n\n\n\n<p>Powered by two NVIDIA Blackwell GPUs and two NVIDIA Grace CPUs, the scale-out single-node architecture seamlessly integrates accelerated computing into existing infrastructure.&nbsp;</p>\n\n\n\n<p>This versatility enables a wide range of system designs and networking options, making the GB200 NVL2 platform an ideal choice for data centers, edge, and cell site locations seeking to harness the power of AI as well as wireless 5G connectivity.&nbsp;</p>\n\n\n\n<p>For instance, half of a GB200 server could be allocated to RAN tasks and the other half to AI processing through <a href=\"https://www.nvidia.com/en-us/technologies/multi-instance-gpu/\">Multi-instance GPU (MIG)</a> technology at a single cell site. For aggregated sites, a full GB200 server could be dedicated to RAN, with another used exclusively for AI. In a centralized deployment, a cluster of GB200 servers could be shared between RAN and AI workloads</p>\n\n\n\n<h3 id=\"nvidia_blackwell_gpu\"  class=\"wp-block-heading\">NVIDIA Blackwell GPU<a href=\"#nvidia_blackwell_gpu\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>NVIDIA Blackwell is a revolutionary architecture that delivers improved performance, efficiency, and scale. NVIDIA Blackwell GPUs pack 208B transistors and are manufactured using a custom-built TSMC 4NP process. All NVIDIA Blackwell products feature two reticle-limited dies connected by a 10-TB/s chip-to-chip interconnect in a unified single GPU.</p>\n\n\n\n<h3 id=\"nvidia_grace_cpu\"  class=\"wp-block-heading\">NVIDIA Grace CPU<a href=\"#nvidia_grace_cpu\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The NVIDIA Grace CPU is a breakthrough processor designed for modern data centers running AI, vRAN, cloud, and high-performance computing (HPC) applications. It provides outstanding performance and memory bandwidth with 2x the energy efficiency of today\u2019s leading server processors.</p>\n\n\n\n<h3 id=\"nvlink2_c2c\"  class=\"wp-block-heading\">NVLink2 C2C<a href=\"#nvlink2_c2c\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The GB200 NVL2 platform uses NVLink-C2C for a groundbreaking 900 GB/s interconnect between each NVIDIA Grace CPU and NVIDIA Blackwell GPU. Combined with fifth-generation NVLink, this delivers a massive 1.4-TB coherent memory model, fueling accelerated AI and vRAN performance.</p>\n\n\n\n<h3 id=\"fifth-generation_nvidia_nvlink\"  class=\"wp-block-heading\">Fifth-generation NVIDIA NVLink<a href=\"#fifth-generation_nvidia_nvlink\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To fully harness the power of exascale computing and trillion-parameter AI models, every GPU in a server cluster must communicate seamlessly and swiftly.&nbsp;</p>\n\n\n\n<p>Fifth-generation NVLink is a high-performance interconnect to deliver accelerated performance from the GB200 NVL2 platform.</p>\n\n\n\n<h3 id=\"key-value_caching\"  class=\"wp-block-heading\">Key-value caching<a href=\"#key-value_caching\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><em>Key-value (KV) caching</em> improves LLM response speeds by storing conversation context and history.&nbsp;</p>\n\n\n\n<p>GB200 NVL2 optimizes KV caching through its fully coherent NVIDIA Grace GPU and NVIDIA Blackwell GPU memory connected by NVLink-C2C, 7x faster than PCIe. This enables LLMs to predict words faster than x86-based GPU implementations.</p>\n\n\n\n<h3 id=\"mgx_reference_architecture\"  class=\"wp-block-heading\">MGX reference architecture<a href=\"#mgx_reference_architecture\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>MGX GB200 NVL2 is a 2:2 configuration with CPU C-Links and GPU NVLinks connected\u200b.</p>\n\n\n\n<p>HPM contains the following components:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>NVIDIA Grace CPUs (2)</li>\n\n\n\n<li>Connectors for GPU pucks and I/O cards</li>\n\n\n\n<li>GPU modules populated in 2U AC Server (2)</li>\n</ul>\n\n\n\n<p>Each pluggable GPU module contains the GPU, B2B connection, and NVLink connectors.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"811\" height=\"523\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/mgx-form-factor.png\" alt=\"Diagram shows NVIDIA Grace CPUs, NVIDIA Blackwell GPUs, NVLink connections, and MGX form factor.\" class=\"wp-image-90030\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/mgx-form-factor.png 811w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/mgx-form-factor-300x193.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/mgx-form-factor-625x403.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/mgx-form-factor-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/mgx-form-factor-768x495.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/mgx-form-factor-645x416.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/mgx-form-factor-465x300.png 465w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/mgx-form-factor-140x90.png 140w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/mgx-form-factor-362x233.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/mgx-form-factor-171x110.png 171w\" sizes=\"(max-width: 811px) 100vw, 811px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA GB200 NVL2 platform layout</em></figcaption></figure></div>\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>GPU Compute</strong></td><td>40 PFLOPS FP4 | 20 PFLOPS FP8/FP6<br>10x GH200</td></tr><tr><td><strong>GPU Memory</strong></td><td>Up to 384 GB</td></tr><tr><td><strong>CPU</strong></td><td>144 Core ARMv9,<br>960 GB LPDDR5,<br>1.4x perf &amp; 30% lower power than 2x SPR</td></tr><tr><td><strong>CPU to GPU<br>NVLink C2C</strong></td><td>Per GPU 900 GB/s bi-dir. &amp; cache-coherent</td></tr><tr><td><strong>GPU to GPU<br>NVLink</strong></td><td>1,800 GB/s bi-dir., NVLink</td></tr><tr><td><strong>Scale-Out</strong></td><td>Spectrum-X Ethernet or InfiniBand Connect-X or BlueField</td></tr><tr><td><strong>OS</strong></td><td>Single OS with unified address space covering 2 CPU + 2 GPU</td></tr><tr><td><strong>System Power</strong></td><td>Full System ~3,500W, configurable</td></tr><tr><td><strong>Schedule</strong></td><td>Sample: Q4 2024<br>MP: Q1 2025</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. GB200 NVL2 platform features</em></figcaption></figure>\n\n\n\n<h3 id=\"real-time_mainstream_llm_inference\"  class=\"wp-block-heading\">Real-time mainstream LLM inference<a href=\"#real-time_mainstream_llm_inference\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The GB200 NVL2 platform introduces massive coherent memory up to 1.3 TB shared between two NVIDIA Grace CPUs and two NVIDIA Blackwell GPUs. This shared memory is coupled with fifth-generation NVIDIA NVLink and high-speed, chip-to-chip (C2C) connections to deliver 5x faster real-time LLM inference performance for mainstream language models, such as Llama3-70B.&nbsp;</p>\n\n\n\n<p>With an input sequence length of 256, an output sequence length of 8000, FP4 precision, the GB200 NVL2 platform can produce up to 25K tokens/sec, which is 2.16B tokens/day.&nbsp;</p>\n\n\n\n<p>Figure 3 shows how GB200 NVL2 performs when supporting AI and RAN workloads.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"611\" height=\"453\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-compute.png\" alt=\"Image shows how the GB200NVL2 performs when supporting AI and RAN. AI and RAN vary with time of day and cell site activity.\" class=\"wp-image-90031\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-compute.png 611w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-compute-300x222.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-compute-155x115.png 155w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-compute-405x300.png 405w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-compute-121x90.png 121w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-compute-362x268.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-compute-148x110.png 148w\" sizes=\"(max-width: 611px) 100vw, 611px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Compute utilization for RAN and AI in GB200 NVL2</em></figcaption></figure></div>\n\n\n<p>Here&#8217;s what platform tenancy looks like for RAN and AI on the GB200 NVL2 platform:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Workload at 100% utilization\n<ul class=\"wp-block-list\">\n<li>RAN: <strong>~36x 100 MHz 64T64R</strong></li>\n\n\n\n<li>*Tokens: <strong>25K tokens/sec</strong></li>\n\n\n\n<li>AI: <strong>~$10/hr. | ~$90K/year</strong></li>\n</ul>\n</li>\n\n\n\n<li>Workload at 50:50 split utilization\n<ul class=\"wp-block-list\">\n<li>RAN: <strong>~18x 100 MHz 64T64R</strong></li>\n\n\n\n<li>*Tokens: <strong>12.5K tokens/sec</strong></li>\n\n\n\n<li>AI: <strong>~$5/hr. | ~$45K/year</strong></li>\n</ul>\n</li>\n</ul>\n\n\n\n<p class=\"has-small-font-size\">*Token AI workload: Llama-3-70B FP4 | Sequence lengths input 256 / output 8K</p>\n\n\n\n<h2 id=\"supporting_hardware_for_aerial_ran_computer-1\"  class=\"wp-block-heading\">Supporting hardware for Aerial RAN Computer-1<a href=\"#supporting_hardware_for_aerial_ran_computer-1\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA BlueField-3 and NVIDIA networking Spectrum-X\u00a0are the supporting hardware for Aerial RAN Computer-1.</p>\n\n\n\n<h3 id=\"nvidia_bluefield-3\"  class=\"wp-block-heading\">NVIDIA BlueField-3<a href=\"#nvidia_bluefield-3\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://resources.nvidia.com/en-us-accelerated-networking-resource-library/datasheet-nvidia-bluefield\">NVIDIA BlueField-3</a> DPUs enable real-time data transmission with precision 5G timing required for fronthaul eCPRI traffic.&nbsp;</p>\n\n\n\n<p>NVIDIA offers a full IEEE 1588v2 Precision Time Protocol (PTP) software solution. NVIDIA PTP software solutions are designed to meet the most demanding PTP profiles. NVIDIA BlueField-3 incorporates an integrated PTP hardware clock (PHC) that enables the device to achieve sub-20 nanosecond accuracy while offering timing-related functions, including time-triggered scheduling and time-based, software-defined networking (SDN) accelerations.&nbsp;</p>\n\n\n\n<p>This technology also enables software applications to transmit fronthaul, RAN-compatible data in high bandwidth.&nbsp;</p>\n\n\n\n<h3 id=\"nvidia_networking_spectrum-x&nbsp;\"  class=\"wp-block-heading\">NVIDIA networking Spectrum-X&nbsp;<a href=\"#nvidia_networking_spectrum-x&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The edge and data center networks play a crucial role in driving AI and wireless advancements and performance, serving as the backbone for distributed AI model inference, generative AI, and world-class vRAN performance.&nbsp;</p>\n\n\n\n<p>NVIDIA BlueField-3 DPUs enable efficient scalability across hundreds and thousands of NVIDIA Blackwell GPUs for optimal application performance.&nbsp;</p>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/networking/spectrumx/\">NVIDIA Spectrum-X</a> Ethernet platform is designed specifically to improve the performance and efficiency of Ethernet-based AI clouds and includes all the required functionality for 5G timing synchronization. It delivers 1.6x better AI networking performance compared to traditional Ethernet, along with consistent, predictable performance in multi-tenant environments.</p>\n\n\n\n<p>When Aerial RAN Computer-1 is deployed in a rack configuration, the Spectrum-X Ethernet switch serves as a dual-purpose fabric. It handles both fronthaul and AI (east-west) traffic on the compute fabric, while also carrying backhaul or midhaul and AI (north-south) traffic on the converged fabric. The remote radio units terminate at the switch in compliance with the eCPRI protocol.</p>\n\n\n\n<h2 id=\"software_stacks_on_aerial_ran_computer-1\"  class=\"wp-block-heading\">Software stacks on Aerial RAN Computer-1<a href=\"#software_stacks_on_aerial_ran_computer-1\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The key software stacks on Aerial RAN Computer-1 include the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>NVIDIA Aerial CUDA-Accelerated RAN</li>\n\n\n\n<li>NVIDIA AI Enterprise and NVIDIA NIM</li>\n\n\n\n<li>NVIDIA Cloud Functions</li>\n</ul>\n\n\n\n<h3 id=\"nvidia_aerial_cuda-accelerated_ran\"  class=\"wp-block-heading\">NVIDIA Aerial CUDA-Accelerated RAN<a href=\"#nvidia_aerial_cuda-accelerated_ran\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://developer.nvidia.com/aerial-cuda-accelerated-ran\">NVIDIA Aerial CUDA-Accelerated RAN</a> is the primary NVIDIA-built RAN software for 5G and private 5G running on Aerial RAN Computer-1.&nbsp;</p>\n\n\n\n<p>It includes NVIDIA GPU-accelerated interoperable PHY and MAC layer libraries that can be easily modified and seamlessly extended with AI components. These hardened RAN software libraries can also be used by other software providers, telcos, cloud service providers (CSPs), and enterprises for building custom commercial-grade, software-defined 5G and future 6G radio access networks (RANs).</p>\n\n\n\n<p>Aerial CUDA-Accelerated RAN is integrated with <a href=\"https://developer.nvidia.com/aerial-ai-radio-frameworks\">NVIDIA Aerial AI Radio Frameworks</a>, which provides a package of AI enhancements to enable training and inference in the RAN using the framework tools\u2014pyAerial, NVIDIA Aerial Data Lake, and <a href=\"https://developer.nvidia.com/sionna/\">NVIDIA Sionna</a>.&nbsp;</p>\n\n\n\n<p>It is also complemented by <a href=\"https://developer.nvidia.com/aerial-omniverse-digital-twin\">NVIDIA Aerial Omniverse Digital Twin</a>, a system-level network digital twin development platform that enables physically accurate simulations of wireless systems.</p>\n\n\n\n<h3 id=\"nvidia_ai_enterprise_and_nvidia_nim\"  class=\"wp-block-heading\">NVIDIA AI Enterprise and NVIDIA NIM<a href=\"#nvidia_ai_enterprise_and_nvidia_nim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> is the software platform for enterprise generative AI. <a href=\"https://developer.nvidia.com/nim\">NVIDIA NIM</a> is a collection of microservices that simplify the deployment of foundation models for generative AI applications.&nbsp;</p>\n\n\n\n<p>Collectively, they provide easy-to-use microservices and blueprints that accelerate data science pipelines and streamline the development and deployment of production-grade co-pilots and other generative AI applications for enterprises.&nbsp;</p>\n\n\n\n<p>Enterprises and telcos can either subscribe to the managed NVIDIA Elastic NIM service or deploy and manage NIM themselves. Aerial RAN Computer-1 can host NVIDIA AI Enterprise and NIM-based AI and generative AI workloads.&nbsp;</p>\n\n\n\n<h3 id=\"nvidia_cloud_functions\"  class=\"wp-block-heading\">NVIDIA Cloud Functions<a href=\"#nvidia_cloud_functions\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://docs.nvidia.com/cloud-functions/index.html\">NVIDIA Cloud Functions</a> offers a serverless platform for GPU-accelerated AI workloads, ensuring security, scalability, and reliability. It supports various communication protocols:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>HTTP polling</li>\n\n\n\n<li>Streaming</li>\n\n\n\n<li>gRPC</li>\n</ul>\n\n\n\n<p>Cloud Functions is primarily suited for shorter running, preemptable workloads, such as inferencing and fine-tuning. This is well-suited for the Aerial RAN Computer-1 platform as the RAN workload resource utilization varies over time of the day.&nbsp;</p>\n\n\n\n<p>The AI workloads that are ephemeral and preemptable can usually fill up those under-used hours of the day, which maintains high utilization of the Aerial RAN Computer-1 platform.</p>\n\n\n\n<h2 id=\"deployment_options_and_performance&nbsp;\"  class=\"wp-block-heading\">Deployment options and performance&nbsp;<a href=\"#deployment_options_and_performance&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Aerial RAN Computer-1 has multiple deployment options that include all points in the radio access network:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Radio base station cell site</li>\n\n\n\n<li>Point of presence locations</li>\n\n\n\n<li>Mobile switching offices</li>\n\n\n\n<li>Baseband hotels&nbsp;</li>\n</ul>\n\n\n\n<p>For private 5G, it can be located on the enterprise premises.&nbsp;</p>\n\n\n\n<p>Aerial RAN Computer-1 can support various configurations and locations, including private, public, or hybrid cloud environments while using the same software regardless of location or interface standard. This ability offers unprecedented flexibility compared to traditional single-purpose RAN computers.\u00a0</p>\n\n\n\n<p>The solution also supports a wide range of network technologies:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Open Radio Access Network (Open-RAN) architectures</li>\n\n\n\n<li>AI-RAN</li>\n\n\n\n<li>3GPP standards</li>\n\n\n\n<li>Other industry-leading specifications</li>\n</ul>\n\n\n\n<p>Aerial RAN Computer-1, based on GB200, delivers continued performance improvements in RAN processing, AI processing, and energy efficiency compared to the earlier NVIDIA H100 and NVIDIA H200 GPUs (Figure 4).</p>\n\n\n\n<p>The GB200 NVL2 platform provides a single MGX server for existing infrastructure, which is easy to deploy and scale out. You get mainstream LLM inference and data processing with high-end RAN compute.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"272\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-performance-625x272.png\" alt=\"Six boxes with the following text: Data processing: 18x; Vector database search: 9x; Llama-3 inference: 5x; RAN processing: 4x; RAN performance/cost: 1.7x; RAN performance/Watt: 2.4x.\" class=\"wp-image-90035\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-performance-625x272.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-performance-300x131.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-performance-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-performance-768x334.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-performance-645x281.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-performance-500x218.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-performance-160x70.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-performance-362x157.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-performance-253x110.png 253w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-performance-1024x445.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gb200-nvl2-performance.png 1508w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. GB200 NVL2 performance compared to previous generations</em></figcaption></figure></div>\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>AI-RAN will revolutionize the telecom industry, enabling telcos to unlock new revenue streams and deliver enhanced experiences through generative AI, robotics, and autonomous technologies. The NVIDIA AI Aerial platform implements AI-RAN, aligning it with NVIDIA\u2019s broader vision to make wireless networks AI-native.&nbsp;</p>\n\n\n\n<p>With Aerial RAN Computer-1, telcos can deploy AI-RAN on a common infrastructure today. You can maximize the utilization by running RAN and AI workloads concurrently and improve RAN performance with AI algorithms.&nbsp;</p>\n\n\n\n<p>Most importantly, with this common computer, you can tap into a completely new opportunity to become the AI fabric of choice for enterprises that need local computing and data sovereignty for their AI workloads. You can start with an AI-first approach and RAN next, with a software upgrade, starting the clock on maximizing ROI from day one.</p>\n\n\n\n<p>T-Mobile and SoftBank have already announced their plans to commercialize AI-RAN together with leading RAN software providers, using hardware and software components of NVIDIA AI Aerial.&nbsp;</p>\n\n\n\n<p>At Mobile World Congress, Americas, Vapor IO and the City of Las Vegas announced the <a href=\"https://www.vapor.io/resources/press-releases/vapor-io-brings-nvidia-ai-aerial-to-the-city-of-las-vegas/\">world\u2019s first private 5G AI-RAN deployment</a> using NVIDIA AI Aerial.</p>\n\n\n\n<p>We are at a turning point in transforming wireless networks for AI, with AI. Join us at the <a href=\"https://register.nvidia.com/flow/nvidia/aisummitdc/virtualregistration/form/contactInfo\">NVIDIA AI Summit</a> in Washington, D.C. and at the <a href=\"https://golive.on24.com/event/4699085/en/ECF80A0589876E8E2FF25D9157BDC063/registration?partnerref=nvshare&amp;es_id=cf6e9bade0\">NVIDIA 6G Developer Day</a> to learn more about NVIDIA Aerial AI and NVIDIA Aerial RAN Computer-1.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Inferencing for generative AI and AI agents will drive the need for AI compute infrastructure to be distributed from edge to central clouds. IDC predicts that &#8220;Business AI (consumer excluded) will contribute $19.9 trillion to the global economy and account for 3.5% of GDP by 2030.&#8221; 5G networks must also evolve to serve this new &hellip; <a href=\"https://developer.nvidia.com/blog/bringing-ai-ran-to-a-telco-near-you/\">Continued</a></p>\n", "protected": false}, "author": 1085, "featured_media": 89962, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1497379", "discourse_permalink": "https://forums.developer.nvidia.com/t/bringing-ai-ran-to-a-telco-near-you/309115", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 2758, 1205], "tags": [817, 453, 549], "coauthors": [2270], "class_list": ["post-89920", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-edge-computing", "category-networking-communications", "tag-5g", "tag-featured", "tag-telecommunications"], "acf": {"post_industry": ["Telecommunications"], "post_products": ["Aerial", "AI Enterprise", "Blackwell", "BlueField DPU", "ConnectX", "GB200", "Grace CPU", "NIM", "NVLink"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nvidia-aerial-ran-computer-1-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nok", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Edge Computing", "link": "https://developer.nvidia.com/blog/category/edge-computing/", "id": 2758}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89920"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1085"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89920"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89920/revisions"}], "predecessor-version": [{"id": 90267, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89920/revisions/90267"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89962"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89920"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89920"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89920"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89920"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89719, "date": "2024-10-07T16:03:48", "date_gmt": "2024-10-07T23:03:48", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89719"}, "modified": "2024-10-17T11:19:11", "modified_gmt": "2024-10-17T18:19:11", "slug": "accelerating-reality-capture-workflows-with-ai-and-nvidia-rtx-gpus", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-reality-capture-workflows-with-ai-and-nvidia-rtx-gpus/", "title": {"rendered": "Accelerating Reality Capture Workflows with AI and NVIDIA RTX GPUs"}, "content": {"rendered": "\n<p>Reality capture creates highly accurate, detailed, and immersive digital representations of environments. Innovations in site scanning and accelerated data processing, and emerging technologies like neural radiance fields (NeRFs) and Gaussian splatting are significantly enhancing the capabilities of reality capture. These technologies are revolutionizing interactions with and analyses of the physical world.&nbsp;</p>\n\n\n\n<p>Site scanning, the first step of reality capture, generates detailed 3D models using methods like Lidar and photogrammetry, while accelerated processing, powered by NVIDIA RTX GPUs, enable faster and more efficient data handling. NeRFs excel in producing photorealistic 3D scenes, and Gaussian splatting offers a novel approach to smooth, efficient rendering. AI enhances these tools by providing deeper insights through advanced algorithms for object detection, segmentation, and classification.&nbsp;</p>\n\n\n\n<p>This post explores how NVIDIA is at the forefront of the integration of AI with reality capture, driving these technological advancements with powerful GPUs, software solutions, and cutting-edge research.</p>\n\n\n\n<h2 id=\"reality_capture_basics\"  class=\"wp-block-heading\">Reality capture basics<a href=\"#reality_capture_basics\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The reality capture process begins with scanning or photographing a physical environment, which is then processed through photogrammetry or Lidar to generate a point cloud\u2014a dense collection of data points that represent precise 3D surface locations. This point cloud is often converted into a 3D model, providing a detailed virtual representation of the physical space.</p>\n\n\n\n<h3 id=\"photogrammetry\"  class=\"wp-block-heading\">Photogrammetry<a href=\"#photogrammetry\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Photogrammetry is a technique that uses photographic images to extract detailed spatial information about physical objects, including their distances, dimensions, shapes, and exact positions in space. By analyzing angles, overlaps, and perspectives from multiple viewpoints, photogrammetry can create point clouds, which are then converted to highly detailed 3D models.&nbsp;</p>\n\n\n\n<p>This method is accessible and cost-effective, particularly when compared to Lidar, as it only requires basic photographic equipment. However, the accuracy of photogrammetry is highly dependent on the quality and quantity of the images captured, and it can struggle with certain surfaces like those that are reflective or transparent, which can result in less reliable outcomes.</p>\n\n\n\n<h3 id=\"lidar\"  class=\"wp-block-heading\">Lidar<a href=\"#lidar\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Lidar (light detection and ranging) technology uses laser pulses to measure distances and create precise 3D models of environments by calculating the time it takes for light to reflect back from surfaces. It offers unparalleled accuracy in capturing detailed spatial data over large areas, even in challenging lighting conditions (such as low light or darkness), and is effective at mapping various materials, including vegetation and surfaces beneath transparent objects.&nbsp;</p>\n\n\n\n<p>Lidar is typically more expensive than photogrammetry due to the specialized hardware required, and it can struggle with highly reflective surfaces like water or glass, potentially distorting data or creating gaps. Additionally, while Lidar excels in geometric accuracy, it provides less texture information compared to photogrammetry, which can limit its effectiveness in applications requiring photorealistic detail.</p>\n\n\n\n<h3 id=\"point_clouds_and_3d_meshes\"  class=\"wp-block-heading\">Point clouds and 3D meshes<a href=\"#point_clouds_and_3d_meshes\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Point clouds and 3D meshes are essential elements of reality capture, converting raw data from Lidar or photogrammetry into detailed, accurate virtual models. Point clouds consist of dense collections of points mapping precise 3D surface locations, which are often converted into 3D meshes that form continuous, textured surfaces for more realistic representations.&nbsp;</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/about-cuda\">CUDA</a>, <a href=\"https://www.nvidia.com/en-us/design-visualization/technologies/rtx/\">NVIDIA RTX</a>, and the <a href=\"https://www.nvidia.com/en-us/omniverse/\">NVIDIA Omniverse</a> platform significantly enhance this process. CUDA accelerates the complex computation needed for processing large datasets, RTX enables real-time ray-traced rendering for highly realistic lighting and shadows, and Omniverse provides a powerful collaborative platform for editing and visualizing 3D meshes seamlessly in real time.&nbsp;</p>\n\n\n\n<h3 id=\"choosing_the_right_reality_capture_technology\"  class=\"wp-block-heading\">Choosing the right reality capture technology<a href=\"#choosing_the_right_reality_capture_technology\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Choosing the right reality capture technology depends on your project&#8217;s specific needs. Lidar is the go-to for high-resolution, detailed spatial data, making it ideal for large-scale surveying, complex sites, and environments where precision is paramount. Software like Autodesk ReCap and Bentley iTwin Capture are commonly used to streamline Lidar data processing and analysis.&nbsp;</p>\n\n\n\n<p>Photogrammetry, on the other hand, excels in capturing detailed color data, which is particularly useful in architectural documentation and cultural heritage preservation. Drones equipped with high-resolution cameras can significantly enhance photogrammetry by capturing images from multiple angles and difficult-to-reach areas, enabling comprehensive 3D models of large or complex sites. Tools like Esri Site Scan for ArcGIS and Pix4D are widely used in photogrammetry, providing robust solutions for processing drone-captured imagery into detailed 3D models.</p>\n\n\n\n<h3 id=\"enhancing_workflows_with_cuda_and_nvidia_rtx\"  class=\"wp-block-heading\">Enhancing workflows with CUDA and NVIDIA RTX<a href=\"#enhancing_workflows_with_cuda_and_nvidia_rtx\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To handle the massive datasets that are typically involved with reality capture, CUDA accelerates the processing of Lidar point clouds and photogrammetric data by leveraging parallel computing, significantly reducing the time required for data conversion, visualization, and analysis. This makes it valuable for high-resolution scanning and 3D reconstruction projects.</p>\n\n\n\n<p>RTX technology enhances the visualization of these 3D models by incorporating ray tracing, which delivers photorealistic lighting, shadows, and reflections. This capability is crucial for creating immersive, high-fidelity visualizations of scanned environments, with tools like Omniverse and Unreal Engine offering RTX-powered rendering for both Lidar and photogrammetry workflows.</p>\n\n\n\n<h3 id=\"nerfs_and_gaussian_splatting\"  class=\"wp-block-heading\">NeRFs and Gaussian splatting<a href=\"#nerfs_and_gaussian_splatting\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>NeRFs<strong> </strong>are transforming 3D scene synthesis by using machine learning to generate highly detailed and realistic views from a vastly reduced number of 2D images compared to traditional photogrammetry. NeRFs can interpolate between sparse data points, creating smooth, photorealistic scenes even from angles that weren\u2019t originally captured.&nbsp;</p>\n\n\n\n<p>This ability to work with fewer images while still delivering exceptional visual fidelity makes NeRFs ideal for applications like architectural visualizations and virtual reality environments. Tools like <a href=\"https://docs.nerf.studio/index.html#\">NeRF Studio</a> enhance the capabilities of NeRFs by enabling developers to add functionalities like semantic embeddings, allowing for more advanced applications and richer interactive experiences.&nbsp;</p>\n\n\n\n<p>Despite their efficiency, NeRFs still require significant computational resources and high-quality images to function effectively, which can limit their practicality for real-time processing or dynamic environments. NVIDIA is advancing NeRF technology with research projects like <a href=\"https://research.nvidia.com/labs/toronto-ai/nerfxl/\">NVIDIA NeRF-XL</a> for large-scale models and<a href=\"https://blogs.nvidia.com/blog/ai-decoded-instant-nerf/\"> NVIDIA Instant-NeRF</a> for accelerated processing, pushing the limits of what&#8217;s possible in reality capture.</p>\n\n\n\n<p>Gaussian splatting is a highly efficient technique for real-time rendering of 3D surfaces or volumes by laying out 2D splats\u2014small, overlapping Gaussian functions\u2014in 2.5D space, enabling smooth, continuous visualizations that balance detail and performance. It excels in scenarios requiring fast and clear visualization of complex 3D point clouds, making it ideal for applications in construction, urban planning, augmented reality, and virtual reality.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-gallery has-nested-images columns-default is-cropped wp-block-gallery-2 is-layout-flex wp-block-gallery-is-layout-flex\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"415\" data-id=\"89727\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/photogrammetric-3d-model-building-625x415.png\" alt=\"Photogrammetry-based 3D model of a building, capturing the main structure but lacking details of the surrounding cityscape and environment, providing limited context.\n\" class=\"wp-image-89727\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/photogrammetric-3d-model-building-625x415.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/photogrammetric-3d-model-building-300x199.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/photogrammetric-3d-model-building-173x115.png 173w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/photogrammetric-3d-model-building-768x510.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/photogrammetric-3d-model-building-645x429.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/photogrammetric-3d-model-building-452x300.png 452w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/photogrammetric-3d-model-building-135x90.png 135w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/photogrammetric-3d-model-building-362x241.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/photogrammetric-3d-model-building-166x110.png 166w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/photogrammetric-3d-model-building-1024x680.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/photogrammetric-3d-model-building.png 1168w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"347\" data-id=\"89728\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gaussian-splatting-3d-model-building-625x347.png\" alt=\"Gaussian splatting 3D model of the same building, including detailed textures and the surrounding cityscape, offering a more comprehensive visualization with full environmental context.\n\" class=\"wp-image-89728\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gaussian-splatting-3d-model-building-625x347.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gaussian-splatting-3d-model-building-300x167.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gaussian-splatting-3d-model-building-179x99.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gaussian-splatting-3d-model-building-768x427.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gaussian-splatting-3d-model-building-645x358.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gaussian-splatting-3d-model-building-500x278.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gaussian-splatting-3d-model-building-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gaussian-splatting-3d-model-building-362x201.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gaussian-splatting-3d-model-building-198x110.png 198w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gaussian-splatting-3d-model-building-1024x569.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gaussian-splatting-3d-model-building.png 1427w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></figure>\n</figure>\n\n\n\n<p class=\"has-text-align-center\"><em>Figure 1. Photogrammetry captures the structure&#8217;s meshes but misses the surroundings and background (left). Gaussian splatting includes the background, providing better visualization for developers and use cases that benefit from full context (right). Photo credit: Ben Stocker, Skender</em></p>\n\n\n\n<p>While it provides exceptional smoothness and speed, there is a trade-off in geometric accuracy, which may limit its use in contexts requiring high fidelity. NVIDIA has advanced this technology with tools like <a href=\"https://research.nvidia.com/labs/avg/publication/fan.cong.etat.cvpr.nri24/\">NVIDIA InstantSplat</a> for rapid 3D reconstruction, NVIDIA 4D-Rotor Gaussian splatting for real-time dynamic scene visualization, and <a href=\"https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/\">NVIDIA Align Your Gaussians (AYG)</a> for generating high-quality 4D visualizations from text descriptions. Supported by the Omniverse platform, these innovations enable efficient, detailed, and real-time visualizations in large-scale projects and dynamic environments, offering significant benefits for architectural visualization, construction monitoring, and digital content creation.&nbsp;</p>\n\n\n\n<p>Startups like <a href=\"https://atomicmaps.io/\">Atomic Maps</a> are pushing the boundaries by integrating Gaussian splats into <a href=\"https://cesium.com/why-cesium/3d-tiles/\">Cesium tiles</a>, providing map-level geographic context that enhances visualization with a comprehensive geographic framework.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"600\" height=\"338\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cesium-splat.gif\" alt=\"A view zooming in from a satellite image to a 3D model of a power transformer.\n\" class=\"wp-image-89760\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Atomic Maps integrates Gaussian splats into Cesium tiles, enhancing geographic context and 3D visualization</em></figcaption></figure>\n\n\n\n<p>These technologies enable more accurate and immersive representations of environments by capturing intricate details and contextual elements that traditional photogrammetry may miss. While photogrammetry excels in precise measurements and surveying, NeRFs and Gaussian splatting offer superior visual fidelity, enabling developers, building owners, and stakeholders to visualize projects with rich background context, such as cityscape views from a building&#8217;s balcony, and to see fine details like phone lines and traffic signs that are often absent in standard photogrammetry. These enhanced visualizations provide a more comprehensive understanding of a project, helping to inform better decisions during the design, planning, and construction phases.</p>\n\n\n\n<h2 id=\"ai_for_reality_capture\"  class=\"wp-block-heading\">AI for reality capture<a href=\"#ai_for_reality_capture\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>AI is transforming reality capture by significantly improving object identification, segmentation, and 3D reconstruction processes. Startups like <a href=\"https://hover.to/\">Hover</a> are leading the charge in using AI to generate detailed 3D models of buildings, enhancing the accuracy and efficiency of structural analysis and categorization.&nbsp;</p>\n\n\n\n<p>NVIDIA Research is advancing segmentation, a critical aspect of reality capture, with the <a href=\"https://research.nvidia.com/labs/dvl/projects/sal/\">SAL (segment anything in Lidar) method</a>, which uses a text-promptable, zero-shot model to segment and classify objects in Lidar data without manual supervision. This streamlines workflows and enables more flexible and scalable segmentation. Tools like <a href=\"https://www.gauzilla.xyz/\">Gauzilla</a> further expand these capabilities by introducing spatial time lapses, which help visualize structural changes over time, offering deeper insights into project development and maintenance needs.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"600\" height=\"337\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gauzilla-timelapse.gif\" alt=\"A 3D time lapse of a building being constructed, growing floor by floor.\n\" class=\"wp-image-89730\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. A Gauzilla timelapse of splats from a Skender Construction site</em></figcaption></figure>\n\n\n\n<p>Companies are increasingly using AI and autonomous robotics to streamline reality capture processes. <a href=\"https://fieldai.com/\">Field AI</a> Field Foundation Models (FFMs) enable autonomous robots to operate in complex, GPS-denied environments, capturing high-quality reality capture data that can be integrated with platforms like <a href=\"http://naska.ai\">Naska.AI</a> through an open partnership model. Naska.AI then uses this data to automate the comparison of laser scans with building information modeling (BIM), highlighting critical information early to reduce costs and prevent schedule overruns, ultimately improving construction accuracy and efficiency.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"324\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan-625x324.png\" alt=\"Screenshot of the NASKA.AI platform displaying a construction site scan, where an automated analysis has identified a discrepancy between the actual construction and the planned BIM model.\n\" class=\"wp-image-89731\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan-625x324.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan-300x155.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan-179x93.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan-768x398.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan-1536x796.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan-645x334.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan-500x259.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan-160x83.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan-362x188.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan-212x110.png 212w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan-1024x531.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/naska-ai-platform-construction-site-scan.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. The Naska.AI platform automatically identifies a construction error by comparing reality capture data with BIM models</em></figcaption></figure>\n\n\n\n<p>NVIDIA is further advancing reality capture with <a href=\"https://developer.nvidia.com/blog/building-spatial-intelligence-from-real-world-3d-data-using-deep-learning-framework-fvdb/\">fVDB</a>, which transforms NeRF and Lidar data into large-scale, AI-ready environments in real time, ideal for urban planning, autonomous systems, and digital twins. <a href=\"https://blogs.nvidia.com/blog/neuralangelo-ai-research-3d-reconstruction/\">Neuralangelo</a>, an AI model from NVIDIA Research, converts 2D video into detailed 3D structures with intricate textures, supporting applications in art, video games, and industrial digital twins.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA development tools enable software developers to significantly accelerate reality capture workflows and embed AI pipelines for object identification, segmentation, classification, and 3D reconstruction. These innovations streamline processes, improve accuracy, and expand the potential of reality capture. With NVIDIA RTX GPU acceleration powered by CUDA, enterprises can now process and visualize reality capture data faster and with greater precision, pushing the boundaries of what&#8217;s possible in architecture and urban development.</p>\n\n\n\n<p>Explore more <a href=\"https://www.nvidia.com/en-us/industries/aec/\">NVIDIA solutions for the AEC industry.</a>&nbsp;</p>\n\n\n\n<h3 id=\"acknowledgments\"  class=\"wp-block-heading\">Acknowledgments<a href=\"#acknowledgments\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><em>Francis Williams, Senior Researcher, NVIDIA</em><br><em>Zan Gojcic, Senior Researcher, NVIDIA</em><br><em>Michael Rubloff, Founder and Managing Editor, Radiancefields.com</em><br><em>Jonathan Stephens, Chief Evangelist and Marketing Director, EveryPoint</em><br><em>Ben Stocker, Senior Construction Technologist, Skender</em><br><em>Michal Gula, Chief Technology Officer, Overhead4D</em><br><em>Chantal Matar, Founder, Studio Chantal Matar</em><br><em>Jim Young, Managing Director, Atomic Maps</em><br><em>Stuart Maggs, CEO, Co Founder Naska.ai</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Reality capture creates highly accurate, detailed, and immersive digital representations of environments. Innovations in site scanning and accelerated data processing, and emerging technologies like neural radiance fields (NeRFs) and Gaussian splatting are significantly enhancing the capabilities of reality capture. These technologies are revolutionizing interactions with and analyses of the physical world.&nbsp; Site scanning, the first &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-reality-capture-workflows-with-ai-and-nvidia-rtx-gpus/\">Continued</a></p>\n", "protected": false}, "author": 2334, "featured_media": 89737, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1496812", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-reality-capture-workflows-with-ai-and-nvidia-rtx-gpus/309051", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 3110, 503], "tags": [453, 3102], "coauthors": [4067, 4068], "class_list": ["post-89719", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-generative-ai", "category-simulation-modeling-design", "tag-featured", "tag-nerf"], "acf": {"post_industry": ["Architecture / Engineering / Construction"], "post_products": ["CUDA", "Omniverse", "RTX GPU"], "post_learning_levels": ["General Interest"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/auto-point-cloud.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nl5", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89719"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2334"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89719"}], "version-history": [{"count": 23, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89719/revisions"}], "predecessor-version": [{"id": 90010, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89719/revisions/90010"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89737"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89719"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89719"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89719"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89719"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89831, "date": "2024-10-07T14:11:06", "date_gmt": "2024-10-07T21:11:06", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89831"}, "modified": "2024-11-14T08:23:01", "modified_gmt": "2024-11-14T16:23:01", "slug": "optimizing-microsoft-bing-visual-search-with-nvidia-accelerated-libraries", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/optimizing-microsoft-bing-visual-search-with-nvidia-accelerated-libraries/", "title": {"rendered": "Optimizing Microsoft Bing Visual Search with NVIDIA Accelerated Libraries"}, "content": {"rendered": "\n<p>Microsoft Bing Visual Search enables people around the world to find content using photographs as queries. The heart of this capability is Microsoft&#8217;s TuringMM visual embedding model that maps images and text into a shared high-dimensional space. Operating on billions of images across the web, performance is critical.&nbsp;</p>\n\n\n\n<p>This post details efforts to optimize the TuringMM pipeline using <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> and NVIDIA acceleration libraries like <a href=\"https://developer.nvidia.com/cv-cuda\">CV-CUDA</a> and <a href=\"https://docs.nvidia.com/cuda/nvimagecodec/\">nvImageCodec</a>. These efforts resulted in a 5.13x speedup and significant TCO reduction. We share how we worked with the Microsoft Bing team to tackle optimization of their core embeddings pipelines that power internet-scale visual search.</p>\n\n\n\n<p>According to Andrew Stewart, PhD and senior data and applied scientist with Microsoft Bing Multimedia, \u201cThe Bing Visual Search team achieved a remarkable 5.13x end-to-end throughput improvement for an offline indexing pipeline running on billions of images using NVIDIA acceleration technology including TensorRT, CV-CUDA, and nvImageCodec, resulting in significant energy and cost savings. For online systems, like Visual Intent from Bing, such improvements mean quicker results for users, or the ability to incorporate additional functionality within the expected latency budget.\u201d </p>\n\n\n\n<h2 id=\"multimodal_ai_and_visual_search\"  class=\"wp-block-heading\">Multimodal AI and visual search<a href=\"#multimodal_ai_and_visual_search\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Multimodal AI powers applications such as visual search. Multimodal AI applications require fusing different data modalities such as text, photo, audio, and video, so that they can interact with one another seamlessly. One popular model for joint image-text understanding is CLIP (Contrastive Language-Image Pretraining). CLIP models employ a dual encoder architecture, one for images and one for text, that consumes hundreds of millions of image-caption pairs.&nbsp;</p>\n\n\n\n<p>The output of each encoder is aligned in a shared high-dimensional space to produce a single high-dimensional vector or an embedding that represents the joint semantic understanding of the input text and image pair. These multimodal embeddings are used to power a wide range of AI-based vision tasks like text-based visual search and retrieval, zero-shot image classification, image captioning and labeling, text-input based content creation/editing, text-assisted content moderation and many more.</p>\n\n\n\n<h2 id=\"microsoft_bing_visual_search\"  class=\"wp-block-heading\">Microsoft Bing Visual Search<a href=\"#microsoft_bing_visual_search\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.bing.com/visualsearch\">Microsoft Bing Visual Search</a> requires computing large-scale vector embeddings for billions of images. Running such an offline system in the cloud or data center can take several weeks to even months to populate the embeddings database. In such cases, where multiple workloads could be running on a shared cluster and resource constraints exist, effective utilization of compute resources and reduction in time taken to complete each workload becomes critical.&nbsp;</p>\n\n\n\n<p>On the other hand, for online tasks like visual search or image captioning, real-time responsiveness through reduced end-to-end latency for each request is a high-priority requirement.&nbsp;</p>\n\n\n\n<h2 id=\"model_and_pipeline_prior_to_optimizations\"  class=\"wp-block-heading\"><strong>Model and pipeline prior to optimizations</strong><a href=\"#model_and_pipeline_prior_to_optimizations\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The visual embeddings model, as with many other models in the Bing infrastructure, runs on a GPU server cluster dedicated to executing inference tasks for a variety of Microsoft\u2019s deep learning services. Within this cluster, the model executes inside an inference server solution on each instance. The inference server receives a batch of images to process from the centralized job dispatcher, and is expected to send back the predicted embeddings for each of these images.</p>\n\n\n\n<p>The workload consists of periodically processing all new images posted to the World Wide Web since the last index. The images to be processed are assembled in request batches of 32 images each, and these batches are sent to the GPU inference servers to be processed. As these images have been collected from the internet from a wide variety of sources, they will vary in size and file format. It is therefore up to the inference task to reformat the images on each batch into a homogenous data layout so that they can be processed in parallel by the model.</p>\n\n\n\n<p>Image processing at this scale is a computationally expensive process. The work starts with reading and decoding the images. The images are in large part found in JPEG format, but they may also include a variety of other common file formats. In addition, image sizes range from small thumbnails to the large professional graphics and high-resolution smartphone photos. The images in the request batch must then all be resized to a common resolution matching the model\u2019s expected input shape of 224&#215;224.&nbsp;</p>\n\n\n\n<p>Furthermore, some basic image preprocessing is applied to the image, consisting of cropping, normalization, and reordering of channels and tensor layout. To deal with the complexity of this wide variety of image specifications, OpenCV was originally chosen to load and process these images.</p>\n\n\n\n<p>The main inference task consists of executing the visual embeddings model on this preprocessed input batch. The model is exported as an ONNX graph and <a href=\"https://github.com/microsoft/onnxruntime\">ONNXRuntime</a> is used as the model execution backend. The ONNXRuntime framework provides a unified set of operations to provide execution compatibility for any model that can be expressed with a standard ONNX opset. The framework provides multiple Execution Providers, which are basically the implementation for each of these operations for a specific hardware platform or accelerator. The default Execution Provider is for CPU execution.</p>\n\n\n\n<p>There are different ways to accelerate ONNX graphs with GPUs. First, ONNX graphs can be run using <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> directly. Second, ONNXRuntime provides two Execution Providers powered by NVIDIA:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html\">CUDA Execution Provider</a>: Offers a general-purpose GPU acceleration solution that provides good performance and flexibility. It is suitable for a wide range of models that conform to the ONNX specification. Bing\u2019s original implementation used ONNXRuntime with CUDA Execution Provider.</li>\n\n\n\n<li><a href=\"https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html\">TensorRT Execution Provider</a>: Offers maximum performance and efficiency for deep learning inference on NVIDIA GPUs by leveraging advanced optimizations like reduced precision and layer fusion. It is ideal for deployment scenarios where performance, throughput, and low latency are critical.\u00a0</li>\n</ul>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"975\" height=\"440\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/original-implementation-bing-visual-embedding-pipeline.png\" alt=\"Schematic diagram showing the original implementation of Bing\u2019s visual embedding pipeline with image decoding and processing steps running on the CPU, and ONNX-Runtime CUDA Execution Provider used for model inference on GPU.\n\" class=\"wp-image-89842\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/original-implementation-bing-visual-embedding-pipeline.png 975w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/original-implementation-bing-visual-embedding-pipeline-300x135.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/original-implementation-bing-visual-embedding-pipeline-625x282.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/original-implementation-bing-visual-embedding-pipeline-179x81.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/original-implementation-bing-visual-embedding-pipeline-768x347.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/original-implementation-bing-visual-embedding-pipeline-645x291.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/original-implementation-bing-visual-embedding-pipeline-500x226.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/original-implementation-bing-visual-embedding-pipeline-160x72.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/original-implementation-bing-visual-embedding-pipeline-362x163.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/original-implementation-bing-visual-embedding-pipeline-244x110.png 244w\" sizes=\"(max-width: 975px) 100vw, 975px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. The original implementation of Bing\u2019s visual embedding pipeline with CPU bottlenecks</em></em></figcaption></figure>\n\n\n\n<h2 id=\"optimizing_the_visual_embeddings_model_pipeline\"  class=\"wp-block-heading\"><strong>Optimizing the visual embeddings model pipeline</strong><a href=\"#optimizing_the_visual_embeddings_model_pipeline\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>We collaborated with the Microsoft Bing Visual Search team to find optimization opportunities in this pipeline. Several possibilities immediately become apparent as providing beneficial performance upside by making better use of the GPU resources.</p>\n\n\n\n<p>The first and obvious opportunity was with the use of TensorRT. Recent versions of TensorRT have added ever-improving support for fused attention layers in transformer architectures. This enables more efficient execution of these computationally expensive layers, which significantly impacts the end-to-end inference performance.</p>\n\n\n\n<p>Decoding the JPEG file format is not often given much thought. However, when dealing with models at scale that have been largely optimized, the loading and decoding of a single megapixel image can often be longer than the inference task itself for that image. Bing\u2019s visual embeddings pipeline was no exception to this, as we could measure significant bottlenecks originating from OpenCV\u2019s <code>imread</code> call, the function that implements image format decoding.</p>\n\n\n\n<p>To alleviate this, we introduced to the pipeline nvImageCodec, the NVIDIA library of image format decoders. This library makes use of GPUs to decode a large variety of file formats. When a dedicated hardware-accelerated decoder is not available, the library transparently falls back to traditional software GPU-accelerated decoding, and if that is not available either, to CPU-accelerated decoding.&nbsp;</p>\n\n\n\n<p>This enables maintaining compatibility and seamless integration across formats, which is essential when dealing with raw image data collected from the internet. Finally, nvImageCodec also supports batch decoding, which enables decoding multiple images simultaneously so some key operations can be parallelized during the decoding stage. This maximizes GPU efficiency.</p>\n\n\n\n<p>The preprocessing of the images themselves was also accelerated with <a href=\"https://developer.nvidia.com/cv-cuda\">CV-CUDA</a>, the NVIDIA library for the GPU-accelerated implementation of common image processing operations. These operations have been highly optimized to work with image batches: the larger the number of images to process, the greater will be the efficiency of these operations.&nbsp;</p>\n\n\n\n<p>Some of the CV-CUDA operations support variable shape image batches. This means that although images may not all conform to a single image size, they can still be batched together and the batch parallelization can be exploited on such collections. Furthermore, the integration of CV-CUDA with nvImageCodec, enables processing the image data directly on GPU buffers, avoiding the need to copy the data back and forth to the host CPU.</p>\n\n\n\n<p>The final implementation was largely simplified due to the Python bindings available in both libraries. The Python APIs for both nvImageCodec and CV-CUDA follow the same conventions and function names as the OpenCV API, for users that may already be familiar with it. The code for these operations is as follows:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ndecoder = nvimgcodec.Decoder\nimages = decoder.decode(batch_queries)\n \nimages_batch = cvcuda.ImageBatchVarShape(len(images))\nfor image in images:\n    images_batch.pushback(cvcuda.as_image(image))\nimages_batch = cvcuda.resize(images_batch, &#x5B;(256, 256)] * len(images), cvcuda.Interp.LINEAR)\n \nstack = cvcuda.stack(&#x5B;cvcuda.as_tensor(image) for image in images_batch])\nstack = cvcuda.customcrop(stack, cvcuda.RectI(16, 16, 224, 224))\nstack = cvcuda.cvtcolor(stack, cvcuda.ColorConversion.RGB2BGR)\nstack = cvcuda.convertto(stack, np.float32)\nstack = cvcuda.reformat(stack, &quot;NCHW&quot;)\nstack = stack.cuda\n</pre></div>\n\n\n<p>Finally, ONNXRuntime has the ability to use IOBindings. This is a feature in the framework that allows pre-allocating and filling memory buffers on the target accelerator directly. In the case of ONNXRuntime-CUDA and ONNXRuntime-TensorRT, this enables providing the model\u2019s input batch data directly to a pre-allocated GPU memory buffer, again avoiding the need for additional memory copies between the CPU and GPU.</p>\n\n\n\n<p>The optimized pipeline fully offloads the majority of the inference task to the GPU device, allowing for more optimal and faster processing, but also for greater power efficiency for the end-to-end task.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"975\" height=\"440\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-optimized-implementation-bing-visual-embedding-pipeline.png\" alt=\"Schematic diagram showing the GPU-optimized implementation of Bing\u2019s visual embedding pipeline with image decoding and processing steps now running on the GPU using CV-CUDA and nvImageCodec, and ONNX-Runtime TensorRT Execution Provider used for optimized model inference on GPU.\" class=\"wp-image-89843\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-optimized-implementation-bing-visual-embedding-pipeline.png 975w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-optimized-implementation-bing-visual-embedding-pipeline-300x135.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-optimized-implementation-bing-visual-embedding-pipeline-625x282.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-optimized-implementation-bing-visual-embedding-pipeline-179x81.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-optimized-implementation-bing-visual-embedding-pipeline-768x347.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-optimized-implementation-bing-visual-embedding-pipeline-645x291.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-optimized-implementation-bing-visual-embedding-pipeline-500x226.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-optimized-implementation-bing-visual-embedding-pipeline-160x72.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-optimized-implementation-bing-visual-embedding-pipeline-362x163.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/gpu-optimized-implementation-bing-visual-embedding-pipeline-244x110.png 244w\" sizes=\"(max-width: 975px) 100vw, 975px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. The GPU-optimized implementation of Bing\u2019s visual embedding pipeline leveraging ONNX-Runtime TRT, CV-CUDA, and nvImageCodec</em></em></figcaption></figure></div>\n\n\n<h2 id=\"results\"  class=\"wp-block-heading\"><strong>Results</strong><a href=\"#results\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The results obtained show how optimal use of GPU resources can largely accelerate deep learning and image processing workloads, even when the baseline is already using GPU acceleration. <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> was responsible for most of the performance improvement, and NVIDIA image decoding and processing libraries were ultimately responsible for an additional 27% end-to-end improvement.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table class=\"has-fixed-layout\"><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Implementation</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Throughput</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Speedup</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Baseline using OpenCV + ONNXRuntime-CUDA</td><td class=\"has-text-align-center\" data-align=\"center\">88 QPS</td><td class=\"has-text-align-center\" data-align=\"center\">&#8211;</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Pipeline using OpenCV + ONNXRuntime-TensorRT</td><td class=\"has-text-align-center\" data-align=\"center\">356 QPS</td><td class=\"has-text-align-center\" data-align=\"center\">4.05</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Pipeline using nvImageCodec + CV-CUDA + ONNXRuntime-TensorRT</td><td class=\"has-text-align-center\" data-align=\"center\">452 QPS</td><td class=\"has-text-align-center\" data-align=\"center\">5.14</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. End-to-end throughput speedup of 5.14x achieved using NVIDIA acceleration libraries and engines</em></figcaption></figure>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"640\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-throughput-chart.png\" alt=\"Bar chart of end-to-end throughput speedup from incremental improvements due to GPU acceleration of the different stages in the pipeline.\" class=\"wp-image-89894\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-throughput-chart.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-throughput-chart-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-throughput-chart-625x391.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-throughput-chart-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-throughput-chart-768x480.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-throughput-chart-645x403.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-throughput-chart-480x300.png 480w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-throughput-chart-144x90.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-throughput-chart-362x226.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/pipeline-throughput-chart-176x110.png 176w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. End-to-end throughput speedup from incremental improvements due to GPU acceleration of the different stages in the pipeline</em></em></figcaption></figure></div>\n\n\n<p>The image processing component of the pipeline contributes considerable latency to the end-to-end inference request. The fact that images of different sizes are used in such inference tasks often results in variable latency numbers, as the time to decode and resize images is proportional to image resolution. Accelerating this process can provide significant performance improvements, resulting in up to 6x speedups, using hardware specifications such as those used in the deployment by Bing.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table class=\"has-fixed-layout\"><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\"></td><td class=\"has-text-align-center\" data-align=\"center\"></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"3\"><strong>Average image size per batch</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Library</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Process</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Small</strong><strong><br></strong>(~400&#215;400)</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Medium</strong><strong><br></strong>(~800&#215;800)</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Large</strong><strong><br></strong>(~1600&#215;1600)</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"3\">OpenCV<br>Single-threaded CPU</td><td class=\"has-text-align-center\" data-align=\"center\">Image decode</td><td class=\"has-text-align-center\" data-align=\"center\">28.4 ms</td><td class=\"has-text-align-center\" data-align=\"center\">162.3 ms</td><td class=\"has-text-align-center\" data-align=\"center\">406.2 ms</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Preprocessing</td><td class=\"has-text-align-center\" data-align=\"center\">6.3 ms</td><td class=\"has-text-align-center\" data-align=\"center\">14.4 ms</td><td class=\"has-text-align-center\" data-align=\"center\">24.0 ms</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Total</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>34.7 ms</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>176.7 ms</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>430.2 ms</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"3\">nvImageCodec + CV-CUDA<br>GPU-accelerated</td><td class=\"has-text-align-center\" data-align=\"center\">Image decode</td><td class=\"has-text-align-center\" data-align=\"center\">5.9 ms</td><td class=\"has-text-align-center\" data-align=\"center\">29.4 ms</td><td class=\"has-text-align-center\" data-align=\"center\">62.7 ms</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Preprocessing</td><td class=\"has-text-align-center\" data-align=\"center\">3.2 ms</td><td class=\"has-text-align-center\" data-align=\"center\">5.2 ms</td><td class=\"has-text-align-center\" data-align=\"center\">6.3 ms</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Total</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>9.1 ms</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>34.6 ms</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>69.0 ms</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">GPU acceleration speedup</td><td class=\"has-text-align-center\" data-align=\"center\">&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>3.8x</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>5.1x</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>6.2x</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Image processing stage accelerated up to 6.2x speedup using the CV-CUDA and nvImageCodec libraries</em></figcaption></figure>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA accelerated libraries, including TensorRT, CV-CUDA, and nvImageCodec, significantly optimized Microsoft Bing Visual Search. The pipeline\u2019s initial implementation used ONNXRuntime with CUDA for GPU acceleration, but bottlenecks in image decoding and preprocessing (through OpenCV) limited performance. NVIDIA accelerated libraries were introduced to improve both image decoding and model inference speeds.</p>\n\n\n\n<p>The introduction of TensorRT, nvImageCodec for decoding, and CV-CUDA for image preprocessing resulted in the 5.13x speedup. These improvements reduced the image processing time by up to 6.2x in some cases. These optimizations enhanced the system\u2019s throughput and allowed Bing to process visual search tasks more efficiently, reducing energy usage and processing times significantly.</p>\n\n\n\n<p><a href=\"https://build.nvidia.com/nvidia/nvclip\">NV-CLIP</a> is an NVIDIA commercial version of the CLIP model available as an<a href=\"https://www.nvidia.com/en-us/ai/\"> NVIDIA NIM microservice</a>. NIM microservices provide models as optimized containers to deploy in the cloud, data centers, workstations, desktops, and laptops. Each NIM container includes the<a href=\"https://blogs.nvidia.com/blog/what-is-a-pretrained-ai-model/\"> pretrained AI models</a> and all the necessary runtime components, making it simple to integrate AI capabilities into applications. <a href=\"https://build.nvidia.com/nvidia/nvclip\">Get started with NV-CLIP</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Microsoft Bing Visual Search enables people around the world to find content using photographs as queries. The heart of this capability is Microsoft&#8217;s TuringMM visual embedding model that maps images and text into a shared high-dimensional space. Operating on billions of images across the web, performance is critical.&nbsp; This post details efforts to optimize the &hellip; <a href=\"https://developer.nvidia.com/blog/optimizing-microsoft-bing-visual-search-with-nvidia-accelerated-libraries/\">Continued</a></p>\n", "protected": false}, "author": 2345, "featured_media": 89876, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1496797", "discourse_permalink": "https://forums.developer.nvidia.com/t/optimizing-microsoft-bing-visual-search-with-nvidia-accelerated-libraries/309045", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 852], "tags": [453, 3739, 562, 4142, 4180], "coauthors": [4078, 2770, 4079, 4080, 3335, 3336], "class_list": ["post-89831", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-data-center-cloud", "tag-featured", "tag-nim", "tag-onnx", "tag-recommenders-personalization", "tag-rtx-ai"], "acf": {"post_industry": ["Cloud Services"], "post_products": ["CV-CUDA", "NIM", "TensorRT"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/microsoft-bing-visual-search-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nmT", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89831"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2345"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89831"}], "version-history": [{"count": 14, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89831/revisions"}], "predecessor-version": [{"id": 90005, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89831/revisions/90005"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89876"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89831"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89831"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89831"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89831"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89773, "date": "2024-10-07T13:00:00", "date_gmt": "2024-10-07T20:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89773"}, "modified": "2024-10-17T11:19:13", "modified_gmt": "2024-10-17T18:19:13", "slug": "generate-image-and-text-embeddings-with-nv-clip", "status": "publish", "type": "post", "link": "https://nvda.ws/47To1Ac", "title": {"rendered": "Generate Image and Text Embeddings with NV-CLIP"}, "content": {"rendered": "\n<p>NV-CLIP, a cutting-edge multimodal embeddings model for image and text, is now generally available.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NV-CLIP, a cutting-edge multimodal embeddings model for image and text, is now generally available.</p>\n", "protected": false}, "author": 1466, "featured_media": 89780, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1496778", "discourse_permalink": "https://forums.developer.nvidia.com/t/generate-image-and-text-embeddings-with-nv-clip/309043", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/47To1Ac", "_links_to_target": "_blank"}, "categories": [2724], "tags": [453, 3739], "coauthors": [2968], "class_list": ["post-89773", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "tag-featured", "tag-nim"], "acf": {"post_industry": ["Manufacturing", "Retail / Consumer Packaged Goods", "Smart Cities / Spaces"], "post_products": ["NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Announcement"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nv-clip-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nlX", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89773"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1466"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89773"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89773/revisions"}], "predecessor-version": [{"id": 89775, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89773/revisions/89775"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89780"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89773"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89773"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89773"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89773"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89474, "date": "2024-10-07T09:00:00", "date_gmt": "2024-10-07T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89474"}, "modified": "2024-10-17T12:06:56", "modified_gmt": "2024-10-17T19:06:56", "slug": "producing-cinematic-content-at-scale-with-a-generative-ai-enabled-openusd-pipeline", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/producing-cinematic-content-at-scale-with-a-generative-ai-enabled-openusd-pipeline/", "title": {"rendered": "Producing Cinematic Content at Scale with a Generative AI-Enabled OpenUSD Pipeline"}, "content": {"rendered": "\n<p>Producing commercials is resource-intensive, requiring physical locations and various props and setups to display products in different settings and environments for more accurate consumer targeting. This traditional process is not only expensive and time-consuming but also can be destructive to the physical environment. It leaves you with no ability to capture a new angle after you return home.&nbsp;</p>\n\n\n\n<p>More organizations are overcoming these obstacles by <a href=\"https://www.nvidia.com/en-us/use-cases/3d-product-configurator/\">developing 3D configurator solutions</a> that provide enhanced flexibility and creativity. Developed with <a href=\"https://www.nvidia.com/en-us/omniverse/usd/\">Universal Scene Description (OpenUSD)</a> and generative AI, <a href=\"http://www.nvidia.com/en-us/glossary/product-configurator/\">product configurator</a> solutions make creating hyper-personalized advertising content at scale a reality for advertising agencies.</p>\n\n\n\n<p>To show how generative AI-enabled configurators can enhance complex design pipelines, the NVIDIA creative team developed a tool called CineBuilder on <a href=\"https://www.nvidia.com/en-us/omniverse/\">NVIDIA Omniverse</a>. Using OpenUSD and generative AI, CineBuilder streamlines the composition, generation, and production of high-quality, cinematic content for use cases such as car commercials.</p>\n\n\n\n<p>This workflow can be used from the initial scaling of concepts to fully configured 30-second commercials. With the workflow, creative teams can tailor commercials to audiences based on lifestyle, preferred accessories, car color, and even regions and time of day based on weather.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/dyciB0h-vmA?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Develop Next-Generation 3D Product Configurators with OpenUSD and Omniverse APIs</em></figcaption></figure>\n\n\n\n<h2 id=\"developing_a_modular_scalable_pipeline_with_openusd\"  class=\"wp-block-heading\">Developing a modular, scalable pipeline with OpenUSD<a href=\"#developing_a_modular_scalable_pipeline_with_openusd\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Built as an extension to Omniverse, CineBuilder facilitates the easy swapping and reconfiguration of various parts of a digital asset or cinematic scene.&nbsp;</p>\n\n\n\n<p>What makes this possible is the OpenUSD-based development pipeline, which enables teams to segment digital assets into distinct components\u2014such as models, materials, and environments\u2014each on separate layers. With these layers established, teams can use CineBuilder to interchange different elements seamlessly, without affecting the overall configuration, all within a unified digital environment.</p>\n\n\n\n<p>To make CineBuilder a powerful, fast, and accurate scene construction engine, the development team structured the data within the OpenUSD pipeline using highly organized patterns and hierarchies. The modular data structure that OpenUSD enables makes it easy to break up assets in numerous ways and then use the composition engine to bring them together in a customized composition.</p>\n\n\n\n<p>The OpenUSD pipeline also makes it possible to manipulate and change data quickly and non-destructively without affecting the work of other team members.</p>\n\n\n\n<h2 id=\"integrating_generative_ai_to_accelerate_production\"  class=\"wp-block-heading\">Integrating generative AI to accelerate production<a href=\"#integrating_generative_ai_to_accelerate_production\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Automating repetitive tasks enables mass production and the management of customized content, ultimately freeing artists to focus more on creative work. The result is a much faster, more efficient production pipeline that requires fewer resources and less time.</p>\n\n\n\n<p>To automate various aspects of the production process, the team integrated numerous generative AI functionalities into the workflow. These functionalities can serve as AI assistants for artists, supporting tasks such as creating diverse virtual sets, generating supporting assets, and producing lighting scenarios.&nbsp;</p>\n\n\n\n<p>Using the <a href=\"https://blogs.nvidia.com/blog/ai-decoded-edify/\">NVIDIA Edify</a> NIM microservice for text-to-360 HDRi, artists can create skydomes to light their scenes from simple text prompts, such as &#8220;blue summer sky.&#8221; The result is infinite lighting scenarios.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"546\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360-1024x546.png\" alt=\"Screenshot shows the UX and the background generated using the prompt \u201cblue summer sky\u201d to generate part of a scene.\" class=\"wp-image-89491\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360-1024x546.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360-300x160.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360-625x333.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360-179x95.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360-768x410.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360-1536x819.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360-645x344.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360-500x267.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360-160x85.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360-362x193.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360-206x110.png 206w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-edify-nim-text-to-360.png 2000w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The NVIDIA Edify NIM microservice for text-to-360 HDRi in action</em></figcaption></figure></div>\n\n\n<p>With Edify-powered text-to-3D, artists can dress a set more efficiently by generating supporting assets for their environments. For a car commercial targeted at outdoor adventurers, these generated objects could be trees or cottages along the side of an alpine road.</p>\n\n\n\n<p>Generative AI also contributes to production processes by giving the Omniverse Viewport eyes, ears, and a brain. Using custom-developed AI agents connecting Omniverse to ChatGPT, the team can use voice commands to control tasks such as zooming in and out and setting the f-stop for a shot.</p>\n\n\n\n<p><a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/multimodal/mllm/neva.html\">NeVA Machine Vision</a> provides LLM vision assistance to naturally describe what is seen on screen. The integration of <a href=\"https://arxiv.org/abs/2201.12086\">BLIP Vision</a> enables real-time object detection so that users can manipulate a scene with voice commands, such as &#8220;rotate the couple standing on the side of the road.&#8221;</p>\n\n\n\n<p>Finally, generative AI is used to customize and tailor the script to the unique elements of each configuration and target market.</p>\n\n\n\n<h2 id=\"delivering_high-quality_content_at_scale\"  class=\"wp-block-heading\">Delivering high-quality content at scale<a href=\"#delivering_high-quality_content_at_scale\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To demonstrate the power of their configurator solution, NVIDIA\u2019s creative team set out to test the scale of generating 30-second car commercials. The result was 630 unique commercial configurations at 4K resolution and 60 FPS. That\u2019s 315 minutes of unique CG-rendered content, equivalent to seven feature films at 24 FPS.</p>\n\n\n\n<p>The following videos show some of the unique configurations that were produced:</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/c-W3zhfehZo?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 2. Customer Commercial Configurator for Automotive &#8211; Rocky Mountains Variant</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/FrMul1k4z-0?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 3. Customer Commercial Configurator for Automotive &#8211; Coastal Variant</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/p-XYYB_mbzM?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 4. Customer Commercial Configurator for Automotive &#8211; Alps Variant</em></figcaption></figure>\n\n\n\n<h2 id=\"best_practices_to_develop_a_configurator_solution\"  class=\"wp-block-heading\">Best practices to develop a configurator solution<a href=\"#best_practices_to_develop_a_configurator_solution\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Developing <a href=\"https://www.nvidia.com/en-us/use-cases/3d-product-configurator/\">product configurator solutions</a> is a complex process involving multiple strategic steps, from defining the scope and objectives to deployment and integration.<strong> </strong>Here are some general steps to help you get started:</p>\n\n\n\n<p><strong>Define the scope and objectives: </strong>This is crucial for setting clear goals and expectations for the configurator&#8217;s capabilities, particularly in handling multiple commercial variants efficiently.</p>\n\n\n\n<p><strong>Establish a modular pipeline: </strong>Configurator solutions should be both flexible and scalable. That\u2019s why it was important to develop pipelines that could handle numerous assets and elements that could be swapped or reconfigured dynamically. The use of OpenUSD plays a critical role here, as it enables the team to manage complex scenes and assets in a non-destructive and efficient manner.&nbsp;</p>\n\n\n\n<p><strong>Integrate with Omniverse: </strong>This provides the essential&nbsp;environment and tools necessary for real-time collaboration and simulation. Omniverse supports the seamless exchange of USD files and facilitates real-time updates and changes across different teams and stages of the production pipeline.&nbsp;</p>\n\n\n\n<p><strong>Build the configurator logic: </strong>The core of the configurator involves developing the logic that enables users to customize and generate commercial variants. Users can swap out elements such as environments, car models, and other accessories based on their inputs or predefined scenarios.</p>\n\n\n\n<p><strong>Design the user interface and experience: </strong>Designing an intuitive user interface is crucial for ensuring that the configurator can be easily used by artists without technical expertise. The interface should provide users with clear options for customization and real-time previews of their work.</p>\n\n\n\n<p><strong>Test and refine: </strong>Before deployment, the configurator underwent rigorous testing to identify any bugs or performance issues. The user experience was refined based on feedback from test users, ensuring that the final product is both robust and user-friendly.</p>\n\n\n\n<p><strong>Deploy and Integrate: </strong>The configurator can be deployed in a commercial setting, integrating it with existing marketing and advertising workflows. This phase requires close collaboration with end-users to ensure that the configurator meets their needs and can be seamlessly incorporated into their daily operations.</p>\n\n\n\n<h2 id=\"learn_more_about_generative_ai_and_openusd\"  class=\"wp-block-heading\">Learn more about generative AI and OpenUSD<a href=\"#learn_more_about_generative_ai_and_openusd\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The pipeline and custom-built tools that use OpenUSD enable the efficient creation of unique variations of commercials. Working this way, you can easily change car materials, parts, accessories, and the environment of the 3D scene in seconds. Generative AI further enhances these workflows by enabling the creation of infinite lighting scenarios and supporting assets from simple text prompts.</p>\n\n\n\n<p>To get started developing your own product configurator solution, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://resources.nvidia.com/en-us-omniverse-product-configurator/ov-product-configura\">Product Configurator Reference Architecture</a></li>\n\n\n\n<li><a href=\"https://docs.omniverse.nvidia.com/auto-config/latest/index.html\">End-to-End Configurator Example Guide</a></li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/use-cases/3d-product-configurator/\">Use Cases: 3D Product Configurators</a></li>\n\n\n\n<li><a href=\"https://www.youtube.com/watch?v=fia6X2lC3-k\">OpenUSD for Marketing and Advertising feat. WPP</a> livestream</li>\n</ul>\n\n\n\n<p><em>Stay up to date by subscribing to </em><a href=\"https://nvda.ws/3u5KPv1\"><em>NVIDIA news</em></a><em> and following NVIDIA Omniverse on </em><a href=\"https://discord.com/channels/827959428476174346/828737081479004230\"><em>Discord</em></a><em>, </em><a href=\"https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA\"><em>YouTube</em></a><em>, and Medium.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Producing commercials is resource-intensive, requiring physical locations and various props and setups to display products in different settings and environments for more accurate consumer targeting. This traditional process is not only expensive and time-consuming but also can be destructive to the physical environment. It leaves you with no ability to capture a new angle after &hellip; <a href=\"https://developer.nvidia.com/blog/producing-cinematic-content-at-scale-with-a-generative-ai-enabled-openusd-pipeline/\">Continued</a></p>\n", "protected": false}, "author": 2343, "featured_media": 89478, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1496703", "discourse_permalink": "https://forums.developer.nvidia.com/t/producing-cinematic-content-at-scale-with-a-generative-ai-enabled-openusd-pipeline/309029", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 3110, 503], "tags": [453, 3700], "coauthors": [4076, 3934, 2762], "class_list": ["post-89474", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-graphics", "category-generative-ai", "category-simulation-modeling-design", "tag-featured", "tag-openusd"], "acf": {"post_industry": ["Media & Entertainment"], "post_products": ["Omniverse"], "post_learning_levels": ["General Interest"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cinematic-content-at-scale-openusd-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nh8", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89474"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2343"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89474"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89474/revisions"}], "predecessor-version": [{"id": 89824, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89474/revisions/89824"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89478"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89474"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89474"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89474"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89474"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89703, "date": "2024-10-07T05:00:00", "date_gmt": "2024-10-07T12:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89703"}, "modified": "2024-10-17T12:06:57", "modified_gmt": "2024-10-17T19:06:57", "slug": "real-time-surgical-guidance-by-fusing-multi-modal-imaging-with-nvidia-holoscan", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/real-time-surgical-guidance-by-fusing-multi-modal-imaging-with-nvidia-holoscan/", "title": {"rendered": "Real-Time Surgical Guidance by Fusing Multi-Modal Imaging with NVIDIA Holoscan"}, "content": {"rendered": "\n<p>Developers in the fields of image-guided surgery and surgical vision face unique challenges in creating systems and applications that can significantly improve surgical workflows. One such challenge is efficiently combining multi-modal imaging data, such as preoperative 3D patient images with intra-operative video. This is key to providing surgeons with real-time, accurate guidance during minimally invasive or robotic-assisted procedures.&nbsp;</p>\n\n\n\n<p>In this post, we walk you through the use of state-of-the-art AI and imaging techniques, with a highlight of\u00a0 <a href=\"https://www.imfusion.com/\">ImFusion</a>\u2019s integration of <a href=\"https://www.nvidia.com/en-us/clara/medical-devices/\">NVIDIA Holoscan</a> for real-time sensor processing, AI, and I/O. We explore how NVIDIA Holoscan enabled us to double the pipeline performance and we explain how combining two imaging modalities can contribute to enhanced surgical accuracy, reduced complications, and better outcomes.\u00a0\u00a0</p>\n\n\n\n<h2 id=\"challenges_in_combining_multi-modal_surgical_data\"  class=\"wp-block-heading\">Challenges in combining multi-modal surgical data<a href=\"#challenges_in_combining_multi-modal_surgical_data\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>During minimally invasive or robotic image-assisted surgery, accurate navigation and detailed understanding of the patient\u2019s anatomy are crucial to the success of a surgical procedure.&nbsp;</p>\n\n\n\n<p>During preoperative planning, surgeons often rely on multi-modal imaging techniques, including 3D diagnostic imaging modalities, such as computed tomography (CT) scans, to identify abnormalities, designate target zones, and pinpoint critical structures such as blood vessels.&nbsp;</p>\n\n\n\n<p>However, combining these preoperative 3D image datasets with intra-operative video seamlessly during live surgical procedures remains a significant challenge, as surgeons often lack adequate access to this valuable preoperative data during surgery.&nbsp;</p>\n\n\n\n<p>The next generation of medical devices serving as procedural guidance in the operating room requires applications that use pre\u2013 and intra-operative multi-modal data. These must simultaneously execute multiple computationally intensive pipelines and perform the following functions:<br></p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Track anatomical structures in real-time: </strong>Accurately monitor the surface of targeted tissues and organs during the procedure.</li>\n\n\n\n<li><strong>Fuse preoperative and intra-operative data:</strong> Seamlessly blend 3D preoperative images with live surgical video feeds.</li>\n\n\n\n<li><strong>Provide low-latency visualization:</strong> Deliver fused, multi-modal information during surgery with sub-100 ms end-to-end latency for adequate hand-eye coordination and real-time decision-making.\u00a0</li>\n</ul>\n\n\n\n<p>This requires a unique combination of AI, accelerated computing, and advanced visualization capabilities.&nbsp;</p>\n\n\n\n<p>NVIDIA Holoscan is a domain-specific computing platform that delivers an accelerated, full-stack infrastructure required for scalable, software-defined, and real-time processing of streaming data at the clinical edge. It includes a library of reference applications to jumpstart the developer\u2019s timeline to build and optimize their own AI applications for production deployment.\u00a0</p>\n\n\n\n<p>NVIDIA partners contribute to this library for the Holoscan AI sensor processing community to share applications, enabling you to reuse and contribute components and sample applications, foster innovation, and accelerate the development of advanced medical devices.&nbsp;</p>\n\n\n\n<h2 id=\"real-time_3d_surgical_guidance_fusing_pre-op_and_live_data&nbsp;\"  class=\"wp-block-heading\">Real-time 3D surgical guidance fusing pre-op and live data&nbsp;<a href=\"#real-time_3d_surgical_guidance_fusing_pre-op_and_live_data&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To address the live data limitation, <a href=\"https://www.imfusion.com/\">ImFusion</a>, a Munich-based company, and <a href=\"https://www.nvidia.com/en-us/programs/isv/\">NVIDIA Connect Program</a> member, used <a href=\"https://www.nvidia.com/en-us/clara/holoscan/\">NVIDIA Holoscan</a> to create a system that can integrate preoperative data with real-time intra-operative imaging.\u00a0</p>\n\n\n\n<p>The novel system tracks the surface of a targeted anatomical structure in the form of a 3D mesh\u2014a digital model that accurately depicts the structure\u2019s shape and contours\u2014and blends it smoothly into the surgeon\u2019s view. The mesh is extracted from a preoperative CT scan and then overlaid in near real-time onto the live video feed from a laparoscopic camera.&nbsp;</p>\n\n\n\n<p>This enables surgeons to visualize the blended intra-operative and preoperative patient information and make more informed real-time decisions.</p>\n\n\n\n<h2 id=\"using_nvidia_holoscan_and_nvidia_igx_for_real-time_surgical_data_fusion\"  class=\"wp-block-heading\">Using NVIDIA Holoscan and NVIDIA IGX for real-time surgical data fusion<a href=\"#using_nvidia_holoscan_and_nvidia_igx_for_real-time_surgical_data_fusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>ImFusion\u2019s solution is built on their proprietary ImFusion SDK, which bundles algorithms for image processing, registration, analysis, and visualization. Integrating NVIDIA Holoscan into the ImFusion SDK unlocked new levels of performance, efficiency, and flexibility.&nbsp;</p>\n\n\n\n<p>As Alexander Ladikos, head of Computer Vision at ImFusion explained, &#8220;Integrating Holoscan into our ImFusion SDK has helped us achieve near real-time performance, crucial for surgical applications. It has accelerated our development process, saving us time and allowing us to reuse existing and custom operators for future projects.&#8221;</p>\n\n\n\n<p>This integration enabled ImFusion to build and run low-latency, AI-enhanced, sensor-streaming applications, setting the stage for next-generation software as a medical device (SaMD) that enables surgeons to simultaneously view live and fused preoperative data.&nbsp;</p>\n\n\n\n<p>At the core of ImFusion\u2019s system are three key neural networks, each using Holoscan acceleration capabilities [AND OPEN SOURCE LIBRARIES OF REF APPS]:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Stereo Depth Estimation:</strong> This network generates depth information from endoscopic stereo video frames, using a state-of-the-art CNN-based model trained on synthetic data. Holoscan\u2019s real-time processing capabilities enabled instant depth estimation from video streams, providing crucial spatial information for surgical guidance.\u00a0</li>\n\n\n\n<li><strong>Optical Flow Estimation:</strong> Calculating 2D pixel displacements between frames, this network ensures robust performance across various surgical scenarios. Holoscan enabled rapid 2D flow estimation for subsequent projection into 3D space, enhancing the system\u2019s ability to track movement within the surgical field.\u00a0</li>\n\n\n\n<li><strong>Segmentation: </strong>Developed by the <a href=\"https://www.orsi-online.com/\">ORSI Academy,</a> a global leading robotic surgery training institute based in Belgium, <a href=\"https://github.com/nvidia-holoscan/holohub/tree/main/applications/orsi\">this deep learning segmentation model</a> identifies surgical instruments and target tissue, crucial for accurate tracking and overlay. Holoscan enabled quick analysis of 3D flow estimates in segmented tissue regions, so the system could precisely identify and track specific anatomical structures and instruments in real time.\u00a0</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"794\" height=\"676\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/models-estimage-depth-optical-flow-segmentation.png\" alt=\"Flow chart diagram shows the algorithmic components that track the target tissue's surface mesh using the endoscope's left and right image streams. Together with the initial pose for the registered surface mesh, these results are used to estimate tissue odometry, transform the mesh, project it to 2d, and visualize the camera stream with the mesh overlay.\" class=\"wp-image-89716\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/models-estimage-depth-optical-flow-segmentation.png 794w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/models-estimage-depth-optical-flow-segmentation-300x255.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/models-estimage-depth-optical-flow-segmentation-625x532.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/models-estimage-depth-optical-flow-segmentation-135x115.png 135w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/models-estimage-depth-optical-flow-segmentation-768x654.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/models-estimage-depth-optical-flow-segmentation-645x549.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/models-estimage-depth-optical-flow-segmentation-352x300.png 352w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/models-estimage-depth-optical-flow-segmentation-106x90.png 106w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/models-estimage-depth-optical-flow-segmentation-362x308.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/models-estimage-depth-optical-flow-segmentation-129x110.png 129w\" sizes=\"(max-width: 794px) 100vw, 794px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Three models estimate depth, optical flow, and segmentation of the target tissue</em></figcaption></figure></div>\n\n\n<p>Building upon these three neural networks, ImFusion\u2019s system achieves impressive real-time performance.&nbsp;</p>\n\n\n\n<p>A surface mesh from a preoperative CT is manually registered with the underlying anatomical structure and then tracked automatically throughout the procedure. Using an NVIDIA IGX Developer Kit equipped with an NVIDIA RTX 6000 Ada GPU, the system achieves a median frame rate of ~13.5 Hz and an end-to-end latency below 75ms.&nbsp;</p>\n\n\n\n<p>While this latency continues to be optimized further, it represents a significant performance improvement, with Holoscan flow benchmarking showing a 50% reduction in end-to-end latency compared to previous hardware configurations and before\u00a0 NVIDIA TensorRT AI model inference optimization.\u00a0</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-1024x576.png\" alt=\"Screenshot of\u00a0the Holoscan tissue tracking application. Within the ImFusion application, using the Holoscan tissue tracking application. the surface mesh is colored magenta, does not occlude any surgical instruments, and fits to the underlying target tissue.\" class=\"wp-image-89715\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-1024x576.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surface-mesh-tracking-endoscopic-feed.png 1920w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>FIGURE 2. Surface mesh tracking in the endoscopic video feed</em></figcaption></figure></div>\n\n\n<h2 id=\"enhanced_surgical_navigation_and_accelerated_development\"  class=\"wp-block-heading\">Enhanced surgical navigation and accelerated development<a href=\"#enhanced_surgical_navigation_and_accelerated_development\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This high level of performance is crucial for live surgical applications, as it enables surgeons to receive instantaneous visual feedback and provides an unprecedented view of the surgical scene.&nbsp;</p>\n\n\n\n<p>ORSI Academy, Europe\u2019s largest robotic surgery training center, contributed to this advancement by partnering with both NVIDIA and ImFusion to guide the development and strengthen its clinical relevance.&nbsp;</p>\n\n\n\n<p>Dr Pieter De Backer, engineer and surgical resident leading Orsi Innotech, the surgical AI department of ORSI Academy, says, \u201cSeamlessly blending live video feeds with overlaid 3D mesh projections can enhance our ability as surgeons to navigate complex anatomical structures during minimally-invasive or robotic-assisted procedures. In challenging renal surgery cases, for example, live visualization of the endophytic tumor surface mesh can enhance tumor delineation, and minimize damage to healthy tissue.\u201c</p>\n\n\n\n<p>The integration of Holoscan-SDK for low-latency tasks and AI inferencing workloads accelerates the development of AI-enhanced SaMD. Its compatibility with domain-specific frameworks such as ImFusion-SDK creates a powerful development environment that shortens development time and reduces costs.</p>\n\n\n\n<h2 id=\"ecosystem_collaboration_and_open-source_contributions\"  class=\"wp-block-heading\">Ecosystem collaboration and open-source contributions<a href=\"#ecosystem_collaboration_and_open-source_contributions\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The collaboration between ImFusion and NVIDIA Holoscan is upleveling the art of the possible in minimally-invasive and robotic-assisted procedures, combining AI, accelerated computing, and domain specificity to enhance precision, performance, and safety. ImFusion\u2019s contributions to Holoscan reference applications can be integrated and built upon by its medtech customers, and its multimodal data fusion application will be available soon.</p>\n\n\n\n<p>We invite you to explore and contribute to the <a href=\"https://github.com/nvidia-holoscan/holohub\">Holoscan reference application repository</a> to expand the ecosystem, accelerate the development of AI-enhanced medical devices, and advance real-time sensor fusion for surgical guidance.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Developers in the fields of image-guided surgery and surgical vision face unique challenges in creating systems and applications that can significantly improve surgical workflows. One such challenge is efficiently combining multi-modal imaging data, such as preoperative 3D patient images with intra-operative video. This is key to providing surgeons with real-time, accurate guidance during minimally invasive &hellip; <a href=\"https://developer.nvidia.com/blog/real-time-surgical-guidance-by-fusing-multi-modal-imaging-with-nvidia-holoscan/\">Continued</a></p>\n", "protected": false}, "author": 2330, "featured_media": 89709, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1496583", "discourse_permalink": "https://forums.developer.nvidia.com/t/real-time-surgical-guidance-by-fusing-multi-modal-imaging-with-nvidia-holoscan/308982", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 503], "tags": [453, 3796, 90], "coauthors": [4066, 4064, 4065, 4063], "class_list": ["post-89703", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-edge-computing", "category-simulation-modeling-design", "tag-featured", "tag-healthcare", "tag-medical-imaging"], "acf": {"post_industry": ["Healthcare & Life Sciences"], "post_products": ["Holoscan"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/surgical-guidance-multimodal-image-fusing-holoscan-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nkP", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89703"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2330"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89703"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89703/revisions"}], "predecessor-version": [{"id": 89717, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89703/revisions/89717"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89709"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89703"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89703"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89703"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89703"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89751, "date": "2024-10-04T14:45:36", "date_gmt": "2024-10-04T21:45:36", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89751"}, "modified": "2024-10-17T12:06:58", "modified_gmt": "2024-10-17T19:06:58", "slug": "just-released-nvidia-tensorrt-llm-0-13-0", "status": "publish", "type": "post", "link": "https://nvda.ws/3ZIj6QO", "title": {"rendered": "Just Released: NVIDIA TensorRT-LLM 0.13.0"}, "content": {"rendered": "\n<p>Updates include tensor parallel support for Mamba2, sparse mixer normalization for MoE models, and more.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Updates include tensor parallel support for Mamba2, sparse mixer normalization for MoE models, and more.</p>\n", "protected": false}, "author": 674, "featured_media": 89865, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1495959", "discourse_permalink": "https://forums.developer.nvidia.com/t/just-released-nvidia-tensorrt-llm-0-13-0/308780", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3ZIj6QO", "_links_to_target": "_blank"}, "categories": [3110], "tags": [453, 2932], "coauthors": [2968], "class_list": ["post-89751", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-large-language-models"], "acf": {"post_industry": ["General"], "post_products": ["AI Foundation Models", "TensorRT-LLM"], "post_learning_levels": ["General Interest"], "post_content_types": ["Announcement"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/dev-tensor-rt-llm-meetup-social-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nlB", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89751"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/674"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89751"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89751/revisions"}], "predecessor-version": [{"id": 89951, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89751/revisions/89951"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89865"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89751"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89751"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89751"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89751"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89756, "date": "2024-10-04T09:00:00", "date_gmt": "2024-10-04T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89756"}, "modified": "2024-10-18T13:10:53", "modified_gmt": "2024-10-18T20:10:53", "slug": "just-released-nvidia-nemo-curator-improvements-for-accelerating-data-curation", "status": "publish", "type": "post", "link": "https://nvda.ws/4ePqHkD", "title": {"rendered": "Just Released: NVIDIA NeMo Curator Improvements for Accelerating Data Curation"}, "content": {"rendered": "\n<p>NeMo Curator now supports images, enabling you to process data for training accurate generative AI models.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NeMo Curator now supports images, enabling you to process data for training accurate generative AI models.</p>\n", "protected": false}, "author": 1921, "featured_media": 88387, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1495849", "discourse_permalink": "https://forums.developer.nvidia.com/t/just-released-nvidia-nemo-curator-improvements-for-accelerating-data-curation/308758", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/4ePqHkD", "_links_to_target": "_blank"}, "categories": [696, 3110], "tags": [3273, 453], "coauthors": [3612], "class_list": ["post-89756", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-generative-ai", "tag-accelerated-data-analytics", "tag-featured"], "acf": {"post_industry": ["General"], "post_products": ["NeMo", "NeMo Curator"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Announcement"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/llm-nemo-curator-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nlG", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89756"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1921"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89756"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89756/revisions"}], "predecessor-version": [{"id": 90610, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89756/revisions/90610"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88387"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89756"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89756"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89756"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89756"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89692, "date": "2024-10-03T13:00:00", "date_gmt": "2024-10-03T20:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89692"}, "modified": "2024-10-17T12:06:59", "modified_gmt": "2024-10-17T19:06:59", "slug": "event-community-over-code", "status": "publish", "type": "post", "link": "https://nvda.ws/3BqUbat", "title": {"rendered": "Event: Community Over Code"}, "content": {"rendered": "\n<p>Learn about accelerating vector search with NVIDIA cuVS and Apache Solr on October 10 at Community Over Code.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn about accelerating vector search with NVIDIA cuVS and Apache Solr on October 10 at Community Over Code.</p>\n", "protected": false}, "author": 1211, "featured_media": 89698, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1495415", "discourse_permalink": "https://forums.developer.nvidia.com/t/event-community-over-code/308687", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3BqUbat", "_links_to_target": "_blank"}, "categories": [696, 3110], "tags": [452, 453], "coauthors": [2497], "class_list": ["post-89692", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-generative-ai", "tag-events", "tag-featured"], "acf": {"post_industry": ["General"], "post_products": ["General"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Announcement"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/white-spheres-green-background.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nkE", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89692"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1211"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89692"}], "version-history": [{"count": 3, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89692/revisions"}], "predecessor-version": [{"id": 89701, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89692/revisions/89701"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89698"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89692"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89692"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89692"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89692"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89792, "date": "2024-10-03T09:24:50", "date_gmt": "2024-10-03T16:24:50", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89792"}, "modified": "2024-10-23T16:36:01", "modified_gmt": "2024-10-23T23:36:01", "slug": "ai-investigates-antarcticas-disappearing-moss-to-uncover-climate-change-clues", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/ai-investigates-antarcticas-disappearing-moss-to-uncover-climate-change-clues/", "title": {"rendered": "AI Investigates Antarctica&#8217;s Disappearing Moss to Uncover Climate Change Clues"}, "content": {"rendered": "\n<p>Antarctica plays a crucial role in regulating \u200cEarth\u2019s climate. Most climate research into the world\u2019s coldest, most windswept continent focuses on the surrounding Southern Ocean\u2019s carbon dioxide absorption, or its vast, sunlight-reflecting glaciers.\u00a0</p>\n\n\n\n<p>A group of Australian scientists is taking a different approach. Researchers are diving deep into Antarctic moss beds, using an AI-powered edge computing platform to look for clues about how warming in Antarctica may impact the rest of the world.</p>\n\n\n\n<p>Less than 1% of Antarctica&#8217;s surface is covered in moss. But the moss\u2019s existence\u2014and its ongoing health\u2014plays an outsized role in its ecosystem.&nbsp;</p>\n\n\n\n<p>Mosses, like other plants, absorb atmospheric CO<sub>2</sub> to grow. Moss beds are like miniature forests, providing habitat for crucial microscopic microbes, fungi, and micro-animals, like tardigrades and mites, which make up some of the lowest rungs on Antarctica\u2019s food chain. These plants and animals can survive in Antarctica because they can dry out and freeze to survive the nine-month Antarctic winters, which also makes them well-suited to space experiments (and travel).</p>\n\n\n\n<p>Over the past two decades, scientists have become increasingly alarmed by the deterioration of moss health in many parts of Antarctica. They\u2019ve observed moss communities drying out, due to changing wind speed patterns linked to climate change and ozone depletion. While moss beds make up a small part of Antarctica\u2019s ecosystem, they\u2019re crucial for soil stabilization\u2014and continued CO<sub>2</sub> sequestration\u2014as well as for maintaining biodiversity.&nbsp;</p>\n\n\n\n<p>Scientists from Securing Antarctica&#8217;s Environmental Future (<a href=\"https://arcsaef.com/\">SAEF</a>), a consortium of researchers funded by the Australian Research Council, developed a small, autonomous year-round monitoring platform to measure and analyze moss health\u2014even when the moss cover is buried in more than three-feet of snow.&nbsp;</p>\n\n\n\n<p>Antarctica\u2019s climate is extremely cold and harsh\u2014there\u2019s no sunlight for half the year\u2014however, mosses create their own warmer microclimates. Therefore, researchers cannot rely on weather stations to investigate why the moss layer is changing.</p>\n\n\n\n<p>The device, named the Artificial Intelligence of Things Platform, or AIoT Platform, has a solar panel and an insulated battery. This means it can be used almost anywhere and collects data year-round to monitor moss beds.</p>\n\n\n\n<p>The platform is built around an <a href=\"https://developer.nvidia.com/embedded-computing\">NVIDIA Jetson Orin Nano</a>, a small yet powerful computer that can handle AI workloads and deliver up to 40 TOPS of AI performance with power options between 7W and 15W. It is housed in a hardened 3\u2019x4\u2019 Pelican case, alongside different sensors, which can collect and analyze data including moss canopy and air temperature, relative humidity, soil moisture and heat flux, solar radiation, and imagery.</p>\n\n\n\n<p>The platform runs an <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/collections/imagesegmentation\">image segmentation mode</a>l (SegFormer, for semantic segmentation), which both ingests and then analyzes all the imagery data at the edge. The model was trained on eight <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 Tensor Core GPUs</a> using <a href=\"https://www.nvidia.com/en-us/launchpad/ai/develop-fine-tune-computer-vision-models-with-tao-automl/\">NVIDIA TAO AutoML</a>, enabling researchers to access state-of-the-art pre-trained image segmentation models that could be easily fine-tuned for the moss health algorithm.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"979\" height=\"552\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Moss.png\" alt=\"An image of a small black plastic Pelican case with sensors extending from it lays on moss-covered Antarctic soil.\" class=\"wp-image-89794\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Moss.png 979w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Moss-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Moss-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Moss-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Moss-768x433.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Moss-645x364.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Moss-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Moss-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Moss-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Moss-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Moss-195x110.png 195w\" sizes=\"(max-width: 979px) 100vw, 979px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The AI of Things Platform (AIoT) is being deployed by researchers to measure the health of Antarctica\u2019s moss colonies, analyzing data inputs like relative humidity and soil moisture (credit Dr Johan Barthelemy)</em></figcaption></figure></div>\n\n\n<p>According to Distinguished Professor Sharon Robinson, the deputy director of science implementation at SAEF, using NVIDIA TensorRT to optimize the image segmentation model lowered the time spent processing the image by 1.6x. It also significantly reduced power consumption and improved battery life for the entire AIoT platform.&nbsp;</p>\n\n\n\n<p>\u201cThe platform continuously processes streams of data and images coming from the sensors, and extracts relevant information, which minimizes the amount of data that needs to be transmitted back to Australia,\u201d Robinson said.&nbsp;</p>\n\n\n\n<p>\u201cThe data that gets sent back lets us immediately understand how climatic events are changing the environment in and above these ancient moss forests, and we can track changes in the health of the moss as well as other biodiversity signals, and understand how that relates to the changes in environmental conditions.\u201d</p>\n\n\n\n<p>Rather than constantly sending large raw datasets across limited bandwidth for processing, the AIoT Platform crunches data at the edge. It then transmits the results\u2014a much smaller amount of data\u2014 back to scientists at an Antarctic research station through long-range, low-power communications, using the LoRaWAN protocol. That data is then forwarded to the SAEF database, by satellite or LoRaWAN.&nbsp;</p>\n\n\n\n<p>When the seasons change and the weather clears, the raw data can be retrieved by scientists during field visits where the platforms are deployed.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow-1024x576.png\" alt=\"An image depicting how the AIoT Platform gathers relevant environmental data and then sends it to researchers using LoRaWAN.\u00a0\" class=\"wp-image-89795\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow-1024x576.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow-625x351.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow-362x203.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/AIoT-Platform-Workflow.png 1297w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. The AIoT Platform\u2019s sensors gather data about Antarctica\u2019s moss year-round, process it at the edge, and send it through LoRaWAN to researchers in Australia (credit Dr. Johan Barthelemy)</em></figcaption></figure></div>\n\n\n<p>To conduct its Antarctic moss research, SAEF used <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 Tensor Core</a> and <a href=\"https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/quadro-product-literature/proviz-print-nvidia-rtx-a6000-datasheet-us-nvidia-1454980-r9-web%20(1).pdf\">NVIDIA RTX A6000</a> GPUs it received from the <a href=\"https://www.nvidia.com/en-us/industries/higher-education-research/academic-grant-program/\">NVIDIA Academic Grant Program</a>. SAEF is currently finalizing the second generation to be deployed on the field this year.&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<p>The team also led the <a href=\"https://www.antarctica.gov.au/news/2024/high-tech-towers-to-monitor-antarctic-biodiversity/\">Antarctic Terrestrial and Nearshore Observing System project</a> of the Australian Antarctic Division to adopt the Jetson platform for its upcoming fleet of remote sensing towers for Antarctica. Making the two remote sensing programs interoperable will expand the coverage of both remote monitoring systems.&nbsp;</p>\n\n\n\n<p>Learn more about how Arctic moss is being impacted by changing <a href=\"https://theconversation.com/antarcticas-moss-forests-are-drying-and-dying-103751\">wind patterns</a>.&nbsp;</p>\n\n\n\n<p>Find out more about <a href=\"https://arcsaef.com/saef-goes-south/\">SAEF</a> and its <a href=\"https://phys.org/news/2024-07-unravel-climate-pattern-impacts-antarctic.html\">climate research</a> in Antarctica</p>\n\n\n\n<p></p>\n\n\n\n<p><br><br><em>Featured image credit Dr. Krystal Randall</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Antarctica plays a crucial role in regulating \u200cEarth\u2019s climate. Most climate research into the world\u2019s coldest, most windswept continent focuses on the surrounding Southern Ocean\u2019s carbon dioxide absorption, or its vast, sunlight-reflecting glaciers.\u00a0 A group of Australian scientists is taking a different approach. Researchers are diving deep into Antarctic moss beds, using an AI-powered edge &hellip; <a href=\"https://developer.nvidia.com/blog/ai-investigates-antarcticas-disappearing-moss-to-uncover-climate-change-clues/\">Continued</a></p>\n", "protected": false}, "author": 2342, "featured_media": 89793, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1495324", "discourse_permalink": "https://forums.developer.nvidia.com/t/ai-investigates-antarcticas-disappearing-moss-to-uncover-climate-change-clues/308660", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 2758], "tags": [3941, 1913, 453, 1877], "coauthors": [4075, 3093, 4083], "class_list": ["post-89792", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-edge-computing", "tag-ai-impact", "tag-climate-weather-ocean-modeling", "tag-featured", "tag-research"], "acf": {"post_industry": ["General", "Academia / Education"], "post_products": ["A100", "Jetson Orin"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Antarctic-Moss.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nmg", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Edge Computing", "link": "https://developer.nvidia.com/blog/category/edge-computing/", "id": 2758}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89792"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2342"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89792"}], "version-history": [{"count": 13, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89792/revisions"}], "predecessor-version": [{"id": 89973, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89792/revisions/89973"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89793"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89792"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89792"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89792"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89792"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89753, "date": "2024-10-03T09:00:00", "date_gmt": "2024-10-03T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89753"}, "modified": "2024-10-17T12:07:01", "modified_gmt": "2024-10-17T19:07:01", "slug": "event-nvidia-cuopt-at-informs-2024", "status": "publish", "type": "post", "link": "https://nvda.ws/3zEFUq3", "title": {"rendered": "Event: NVIDIA cuOpt at INFORMS 2024"}, "content": {"rendered": "\n<p>Join NVIDIA cuOpt engineers at INFORMS 2024 on October 22-23 to learn how to revolutionize accelerated computing.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Join NVIDIA cuOpt engineers at INFORMS 2024 on October 22-23 to learn how to revolutionize accelerated computing.</p>\n", "protected": false}, "author": 1997, "featured_media": 89754, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1495310", "discourse_permalink": "https://forums.developer.nvidia.com/t/event-nvidia-cuopt-at-informs-2024/308657", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3zEFUq3", "_links_to_target": "_blank"}, "categories": [696], "tags": [1932, 453, 126], "coauthors": [3699], "class_list": ["post-89753", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "tag-development-tools-and-libraries", "tag-featured", "tag-optimization"], "acf": {"post_industry": ["General"], "post_products": ["cuOpt"], "post_learning_levels": ["General Interest"], "post_content_types": ["Announcement"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuopt-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nlD", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89753"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1997"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89753"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89753/revisions"}], "predecessor-version": [{"id": 89787, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89753/revisions/89787"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89754"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89753"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89753"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89753"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89753"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89655, "date": "2024-10-03T09:00:00", "date_gmt": "2024-10-03T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89655"}, "modified": "2024-10-21T16:56:04", "modified_gmt": "2024-10-21T23:56:04", "slug": "new-reward-model-helps-improve-llm-alignment-with-human-preferences", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/new-reward-model-helps-improve-llm-alignment-with-human-preferences/", "title": {"rendered": "New Reward Model Helps Improve LLM Alignment with Human Preferences"}, "content": {"rendered": "\n<p>Reinforcement learning from human feedback (RLHF) is essential for developing AI systems that are aligned with human values and preferences. RLHF enables the most capable LLMs, including ChatGPT, Claude, and Nemotron families, to generate exceptional responses.&nbsp;</p>\n\n\n\n<p>By integrating human feedback into the training process, RLHF enables models to learn more nuanced behaviors and make decisions that better reflect user expectations. This approach enhances the quality of AI-generated responses and fosters trust and reliability in AI applications.&nbsp;</p>\n\n\n\n<p>To help the AI community easily adopt RLHF to build and customize models, NVIDIA released <a href=\"https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-reward\">Llama 3.1-Nemotron-70B-Reward</a>, a state-of-the-art reward model that scores the responses generated by LLMs. Such scores can be used to improve LLM response quality, making a more positive and impactful interaction between humans and AI.</p>\n\n\n\n<p>NVIDIA researchers leveraged the reward model to train <a href=\"https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct\" target=\"_blank\" rel=\"noreferrer noopener\">Llama 3.1-Nemotron-70B-Instruct model</a>, which is among the top models on <a href=\"https://github.com/lm-sys/arena-hard-auto/\" target=\"_blank\" rel=\"noreferrer noopener\">Arena Hard leaderboard</a>.</p>\n\n\n\n<h2 id=\"#1_reward_model\"  class=\"wp-block-heading\">#1 reward model<a href=\"##1_reward_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The Llama 3.1-Nemotron-70B-Reward model currently is in first place on the<a href=\"https://huggingface.co/spaces/allenai/reward-bench\"> Hugging Face RewardBench leaderboard</a> for evaluating the capabilities, safety, and pitfalls of reward models.</p>\n\n\n\n<p>The model scored 94.1% on Overall RewardBench, meaning that it can identify responses that align with human preferences 94% of the time.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"382\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Llama-3.1-Nemotron-70B-Reward-leaderboard-1-1024x382.png\" alt=\"Screenshot of the leaderboard shows the ranking of various reward models and their accuracy across different categories. The model on the top of the RewardBench leaderboard is NVIDIA\u2019s Llama-3.1-Nemotron-70B Reward model.\" class=\"wp-image-89656\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Llama-3.1-Nemotron-70B-Reward-leaderboard-1-1024x382.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Llama-3.1-Nemotron-70B-Reward-leaderboard-1-300x112.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Llama-3.1-Nemotron-70B-Reward-leaderboard-1-625x233.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Llama-3.1-Nemotron-70B-Reward-leaderboard-1-179x67.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Llama-3.1-Nemotron-70B-Reward-leaderboard-1-768x286.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Llama-3.1-Nemotron-70B-Reward-leaderboard-1-645x241.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Llama-3.1-Nemotron-70B-Reward-leaderboard-1-500x186.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Llama-3.1-Nemotron-70B-Reward-leaderboard-1-160x60.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Llama-3.1-Nemotron-70B-Reward-leaderboard-1-362x135.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Llama-3.1-Nemotron-70B-Reward-leaderboard-1-295x110.png 295w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Llama-3.1-Nemotron-70B-Reward-leaderboard-1.png 1475w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Llama-3.1-Nemtron-70B-Reward tops RewardBench leaderboard across various categories</em></figcaption></figure></div>\n\n\n<p>The model scores well across all four categories: Chat, Chat-Hard, Safety, and Reasoning. It has an impressive performance for Safety and Reasoning, achieving 95.1% and 98.1% accuracy, respectively. This means that the model can safely reject potential unsafe responses and support RLHF in domains like math and code.</p>\n\n\n\n<p>With just a fifth the size of Nemotron-4 340B Reward, this model delivers high compute efficiency coupled with superior accuracy. This model is also trained only on CC-BY-4.0-licensed <a href=\"https://huggingface.co/datasets/nvidia/HelpSteer2\">HelpSteer2 data</a>, which makes it feasible for enterprise use cases.</p>\n\n\n\n<h2 id=\"implementation\"  class=\"wp-block-heading\">Implementation<a href=\"#implementation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To train this model, we combined two popular approaches to make the best of both worlds:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://arxiv.org/abs/2406.08673\">Regression-style reward models</a></li>\n\n\n\n<li><a href=\"https://arxiv.org/abs/2203.02155\">Bradley-Terry reward model</a>&nbsp;</li>\n</ul>\n\n\n\n<p>We trained with both approaches using data that we released in HelpSteer2. An important contributor to the model performance is high data quality, which we meticulously curated and then released to advance AI for all.&nbsp;&nbsp;</p>\n\n\n\n<h2 id=\"leading_large_language_model\"  class=\"wp-block-heading\">Leading large language model<a href=\"#leading_large_language_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Using the trained Reward Model and HelpSteer2-Preference Prompts for RLHF training (specifically with the REINFORCE algorithm) produces a model that scores 85 on Arena Hard, a popular automatic evaluation tool for instruction-tuned LLMs. This makes this the best leading models on the <a href=\"https://github.com/lmarena/arena-hard-auto/\" target=\"_blank\" rel=\"noreferrer noopener\">Arena Hard Leaderboard</a>, among models that do not require additional test-time compute.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"624\" height=\"265\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-2.-Llama-3.1-Nemotron-70B-Instruct-reached-3-on-Arena-Hard-leaderboard.png\" alt=\"The image shows leaderboard ranking of the top models. It highlights Llama-3.1-Nemotron-70B-Instruct model ranked #3 with a score of 84.9\" class=\"wp-image-90725\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-2.-Llama-3.1-Nemotron-70B-Instruct-reached-3-on-Arena-Hard-leaderboard.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-2.-Llama-3.1-Nemotron-70B-Instruct-reached-3-on-Arena-Hard-leaderboard-300x127.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-2.-Llama-3.1-Nemotron-70B-Instruct-reached-3-on-Arena-Hard-leaderboard-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-2.-Llama-3.1-Nemotron-70B-Instruct-reached-3-on-Arena-Hard-leaderboard-500x212.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-2.-Llama-3.1-Nemotron-70B-Instruct-reached-3-on-Arena-Hard-leaderboard-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-2.-Llama-3.1-Nemotron-70B-Instruct-reached-3-on-Arena-Hard-leaderboard-362x154.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/Figure-2.-Llama-3.1-Nemotron-70B-Instruct-reached-3-on-Arena-Hard-leaderboard-259x110.png 259w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Llama-3.1-Nemotron-70B-Instruct reached #3 on Arena Hard leaderboard.</em></figcaption></figure></div>\n\n\n<p>The Llama-3.1-Nemotron-70B-Instruct model comes with Llama-3.1 License, making it easy for research and enterprises to customize and integrate this model in their applications.</p>\n\n\n\n<h2 id=\"easy_deployment_with_nvidia_nim\"  class=\"wp-block-heading\">Easy deployment with NVIDIA NIM<a href=\"#easy_deployment_with_nvidia_nim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The Nemotron Reward model is packaged as an <a href=\"http://ai.nvidia.com\">NVIDIA NIM inference microservice</a> to streamline and accelerate the deployment of generative AI models across NVIDIA-accelerated infrastructure anywhere, including cloud, data center, and workstations.</p>\n\n\n\n<p>NIM uses inference optimization engines, industry-standard APIs, and prebuilt containers to provide high-throughput AI inference that scales with demand.</p>\n\n\n\n<h2 id=\"getting_started\"  class=\"wp-block-heading\">Getting started<a href=\"#getting_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Experience the <a href=\"https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-reward\" target=\"_blank\" rel=\"noreferrer noopener\">Llama 3.1-Nemotron-70B-Reward model</a> from a browser today or test it at scale and build a proof of concept (PoC) with the NVIDIA-hosted API endpoint running on a fully accelerated stack. The Llama 3.1-Nemotron-70B-Instruct model can also be accessed <a href=\"https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct\">here</a>.</p>\n\n\n\n<p>Get started at <a href=\"https://ai.nvidia.com\">ai.nvidia.com</a> with free NVIDIA cloud credits or download the model from <a href=\"https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward\">Hugging Face</a>.</p>\n\n\n\n<p>For more information about how the model was trained and can be used for RLHF, see <a href=\"https://arxiv.org/abs/2410.01257\">HelpSteer2-Preference: Complementing Ratings with Preferences</a>.</p>\n\n\n\n<p><em>This blog was updated on 10/21/2024.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Reinforcement learning from human feedback (RLHF) is essential for developing AI systems that are aligned with human values and preferences. RLHF enables the most capable LLMs, including ChatGPT, Claude, and Nemotron families, to generate exceptional responses.&nbsp; By integrating human feedback into the training process, RLHF enables models to learn more nuanced behaviors and make decisions &hellip; <a href=\"https://developer.nvidia.com/blog/new-reward-model-helps-improve-llm-alignment-with-human-preferences/\">Continued</a></p>\n", "protected": false}, "author": 1833, "featured_media": 89657, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1495309", "discourse_permalink": "https://forums.developer.nvidia.com/t/new-reward-model-helps-improve-llm-alignment-with-human-preferences/308656", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [453, 2136], "coauthors": [3462, 610], "class_list": ["post-89655", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-huggingface"], "acf": {"post_industry": ["General"], "post_products": ["AI Foundation Models"], "post_learning_levels": ["General Interest"], "post_content_types": ["Benchmark"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/nemotron-reward-model-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nk3", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89655"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1833"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89655"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89655/revisions"}], "predecessor-version": [{"id": 90731, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89655/revisions/90731"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89657"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89655"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89655"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89655"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89655"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89659, "date": "2024-10-02T11:00:00", "date_gmt": "2024-10-02T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89659"}, "modified": "2024-10-17T12:07:02", "modified_gmt": "2024-10-17T19:07:02", "slug": "webinar-accelerating-python-with-gpus", "status": "publish", "type": "post", "link": "https://nvda.ws/3UfgTsH", "title": {"rendered": "Webinar: Accelerating Python with GPUs"}, "content": {"rendered": "\n<p>Join us on October 9 to learn how your applications can benefit from NVIDIA CUDA Python software initiatives.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Join us on October 9 to learn how your applications can benefit from NVIDIA CUDA Python software initiatives.</p>\n", "protected": false}, "author": 1466, "featured_media": 89661, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1494868", "discourse_permalink": "https://forums.developer.nvidia.com/t/webinar-accelerating-python-with-gpus/308572", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3UfgTsH", "_links_to_target": "_blank"}, "categories": [852, 696], "tags": [453, 61, 1981], "coauthors": [2968], "class_list": ["post-89659", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-data-science", "tag-featured", "tag-python", "tag-webinar"], "acf": {"post_industry": ["General"], "post_products": ["CUDA", "RAPIDS"], "post_learning_levels": ["General Interest"], "post_content_types": ["Announcement"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/cuda-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nk7", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89659"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1466"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89659"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89659/revisions"}], "predecessor-version": [{"id": 89662, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89659/revisions/89662"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89661"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89659"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89659"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89659"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89659"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]