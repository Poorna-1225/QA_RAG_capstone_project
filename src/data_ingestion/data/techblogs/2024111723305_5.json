[{"id": 89552, "date": "2024-10-02T10:00:00", "date_gmt": "2024-10-02T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89552"}, "modified": "2024-10-17T12:07:03", "modified_gmt": "2024-10-17T19:07:03", "slug": "building-llm-powered-production-systems-with-nvidia-nim-and-outerbounds", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/building-llm-powered-production-systems-with-nvidia-nim-and-outerbounds/", "title": {"rendered": "Building LLM-Powered Production Systems with NVIDIA NIM and Outerbounds"}, "content": {"rendered": "\n<p>With the rapid expansion of language models over the past 18 months, hundreds of variants are now available. These include <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models (LLMs)</a>, small language models (SLMs), and domain-specific models\u2014many of which are freely accessible for commercial use. For LLMs in particular, the process of fine-tuning with custom datasets has also become increasingly affordable and straightforward.</p>\n\n\n\n<p>As AI models become less expensive and more accessible, an increasing number of real-world processes and products emerge as potential applications. Consider any process that involves unstructured data\u2014support tickets, medical records, incident reports, screenplays, and much more.&nbsp;</p>\n\n\n\n<p>The data involved is often sensitive, and the outcomes are critical to the business. While LLMs make hacking quick demos deceptively easy, establishing the proper processes and infrastructure for developing and deploying LLM-powered applications is not trivial. All the usual enterprise concerns still apply, including how to:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Access data, deploy, and operate the system safely and securely.</li>\n\n\n\n<li>Set up rapid, productive development processes across the organization.</li>\n\n\n\n<li>Measure and facilitate continuous improvement as the field keeps developing rapidly.</li>\n</ol>\n\n\n\n<p>Deploying LLMs in enterprise environments requires a secure and well-structured approach to <a href=\"https://www.nvidia.com/en-us/glossary/machine-learning/\">machine learning (ML)</a> infrastructure, development, and deployment.&nbsp;</p>\n\n\n\n<p>This post explains how <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a> microservices and the <a href=\"https://outerbounds.com/\">Outerbounds</a> platform together enable efficient, secure management of LLMs and systems built around them. In particular, we focus on integrating LLMs into enterprise environments while following established continuous integration and continuous deployment (CI/CD) best practices.</p>\n\n\n\n<p>NVIDIA NIM provides containers to self-host GPU-accelerated microservices for pretrained and customized AI models across clouds, data centers, and workstations. Outerbounds is a leading MLOps and AI platform born out of Netflix, powered by the popular open-source framework <a href=\"https://metaflow.org\">Metaflow</a>.</p>\n\n\n\n<h2 id=\"building_llm-powered_enterprise_applications_with_nvidia_nim\"  class=\"wp-block-heading\">Building LLM-powered enterprise applications with NVIDIA NIM<a href=\"#building_llm-powered_enterprise_applications_with_nvidia_nim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>A substantial share of security and data governance concerns can be readily mitigated by avoiding the need to send data to third-party services. This is a key value proposition of NVIDIA NIM\u2014microservices that offer a large selection of prepackaged and optimized community-created LLMs, deployable in the company\u2019s private environment.</p>\n\n\n\n<p>Since the original release of NIM, Outerbounds has been enabling companies to develop LLM-powered enterprise applications, as well as <a href=\"https://outerbounds.com/blog/hacker-news-sentiment/\">public examples</a>. NIM is <a href=\"https://outerbounds.com/blog/nim-announcement/\">now integrated in the Outerbounds platform</a>, enabling you, as the developer, to deploy securely <a href=\"https://outerbounds.com/blog/obp-on-all-clouds/\">across cloud and on-premises resources</a>. In the course of doing this, Outerbounds has begun to identify emerging patterns and best practices, particularly around infrastructure setup and development workflows.</p>\n\n\n\n<p>The term large language model operations (LLMOps) has been coined to encompass many of these practices. However, don&#8217;t let the name mislead you. LLMOps centers around the challenges of managing large language model dependencies and operations, while MLOps casts a wider net, covering a broad spectrum of tasks related to overseeing machine learning models across diverse domains and applications.</p>\n\n\n\n<p>Many of the topics discussed are established best practices borrowed from software engineering. In fact, it\u2019s advantageous to <a href=\"https://outerbounds.com/blog/code-data-models\">develop LLM-powered systems using the same principles as any robust software</a>, with particular attention given to the additional challenges posed by the stochastic nature of LLMs and natural language prompts, as well as their unique computational demands.</p>\n\n\n\n<p>The following sections highlight learnings in the three main areas that need to be addressed by any serious system:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Productive development practices</li>\n\n\n\n<li>Collaboration and continuous improvement</li>\n\n\n\n<li>Robust production deployments&nbsp;</li>\n</ul>\n\n\n\n<p>These three areas are integral to building LLM-powered enterprise applications with NVIDIA NIM. Productive development practices leverage NIM&#8217;s microservices to experiment, fine-tune, and test LLMs securely in private environments. Collaboration and continuous improvement ensure teams can iterate on models, monitor performance, and adapt to changes efficiently. Production deployments with NIM allow enterprises to scale LLM-powered applications securely, whether in the cloud or on-premises, ensuring stability and performance as these systems move from development to real-world use.</p>\n\n\n\n<p>This discussion focuses on batch use cases like document understanding, but many elements discussed apply to real-time use cases as well.</p>\n\n\n\n<h3 id=\"stage_1_developing_systems_backed_by_llms\"  class=\"wp-block-heading\">Stage 1: Developing systems backed by LLMs<a href=\"#stage_1_developing_systems_backed_by_llms\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The first stage in building LLM-powered systems focuses on setting up a productive development environment for rapid iteration and experimentation. NVIDIA NIM microservices play a key role by providing optimized LLMs that can be deployed in secure, private environments. This stage involves fine-tuning models, building workflows, and testing with real-world data while ensuring data control and maximizing LLM performance. The goal is to establish a solid development pipeline that supports isolated environments and seamless LLM integration.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"559\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram-625x559.jpg\" alt=\"A diagram showing a development environment that includes workstations, local GPU resources, and a shared compute pool hosting NVIDIA NIM containers and additional GPU resources.\n\" class=\"wp-image-89562\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram-625x559.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram-300x268.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram-129x115.jpg 129w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram-768x687.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram-1536x1374.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram-645x577.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram-335x300.jpg 335w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram-101x90.jpg 101w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram-362x324.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram-123x110.jpg 123w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram-1024x916.jpg 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-enviroment-nvidia-nim-diagram.jpg 2000w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Development environment with NVIDIA NIM microservices</em></figcaption></figure>\n\n\n\n<p>Focusing on the development stage first, Outerbounds has found the following elements to be beneficial for developing LLM-powered applications, in particular when dealing with sensitive data:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Operating within your cloud premises</li>\n\n\n\n<li>Using local compute resources for isolated development environments</li>\n\n\n\n<li>Maximizing LLM throughput to minimize cost</li>\n\n\n\n<li>Supporting domain-specific evaluation</li>\n\n\n\n<li>Customizing models with fine-tuning</li>\n</ul>\n\n\n\n<h4 class=\"wp-block-heading\">Operate within your cloud premises</h4>\n\n\n\n<p>Outerbounds helps you deploy the development environment shown in Figure 1 in your own cloud account(s), so you can develop AI applications, powered by NIM, with your existing data governance rules and boundaries. Furthermore, you can use your existing compute resources for hosting the models without having to pay extra margin for <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\">LLM inference</a>.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Flexible, isolated development environments with local compute resources</h4>\n\n\n\n<p>The two examples in Figure 1 provide personal development environments, where you can operate freely without the risk of interfering with others. This helps to maximize development velocity. NIM exposes an OpenAI-compatible API, which enables hitting the private endpoints using off-the-shelf frameworks, choosing the best tool for each job.In addition to exploring and developing in personal Jupyter Notebooks, you can build end-to-end workflows using <a href=\"https://docs.metaflow.org\">open-source Metaflow</a>. Metaflow is a Python library for developing, deploying, and operating various data-intensive applications, in particular those involving data science, ML, and AI. Outerbounds extends Metaflow with an @nim decorator, which makes it straightforward to embed LLM NIM microservices in larger workflows:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nMODEL = &quot;meta/llama3-70b-instruct&quot;\n\nPROMPT = &quot;answer with one word HAPPY if the sentiment of the following sentence is positive, otherwise answer with one word SAD&quot;\n\n@nim(models=&#x5B;MODEL])\nclass NIMExample(FlowSpec):\n  ...\n  @step\n  def prompt(self):\n    llm = current.nim.models&#x5B;MODEL]\n    prompt = {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{PROMPT}---{doc}&quot;}\n    chat_completion = llm(messages=&#x5B;prompt], max_tokens=1)\n    print(&#039;response&#039;, chat_completion&#x5B;&#039;choices&#039;]&#x5B;0]&#x5B;&#039;message&#039;]&#x5B;&#039;content&#039;])\n</pre></div>\n\n\n<p>As a developer, you can execute flows like this locally on your workstation, accessing test data and using any NIM microservices available in the development environment, quickly iterating on prompts and models. For an end-to-end example involving @nim, see <a href=\"https://outerbounds.com/blog/hacker-news-sentiment\">350M Tokens Don&#8217;t Lie</a> and the <a href=\"https://github.com/outerbounds/hacker-news-sentiment/blob/main/hnposts.py#L40\">accompanying source code</a>.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">Maximizing LLM throughput to minimize cost</h4>\n\n\n\n<p>In contrast to many third-party APIs, you can hit NIM endpoints in your own environment without rate limiting. Thanks to various NIM optimizations such as <a href=\"https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_2-improving_resource_utilization#what-is-dynamic-batching\">dynamic batching</a>, you can increase total throughput by parallelizing prompting. For more details, see <a href=\"https://developer.nvidia.com/blog/optimizing-inference-efficiency-for-llms-at-scale-with-nvidia-nim-microservices/\">Optimizing Inference Efficiency for LLMs at Scale with NVIDIA NIM Microservices</a>.</p>\n\n\n\n<p>For the <a href=\"https://outerbounds.com/blog/hacker-news-sentiment\">350M Tokens Don&#8217;t Lie</a> example, Outerbounds processed 230 million input tokens in about 9 hours with a LLama 3 70B model by hitting the NIM container with five concurrent worker tasks (Figure 2). The model was running on four <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100 Tensor Core</a> GPUs.</p>\n\n\n\n<p>Since NIM microservices run and autoscale on NVIDIA GPUs hosted in your environment, higher throughput time results in lower cost.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1869\" height=\"950\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim.png\" alt=\"Figure shows a timeline of multiple batches of five tasks prompting NIM microservices.\n\" class=\"wp-image-89567\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim.png 1869w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim-300x152.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim-625x318.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim-179x91.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim-768x390.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim-1536x781.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim-645x328.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim-500x254.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim-160x81.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim-362x184.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim-216x110.png 216w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/timeline-multiple-batches-five-tasks-prompting-nvidia-nim-1024x520.png 1024w\" sizes=\"(max-width: 1869px) 100vw, 1869px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Prompting NIM microservices with five concurrent workers running on Outerbounds</em></figcaption></figure>\n\n\n\n<h4 class=\"wp-block-heading\">Support domain-specific evaluation</h4>\n\n\n\n<p>While it may be easy to measure and benchmark raw performance, it\u2019s more difficult to be prescriptive when it comes to evaluating the quality of LLM responses. In general, you need to evaluate responses in the context of your data and application instead of relying on off-the-shelf benchmarks or datasets.&nbsp;</p>\n\n\n\n<p>Outerbounds finds it useful to construct case-specific evaluation sets, sometimes supported by a custom UI. This enables you to quickly evaluate results and iterate on prompts. For an example, see <a href=\"https://outerbounds.com/blog/document-understanding\">Scaling LLM-Powered Document Understanding.</a></p>\n\n\n\n<h4 class=\"wp-block-heading\">Support model customization through fine-tuning</h4>\n\n\n\n<p>With the capable LLMs now available, you can often get excellent results with prompt engineering. However, it is beneficial to have the capability to fine-tune the model with custom datasets should a need arise.</p>\n\n\n\n<p>While it is possible to fine-tune by resuming training of an LLM, this requires significant compute resources. As an alternative to fine-tuning all of the model\u2019s parameters, it is common to leverage a <a href=\"https://github.com/huggingface/peft\">Parameter-Efficient Fine Tuning (PEFT)</a> technique, which alters only a subset of the model parameters. This can be done with significantly fewer computational resources.</p>\n\n\n\n<p>Fortunately, <a href=\"https://docs.nvidia.com/nim/large-language-models/latest/peft.html\">NIM supports PEFT fine-tuning out of the box</a>, so you can use NIM with custom models without having to manually set up an inference stack.&nbsp;</p>\n\n\n\n<p>If you\u2019re interested in technical details, take a look at <a href=\"https://outerbounds.com/blog/scale-ml-ai-smoothly\">new features that Outerbounds provides for fine-tuning</a>, including an end-to-end example of <a href=\"https://github.com/outerbounds/nim-lora/tree/main\">creating adapters using Metaflow and Hugging Face, and serving them with NIM</a>. The workflow and instructions show how, with a few commands and a few hundred lines of Python code, you can retain complete control over custom workflows for customizing the most potent open-source LLMs.&nbsp;</p>\n\n\n\n<p>For instance, consider the development environment depicted in Figure 1. To support fast iterations, a workstation is equipped with a modest local GPU, enabling fast code development. For larger-scale fine-tuning, developers can leverage a larger GPU cluster in the company\u2019s compute pool. In both these cases, fine-tuned models can be quickly evaluated in the development environment prior to deployment to production.</p>\n\n\n\n<h3 id=\"stage_2_continuous_improvement_for_llm_systems\"  class=\"wp-block-heading\">Stage 2: Continuous improvement for LLM systems<a href=\"#stage_2_continuous_improvement_for_llm_systems\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Productivity-boosting development environments powered by high-performance LLMs enable rapid development iterations. But speed is not everything. Outerbounds wants to ensure that developers can move fast without breaking things, and strive for coherent, continuous improvement over a long period of time, not just short-term experiments. GitOps, a framework for maintaining version control and continuous improvement using Git is one good way of ensuring this.</p>\n\n\n\n<p>Introducing proper version control, tracking, and monitoring to the development environment helps to achieve this (Figure 2).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"559\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring-625x559.jpg\" alt=\"A diagram showing a development environment that supports iterative development through versioning, tracking and monitoring, building on Figure 1.\n\" class=\"wp-image-89568\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring-625x559.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring-300x268.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring-129x115.jpg 129w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring-768x687.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring-1536x1374.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring-645x577.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring-335x300.jpg 335w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring-101x90.jpg 101w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring-362x324.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring-123x110.jpg 123w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring-1024x916.jpg 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/interative-development-environment-versioning-tracking-monitoring.jpg 2000w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Adding versioning, tracking, and monitoring to the development environment</em></figcaption></figure>\n\n\n\n<h4 class=\"wp-block-heading\">Versioning code, data, models, and prompts</h4>\n\n\n\n<p>When using Git or a similar tool for version control, what\u2019s the best way to keep track of prompts, their responses, and models used?</p>\n\n\n\n<p>This is where <a href=\"https://docs.metaflow.org/metaflow/basics#artifacts\">Metaflow built-in artifacts</a> come in handy. Metaflow persists the full workflow state automatically, so by treating prompts and responses as artifacts, everything gets versioned by default and <a href=\"https://docs.metaflow.org/metaflow/client\">made easily accessible for post-hoc analysis</a>. Notably, <a href=\"https://docs.metaflow.org/scaling/tagging\">artifacts are automatically namespaced and organized</a>, so developer teams can work concurrently and cooperatively.</p>\n\n\n\n<p>In addition, you can use <a href=\"http://ys-to-use-the-new-metaflow-tags/\">Metaflow tags to aid collaboration</a> by annotating particularly successful prompts and runs, so they can be reused easily by others.&nbsp;</p>\n\n\n\n<h4 class=\"wp-block-heading\">Including LLMs in the software supply chain</h4>\n\n\n\n<p>A crucial but often overlooked part of modern systems powered by LLMs is that the LLM needs to be treated as a core dependency of the system, similar to any other software library. In other words, the LLM\u2014a particular version of the LLM to be exact\u2014becomes a part of the software supply chain. To learn more, see <a href=\"https://outerbounds.com/blog/secure-ml-secure-software-dependencies\">Secure ML with Secure Software Dependencies.</a></p>\n\n\n\n<p>Imagine a system that works well during development and passes the initial evaluation may suddenly break or slowly erode as the <a href=\"https://huyenchip.com/2023/04/11/llm-engineering.html#backward_and_forward_compatibility\">backing LLM changes uncontrollably</a>. When using third-party APIs, the vendor must be trusted not to change the model. This is difficult as models evolve and advance at a rapid pace.</p>\n\n\n\n<p>Deploying NIM microservices in your environment gives you control of the full model lifecycle. In particular, you can treat the models as a proper dependency of your systems, associating particular prompts and evaluations with an exact model version, down to the hash of the container image producing the results.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Monitoring NIM microservices</h4>\n\n\n\n<p>Aside from tracking and storing artifacts and metrics, you can instrument critical points throughout a flow, making everything easily observable. One way of doing this is with <a href=\"https://docs.metaflow.org/metaflow/visualizing-results\">Metaflow cards</a>, which enable you to attach custom, versioned visualizations to workflows.</p>\n\n\n\n<p>When running on Outerbounds, @nim collects metrics about NIM performance automatically, as shown in Figure 4.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1686\" height=\"1234\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies.png\" alt=\"A visualization showing NIM response successes and latencies.\n\" class=\"wp-image-89569\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies.png 1686w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies-300x220.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies-625x457.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies-157x115.png 157w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies-768x562.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies-1536x1124.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies-645x472.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies-410x300.png 410w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies-123x90.png 123w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies-362x265.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies-150x110.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metaflow-card-nim-response-successes-latencies-1024x749.png 1024w\" sizes=\"(max-width: 1686px) 100vw, 1686px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. A Metaflow card visualizing NIM response successes and latencies</em></figcaption></figure>\n\n\n\n<p>In addition to low-level metrics, you can <a href=\"https://docs.metaflow.org/metaflow/visualizing-results\">customize the Metaflow card</a> to show metrics related to the use case. For example, to monitor data drift and alert when the accuracy of responses degrades below a certain threshold.</p>\n\n\n\n<h3 id=\"stage_3_cicd_and_production_roll-outs\"  class=\"wp-block-heading\">Stage 3: CI/CD and production roll-outs<a href=\"#stage_3_cicd_and_production_roll-outs\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In this final stage, the focus shifts to integrating continuous integration and continuous delivery practices to ensure smooth, reliable production roll-outs of LLM-powered systems. By implementing automated pipelines, organizations can continuously improve and update their LLM models while maintaining stability. This stage emphasizes the importance of gradual deployments, monitoring, and version control to manage the complexities of LLM systems in live environments.</p>\n\n\n\n<p>Often, moving an LLM to production is perceived as a binary milestone, the crossing of which signals the successful completion of a project. Yet, modern business-critical systems remain incomplete after pushing the deploy button.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"559\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system-625x559.jpg\" alt=\"A figure showing development and production environments side by side, connected through a CI/CD system, building on Figure 3.\n\" class=\"wp-image-89570\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system-625x559.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system-300x268.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system-129x115.jpg 129w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system-768x687.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system-1536x1374.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system-645x577.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system-335x300.jpg 335w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system-101x90.jpg 101w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system-362x324.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system-123x110.jpg 123w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system-1024x916.jpg 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/development-production-environments-cicd-system.jpg 2000w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Adding a production environment, connected to the development environment through a CI/CD system</em></figcaption></figure>\n\n\n\n<h4 class=\"wp-block-heading\">Continuous delivery with CI/CD systems</h4>\n\n\n\n<p>Following DevOps best practices, LLM-powered systems should be deployed through a CI/CD pipeline, such as GitHub Actions. This setup enables continuous deployment of system improvements, which is crucial for systems undergoing rapid iterations, a common scenario with LLMs. Over time, refine prompts, fine-tune models, and upgrade the underlying LLMs, especially as new models are released every few months.</p>\n\n\n\n<p>Instead of considering \u201ca deployment\u201d as a one-off action, think of it as a gradual roll-out. Due to the stochastic nature of LLMs, it\u2019s difficult to know whether a new version works better than an old one without exposing it to live data. In other words, deploy it first as an A/B experiment of sorts, or as a shadow deployment running in parallel with production.&nbsp;</p>\n\n\n\n<p>This approach leads to multiple versions of LLMs in production concurrently: an existing production version as well as a number of challenger models. Because you control the NIM models, you can more reliably manage roll-outs like this in your own environment. For more details, see <a href=\"https://outerbounds.com/blog/continuous-delivery-of-ml-ai\">How To Organize Continuous Delivery of ML/AI Systems: a 10-Stage Maturity Model</a>.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Isolating business logic and models, unifying compute</h4>\n\n\n\n<p>To enable stable, highly-available production deployments, they must be securely isolated from development environments. Under no circumstances should development interfere with production (and vice versa).</p>\n\n\n\n<p>In the case of LLMs, you may want to experiment with the latest and greatest models, while being more conservative with models in production. Furthermore, when tracking and monitoring production systems, you may need to control who has access to production model responses when sensitive data is involved. You can achieve this by, for example, setting up separate cloud accounts for production and experimentation. You could also use <a href=\"https://outerbounds.com/features/security-and-compliance/\">Outerbounds perimeters with role-based access control</a> to set up the desired permission boundaries.</p>\n\n\n\n<p>While it\u2019s often a strict requirement to keep logic and data isolated, it\u2019s often beneficial to use shared compute pools across development and production to drive up utilization and hence lower the cost of valuable GPU resources. For example, you can have a unified cluster of GPU (cloud) hardware but deploy a separate set of NIM models in the two environments to guarantee sufficient capacity and isolation for production. To learn more, see <a href=\"https://outerbounds.com/blog/six-steps-to-cost-optimization\">The 6 Steps to Cost-Optimized ML/AI/Data Workloads</a>.</p>\n\n\n\n<h4 class=\"wp-block-heading\">Integrating LLM-powered systems into their surroundings</h4>\n\n\n\n<p>The LLM-powered systems on Outerbounds are not isolated islands. They are connected to upstream data sources, such as data warehouses, and downstream systems consuming their results. This poses additional challenges to deployments, as they have to behave well in the context of other systems too.&nbsp;</p>\n\n\n\n<p>You can use Metaflow eventcoming triggering to make your systems <a href=\"https://outerbounds.com/blog/metaflow-event-triggering/\">react to changes in upstream data sources in real time</a>. When integrating with downstream systems, strong versioning and isolated deployments are a must, to avoid inadvertently breaking compatibility for consumers of your results.</p>\n\n\n\n<h2 id=\"start_building_llm-powered_production_systems_with_nvidia_nim_and_outerbounds\"  class=\"wp-block-heading\">Start building LLM-powered production systems with NVIDIA NIM and Outerbounds<a href=\"#start_building_llm-powered_production_systems_with_nvidia_nim_and_outerbounds\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In many ways, systems powered by LLMs should be approached like any other large software system that is subject to stochastic inputs and outputs. The presence of LLMs is similar to a built-in chaos monkey which, when approached correctly, forces building more resilient systems by design.</p>\n\n\n\n<p>LLMs are a new kind of a software dependency that is particularly fast-evolving and must be managed as such. NVIDIA NIM delivers LLMs as standard container images, which enables building stable and secure production systems by leveraging battle-hardened best practices, without sacrificing the speed of innovation.</p>\n\n\n\n<p><a href=\"https://outerbounds.com/get-started\">Get started with NVIDIA NIM and Outerbounds</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>With the rapid expansion of language models over the past 18 months, hundreds of variants are now available. These include large language models (LLMs), small language models (SLMs), and domain-specific models\u2014many of which are freely accessible for commercial use. For LLMs in particular, the process of fine-tuning with custom datasets has also become increasingly affordable &hellip; <a href=\"https://developer.nvidia.com/blog/building-llm-powered-production-systems-with-nvidia-nim-and-outerbounds/\">Continued</a></p>\n", "protected": false}, "author": 2319, "featured_media": 89557, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1494832", "discourse_permalink": "https://forums.developer.nvidia.com/t/building-llm-powered-production-systems-with-nvidia-nim-and-outerbounds/308565", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 3110], "tags": [453, 2932, 3739], "coauthors": [4053, 3672], "class_list": ["post-89552", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-generative-ai", "tag-featured", "tag-large-language-models", "tag-nim"], "acf": {"post_industry": ["General"], "post_products": ["NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-outerbounds-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nio", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89552"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2319"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89552"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89552/revisions"}], "predecessor-version": [{"id": 89591, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89552/revisions/89591"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89557"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89552"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89552"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89552"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89552"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89672, "date": "2024-10-02T09:25:36", "date_gmt": "2024-10-02T16:25:36", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89672"}, "modified": "2024-10-17T12:07:03", "modified_gmt": "2024-10-17T19:07:03", "slug": "ai-uses-zero-shot-learning-to-find-existing-drugs-for-treating-rare-diseases", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/ai-uses-zero-shot-learning-to-find-existing-drugs-for-treating-rare-diseases/", "title": {"rendered": "AI Uses Zero-Shot Learning to Find Existing Drugs for Treating Rare Diseases"}, "content": {"rendered": "\n<p>A groundbreaking drug-repurposing AI model could bring new hope to doctors and patients trying to treat diseases with limited or no existing treatment options. Called <a href=\"http://txgnn.org\">TxGNN</a>, this zero-shot tool helps doctors find new uses for existing drugs for conditions that might otherwise go untreated. </p>\n\n\n\n<p>The <a href=\"https://www.nature.com/articles/s41591-024-03233-x\">study</a>, recently published in <em>Nature Medicine</em> and led by scientists from Harvard University, could reduce the time and cost for drug development\u2014delivering effective treatment to patients much more quickly.</p>\n\n\n\n<p>\u201cWith this tool, we aim to identify new therapies across the disease spectrum but when it comes to rare, ultrarare, and neglected conditions, we foresee this model could help close, or at least narrow, a gap that creates serious health disparities,\u201d study lead Marinka Zitnik, an assistant professor of biomedical informatics in the Blavatnik Institute at Harvard Medical School and Kempner Institute associate faculty member, said in a <a href=\"https://hms.harvard.edu/news/researchers-harness-ai-repurpose-existing-drugs-treatment-rare-diseases\">recent news post</a>.\u00a0</p>\n\n\n\n<p>Globally, more than 300M people are affected by over 7,000 rare or undiagnosed diseases. Of these rare diseases, around 7% have an FDA-approved drug treatment. This means that many patients are waiting and hoping for new therapies.&nbsp;</p>\n\n\n\n<p>This new AI tool addresses a limitation of most current models, which rely on known drugs for similar diseases. These models struggle with rare and poorly understood conditions due to a lack of data.</p>\n\n\n\n<p>TxGNN overcomes this issue by using graph neural networks (GNNs) to analyze and identify complex relationships and patterns in large medical data sets, which include information on diseases, drugs, and proteins.&nbsp;</p>\n\n\n\n<p>The researchers used the Kempner Institute\u2019s AI cluster, which includes <a href=\"https://www.nvidia.com/en-us/data-center/v100/\">NVIDIA V100 Tensor Core GPUs</a> and <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100 Tensor Core GPUs</a>, to train and fine-tune the model. According to Zitnik, the GPUs were critical in processing the large medical knowledge graph spanning 17,080 diseases and almost 8,000 drugs.\u00a0</p>\n\n\n\n<p>TxGNN relies on graph neural networks (GNN) to reason over complex biological data and generate transparent explanations for experts to review insights into its predictions. By analyzing these underlying connections, the AI model can understand and predict how a drug could influence a specific condition.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1828\" height=\"1231\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN.jpeg\" alt=\"A workflow illustration showing how TxGNN reasons.\" class=\"wp-image-89676\" style=\"width:728px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN.jpeg 1828w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN-300x202.jpeg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN-625x421.jpeg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN-171x115.jpeg 171w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN-768x517.jpeg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN-1536x1034.jpeg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN-645x434.jpeg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN-445x300.jpeg 445w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN-134x90.jpeg 134w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN-362x244.jpeg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN-163x110.jpeg 163w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/TxGNN-1024x690.jpeg 1024w\" sizes=\"(max-width: 1828px) 100vw, 1828px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. A workflow showing the steps used in the TxGNN model&nbsp;</em>(<a href=\"https://rdcu.be/dVJii\">Huang, K., Chandak, P., Wang, Q.&nbsp;<em>et al.</em></a>)</figcaption></figure></div>\n\n\n<p>In testing, the AI model improved treatment predictions by up to 19% without being trained on the specific disease. It also performed better than existing models at predicting contraindications, which are situations when a drug shouldn&#8217;t be used.</p>\n\n\n\n<p>Its treatment suggestions also matched medications that doctors often prescribe even when they aren&#8217;t approved for a specific condition.&nbsp;</p>\n\n\n\n<p>Visit the <a href=\"http://txgnn.org\">TxGNN Explorer</a> to learn more and experience the visual interface.&nbsp;</p>\n\n\n\n<p>Read the news coverage from <a href=\"https://hms.harvard.edu/news/researchers-harness-ai-repurpose-existing-drugs-treatment-rare-diseases\">Harvard Medical School</a> and <a href=\"https://kempnerinstitute.harvard.edu/news/txgnn-ai-dr-house-for-disease-treatment/\">Kempner Institute</a>. </p>\n", "protected": false}, "excerpt": {"rendered": "<p>A groundbreaking drug-repurposing AI model could bring new hope to doctors and patients trying to treat diseases with limited or no existing treatment options. Called TxGNN, this zero-shot tool helps doctors find new uses for existing drugs for conditions that might otherwise go untreated. The study, recently published in Nature Medicine and led by scientists &hellip; <a href=\"https://developer.nvidia.com/blog/ai-uses-zero-shot-learning-to-find-existing-drugs-for-treating-rare-diseases/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 89674, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1494817", "discourse_permalink": "https://forums.developer.nvidia.com/t/ai-uses-zero-shot-learning-to-find-existing-drugs-for-treating-rare-diseases/308557", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 1903], "tags": [3941, 2385, 453, 1877], "coauthors": [2315], "class_list": ["post-89672", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-features", "tag-ai-impact", "tag-drug-discovery", "tag-featured", "tag-research"], "acf": {"post_industry": ["General", "Healthcare & Life Sciences"], "post_products": ["H100", "V100"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/drug-repurposing-TxGNN.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nkk", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89672"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89672"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89672/revisions"}], "predecessor-version": [{"id": 89813, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89672/revisions/89813"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89674"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89672"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89672"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89672"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89672"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89663, "date": "2024-10-02T06:00:00", "date_gmt": "2024-10-02T13:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89663"}, "modified": "2024-11-14T08:07:17", "modified_gmt": "2024-11-14T16:07:17", "slug": "accelerating-llms-with-llama-cpp-on-nvidia-rtx-systems", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-llms-with-llama-cpp-on-nvidia-rtx-systems/", "title": {"rendered": "Accelerating LLMs with llama.cpp on NVIDIA RTX Systems"}, "content": {"rendered": "\n<p>The <a href=\"https://www.nvidia.com/en-us/ai-on-rtx/\">NVIDIA RTX AI for Windows PCs platform</a> offers a thriving ecosystem of thousands of open-source models for application developers to leverage and integrate into Windows applications. Notably, llama.cpp is one popular tool, with over 65K GitHub stars at the time of writing. Originally released in 2023, this open-source repository is a lightweight, efficient framework for <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language model (LLM)</a> inference that runs across a range of hardware platforms, including RTX PCs.&nbsp;</p>\n\n\n\n<p>This post explains how llama.cpp on RTX PCs offers a compelling solution for building cross-platform or Windows-native applications that require LLM functionality.&nbsp;</p>\n\n\n\n<h2 id=\"overview_of_llamacpp\"  class=\"wp-block-heading\">Overview of llama.cpp<a href=\"#overview_of_llamacpp\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>While LLMs have shown promise in unlocking exciting new use cases, their large memory and compute-intensive nature often make it challenging for developers to deploy them into production applications. To address this problem, llama.cpp provides a vast array of functionality to optimize model performance and deploy efficiently on a wide range of hardware.&nbsp;</p>\n\n\n\n<p>At its core, llama.cpp leverages the ggml tensor library for machine learning. This lightweight software stack enables cross-platform use of llama.cpp without external dependencies. Extremely memory efficient, it\u2019s an ideal choice for local on-device inference. The model data is packaged and deployed in a customized file format called GGUF, specifically designed and implemented by llama.cpp contributors.&nbsp;</p>\n\n\n\n<p>Developers building projects on top of llama.cpp can choose from thousands of prepackaged models, covering a wide range of high-quality quantizations. A growing open-source community is actively developing the llama.cpp and ggml projects.&nbsp;</p>\n\n\n\n<h2 id=\"accelerated_performance_of_llamacpp_on_nvidia_rtx\"  class=\"wp-block-heading\">Accelerated performance of llama.cpp on NVIDIA RTX<a href=\"#accelerated_performance_of_llamacpp_on_nvidia_rtx\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA continues to collaborate on improving and optimizing llama.cpp performance when running on RTX GPUs, as well as the developer experience. Some key contributions include:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://developer.nvidia.com/blog/optimizing-llama-cpp-ai-inference-with-cuda-graphs/\">Implementing CUDA Graphs in llama.cpp</a> to reduce overheads and gaps between kernel execution times to generate tokens.</li>\n\n\n\n<li>Reducing CPU overheads when preparing ggml graphs.</li>\n</ul>\n\n\n\n<p>For more information on the latest contributions, see <a href=\"https://developer.nvidia.com/blog/optimizing-llama-cpp-ai-inference-with-cuda-graphs/\">Optimizing llama.cpp AI Inference with CUDA Graphs</a>.</p>\n\n\n\n<p>Figure 1 shows NVIDIA internal measurements showcasing throughput performance on NVIDIA GeForce RTX GPUs using a Llama 3 8B model on llama.cpp. On the NVIDIA RTX 4090 GPU, users can expect ~150 tokens per second, with an input sequence length of 100 tokens and an output sequence length of 100 tokens.&nbsp;</p>\n\n\n\n<p>To build the llama.cpp library using NVIDIA GPU optimizations with the CUDA backend, visit <a href=\"https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#cuda\">llama.cpp/docs</a> on GitHub.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1531\" height=\"944\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image2.png\" alt=\"Bar graph showing inference performance of Llama 3 8B int4 with llama.cpp on four different NVIDIA GeForce RTX GPUs.\" class=\"wp-image-89668\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image2.png 1531w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image2-300x185.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image2-625x385.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image2-179x110.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image2-768x474.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image2-645x398.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image2-487x300.png 487w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image2-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image2-362x223.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image2-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image2-1024x631.png 1024w\" sizes=\"(max-width: 1531px) 100vw, 1531px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. NVIDIA internal throughput performance measurements on NVIDIA GeForce RTX GPUs, featuring a Llama 3 8B model with an input sequence lengths of 100 tokens,\u00a0 generating 100 tokens</em></em></figcaption></figure></div>\n\n\n<h2 id=\"ecosystem_of_developers_building_with_llamacpp\"  class=\"wp-block-heading\">Ecosystem of developers building with llama.cpp<a href=\"#ecosystem_of_developers_building_with_llamacpp\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>A vast ecosystem of developer frameworks and abstractions are built on top of llama.cpp for developers to further accelerate their application development journey. Popular developer tools such as <a href=\"https://ollama.com/\">Ollama</a>, <a href=\"http://jan.ai\">Homebrew</a>, and <a href=\"https://lmstudio.ai/\">LMStudio</a> all extend and leverage the capabilities of llama.cpp under-the-hood to offer abstracted developer experiences. Key functionalities of some of these tools include configuration and dependency management, bundling of model weights, abstracted UIs, and a locally run API endpoint to an LLM.&nbsp;</p>\n\n\n\n<p>Additionally, there is a broad ecosystem of models that are already pre-optimized and available for developers to leverage using llama.cpp on RTX systems. Notable models include the latest GGUF quantized versions of <a href=\"https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\">Llama 3.2 available on Hugging Face.</a></p>\n\n\n\n<p>In addition, llama.cpp is offered as an inference deployment mechanism as part of the <a href=\"https://github.com/NVIDIA/RTX-AI-Toolkit/blob/main/llm-deployment/llama.cpp_deployment.md\">NVIDIA RTX AI Toolkit</a>.&nbsp;</p>\n\n\n\n<h2 id=\"applications_accelerated_with_llamacpp_on_the_rtx_platform\"  class=\"wp-block-heading\">Applications accelerated with llama.cpp on the RTX platform<a href=\"#applications_accelerated_with_llamacpp_on_the_rtx_platform\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>More than 50 tools and apps are now accelerated with llama.cpp, including:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Backyard.ai:</strong> With Backyard.ai, users can unleash their creativity with AI by interacting with their favorite characters virtually, in a private environment, with full ownership and control. This platform leverages llama.cpp to accelerate LLM models on RTX systems.\u00a0\u00a0</li>\n\n\n\n<li><strong>Brave</strong>: Brave has built Leo, a smart AI assistant, directly into the Brave browser. With privacy-preserving Leo, users can now ask questions, summarize pages and PDFs, write code, and create new text. With Leo, users can leverage Ollama, which utilizes llama.cpp for acceleration on RTX systems, to interact with local LLMs on their devices.\u00a0\u00a0\u00a0\u00a0\u00a0</li>\n\n\n\n<li><strong>Opera</strong>: Opera has now integrated Local AI Models to augment a user\u2019s browsing needs as part of the developer version of Opera One. Opera has integrated these capabilities using Ollama, leveraging the llama.cpp backend running entirely locally on NVIDIA RTX systems. In Opera&#8217;s browser AI, Aria, users can also ask the engine about web pages for summaries and translations, get more information with additional searches, generate text and images, and read the responses out loud with support for over 50 languages.</li>\n\n\n\n<li><strong>Sourcegraph</strong>: Sourcegraph Cody is an AI coding assistant that supports the latest LLMs and uses the best developer context to provide accurate code suggestions. Cody can also work with models running on the local machine and in air-gapped environments. It leverages Ollama, which uses llama.cpp, for local inference support accelerated on NVIDIA RTX GPUs.</li>\n</ul>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Using <a href=\"https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#cuda\">llama.cpp on RTX AI PCs</a> offers developers a compelling solution to accelerate AI workloads on GPUs. With llama.cpp, developers can leverage a C++ implementation for LLM inferencing with a lightweight installation package. Learn more and get started with the <a href=\"https://github.com/NVIDIA/RTX-AI-Toolkit/blob/main/llm-deployment/llama.cpp_deployment.md\">llama.cpp on RTX AI Toolkit</a>.</p>\n\n\n\n<p>NVIDIA is committed to contributing to and accelerating open-source software on the <a href=\"https://www.nvidia.com/en-us/ai-on-rtx/\">RTX AI platform</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The NVIDIA RTX AI for Windows PCs platform offers a thriving ecosystem of thousands of open-source models for application developers to leverage and integrate into Windows applications. Notably, llama.cpp is one popular tool, with over 65K GitHub stars at the time of writing. Originally released in 2023, this open-source repository is a lightweight, efficient framework &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-llms-with-llama-cpp-on-nvidia-rtx-systems/\">Continued</a></p>\n", "protected": false}, "author": 1475, "featured_media": 89667, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1494729", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-llms-with-llama-cpp-on-nvidia-rtx-systems/308539", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 3110, 1903], "tags": [2499, 453, 4179, 2932, 4180], "coauthors": [2931], "class_list": ["post-89663", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-graphics", "category-generative-ai", "category-features", "tag-cuda-graphs", "tag-featured", "tag-llama-cpp", "tag-large-language-models", "tag-rtx-ai"], "acf": {"post_industry": ["General"], "post_products": ["CUDA", "RTX GPU"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Best practice"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/10/image1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nkb", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89663"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1475"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89663"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89663/revisions"}], "predecessor-version": [{"id": 89671, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89663/revisions/89671"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89667"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89663"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89663"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89663"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89663"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89597, "date": "2024-10-01T10:00:00", "date_gmt": "2024-10-01T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89597"}, "modified": "2024-11-04T14:51:46", "modified_gmt": "2024-11-04T22:51:46", "slug": "simplify-and-scale-ai-powered-metahuman-deployment-with-nvidia-ace-and-unreal-engine-5", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/simplify-and-scale-ai-powered-metahuman-deployment-with-nvidia-ace-and-unreal-engine-5/", "title": {"rendered": "Simplify and Scale AI-Powered MetaHuman Deployment with NVIDIA ACE and Unreal Engine 5"}, "content": {"rendered": "\n<p>At Unreal Fest 2024, NVIDIA released new Unreal Engine 5 on-device plugins for <a href=\"https://developer.nvidia.com/ace\">NVIDIA ACE</a>, making it easier to build and deploy AI-powered <a href=\"https://www.unrealengine.com/en-US/metahuman\">MetaHuman</a> characters on Windows PCs. ACE is a suite of digital human technologies that provide speech, intelligence, and animation powered by generative AI.</p>\n\n\n\n<p>Developers can also now access a new Audio2Face-3D plugin for AI-powered facial animations in Autodesk Maya. This plugin provides a simple, streamlined interface to help develop avatars in Maya easier and faster. The plugin comes with source code so that you, as a developer, can dive in and develop a plugin for the digital content creation (DCC) tool of your choice.&nbsp;</p>\n\n\n\n<p>NVIDIA has also built an Unreal Engine 5 renderer microservice that leverages Epic\u2019s Unreal Pixel Streaming technology. This microservice now supports the NVIDIA ACE <a href=\"https://catalog.ngc.nvidia.com/orgs/eevaigoeixww/teams/animation/containers/ia-animation-graph-microservice\">Animation Graph Microservice</a> and Linux operating system in early access. The Animation Graph Microservice enables realistic and responsive character movements, and with Unreal Pixel Streaming support, you can stream your MetaHuman creations to any device.</p>\n\n\n\n<h2 id=\"bring_life_to_metahuman_characters_with_the_latest_nvidia_ace_advancements\"  class=\"wp-block-heading\">Bring life to MetaHuman characters with the latest NVIDIA ACE advancements<a href=\"#bring_life_to_metahuman_characters_with_the_latest_nvidia_ace_advancements\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The NVIDIA ACE Unreal Engine 5 sample project serves as a guide for developers looking to integrate ACE into their games and applications. This sample project expands the number of on-device ACE plugins, including:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Audio2Face-3D for lip sync and facial animation</li>\n\n\n\n<li>The Nemotron Mini 4B Instruct model for response generation</li>\n\n\n\n<li>Retrieval-augmented generation (RAG) for contextual information</li>\n</ul>\n\n\n\n<p>As a developer, you can build a database for your intellectual property, generate relevant responses at low latency, and have those responses drive corresponding MetaHuman facial animations seamlessly in Unreal Engine 5. Each of these microservices are optimized to run on Windows PCs with low latency and a minimal memory footprint.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/qr7f1e-7g8Y?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 1. Get started with the Unreal Engine 5 plugin</em></em></figcaption></figure>\n\n\n\n<p>These new plugins are coming soon. To <a href=\"https://developer.nvidia.com/ace/get-started/\">get started</a>, ensure you have the appropriate ACE plugin and Unreal Engine sample downloaded alongside a MetaHuman character.</p>\n\n\n\n<h2 id=\"streamline_3d_animation_with_the_maya_ace_plugin\"  class=\"wp-block-heading\">Streamline 3D animation with the Maya ACE plugin<a href=\"#streamline_3d_animation_with_the_maya_ace_plugin\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Autodesk Maya offers high-performance animation functions for game developers and technical artists to create high-quality 3D graphics. Now you can generate high-quality, audio-driven facial animation for any character more easily with the Audio2Face-3D plugin. The streamlined user interface enables you to seamlessly transition to the Unreal Engine 5 environment. The source code and scripts are highly customizable and can be modified for use in other digital content creation tools.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/FsY81c3dDbI?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 2. Learn how to download and install Autodesk Maya for NVIDIA ACE</em></em></figcaption></figure>\n\n\n\n<p>To get started, <a href=\"https://build.nvidia.com/nvidia/audio2face/api\">generate an API key</a> or download the <a href=\"https://catalog.ngc.nvidia.com/orgs/eevaigoeixww/teams/animation/helm-charts/a2f-service\">Audio2Face-3D microservice</a>. The Audio2Face-3D microservice is part of <a href=\"https://developer.nvidia.com/nim\">NVIDIA NIM</a>, a set of easy-to-use microservices that speed up the deployment of foundation models on any cloud or data center.&nbsp;</p>\n\n\n\n<p>Next, ensure you have Autodesk Maya 2023, 2024, or 2025. Access the <a href=\"https://github.com/NVIDIA/Maya-ACE\">NVIDIA/Maya-ACE</a> GitHub repo, which includes the Maya plugin, gRPC client libraries, test assets, and a sample scene\u2014everything you need to explore, learn, and innovate with Audio2Face-3D.</p>\n\n\n\n<h2 id=\"scale_digital_human_technology_deployment_with_ue5_pixel_streaming\"  class=\"wp-block-heading\">Scale digital human technology deployment with UE5 Pixel Streaming<a href=\"#scale_digital_human_technology_deployment_with_ue5_pixel_streaming\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>When deploying digital human technology through the cloud, the goal is to simultaneously reach as many customers as possible. Streaming high-fidelity characters requires significant compute resources. The latest Unreal Engine 5 renderer microservice in NVIDIA ACE adds support for the NVIDIA Animation Graph Microservice and Linux operating systems in early access.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"393\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams-625x393.png\" alt=\"Bar chart showing number of concurrent streams with Unreal Engine presets. \n\" class=\"wp-image-89601\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams-625x393.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams-768x483.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams-1536x965.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams-645x405.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams-477x300.png 477w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams-143x90.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams-362x227.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams-175x110.png 175w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams-1024x643.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/unreal-engine-presets-concurrent-streams.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Number of concurrent streams with Unreal Engine presets. Note that performance may change after early access</em></em></figcaption></figure>\n\n\n\n<p>Animation Graph is a microservice that interacts with other AI models to create a conversational pipeline for characters. It\u2019s responsible for connecting developer RAG architectures, maintaining both context and conversational history. With the new UE5 pixel streaming compatibility, you can run a MetaHuman character on a server in the cloud and stream its rendered frames and audio to any browser and edge device over Web Real-Time Communication (WebRTC).&nbsp;</p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To get started with the Unreal Engine 5 renderer microservice, <a href=\"https://developer.nvidia.com/ace/early-access-form\">apply for early access</a>. Learn more about <a href=\"https://developer.nvidia.com/ace\">NVIDIA ACE</a> and <a href=\"https://developer.nvidia.com/ace/get-started/\">download the NIM microservices</a> to begin building game characters powered by generative AI.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>At Unreal Fest 2024, NVIDIA released new Unreal Engine 5 on-device plugins for NVIDIA ACE, making it easier to build and deploy AI-powered MetaHuman characters on Windows PCs. ACE is a suite of digital human technologies that provide speech, intelligence, and animation powered by generative AI. Developers can also now access a new Audio2Face-3D plugin &hellip; <a href=\"https://developer.nvidia.com/blog/simplify-and-scale-ai-powered-metahuman-deployment-with-nvidia-ace-and-unreal-engine-5/\">Continued</a></p>\n", "protected": false}, "author": 1046, "featured_media": 89611, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1494446", "discourse_permalink": "https://forums.developer.nvidia.com/t/simplify-and-scale-ai-powered-metahuman-deployment-with-nvidia-ace-and-unreal-engine-5/308469", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 3110], "tags": [3063, 453, 582], "coauthors": [2195], "class_list": ["post-89597", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-graphics", "category-generative-ai", "tag-omniverse-ace", "tag-featured", "tag-unreal-engine"], "acf": {"post_industry": ["Gaming", "Media & Entertainment"], "post_products": ["Avatar Cloud Engine (ACE)", "NIM"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/metahuman-character-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nj7", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89597"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1046"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89597"}], "version-history": [{"count": 14, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89597/revisions"}], "predecessor-version": [{"id": 89746, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89597/revisions/89746"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89611"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89597"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89597"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89597"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89597"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89574, "date": "2024-10-01T10:00:00", "date_gmt": "2024-10-01T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89574"}, "modified": "2024-11-04T14:51:09", "modified_gmt": "2024-11-04T22:51:09", "slug": "evolving-ai-powered-game-development-with-retrieval-augmented-generation", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/evolving-ai-powered-game-development-with-retrieval-augmented-generation/", "title": {"rendered": "Evolving AI-Powered Game Development with Retrieval-Augmented Generation"}, "content": {"rendered": "\n<p>Game development is a complex and resource-intensive process, particularly when using advanced tools like <a href=\"https://developer.nvidia.com/game-engines/unreal-engine\">Unreal Engine</a>. Developers find themselves navigating through vast amounts of information, often scattered across tutorials, user manuals, API documentation, and the source code itself. This multifaceted journey requires expertise in programming, design, and project management, all while balancing innovation with practical implementation to meet tight deadlines and player expectations.</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\">Large language models (LLMs)</a> are being integrated into various stages of the development pipeline.These models are transforming workflows by driving intelligent non-player characters (NPCs), assisting with code generation, and minimizing the time spent on repetitive tasks. However, the effectiveness of LLMs is limited when they lack access to specific domain knowledge\u2014be it a character&#8217;s backstory or the intricacies of a game engine&#8217;s source code. While fine-tuning these models with specialized data can help overcome these limitations, the process is often time-consuming and expensive, presenting a significant challenge for developers seeking to fully leverage AI in their workflows.</p>\n\n\n\n<p>This is where <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation (RAG)</a> comes into play. RAG is a software architecture that combines the capabilities of LLMs with information sources specific to a business, offering a more efficient alternative to model retraining. Using RAG, developers can supplement the internal knowledge of an LLM on the fly, grounding responses with accurate, up-to-date information without the need to retrain the entire model.</p>\n\n\n\n<p>This post explains how RAG is transforming game development by improving AI-generated content accuracy, reducing bias and hallucinations, and providing domain-specific responses.</p>\n\n\n\n<h2 id=\"what_is_retrieval-augmented_generation\"  class=\"wp-block-heading\">What is retrieval-augmented generation?<a href=\"#what_is_retrieval-augmented_generation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>RAG is designed to enhance the capabilities of LLMs by integrating them with additional data sources. RAG operates through four main components:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li><strong>User prompt</strong>: The process begins with an initial query or instruction from the user.</li>\n\n\n\n<li><strong>Information retrieval</strong>: RAG searches relevant datasets to find the most pertinent information.</li>\n\n\n\n<li><strong>Augmentation</strong>: The retrieved data is combined with the user prompt to enrich the input given to the LLM.</li>\n\n\n\n<li><strong>Content generation</strong>: The LLM generates a response based on the augmented prompt.</li>\n</ol>\n\n\n\n<p>RAG systems can use the latest information available on the web, within enterprise databases, or from file systems to produce informative and contextually relevant answers. This technique is particularly valuable in scenarios where up-to-date and domain-specific knowledge is crucial.</p>\n\n\n\n<p>RAG is an ideal solution for enterprises looking to maximize the value of their data and create more immersive gaming experiences. Some key benefits include:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Improved accuracy</strong>: RAG ensures that NPCs and game elements behave consistently with the latest game lore and mechanics, generating realistic and contextually appropriate dialogue and narrative elements.&nbsp;</li>\n\n\n\n<li><strong>Domain-specific responses</strong>: By integrating proprietary game design documents and lore, RAG enables tailored AI behavior that aligns with the game&#8217;s unique universe and style.</li>\n\n\n\n<li><strong>Reduced bias and hallucinations</strong>: By grounding responses in real data, RAG minimizes the risk of generating biased or inaccurate content.</li>\n\n\n\n<li><strong>Cost-effective implementation</strong>: RAG eliminates the need for frequent model retraining, enabling developers to quickly adapt AI systems to new game updates and expansions while reducing manual content creation efforts.</li>\n</ul>\n\n\n\n<h2 id=\"demonstrating_rag_with_unreal_engine_5\"  class=\"wp-block-heading\">Demonstrating RAG with Unreal Engine 5<a href=\"#demonstrating_rag_with_unreal_engine_5\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Game engine developers often deal with vast and frequently updated datasets. By embedding source code, documentation, and tutorials into locally running vector databases, they can use RAG to run inference and \u201cchat\u201d with their data.&nbsp;</p>\n\n\n\n<p>To showcase the power of RAG, we developed a demo using Epic Games\u2019 Unreal Engine 5, leveraging its extensive publicly available data. This demo is hosted on the OCI cloud infrastructure and powered by <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 Tensor Core GPU</a> instances. It features Code Llama 34 B, an LLM tuned for code generation, optimized by <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a> and <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">NVIDIA TensorRT-LLM</a>.</p>\n\n\n\n<p>The demo features three separate databases: user documentation, API documentation, and the source code itself. The RAG system retrieves relevant information from these databases and ranks the most useful results before presenting them to the LLM. While Code Llama can handle some basic Unreal Engine questions, its responses can be outdated or too generic for practical use. By integrating RAG, the system significantly enhances the accuracy and relevance of the responses, often including code examples and references to the original source materials.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/WJESpqM9QOc?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 1. Learn more about building an intelligent chatbot with RAG for game development</em></em></figcaption></figure>\n\n\n\n<p>Additionally, developers can build RAG-powered applications using the <a href=\"https://github.com/NVIDIA/workbench-example-hybrid-rag/tree/main\">NVIDIA AI Workbench Hybrid RAG Project</a>. This project seamlessly integrates with Unreal Engine 5 documentation, enabling developers to create a comprehensive knowledge base that enhances game development workflows. With <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/\">NVIDIA AI Workbench</a>, developers can leverage both local and cloud resources efficiently, and enjoy the flexibility to easily run embedding and retrieval processes on NVIDIA RTX GPUs while offloading inference to the cloud.&nbsp;</p>\n\n\n\n<p>This hybrid approach enables game creators to quickly access relevant information about engine features, blueprint scripting, and rendering techniques directly within their development environment, streamlining the process so they can focus more on creativity and innovation. Learn more about <a href=\"https://developer.nvidia.com/blog/optimize-ai-model-performance-and-maintain-data-privacy-with-hybrid-rag/\">building hybrid RAG applications using AI Workbench</a>.</p>\n\n\n\n<p>When you\u2019re ready to deploy a RAG-powered application into production, <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> provides enterprise support for the software used as part of the NVIDIA RAG pipeline. This includes <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM microservices</a>, which provide pre-built optimized inference engines with standard APIs in easy-to-deploy software containers.&nbsp;&nbsp;</p>\n\n\n\n<h2 id=\"real-world_rag_use_cases_for_game_development\"  class=\"wp-block-heading\">Real-world RAG use cases for game development<a href=\"#real-world_rag_use_cases_for_game_development\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>RAG offers significant benefits for game developers, enhancing the development process and improving the overall developer experience:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Enhanced documentation access</strong>: RAG streamlines interaction with Unreal Engine 5 documentation, enabling developers to quickly find answers about engine features, blueprint scripting, and rendering techniques directly within their development environment.</li>\n\n\n\n<li><strong>Intelligent code assistance</strong>: By leveraging vast codebases and best practices, RAG can provide context-aware code suggestions, improving coding efficiency and reducing errors.</li>\n\n\n\n<li><strong>Rapid prototyping</strong>: RAG assists in generating placeholder content, such as temporary dialogue or level descriptions, enabling faster iteration during the early stages of development.</li>\n\n\n\n<li><strong>Developer onboarding and training</strong>: Personalized tutorial systems powered by RAG can guide new team members based on their skill levels, significantly improving the onboarding process and supporting ongoing learning.</li>\n\n\n\n<li><strong>Automated bug resolution</strong>: RAG can help developers troubleshoot issues by retrieving relevant solutions from internal documentation, known issues databases, and community forums.</li>\n</ul>\n\n\n\n<h2 id=\"get_started_with_rag&nbsp;\"  class=\"wp-block-heading\">Get started with RAG&nbsp;<a href=\"#get_started_with_rag&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>RAG represents the next step in the evolution of AI-driven game development. By seamlessly integrating additional datasets with a foundation LLM, RAG enhances the accuracy, relevance, and timeliness of generated content. Whether for game development, lore retrieval, customer service, or countless other applications, RAG offers a cost-effective and powerful solution that can transform how enterprises and developers interact with their data.</p>\n\n\n\n<p>Join NVIDIA and Dell at Unreal Fest to discover how to build and scale RAG-powered chatbots to enhance the game development workflow and accelerate creative processes. Visit us at the NVIDIA/Dell booth in the expo area, and join us for our session, <a href=\"https://t.co/RiYmZzy8DM\">Bringing MetaHumans to Life with Generative AI</a>. We can\u2019t wait to see you there!</p>\n\n\n\n<p>Join the NVIDIA game development community and sign up to <a href=\"https://www.nvidia.com/en-us/industries/game-development/newsletter/\">receive the latest news</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Game development is a complex and resource-intensive process, particularly when using advanced tools like Unreal Engine. Developers find themselves navigating through vast amounts of information, often scattered across tutorials, user manuals, API documentation, and the source code itself. This multifaceted journey requires expertise in programming, design, and project management, all while balancing innovation with practical &hellip; <a href=\"https://developer.nvidia.com/blog/evolving-ai-powered-game-development-with-retrieval-augmented-generation/\">Continued</a></p>\n", "protected": false}, "author": 1629, "featured_media": 89580, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1494445", "discourse_permalink": "https://forums.developer.nvidia.com/t/evolving-ai-powered-game-development-with-retrieval-augmented-generation/308468", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [453, 2932, 3613, 582], "coauthors": [3159, 3909], "class_list": ["post-89574", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-large-language-models", "tag-retrieval-augmented-generation-rag", "tag-unreal-engine"], "acf": {"post_industry": ["Media & Entertainment"], "post_products": ["A100", "AI Workbench", "NIM", "TensorRT-LLM", "Triton Inference Server"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/RAG-Gaming.png", "jetpack_shortlink": "https://wp.me/pcCQAL-niK", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89574"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1629"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89574"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89574/revisions"}], "predecessor-version": [{"id": 89654, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89574/revisions/89654"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89580"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89574"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89574"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89574"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89574"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89521, "date": "2024-10-01T10:00:00", "date_gmt": "2024-10-01T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89521"}, "modified": "2024-10-17T12:07:06", "modified_gmt": "2024-10-17T19:07:06", "slug": "revolutionizing-cloud-gaming-and-graphics-rendering-with-nvidia-gdn", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/revolutionizing-cloud-gaming-and-graphics-rendering-with-nvidia-gdn/", "title": {"rendered": "Revolutionizing Cloud Gaming and Graphics Rendering with NVIDIA GDN"}, "content": {"rendered": "\n<p>Gaming has always pushed the boundaries of graphics hardware. The most popular games typically required robust GPU, CPU, and RAM resources on a user\u2019s PC or console\u2014that is, until the advent of <a href=\"https://developer.nvidia.com/industries/game-development/geforce-now\">GeForce NOW</a> and cloud gaming.</p>\n\n\n\n<p>Today, with the power of interactive streaming from the cloud, any user on almost any device can play the latest and greatest of today\u2019s games.&nbsp;</p>\n\n\n\n<p>However, in a typical cloud gaming arrangement, there are typically still two friction points remaining when the user wants to play:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>They must own the game.</li>\n\n\n\n<li>They must have one or more accounts, such as the cloud gaming service to which they are subscribed.&nbsp;</li>\n</ul>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/omniverse/solutions/stream-3d-apps/\">NVIDIA Graphics Delivery Network (GDN)</a> is a turnkey platform that removes the remaining friction, enabling seamless one-click play. With GDN, game publishers can stream games built with any 3D engine to almost any device, using the same underlying technology as GeForce NOW.&nbsp;</p>\n\n\n\n<p>Publishers choose a virtual machine type that matches the requirements of the game, upload their content to a developer portal, and then let NVIDIA GDN do the rest to stream it to audiences worldwide.&nbsp;</p>\n\n\n\n<p>Users can access the content through SDK integration points, such as a deep-link URL, or embedded in a webpage or native application built with the GDN Client SDK.</p>\n\n\n\n<p>GDN benefits from over 10 years of research and development of cloud graphics infrastructure. Starting with the Kepler generation of GPUs in 2013 (which powered GRID, NVIDIA\u2019s first cloud gaming service), NVIDIA has continually innovated best-in-class streaming technologies. In 2020, that investment culminated in the launch of the flagship cloud gaming service, GeForce NOW.&nbsp;</p>\n\n\n\n<p>In 2023, GDN came to life, using the same cloud infrastructure to serve enterprise needs. Designed originally for other verticals to stream high-fidelity NVIDIA Omniverse experiences such as digital twins and configurators, it wasn\u2019t long before gaming companies also realized the unique potential of GDN for user acquisition and engagement.&nbsp;</p>\n\n\n\n<h2 id=\"high-performance_streaming_events&nbsp;\"  class=\"wp-block-heading\">High-performance streaming events&nbsp;<a href=\"#high-performance_streaming_events&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>GDN provides you with a unique opportunity to host massive, multi-platform events, reaching new users outside traditional platforms or stores. With data centers located in major metropolitan cities globally, GDN ensures that most users can have a low-latency, near-native experience.</p>\n\n\n\n<h3 id=\"epic_games\"  class=\"wp-block-heading\">Epic Games<a href=\"#epic_games\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Recently, GDN streamed <a href=\"https://www.epicgames.com\">Epic Games</a>\u2019 <em>Metallica: Fuel. Fire. Fury</em> Fortnite concert to mobile devices in the USA, enabling a one-click join.\u00a0GDN enabled mobile users to easily participate in the touch-enabled virtual concert without requiring a cloud gaming subscription or any Fortnite installation. By clicking a link on their iPhone or Android devices, users instantly joined the touch-friendly experience from within Safari or Chrome mobile browsers. Because the game was already preloaded on reserved virtual machines, users went from click to Fortnite within seconds.\u00a0</p>\n\n\n\n<h3 id=\"zenos\"  class=\"wp-block-heading\">ZENOS<a href=\"#zenos\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Another showcase of the virtual event engagement opportunity is <a href=\"https://www.virtexstadium.com/\">ZENOS</a> (then known as Virtex Stadium), a virtual stadium available on Steam that provides immersive VR and 3D spectating experiences for esports.&nbsp;</p>\n\n\n\n<p>In 2023, ZENOS offered a way for users to join the virtual watch party experience for <em>League of Legends\u2019 Worlds Championship</em> without any downloads, installations, or logins, using GDN&#8217;s powerful GPUs and global cloud infrastructure.\u00a0Fans accessed ZENOS in Chrome, Safari, or Edge browsers through GDN when an event was live, and enjoyed the excitement of <em>Worlds</em> with exceptional visual quality, regardless of what GPU or CPU powered their local device.</p>\n\n\n\n<p>Here\u2019s what Tim Mcguinness, CEO and co-founder at ZENOS, had to say. \u201cZENOS transforms the entertainment viewing experience by creating fully immersive 3D environments for live esports events. GDN&#8217;s low-latency, high-fidelity streaming ensures that our fans can enter our virtual stadiums in one click and enjoy smooth, visually stunning experiences from their web browsers. Together, we aim to make all gaming events more accessible and engaging for audiences worldwide.\u201d</p>\n\n\n\n<h3 id=\"ludeo\"  class=\"wp-block-heading\">Ludeo<a href=\"#ludeo\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://www.ludeo.com/\">Ludeo</a> enhances game discovery by enabling players to access and interact with playable game highlights through social media, using GDN for seamless, high-fidelity streaming. Together, Ludeo and GDN support user acquisition and engagement by enabling you to deliver immediate, bite-sized experiences of core gameplay directly created by gamers themselves.</p>\n\n\n\n<p>Integrating Ludeo\u2019s SDK enables players to join a specific state of the game and play user-created game highlights. The content is then streamed from GDN data centers to end users, who can join a specific game challenge with a shared deep link. They can experience that exact game moment within seconds. The seamless interaction creates a frictionless entry point into the most engaging parts of the game.</p>\n\n\n\n<p>\u201cLudeo revolutionizes game discovery by enabling players to discover, play, create, and share playable gaming moments on social media. Together with NVIDIA\u2019s GDN, we are making it easier than ever before for developers to engage users with core gameplay,\u201d said Asaf Gazit, co-founder and CEO at Ludeo.</p>\n\n\n\n<h2 id=\"infrastructure_behind_gdn\"  class=\"wp-block-heading\">Infrastructure behind GDN<a href=\"#infrastructure_behind_gdn\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The backbone of GDN is NVIDIA L40 series GPUs based on the NVIDIA Ada Lovelace architecture, including NVIDIA <a href=\"https://www.nvidia.com/en-us/data-center/l40/\">L40</a> and <a href=\"https://www.nvidia.com/en-us/data-center/l40s/\">L40S</a>. With 48 GB of graphics memory and 142 third-generation RT Cores that enhance real-time ray tracing capabilities, these GPUs are uniquely suited for heavy rendering workloads and high-definition streaming.&nbsp;</p>\n\n\n\n<p>GDN offers various instance types or virtual machine profiles, ranging from full to fractional GPUs. You don&#8217;t need any orchestration or manual virtualization. Everything is already done by GDN, with one instance streaming to one user.&nbsp;</p>\n\n\n\n<p>Making use of <a href=\"https://www.nvidia.com/en-us/data-center/virtual-solutions/\">NVIDIA virtual GPU (vGPU)</a> technology, the GPU frame buffer is partitioned, dedicating memory to each virtual machine. GPU cycles are time-sliced between each instance, ensuring efficient resource sharing. Dedicated CPU cores, temp storage, and system RAM are also allocated as part of the profile.</p>\n\n\n\n<p>Meanwhile, more than 30 strategically placed data centers worldwide\u2014in more than 130 countries\u2014ensure low-latency performance for a large population coverage, crucial for interactive-intensive applications like gaming.</p>\n\n\n\n<h2 id=\"supported_devices\"  class=\"wp-block-heading\">Supported devices<a href=\"#supported_devices\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>GDN supports a wide range of client devices. It can also stream in low-bandwidth environments as low as 5 Mbps/user. Supported devices include the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>PCs and laptops: </strong>GDN can stream up to 1600p/1440p and up to 120 FPS in a web browser. Officially supported browsers include Chrome, Edge, and Safari.&nbsp;</li>\n\n\n\n<li><strong>Tablets and smartphones:</strong> Both Android and iOS devices are supported with Chrome and Safari browsers, respectively.</li>\n\n\n\n<li><strong>Smart TVs:</strong> <a href=\"https://nvidia.custhelp.com/app/answers/detail/a_id/5410/~/how-can-i-set-up-4k-streaming-with-geforce-now-on-my-smart-tv%3F#:~:text=GeForce%20NOW%20is%20available%20on,with%20the%20latest%20software%20version\">LG, Samsung, and Android TVs</a> can stream 4K interactive 3D content with the help of the GeForce NOW application.</li>\n\n\n\n<li><strong>Apple Vision Pro (coming soon):</strong> As <a href=\"https://blogs.nvidia.com/blog/omniverse-apple-vision-pro/\">showcased at GTC</a>, Omniverse-based applications will soon be streamed to Apple Vision Pro headsets, with Unreal and Unity support planned.&nbsp;</li>\n</ul>\n\n\n\n<h2 id=\"gdn_billing\"  class=\"wp-block-heading\">GDN billing<a href=\"#gdn_billing\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>GDN billing operates on a deposit system where a minimum payment is made in advance, and streaming use is tracked and debited regularly from the account. Pricing takes into account various factors such as instance type, region, and consumption model.&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Instance type:</strong> Costs vary depending on the type of virtual machine profile selected. Full GPU instance types cost more than those with fractional vGPUs.</li>\n\n\n\n<li><strong>Region: </strong>There may be a different cost depending on the location of the data center.</li>\n\n\n\n<li><strong>Consumption model:</strong>\n<ul class=\"wp-block-list\">\n<li><strong>On-demand:</strong> Billing is based on the amount of time that the virtual machines are actually used, tracked per minute. On-demand instances are not guaranteed and users may experience queuing during peak hours. When a user joins a session, it will not be revoked.&nbsp;</li>\n\n\n\n<li><strong>Reserved:</strong> These instances are booked in advance to guarantee access and to prewarm the application to reduce loading times. There is a short-term hourly reserved option for events, and a long-term reserved option (1 month or longer). There is also the possibility to schedule a reservation for a recurring period such as every Monday to Friday, 8AM to 5PM.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h2 id=\"how_to_get_started&nbsp;\"  class=\"wp-block-heading\">How to get started&nbsp;<a href=\"#how_to_get_started&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To start streaming your content, see the <a href=\"https://www.nvidia.com/en-us/omniverse/solutions/stream-3d-apps/\">NVIDIA Graphics Delivery Network</a> page to explore and compare the GDN instance types available. Contact us at <a href=\"mailto:gdn@nvidia.com\">gdn@nvidia.com</a> with your requirements and a sales representative will be in touch.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Gaming has always pushed the boundaries of graphics hardware. The most popular games typically required robust GPU, CPU, and RAM resources on a user\u2019s PC or console\u2014that is, until the advent of GeForce NOW and cloud gaming. Today, with the power of interactive streaming from the cloud, any user on almost any device can play &hellip; <a href=\"https://developer.nvidia.com/blog/revolutionizing-cloud-gaming-and-graphics-rendering-with-nvidia-gdn/\">Continued</a></p>\n", "protected": false}, "author": 2324, "featured_media": 89533, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1494447", "discourse_permalink": "https://forums.developer.nvidia.com/t/revolutionizing-cloud-gaming-and-graphics-rendering-with-nvidia-gdn/308470", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 852], "tags": [453, 3592], "coauthors": [4057, 4058], "class_list": ["post-89521", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-graphics", "category-data-center-cloud", "tag-featured", "tag-media-streaming"], "acf": {"post_industry": ["Gaming"], "post_products": ["Omniverse"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvdia-gdn-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nhT", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89521"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2324"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89521"}], "version-history": [{"count": 3, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89521/revisions"}], "predecessor-version": [{"id": 89739, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89521/revisions/89739"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89533"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89521"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89521"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89521"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89521"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89625, "date": "2024-10-01T09:00:00", "date_gmt": "2024-10-01T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89625"}, "modified": "2024-11-07T15:29:42", "modified_gmt": "2024-11-07T23:29:42", "slug": "evaluating-medical-rag-with-nvidia-ai-endpoints-and-ragas", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/evaluating-medical-rag-with-nvidia-ai-endpoints-and-ragas/", "title": {"rendered": "Evaluating Medical RAG with NVIDIA AI Endpoints and Ragas"}, "content": {"rendered": "\n<p>In the rapidly evolving field of medicine, the integration of cutting-edge technologies is crucial for enhancing patient care and advancing research. One such innovation is <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation (RAG)</a>, which is transforming how medical information is processed and used.&nbsp;</p>\n\n\n\n<p>RAG combines the capabilities of <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models (LLMs)</a> with external knowledge retrieval, addressing critical limitations such as outdated information and the generation of inaccurate data, known as hallucinations. By retrieving up-to-date and relevant information from structured databases, scientific literature, and patient records, RAG provides a more accurate and contextually aware foundation for medical applications. This hybrid approach improves the accuracy and reliability of generated outputs and enhances interpretability, making it a valuable tool in areas like drug discovery and clinical trial screening.&nbsp;</p>\n\n\n\n<p>As we continue to explore the potential of <a href=\"https://developer.nvidia.com/blog/develop-secure-reliable-medical-apps-with-rag-and-nvidia-nemo-guardrails/\">RAG in medicine</a>, it is essential to evaluate its performance rigorously, considering both the retrieval and generation components to ensure the highest standards of accuracy and relevance in medical applications. Medical RAG systems have unique demands and requirements, highlighting the need for comprehensive evaluation frameworks that can robustly address them.&nbsp;</p>\n\n\n\n<p>In this post, I show you how to address medical evaluation challenges using LangChain NVIDIA AI endpoints and Ragas. You use the <a href=\"https://figshare.com/articles/dataset/MACCROBAT2018/9764942\">MACCROBAT dataset</a>, a dataset of detailed patient medical reports taken from PubMed Central, enriched with meticulously annotated information.</p>\n\n\n\n<h2 id=\"challenges_of_medical_rag\"  class=\"wp-block-heading\">Challenges of Medical RAG<a href=\"#challenges_of_medical_rag\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>One primary challenge is scalability. As the volume of <a href=\"https://www.lek.com/insights/hea/eu/ei/tapping-new-potential-realising-value-data-healthcare-sector\">medical data grows at a CAGR of &gt;35%</a>, RAG systems must efficiently process and retrieve relevant information without compromising speed or accuracy. This is crucial in real-time applications where timely access to information can directly impact patient care.&nbsp;</p>\n\n\n\n<p>The specific language and knowledge required for medical applications can differ vastly from other domains, such as legal or financial sectors, limiting the system&#8217;s versatility and requiring domain-specific tuning.&nbsp;</p>\n\n\n\n<p>Another critical challenge is the lack of medical RAG benchmarks and the inadequacy of general evaluation metrics for this domain. The lack of benchmarks requires the generation of synthetic test and ground truth data based on medical texts and health records.&nbsp;</p>\n\n\n\n<p>Traditional metrics like BLEU or ROUGE, which focus on text similarity, do not adequately capture the nuanced performance of RAG systems. These metrics often fail to reflect the factual accuracy and contextual relevance of the generated content, which are crucial in medical applications.&nbsp;</p>\n\n\n\n<p>Finally, evaluating RAG systems also involves assessing both the retrieval and generation components independently and as a whole. The retrieval component must be evaluated for its ability to fetch relevant and current information from vast and dynamic knowledge bases. This includes measuring precision, recall, and relevance, while also considering the temporal aspects of information.&nbsp;</p>\n\n\n\n<p>The generation component, powered by large language models, must be evaluated for the faithfulness and accuracy of the content it produces, ensuring that it aligns with the retrieved data and the original query.&nbsp;</p>\n\n\n\n<p>Overall, these challenges highlight the need for comprehensive evaluation frameworks that can address the unique demands of medical RAG systems, ensuring they provide accurate, reliable, and contextually appropriate information.</p>\n\n\n\n<h2 id=\"what_is_ragas\"  class=\"wp-block-heading\">What is Ragas?<a href=\"#what_is_ragas\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://docs.ragas.io/\">Ragas</a> (retrieval-augmented generation assessment) is a popular, open-source, automated evaluation framework designed to evaluate RAG pipelines.&nbsp;</p>\n\n\n\n<p>The Ragas framework provides <a href=\"https://docs.ragas.io/en/latest/concepts/metrics/index.html\">tools and metrics</a> to assess the performance of these pipelines, focusing on aspects such as context relevancy, context recall, faithfulness, and answer relevancy. It employs LLM-as-a-judge for reference-free evaluations, which minimizes the need for human-annotated data and provides human-like feedback. This makes the evaluation process more efficient and cost-effective.</p>\n\n\n\n<h2 id=\"strategies_for_evaluating_rag\"  class=\"wp-block-heading\">Strategies for evaluating RAG<a href=\"#strategies_for_evaluating_rag\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>A typical strategy for robust evaluation of RAG follows this process:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Generate a set of synthetically generated triplets (question-answer-context) based on the documents in the vector store.&nbsp;</li>\n\n\n\n<li>Run evaluation precision/recall metrics for each sample question by running it through the RAG and comparing the response and context to ground truth.</li>\n\n\n\n<li>Filter out low-quality synthetic samples.</li>\n\n\n\n<li>Run the sample queries on the actual RAG and evaluate using the metrics using synthetic context and response as ground truth.</li>\n</ol>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"527\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag-1024x527.png\" alt=\"Diagram shows a question, such as \u2018What are typical BP measurements in the case of congestive heart failure?\u201d The system asks whether the retrieved context is relevant to the question and then whether the retrieved context contains information relevant to the question. The response might start with something like, \u201cWhile normal blood pressure is generally considered below 120/80 mmHg, heart failure patients often require careful management within a target range\u2026.\u201d The system then asks if the response is accurate and relevant to the question.\" class=\"wp-image-89633\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag-1024x527.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag-300x154.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag-625x321.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag-179x92.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag-768x395.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag-1536x790.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag-645x332.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag-500x257.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag-160x82.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag-362x186.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag-214x110.png 214w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-component-flow-medical-rag.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Evaluation component flow for RAG and search system</em></figcaption></figure></div>\n\n\n<p>To make the best use of this tutorial, you need a basic knowledge of LLM inference pipelines.</p>\n\n\n\n<h3 id=\"set_up&nbsp;\"  class=\"wp-block-heading\">Set up&nbsp;<a href=\"#set_up&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To get started, create a free account with the <a href=\"https://build.nvidia.com/\">NVIDIA API Catalog</a> and follow these steps:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Select any model.</li>\n\n\n\n<li>Choose <strong>Python,</strong><em> </em><strong>Get API Key</strong><em>.</em></li>\n\n\n\n<li>Save the generated key as <code>NVIDIA_API_KEY</code>.</li>\n</ol>\n\n\n\n<p>From there, you should have access to the endpoints.</p>\n\n\n\n<p>Now, install LangChain, NVIDIA AI endpoints, and Ragas:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: bash; title: ; notranslate\" title=\"\">\npip install langchain\npip install langchain_nvidia_ai_endpoints\npip install ragas=0.1.10\n</pre></div>\n\n\n<h3 id=\"download_the_medical_dataset\"  class=\"wp-block-heading\">Download the medical dataset<a href=\"#download_the_medical_dataset\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Next, download the <a href=\"https://www.kaggle.com/datasets/okolojeremiah/maccrobat\">Kaggle MACCROBAT dataset</a>. You can either download the dataset directly from Kaggle (requires the Kaggle API token) or use the <a href=\"https://huggingface.co/datasets/singh-aditya/MACCROBAT_biomedical_ner\">/MACCROBAT_biomedical_ner</a> version from <a href=\"https://huggingface.co/\">Hugging Face</a>.&nbsp;</p>\n\n\n\n<p>For this post, you use the full text from the medical reports and ignore the NER annotations:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom langchain_community.document_loaders import HuggingFaceDatasetLoader\nfrom datasets import load_dataset\n\ndataset_name = &quot;singh-aditya/MACCROBAT_biomedical_ner&quot;\npage_content_column = &quot;full_text&quot;\n\nloader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\ndataset = loader.load()\n</pre></div>\n\n\n<h3 id=\"generate_synthetic_data\"  class=\"wp-block-heading\">Generate synthetic data<a href=\"#generate_synthetic_data\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>One of the key challenges in RAG evaluation is the generation of synthetic data. This is required for robust evaluation, as you want to test your RAG system on questions relevant to the data in the vector database.&nbsp;</p>\n\n\n\n<p>A key benefit of this approach is that it enables wide testing while not requiring costly human-annotated data. A set of LLMs (<code>generator</code>, <code>critic</code>, <code>embedding</code>) are used to generate representative synthetic data based on relevant data. Ragas uses OpenAI by default so you override this to use NVIDIA AI endpoints instead.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n\ncritic_llm = ChatNVIDIA(model=&quot;meta/llama-3.1-8b-instruct&quot;)\ngenerator_llm = ChatNVIDIA(model=&quot;mistralai/mixtral-8x7b-instruct-v0.1&quot;)\nembeddings = NVIDIAEmbeddings(model=&quot;nv-embedqa-e5-v5&quot;, truncate=&quot;END&quot;)\n\ngenerator = TestsetGenerator.from_langchain(\n     generator_llm,\n     critic_llm,\n     embeddings,\n     chunk_size=512\n)\n\n# generate testset\ntestset = generator.generate_with_langchain_docs(dataset,  test_size=10, is_async=False, raise_exceptions=False, distributions={simple: 1.0})\n</pre></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"255\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline-1024x255.png\" alt=\"Diagram showing an evaluation pipeline for medical RAG, consisting of input from the EHR database; synthetic questions, answers, and contexts; and output and metrics.\" class=\"wp-image-89635\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline-1024x255.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline-300x75.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline-625x155.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline-179x45.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline-768x191.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline-1536x382.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline-645x160.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline-500x124.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline-160x40.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline-362x90.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline-442x110.png 442w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/evaluation-system-medical-rag-pipeline.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Pipeline of evaluation system for medical RAG</em></figcaption></figure></div>\n\n\n<p>Deploy the code on a vector store based on medical reports from the <a href=\"https://www.kaggle.com/datasets/okolojeremiah/maccrobat\">MACCROBAT dataset</a>. This generates a list of sample questions based on the actual documents in the vector store:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n&#x5B;\u201cWhat are typical BP measurements in the case of congestive heart failure?\u201d,\n\u201cWhat can scans reveal in patients with severe acute pain in the periumbilical region?\u201d\n\u201cIs surgical intervention required for the treatment of a metachronous solitary liver metastasis?\u201d  \n\u201cWhat are the most effective procedures for detecting gastric cancer?\u201d]\n</pre></div>\n\n\n<p>In addition, each of the questions is associated with a retrieved context and generated ground truth answer, which you can later use to independently&nbsp;evaluate and grade the retrieval and generation components of the medical RAG.</p>\n\n\n\n<h3 id=\"evaluate_the_input_data\"  class=\"wp-block-heading\">Evaluate the input data<a href=\"#evaluate_the_input_data\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>You can now use the synthetic data as input data for evaluation. Populate the input data with the generated questions (<code>question</code>) and responses (<code>ground_truth</code>), as well as the actual retrieved contexts from your medical RAG system (<code>contexts</code>) and their respective responses (<code>answer</code>).&nbsp;</p>\n\n\n\n<p>In this code example, you evaluate generation-specific metrics (<code>answer_relevancy</code>, <code>faithfulness</code>):</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n   # answer relevance and faithfulness metrics ignore ground truth, so just fill it with empty values\n    ground_truth = &#x5B;&#039;&#039;]*len(queries)\n    answers = &#x5B;]\n    contexts = &#x5B;]\n\n    # Run queries in search endpoint and collect context and results \n    for query in queries:\n        json_data = query_rag(query)\n\n        response =json_data&#x5B;&#039;results&#039;]&#x5B;0]&#x5B;&#039;answer&#039;]\n        answers.append(response)\n  \n        seq_str = &#x5B;]\n        seq_str.append(json_data&#x5B;&#039;results&#039;]&#x5B;0]&#x5B;&#039;retrieved _document_context&#039;])\n        contexts.append(seq_str)\n\n    # Store all data in HF dataset for RAGAS\n    data = {\n        &quot;question&quot;: queries,\n        &quot;answer&quot;: answers,\n        &quot;contexts&quot;: contexts,\n        &quot;ground_truth&quot;: ground_truth\n    }\n    dataset= DatasetDict()\n    dataset&#x5B;&#039;eval&#039;]=Dataset.from_dict(data)\n\n    # Override OpenAI LLM and embedding with NVIDIA AI endpoints\n    nvidia_llm = ChatNVIDIA(model=&quot;meta/llama-3.1-8b-instruct&quot;)    \nnvidia_embeddings = NVIDIAEmbeddings(model=&quot;nvidia/nv-embedqa-e5-v5&quot;, truncate=&quot;END&quot;)\n\n   result = evaluate(\n            dataset&#x5B;&quot;eval&quot;],\n            metrics=&#x5B;answer_relevancy, \n                faithfulness\n                ],\n            llm=nvidia_llm,\n            embeddings=nvidia_embeddings,\n            raise_exceptions=False,\n            is_async=False,\n        )\n</pre></div>\n\n\n<h3 id=\"apply_to_semantic_search\"  class=\"wp-block-heading\">Apply to semantic search<a href=\"#apply_to_semantic_search\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>You can further modify the system to evaluate semantic search based on keywords, as opposed to question/answer pairs. In this case, you extract the keyphrases from Ragas and ignore the generated testset of question/answer data. This is often useful in medical systems where a full RAG pipeline is not yet deployed.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ntestset = generator.generate_with_langchain_docs(&#x5B;doc], test_size=10, is_async=False, raise_exceptions=False, distributions={simple: 1.0})\n        queries = &#x5B;]\n        for node in generator.docstore.nodes:\n            queries += node.keyphrases\n        return queries\n</pre></div>\n\n\n<p>This now generates queries, as opposed to questions, which you can feed into any medical semantic search system for evaluation:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n&#x5B;\u201clesion\u201d, \u201cintraperitoneal fluid\u201d, \u201cRF treatment\u201d, \u201cpalpitations\u201d, \u201cthoracoscopic lung biopsy\u201d, \u201cpreoperative chemoradiotherapy\u201d, \u201chaemoglobin level\u201d, \u201cneedle biopsy specimen\u201d, \u201chypotension\u201d, \u201ctachycardia\u201d,  \u201cabdominal radiograph\u201d, \u201cpneumatic dilatation balloon\u201d, \u201ccomputed tomographic (CT) scan\u201d, \u201ctumor cells\u201c, \u201cradiologic examinations\u201c, \u201cS-100 protein\u201c, \u201cultrastructural analysis\u201d, \u201cBirbeck granules\u201d, \u201cdiastolic congestive heart failure (CHF)\u201d, \u201cBrachial blood pressure\u201d, \u201cventricular endomyocardial biopsy\u201d, \u201cmyocarditis\u201d, \u201cinfiltrative cardiomyopathies\u201d, \u201cstenosis\u201d, \u201cdiastolic dysfunction\u201d,  \u201cautoimmune hepatitis\u201d]\n</pre></div>\n\n\n<h2 id=\"customizing_for_semantic_search\"  class=\"wp-block-heading\">Customizing for semantic search<a href=\"#customizing_for_semantic_search\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>As mentioned earlier, default evaluation metrics are not always sufficient for medical systems, and often must be customized to support domain-specific challenges.&nbsp;</p>\n\n\n\n<p>To this end, you can create custom metrics in Ragas. This requires creating a custom prompt. In this example, you create a custom prompt to measure retrieval precision for a semantic search query:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nRETRIEVAL_PRECISION = Prompt(\n    name=&quot;retrieval_precision&quot;,\n    instruction=&quot;&quot;&quot;if a user put this query into a search engine, is this result relevant enough that it could be in the first page of results? Answers should STRICTLY be either &#039;1&#039; or &#039;0&#039;. Answer &#039;0&#039; if the provided summary does not contain enough information to answer the question and answer &#039;1&#039; if the provided summary can answer the question.&quot;&quot;&quot;,\n    input_keys=&#x5B;&quot;question&quot;, &quot;context&quot;],\n    output_key=&quot;answer&quot;,\n    output_type=&quot;json&quot;,\n)\n</pre></div>\n\n\n<p>Next, build a new class inheriting from <code>MetricWithLLM</code> and override the <code>_ascore</code> function to compute a score based on the prompt response:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n@dataclass\nclass RetrievalPrecision(MetricWithLLM):\n\n    name: str = &quot;retrieval_precision&quot;  # type: ignore\n    evaluation_mode: EvaluationMode = EvaluationMode.qc  # type: ignore\n    context_relevancy_prompt: Prompt = field(default_factory=lambda: RETRIEVAL_PRECISION)\n\n    async def _ascore(self, row: t.Dict, callbacks: Callbacks, is_async: bool) -&gt; float:\n        score=response&#x5B;0] # first token is the result &#x5B;0,1]\n        if score.isnumeric():\n            return int(score)\n        else:\n            return 0\n\n    retrieval_precision = RetrievalPrecision()\n</pre></div>\n\n\n<p>Now your new custom metric is defined as <code>retrieval_precision</code> and you can use it in the standard Ragas evaluation pipeline:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nnvidia_llm = ChatNVIDIA(model=&quot;meta/llama-3.1-8b-instruct&quot;)\nnvidia_embeddings = NVIDIAEmbeddings(model=&quot;nvidia/nv-embedqa-e5-v5&quot;, truncate=&quot;END&quot;)\n\nscore = evaluate(dataset&#x5B;&quot;eval&quot;], metrics=&#x5B;retrieval_precision], llm=nvidia_llm, embeddings=nvidia_embeddings, raise_exceptions=False, is_async=False)\n</pre></div>\n\n\n<h2 id=\"refining_with_structured_output\"  class=\"wp-block-heading\">Refining with structured output<a href=\"#refining_with_structured_output\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>RAG and LLM evaluation frameworks employ LLM-as-a-judge techniques, often requiring long and complicated prompts. As you saw in the earlier example of a custom metric prompt, this also requires parsing and post-processing of the LLM response.&nbsp;</p>\n\n\n\n<p>You can refine this process, making it more robust, by using the <a href=\"https://api.python.langchain.com/en/latest/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html#langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.with_structured_output\">structured output</a> feature of <a href=\"https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints\">LangChain NVIDIA AI endpoints</a>. Modifying the earlier prompt yields a simplified pipeline:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nimport enum\n\nclass Choices(enum.Enum):\n    Y = &quot;Y&quot;\n    N = &quot;N&quot;\n\nstructured_llm = nvidia_llm.with_structured_output(Choices)\n\nstructured_llm.invoke(&quot;if a user put this query into a search engine, is this result relevant enough that it could be in the first page of results? Answer &#039;N&#039; if the provided summary does not contain enough information to answer the question and answer &#039;Y&#039; if the provided summary can answer the question.&quot;)\n</pre></div>\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>RAG has emerged as a powerful approach, combining the strengths of LLMs and dense vector representations.&nbsp; By using dense vector representations, RAG models can scale efficiently, making them well-suited for large-scale enterprise applications, such as multilingual customer service chatbots and code generation agents.&nbsp;</p>\n\n\n\n<p>As LLMs continue to evolve, it is clear that RAG will play an increasingly important role in driving innovation and delivering high-quality, intelligent systems in medicine.&nbsp;</p>\n\n\n\n<p>When evaluating a medical RAG system, it&#8217;s crucial to consider several key factors:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>The system should provide accurate, relevant, and up-to-date information while remaining faithful to the retrieved context.&nbsp;</li>\n\n\n\n<li>It must demonstrate robustness in handling specialized medical terminology and concepts, as well as noisy or imperfect inputs.&nbsp;</li>\n\n\n\n<li>Proper evaluation involves using appropriate metrics for both retrieval and generation components, benchmarking against specialized medical datasets, and considering cost-effectiveness.&nbsp;</li>\n\n\n\n<li>Incorporating feedback from healthcare professionals and conducting continuous evaluations are essential to ensure the system&#8217;s practical utility and relevance in clinical settings.</li>\n</ul>\n\n\n\n<p>The pipeline described in this post addresses all these points and can be further embellished to include additional metrics and features.&nbsp;</p>\n\n\n\n<p>For more information about a reference evaluation tool using Ragas, see the <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG/tools/evaluation\">evaluation example</a> on the <a href=\"https://github.com/NVIDIA/GenerativeAIExamples\">/NVIDIA/GenerativeAIExamples</a> GitHub repo.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In the rapidly evolving field of medicine, the integration of cutting-edge technologies is crucial for enhancing patient care and advancing research. One such innovation is retrieval-augmented generation (RAG), which is transforming how medical information is processed and used.&nbsp; RAG combines the capabilities of large language models (LLMs) with external knowledge retrieval, addressing critical limitations such &hellip; <a href=\"https://developer.nvidia.com/blog/evaluating-medical-rag-with-nvidia-ai-endpoints-and-ragas/\">Continued</a></p>\n", "protected": false}, "author": 1992, "featured_media": 89632, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1494426", "discourse_permalink": "https://forums.developer.nvidia.com/t/evaluating-medical-rag-with-nvidia-ai-endpoints-and-ragas/308465", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [296, 453, 1948, 3813, 2932, 3613], "coauthors": [3689], "class_list": ["post-89625", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "tag-ai-inference-microservices", "tag-featured", "tag-healthcare-and-lifesciences", "tag-langchain", "tag-large-language-models", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["Healthcare & Life Sciences"], "post_products": ["AI Enterprise"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/medical-rag-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-njz", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89625"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1992"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89625"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89625/revisions"}], "predecessor-version": [{"id": 91617, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89625/revisions/91617"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89632"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89625"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89625"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89625"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89625"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89541, "date": "2024-09-30T14:50:06", "date_gmt": "2024-09-30T21:50:06", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89541"}, "modified": "2024-10-17T12:07:07", "modified_gmt": "2024-10-17T19:07:07", "slug": "managing-ai-inference-pipelines-on-kubernetes-with-nvidia-nim-operator", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/managing-ai-inference-pipelines-on-kubernetes-with-nvidia-nim-operator/", "title": {"rendered": "Managing AI Inference Pipelines on Kubernetes with NVIDIA NIM Operator"}, "content": {"rendered": "\n<p>Developers have shown a lot of excitement for <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM microservices</a>, a set of easy-to-use cloud-native microservices that shortens the time-to-market and simplifies the deployment of generative AI models anywhere, across cloud, data centers, cloud, and GPU-accelerated workstations.&nbsp;</p>\n\n\n\n<p>To meet the demands of diverse use cases, NVIDIA is bringing to market a variety of different AI models packaged as NVIDIA NIM microservices, which enable key functionality in a <a href=\"https://build.nvidia.com/nim/agent-blueprints\">generative AI inference workflow</a>.&nbsp;</p>\n\n\n\n<p>A typical generative AI application integrates multiple different NIM microservices. For instance, <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/blob/main/RAG/examples/advanced_rag/multi_turn_rag/README.md\">multi-turn conversational AI in a RAG pipeline</a> uses the LLM, embedding, and re-ranking NIM microservices. The deployment and lifecycle management of these microservices and their dependencies for production generative AI pipelines can lead to additional toil for the MLOps and LLMOps engineers and Kubernetes cluster admins.&nbsp;</p>\n\n\n\n<p>This is why NVIDIA is announcing the <a href=\"https://github.com/NVIDIA/k8s-nim-operator\">NVIDIA NIM Operator</a>, a Kubernetes operator designed to facilitate the deployment, scaling, monitoring, and management of NVIDIA NIM microservices on Kubernetes clusters. With NIM Operator, you can deploy, auto-scale, and manage the lifecycle of NVIDIA NIM microservices with just a few clicks or commands.&nbsp;</p>\n\n\n\n<p>Cluster admins and MLOps and LLMOps engineers don\u2019t have to put effort into the manual deployment, scaling, and lifecycle management of AI inference pipelines. NIM Operator handles all of this and more.&nbsp;</p>\n\n\n\n<h2 id=\"core_capabilities_and_benefits\"  class=\"wp-block-heading\">Core capabilities and benefits<a href=\"#core_capabilities_and_benefits\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Developers are looking to reduce the effort of deploying AI inference pipelines at scale in local deployments. NIM Operator facilitates this with simplified, lightweight deployment and manages the lifecycle of AI NIM inference pipelines on Kubernetes. NIM Operator also supports pre-caching models to enable faster initial inference and autoscaling.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1466\" height=\"810\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-architecture.png\" alt=\"The image depicts a stack diagram highlighting NVDIA NIM Operator, a Kubernetes Operator that is designed to facilitate the deployment, management, and scaling of NVIDIA NIM microservices on Kubernetes clusters.\" class=\"wp-image-89548\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-architecture.png 1466w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-architecture-300x166.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-architecture-625x345.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-architecture-179x99.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-architecture-768x424.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-architecture-645x356.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-architecture-500x276.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-architecture-160x88.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-architecture-362x200.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-architecture-199x110.png 199w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-architecture-1024x566.png 1024w\" sizes=\"(max-width: 1466px) 100vw, 1466px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NIM Operator architecture</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img loading=\"lazy\" decoding=\"async\" width=\"800\" height=\"494\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/image-2.gif\" alt=\"NIM Operator Deployment\" class=\"wp-image-89641\"/></figure></div>\n\n\n<p class=\"has-text-align-center\"><em>Figure 2. NIM Operator Helm deployment</em></p>\n\n\n\n<h3 id=\"intelligent_model_pre-caching\"  class=\"wp-block-heading\">Intelligent model pre-caching<a href=\"#intelligent_model_pre-caching\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>NIM Operator offers <a href=\"https://docs.nvidia.com/nim-operator/latest/cache.html\">pre-caching of models</a> that reduces initial inference latency and enables faster autoscaling. It also enables model deployments in air-gapped environments.&nbsp;</p>\n\n\n\n<p>Use NIM intelligent model pre-caching by specifying NIM profiles and tags, or let NIM Operator auto-detect the best model based on the GPUs available on the Kubernetes cluster. You can pre-cache models on any available node based on your requirements, either on CPU-only or on GPU-accelerated nodes.</p>\n\n\n\n<p>When this option is selected, NIM Operator creates a persistent volume claim (PVC) in Kubernetes and then downloads and caches the NIM models in the cluster. Then, NIM Operator deploys and manages the lifecycle of this PVC using the <code>NIMCache</code> custom resource.&nbsp;&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img loading=\"lazy\" decoding=\"async\" width=\"800\" height=\"494\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/image.gif\" alt=\"NIM Cache\" class=\"wp-image-89639\"/></figure></div>\n\n\n<p class=\"has-text-align-center\"><em>Figure 3. NIM microservice cache deployment</em></p>\n\n\n\n<h3 id=\"automated_ai_nim_pipeline_deployments\"  class=\"wp-block-heading\">Automated AI NIM pipeline deployments<a href=\"#automated_ai_nim_pipeline_deployments\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>NVIDIA is introducing two Kubernetes custom resource definitions (CRDs) to deploy NVIDIA NIM microservices: <a href=\"https://docs.nvidia.com/nim-operator/latest/service.html\">NIMService</a> and <a href=\"https://docs.nvidia.com/nim-operator/latest/pipelines.html\">NIMPipeline</a>.&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><code>NIMService</code>, when deployed, manages each NIM microservice as a standalone microservice.&nbsp;</li>\n\n\n\n<li><code>NIMPipeline</code> enables the deployment and management of several NIM microservices collectively.&nbsp;</li>\n</ul>\n\n\n\n<p>Figure 4 shows a RAG pipeline managed as a microservice pipeline. You can manage multiple pipelines as a collection instead of individual services.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img loading=\"lazy\" decoding=\"async\" width=\"800\" height=\"494\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/image-2.gif\" alt=\"NIM Pipelines\" class=\"wp-image-89642\"/></figure></div>\n\n\n<p class=\"has-text-align-center\"><em>Figure 4. NIM microservice pipeline deployment</em></p>\n\n\n\n<h3 id=\"autoscaling&nbsp;\"  class=\"wp-block-heading\">Autoscaling&nbsp;<a href=\"#autoscaling&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>NIM Operator supports <a href=\"https://docs.nvidia.com/nim-operator/latest/service.html#autoscaling-nim-for-llms\">auto-scaling the NIMService deployment</a> and its <code>ReplicaSet</code> using Kubernetes Horizontal Pod Autoscaler (HPA).&nbsp;</p>\n\n\n\n<p>The <code>NIMService</code> and <code>NIMPipeline</code> CRDs support all the familiar HPA metrics and scaling behaviors, such as the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Specify minimum and maximum replica counts</li>\n\n\n\n<li>Scale using the following metrics:\n<ul class=\"wp-block-list\">\n<li>Per-pod resource metrics, such as CPU</li>\n\n\n\n<li>Per-pod custom metrics, such as GPU memory usage</li>\n\n\n\n<li>Object metrics, such as NIM max requests or <code>KVCache</code></li>\n\n\n\n<li>External metrics</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>You can also specify any HPA scale-up and scale-down behavior, for example, a stabilization window to prevent flapping and scaling policies to control the rate of change of replicas while scaling.&nbsp;</p>\n\n\n\n<p>For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/gpu-telemetry/latest/dcgm-exporter.html\">GPU Metrics</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter\"><img loading=\"lazy\" decoding=\"async\" width=\"800\" height=\"494\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/image-1.gif\" alt=\"NIM AutoScaling\" class=\"wp-image-89640\"/></figure></div>\n\n\n<p class=\"has-text-align-center\"><em>Figure 5. NIM Auto-scaling</em></p>\n\n\n\n<h2 id=\"day_2_operations\"  class=\"wp-block-heading\">Day 2 operations<a href=\"#day_2_operations\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><code>NIMService</code> and <code>NIMPipeline</code> support easy rolling upgrades of NIM with a customizable rolling strategy. Change the version number of the NIM in the <code>NIMService</code> or <code>NIMPipeline</code> CRD and NIM Operator updates the NIM deployments in the cluster.&nbsp;</p>\n\n\n\n<p>Any changes in <code>NIMService</code> pods are reflected in the <code>NIMService</code> and <code>NIMPipeline</code> status. You can also add Kubernetes ingress for <code>NIMService</code>.&nbsp;</p>\n\n\n\n<h2 id=\"support_matrix&nbsp;\"  class=\"wp-block-heading\">Support matrix&nbsp;<a href=\"#support_matrix&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>At launch, NIM Operator supports the reasoning LLM and the retrieval\u2014embedding NIM microservice.</p>\n\n\n\n<p>We are continuously expanding the list of supported NVIDIA NIM microservices. For more information about the full list of supported NIM microservices, see <a href=\"https://docs.nvidia.com/nim-operator/latest/platform-support.html\">Platform Support</a><strong>.</strong></p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>By automating the deployment, scaling, and lifecycle management of NVIDIA NIM microservices, NIM Operator makes it easier for enterprise teams to adopt NIM microservices and accelerate AI adoption.&nbsp;</p>\n\n\n\n<p>This effort aligns with our commitment to make NIM microservices easy to adopt, production-ready, and secure. NIM Operator will be part of future releases of NVIDIA AI Enterprise to provide enterprise support, API stability, and proactive security patching.</p>\n\n\n\n<p>Get started with <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/helm-charts/k8s-nim-operator\">NIM Operator through NGC today</a>, or get it from the <a href=\"https://github.com/NVIDIA/k8s-nim-operator\">GitHub repo</a>. For technical questions on installation, usage, or issues, please file an issue on the repo.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Developers have shown a lot of excitement for NVIDIA NIM microservices, a set of easy-to-use cloud-native microservices that shortens the time-to-market and simplifies the deployment of generative AI models anywhere, across cloud, data centers, cloud, and GPU-accelerated workstations.&nbsp; To meet the demands of diverse use cases, NVIDIA is bringing to market a variety of different &hellip; <a href=\"https://developer.nvidia.com/blog/managing-ai-inference-pipelines-on-kubernetes-with-nvidia-nim-operator/\">Continued</a></p>\n", "protected": false}, "author": 2320, "featured_media": 89546, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1494091", "discourse_permalink": "https://forums.developer.nvidia.com/t/managing-ai-inference-pipelines-on-kubernetes-with-nvidia-nim-operator/308376", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110], "tags": [453, 572], "coauthors": [4052, 4054, 1352, 4055, 4056], "class_list": ["post-89541", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "tag-featured", "tag-kubernetes"], "acf": {"post_industry": ["General"], "post_products": ["NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nim-operator-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nid", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89541"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2320"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89541"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89541/revisions"}], "predecessor-version": [{"id": 89868, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89541/revisions/89868"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89546"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89541"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89541"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89541"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89541"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89583, "date": "2024-09-30T12:21:18", "date_gmt": "2024-09-30T19:21:18", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89583"}, "modified": "2024-11-04T14:57:33", "modified_gmt": "2024-11-04T22:57:33", "slug": "improve-reinforcement-learning-from-human-feedback-with-leaderboard-topping-reward-model", "status": "publish", "type": "post", "link": "https://nvda.ws/3TOzjQL", "title": {"rendered": "Improve Reinforcement Learning from Human Feedback with Leaderboard-Topping Reward Model"}, "content": {"rendered": "\n<p>Llama 3.1 Nemotron 70B Reward model helps generate high-quality training data that aligns with human preferences for finance, retail, healthcare, scientific research, telecommunications, and sovereign AI.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Llama 3.1 Nemotron 70B Reward model helps generate high-quality training data that aligns with human preferences for finance, retail, healthcare, scientific research, telecommunications, and sovereign AI.</p>\n", "protected": false}, "author": 492, "featured_media": 89585, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1494014", "discourse_permalink": "https://forums.developer.nvidia.com/t/improve-reinforcement-learning-from-human-feedback-with-leaderboard-topping-reward-model/308364", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3TOzjQL", "_links_to_target": "_blank"}, "categories": [3110], "tags": [453, 3933, 2932, 1718], "coauthors": [610, 3872], "class_list": ["post-89583", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-llama", "tag-large-language-models", "tag-synthetic-data"], "acf": {"post_industry": ["Financial Services", "Healthcare & Life Sciences", "HPC / Scientific Computing", "Retail / Consumer Packaged Goods", "Smart Cities / Spaces", "Telecommunications"], "post_products": ["AI Foundation Models"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/rag-visual-blog-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-niT", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89583"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/492"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89583"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89583/revisions"}], "predecessor-version": [{"id": 89590, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89583/revisions/89590"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89585"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89583"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89583"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89583"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89583"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89173, "date": "2024-09-30T09:00:00", "date_gmt": "2024-09-30T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89173"}, "modified": "2024-10-17T12:07:09", "modified_gmt": "2024-10-17T19:07:09", "slug": "advancing-quantum-algorithm-design-with-gpt", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/advancing-quantum-algorithm-design-with-gpt/", "title": {"rendered": "Advancing Quantum Algorithm Design with GPTs"}, "content": {"rendered": "\n<p>AI techniques like large language models (LLMs) are rapidly transforming many scientific disciplines. <a href=\"https://www.nvidia.com/en-us/glossary/quantum-computing/\">Quantum computing</a> is no exception. A collaboration between NVIDIA, the University of Toronto, and Saint Jude Children\u2019s Research Hospital is bringing generative pre-trained transformers (GPTs) to the <a href=\"https://arxiv.org/abs/2401.09253\">design of new quantum algorithms</a>, including the <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s62497/\">Generative Quantum Eigensolver (GQE)</a> technique.&nbsp;</p>\n\n\n\n<p>The GQE technique is the latest in a wave of so-called <a href=\"https://developer.nvidia.com/blog/enabling-quantum-computing-with-ai/\">AI for Quantum</a> techniques. Developed with the NVIDIA CUDA-Q platform, GQE is the first method enabling you to use your own GPT model for creating complex quantum circuits.&nbsp;</p>\n\n\n\n<p>The CUDA-Q platform has been instrumental in developing GQE. Training and using GPT models in quantum computing requires hybrid access to CPUs, GPUs, and QPUs. The CUDA-Q focus on <a href=\"https://developer.nvidia.com/blog/an-introduction-to-quantum-accelerated-supercomputing/\">accelerated quantum supercomputing</a> makes it a fully hybrid computing environment perfectly suited for GQE.&nbsp;</p>\n\n\n\n<p>According to GQE co-author <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-ep64025/?playlistId=playList-d63e5c44-196c-4246-bdca-d2a7bd2e00c1\">Alan Aspuru-Guzik</a>, these abilities position CUDA-Q as a scalable standard.</p>\n\n\n\n<h2 id=\"learning_the_grammar_of_quantum_circuits\"  class=\"wp-block-heading\">Learning the grammar of quantum circuits<a href=\"#learning_the_grammar_of_quantum_circuits\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/#:~:text=Large%20language%20models%20largely%20represent,the%20words%20in%20this%20sentence.\">Conventional LLMs</a> can be a useful analogy for understanding GQE. In general, the goal of an LLM is to take a vocabulary of many words; train a transformer model with text samples to understand things like meaning, context, and grammar; and then sample the trained model to produce words, which are then strung together to generate a new document.</p>\n\n\n\n<p>Where LLMs deal with words, GQE deals with quantum circuit operations. GQE takes a pool of unitary operations (vocabulary) and trains a transformer model to generate a sequence of indices corresponding to unitary operations (words) that define a resulting quantum circuit (document). The grammar for generating these indices is a set of rules trained by minimizing a cost function, which is evaluated by computing expectation values using previously generated circuits.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"322\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm-1024x322.png\" alt=\"Diagram shows generated outputs from the GQE (quantum circuit) and an LLM (sentence).\" class=\"wp-image-89187\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm-1024x322.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm-300x94.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm-625x197.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm-179x56.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm-768x241.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm-1536x483.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm-645x203.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm-500x157.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm-160x50.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm-362x114.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm-350x110.png 350w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-llm.png 1832w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Comparing GQE to a LLM</em></figcaption></figure></div>\n\n\n<p>Figure 1 shows that GQE is analogous to a LLM. Instead of adding individual words to construct a sentence, unitary operations are added to generate a quantum circuit.</p>\n\n\n\n<h2 id=\"gqe-enabled_algorithms\"  class=\"wp-block-heading\">GQE-enabled algorithms.<a href=\"#gqe-enabled_algorithms\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In the era of noisy, small-scale quantum (NISQ) computers, quantum algorithms are limited by several hardware constraints. This has motivated the development of hybrid quantum-classical algorithms like the Variational Quantum Eigensolver (VQE), which attempts to circumvent these limitations by offloading onerous tasks to a conventional computer.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"675\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-vqe-1024x675.png\" alt=\"For GQE, the parameters are only in the GPT model. For VQE, the variational parameters are in the quantum circuit.\" class=\"wp-image-89188\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-vqe-1024x675.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-vqe-300x198.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-vqe-625x412.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-vqe-174x115.png 174w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-vqe-768x506.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-vqe-645x425.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-vqe-455x300.png 455w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-vqe-137x90.png 137w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-vqe-362x239.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-vqe-167x110.png 167w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gqe-vs-vqe.png 1438w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Comparison between the GQE and VQE</em></figcaption></figure></div>\n\n\n<p>All optimized parameters are handled classically in the GPT model and are updated based on the expected values of the generated circuits. This enables optimization to occur in a more favorable deep neural network landscape and offers a potential route to avoiding the <a href=\"https://www.nature.com/articles/s41467-018-07090-4\">barren plateaus</a> that impede variational algorithms. This also eliminates the need for the many intermediate circuit evaluations required in techniques like reinforcement learning.</p>\n\n\n\n<p>The GQE method is the first hybrid quantum-classical algorithm leveraging the power of AI to accelerate NISQ applications. GQE extends NISQ algorithms in several ways:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Ease of optimization:</strong> GQE builds quantum circuits without quantum variational parameters (Figure 2).&nbsp;</li>\n\n\n\n<li><strong>Quantum resource efficiency:</strong> By replacing quantum gradient evaluation with sampling and backpropagation, GQE is expected to provide greater utility with fewer quantum circuit evaluations.</li>\n\n\n\n<li><strong>Customizability:</strong> The GQE is very flexible and can be modified to incorporate <em>a priori</em> domain knowledge, or applied to target applications outside of chemistry.</li>\n\n\n\n<li><strong>Pretrainability:</strong> The GQE transformer can be pretrained, eliminating the need for additional quantum circuit evaluations. We discuss this later in this post.</li>\n</ul>\n\n\n\n<h2 id=\"results_from_gpt-qe\"  class=\"wp-block-heading\">Results from GPT-QE<a href=\"#results_from_gpt-qe\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>For the inaugural application of GQE, the authors built a specific model inspired by GPT-2 (referred to explicitly as GPT-QE) and used it to estimate the ground state energies of a set of small molecules.</p>\n\n\n\n<p>The operator pool of vocabulary was built from chemically inspired operations such as excitations and time evolution steps that were derived from a standard <a href=\"https://arxiv.org/abs/2402.15879\">ansatz</a> known as \u2018unitary coupled-clusters with single and double excitations\u2019 (UCCSD). An <em>ansatz</em> is an approach to parameterizing quantum circuits.</p>\n\n\n\n<p>Variational algorithms must be started with a \u2018best guess\u2019 initial state, generated with existing classical methods. To demonstrate GPT-QE, the authors generated an initial state using the Hartree-Fock method with an STO-3G basis set. The GPT model used in this work was identical to OpenAI\u2019s GPT-2 model, including 12 attention layers, 12 attention heads, and 768 embedding dimensions. For more information and a comprehensive technical explanation of the training process, see <em>2.2. GPT Quantum Eigensolver</em> in <a href=\"https://arxiv.org/pdf/2401.09253.pdf\">The generative quantum eigensolver (GQE) and its application for ground state search</a>.&nbsp;</p>\n\n\n\n<p>A great advantage of this technique is that it is highly parallelizable, both in terms of using GPU acceleration for the classical component and in using multiple QPUs for the quantum calculations. Since the publication of the paper, the workflow has been accelerated by parallelizing the expectation value computations of the GPT-QE sampled circuits using the NVIDIA CUDA-Q multi-QPU backend, mqpu.</p>\n\n\n\n<p>The <code>mqpu</code> backend is designed for parallel and asynchronous quantum co-processing, enabling multiple GPUs to simulate multiple QPUs. As the availability of physical quantum hardware increases, these backends can trivially be replaced with access to multiple instances of potentially varying QPU hardware.</p>\n\n\n\n<p>Figure 3 shows the speedup realized by using the <code>nvidia-mqpu</code> backend on a much larger 18-qubit CO<sub>2 </sub>GQE experiment. Baseline CPU computations were obtained by calculating the expectation value of 48 sampled circuits on a 56-core Intel Xeon Platinum 8480CL E5.</p>\n\n\n\n<p>Using a single NVIDIA H100 GPU instead of the CPU provided a 40x speedup. The CUDA-Q <code>mqpu</code> backend provides an additional 8x speedup by enabling asynchronous computation of the expectation values across eight GPUs using an NVIDIA DGX-H100 system.&nbsp;</p>\n\n\n\n<p>The authors also trained a 30-qubit CO<sub>2 </sub>GQE experiment for which the CPU failed. The model trained in 173 hours on a single NVIDIA H100 GPU, which was reduced to 3.5 hours when parallelized across 48 H100 GPUs.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"643\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/expectation-value-computation-gqe-1024x643.png\" alt=\"The bar chart shows a 40x speedup for an NVIDIA H100 GPU and 320x for an NVIDIA DGX-H100 system.\" class=\"wp-image-89189\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/expectation-value-computation-gqe-1024x643.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/expectation-value-computation-gqe-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/expectation-value-computation-gqe-625x393.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/expectation-value-computation-gqe-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/expectation-value-computation-gqe-768x482.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/expectation-value-computation-gqe-645x405.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/expectation-value-computation-gqe-478x300.png 478w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/expectation-value-computation-gqe-143x90.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/expectation-value-computation-gqe-362x227.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/expectation-value-computation-gqe-175x110.png 175w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/expectation-value-computation-gqe.png 1280w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Expectation value computation for GQE circuit samples</em></figcaption></figure></div>\n\n\n<p>Figure 3 shows GQE circuit samples accelerated with a single NVIDIA H100 GPU or asynchronous evaluation across multiple GPUs using an NVIDIA DGX-H100.</p>\n\n\n\n<p>As the scale of quantum computations continues to increase, the ability to parallelize simulation workloads across multiple GPUs, and eventually QPUs, will become increasingly important.&nbsp;</p>\n\n\n\n<p>Beyond access to these hardware capabilities, implementing GPT-QE using CUDA-Q provided additional benefits like interoperability with GPU-accelerated libraries such as PyTorch to accelerate the classical parts of the algorithm. This is a huge benefit of the CUDA-Q platform, which also has access to the world&#8217;s fastest implementations of conventional mathematical operations through the GPU-accelerated <a href=\"https://www.nvidia.com/en-us/technologies/cuda-x\">CUDA-X</a> libraries.&nbsp;</p>\n\n\n\n<p>The CUDA-Q QPU agnosticism is also key in enabling future experiments on multiple physical QPUs. Most importantly, by embodying hybrid quantum computing and offloading gradient calculations to classical processors, large-scale systems can be explored and open the door to useful quantum computing applications enabled by AI.</p>\n\n\n\n<h2 id=\"opportunities_to_extend_the_gqe_framework\"  class=\"wp-block-heading\">Opportunities to extend the GQE framework<a href=\"#opportunities_to_extend_the_gqe_framework\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This collaboration is a first step towards understanding the broad opportunities for how GPT models can enable quantum supercomputing applications.</p>\n\n\n\n<p>Future research will hone exploring different operator pools for GQE and optimal strategies for training. This includes a focus on <em>pretraining</em>, a process where existing datasets can be used to either make the transformer training more efficient or aid in the convergence of the training process. This is possible if there is a sufficiently large data set available containing generated circuits and their associated expectation values. Pretrained models can also provide a warm start for training other similar models.</p>\n\n\n\n<p>For example, the output from a prior run would create a database of circuits and their associated ground state energies. Poorly performing circuits can be thrown away and the transformer can be trained using only the better-performing circuits, without the need for a quantum computer or simulator. This pretrained transformer can then be used as the initialization point for further training, which is expected to converge quicker and exhibit better performance.</p>\n\n\n\n<p>There is also a huge scope for applications using GQE outside of quantum chemistry. A collaboration between NVIDIA and Los Alamos National Lab is exploring using the ideas of GQE for geometric quantum machine learning.</p>\n\n\n\n<p>For more information about the GQE code, including examples, see the <a href=\"https://github.com/cudaq-libraries/cudaqlib/tree/main/examples/python\">GQE</a> GitHub repo.</p>\n\n\n\n<h2 id=\"explore_nvidia_tools_for_quantum_research\"  class=\"wp-block-heading\">Explore NVIDIA tools for quantum research<a href=\"#explore_nvidia_tools_for_quantum_research\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The GQE is a novel example of how GPT models and AI in general can be used to enable many aspects of quantum computing.</p>\n\n\n\n<p>NVIDIA is developing hardware and software tools such as CUDA-Q to ensure scalability and acceleration of both the classical and quantum parts of hybrid workflows. For more information about NVIDIA\u2019s quantum efforts, visit the <a href=\"https://www.nvidia.com/en-us/solutions/quantum-computing/\">Quantum Computing</a> page.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>AI techniques like large language models (LLMs) are rapidly transforming many scientific disciplines. Quantum computing is no exception. A collaboration between NVIDIA, the University of Toronto, and Saint Jude Children\u2019s Research Hospital is bringing generative pre-trained transformers (GPTs) to the design of new quantum algorithms, including the Generative Quantum Eigensolver (GQE) technique.&nbsp; The GQE technique &hellip; <a href=\"https://developer.nvidia.com/blog/advancing-quantum-algorithm-design-with-gpt/\">Continued</a></p>\n", "protected": false}, "author": 1950, "featured_media": 89473, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1493930", "discourse_permalink": "https://forums.developer.nvidia.com/t/advancing-quantum-algorithm-design-with-gpts/308355", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 503], "tags": [453, 2735, 1914], "coauthors": [3645, 4040, 4050, 4051], "class_list": ["post-89173", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "category-simulation-modeling-design", "tag-featured", "tag-quantum-computing", "tag-cluster-supercomputing"], "acf": {"post_industry": ["HPC / Scientific Computing"], "post_products": ["CUDA"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantum-algorithm-gpt-featured-2.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nch", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89173"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1950"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89173"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89173/revisions"}], "predecessor-version": [{"id": 89743, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89173/revisions/89743"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89473"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89173"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89173"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89173"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89173"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89513, "date": "2024-09-27T11:10:11", "date_gmt": "2024-09-27T18:10:11", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89513"}, "modified": "2024-10-17T12:07:10", "modified_gmt": "2024-10-17T19:07:10", "slug": "ai-chatbot-delivers-multilingual-support-to-african-farmers", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/ai-chatbot-delivers-multilingual-support-to-african-farmers/", "title": {"rendered": "AI Chatbot Delivers Multilingual Support to African Farmers"}, "content": {"rendered": "\n<p>Some of Africa\u2019s most resource-constrained farmers are gaining access to on-demand, AI-powered advice through a multimodal chatbot\u00a0that gives detailed recommendations\u00a0about how to increase yields or fight common pests and crop diseases.</p>\n\n\n\n<p>Since February, farmers in the East African nation of Malawi have had access to the chatbot, named UlangiziAI, through WhatsApp on mobile phones. The chatbot understands agricultural-related questions sent by text or voice notes, and can also recognize and interpret photographs.</p>\n\n\n\n<p>The app responds to queries in either English or Chichewa, the native language of about half of the country\u2019s 20M people. In Malawi, where around 30% of the population doesn\u2019t read or write, the apps ability to respond in natural language is crucial.\u00a0</p>\n\n\n\n<p>It was developed by NGO <a href=\"https://opportunity.org/\">Opportunity Internationa</a>l, which has worked in Malawi for 21 years. Because smartphone access remains low in Malawi, the NGO hired a constellation of local Malawians to work with rural farming communities. These farmer support agents receive training and a smartphone, and then help farmers access the UlangiziAI app (\u201cUlangizi\u201d in Chichewa means \u201cguidance\u201d or \u201ccounseling\u201d).&nbsp;</p>\n\n\n\n<p>Paul Essene, who helped develop the technology behind UlangiziAI, said the chatbot can help overcome endemic, structural challenges Malawian farmers routinely face.&nbsp;</p>\n\n\n\n<p>Most Malawians live on around $2 per day. More than 80% of the country lives in rural or remote areas, and around three in four Malawians work on or own farms. Government offices designed to offer farmers advice are inconsistently open. Even getting to these offices can be difficult for farmers. In a country where walking is often the primary\u2014and most reliable\u2014means of transportation, farmers regularly walk ten miles or more to town, only to find government offices closed.&nbsp;</p>\n\n\n\n<p>By using UlangiziAI farmers can quickly find answers to agricultural questions, including about their livestock, plants, fertilizers and fungicides, and overall growing strategies.</p>\n\n\n\n<p>\u201cIn our initial three month pilot program, we saw more than 4,000 questions come into the app in both English and Chichewa,\u201d Essene said. \u201cThe types of questions we\u2019re able to field, from farming practices, to crop related questions to how to test for diseases\u2014they\u2019re coming in a mixture modalites and the app can respond really quickly and accurately.\u201d</p>\n\n\n\n<p>To create the chatbot, Opportunity International worked with <a href=\"https://gooey.ai/\">Gooey.AI</a>, an AI workflow platform.</p>\n\n\n\n<p>UlangiziAI runs on <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 Tensor Core GPUs</a> in the Azure Cloud. Because Chichewa is a low-resource language, there are relatively few data assets available in Chichewa to train LLMs or speech recognition models. As a result, Gooey.AI has had to bootstrap techniques to optimize UlangiziAI.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-video\"><video controls src=\"https://developer.download.nvidia.com/video/devblog/Ulangizi demo (soya rust).mp4\"></video></figure>\n\n\n\n<p class=\"has-text-align-center\"><em>Video 1. A video showing the UlangiziAI app responding in real time</em><a href=\"https://nvidia-my.sharepoint.com/personal/ewolfberg_nvidia_com/_layouts/15/stream.aspx?id=%2Fpersonal%2Fewolfberg%5Fnvidia%5Fcom%2FDocuments%2FVideos%2FClipchamp%2FVideo%20Project%206%2FExports%2FVideo%20Project%206%2Emp4&amp;referrer=StreamWebApp%2EWeb&amp;referrerScenario=AddressBarCopied%2Eview%2Ed6c71852%2Dd42a%2D42a4%2D9a84%2D040782728339\"></a></p>\n\n\n\n<p>When a question is submitted \u200cthrough a WhatsApp voice memo, it\u2019s transcribed and translated into English by an LLM, such as Meta\u2019s MMS Large or a fine-tuned version of OpenAI\u2019s Whisper 3. When farmers upload pictures they\u2019ve taken of a crop or pest, they are automatically routed to OpenAI\u2019s GPT4o. That data is then combined, and sent to generate vector database queries.&nbsp;&nbsp;</p>\n\n\n\n<p>The app then queries a RAG pipeline grounded in the Malawi Ministry of Agriculture Guide to Agriculture Production Manual\u2014a 450-page book of in-depth information focused on the specific needs of Malawian farms and farmers. </p>\n\n\n\n<p>Once the retriever has identified and extracted the relevant data from the vector database, it\u2019s sent back to the language model, which translates it into English or Chichewa. It then shares the answer back to the farmer in either text or audio format \u200cthrough WhatsApp. Each answer also includes citations and hyperlinks to the relevant data source.&nbsp;</p>\n\n\n\n<p>Gooey.AI regularly hot swaps the open source LLMs and automatic speech recognition tools UlangiziAI runs on, using the near step-change improvements each successive model generation has demonstrated understanding low-resource languages.&nbsp;</p>\n\n\n\n<p>UlangiziAI has reduced the time it takes farmers to get critical farming guidance from days to seconds. So far, farmers who have used the app have rated it highly. And later this year or early next year, Opportunity International plans to introduce it in Kenya and make it available in Swahili as well as other Kenyan languages.</p>\n\n\n\n<p>Check out <a href=\"https://opportunity.org/what-we-do/\">Opportunity International global programming</a>.</p>\n\n\n\n<p>Learn more about the work Gooey.AI is doing to<a href=\"https://blog.gooey.ai/global-language-understanding-for-ais\"> evaluate AI models</a> and<a href=\"https://gooey.ai/copilot\"> build chatbots</a> for <a href=\"https://gooey.ai/impact\">low-resource languages</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Some of Africa\u2019s most resource-constrained farmers are gaining access to on-demand, AI-powered advice through a multimodal chatbot\u00a0that gives detailed recommendations\u00a0about how to increase yields or fight common pests and crop diseases. Since February, farmers in the East African nation of Malawi have had access to the chatbot, named UlangiziAI, through WhatsApp on mobile phones. The &hellip; <a href=\"https://developer.nvidia.com/blog/ai-chatbot-delivers-multilingual-support-to-african-farmers/\">Continued</a></p>\n", "protected": false}, "author": 2156, "featured_media": 89516, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1492977", "discourse_permalink": "https://forums.developer.nvidia.com/t/ai-chatbot-delivers-multilingual-support-to-african-farmers/308113", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 696, 3110, 1903], "tags": [2415, 3941, 3266, 453, 2932, 3613], "coauthors": [3876], "class_list": ["post-89513", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-data-science", "category-generative-ai", "category-features", "tag-agriculture", "tag-ai-impact", "tag-chatbot", "tag-featured", "tag-large-language-models", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["General"], "post_products": ["General", "A100"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/AI-farming-chatbot.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nhL", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89513"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2156"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89513"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89513/revisions"}], "predecessor-version": [{"id": 89543, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89513/revisions/89543"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89516"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89513"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89513"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89513"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89513"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89509, "date": "2024-09-27T08:44:55", "date_gmt": "2024-09-27T15:44:55", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89509"}, "modified": "2024-10-17T12:07:11", "modified_gmt": "2024-10-17T19:07:11", "slug": "just-released-nvidia-hpc-sdk-v24-9", "status": "publish", "type": "post", "link": "https://nvda.ws/3ZFB1HC", "title": {"rendered": "Just Released: NVIDIA HPC SDK v24.9"}, "content": {"rendered": "\n<p>\u00a0The new release includes several new features including improved stdpar programming and Arm processor support.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>\u00a0The new release includes several new features including improved stdpar programming and Arm processor support.</p>\n", "protected": false}, "author": 759, "featured_media": 85792, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1492933", "discourse_permalink": "https://forums.developer.nvidia.com/t/just-released-nvidia-hpc-sdk-v24-9/308107", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3ZFB1HC", "_links_to_target": "_blank"}, "categories": [503], "tags": [453, 1958], "coauthors": [1262], "class_list": ["post-89509", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-simulation-modeling-design", "tag-featured", "tag-news"], "acf": {"post_industry": ["Academia / Education", "Energy", "HPC / Scientific Computing"], "post_products": ["HPC SDK"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/HPC_SW_KV.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nhH", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89509"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/759"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89509"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89509/revisions"}], "predecessor-version": [{"id": 89512, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89509/revisions/89512"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/85792"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89509"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89509"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89509"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89509"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88938, "date": "2024-09-26T14:44:00", "date_gmt": "2024-09-26T21:44:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88938"}, "modified": "2024-11-05T18:26:47", "modified_gmt": "2024-11-06T02:26:47", "slug": "low-latency-inference-chapter-2-blackwell-is-coming-nvidia-gh200-nvl32-with-nvlink-switch-gives-signs-of-big-leap-in-time-to-first-token-performance", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/low-latency-inference-chapter-2-blackwell-is-coming-nvidia-gh200-nvl32-with-nvlink-switch-gives-signs-of-big-leap-in-time-to-first-token-performance/", "title": {"rendered": "Low Latency Inference Chapter 2: Blackwell is Coming. NVIDIA GH200 NVL32 with NVLink Switch Gives Signs of Big Leap in Time to First Token Performance"}, "content": {"rendered": "\n<p>Many of the most exciting applications of large language models (LLMs), such as interactive speech bots, coding co-pilots, and search, need to begin responding to user queries quickly to deliver positive user experiences. The time that it takes for an LLM to ingest a user prompt (and context, which can be sizable) and begin outputting a response is called time to first token (TTFT).</p>\n\n\n\n<p>As LLMs continue to grow in size \u2013 with the latest community models now featuring hundreds of billions of parameters \u2013 they deliver&nbsp;more accurate responses and also support even larger context windows to allow users to ask longer, information-rich queries. For example, the Llama 3.1 and 3.2 family of LLMs supports up to 128K token context windows, or roughly the length of a novel. These capabilities make LLMs more useful, but they also require more delivered parallel compute performance for good interactivity.</p>\n\n\n\n<p>Until now, AI has scaled with model pre-training. Recent advances will also scale with post-training synthetic data generation and inference-time reasoning. Inference performance and scaling is now critically important.</p>\n\n\n\n<p>In this post, we show how the NVIDIA GH200 NVL32 system, powered by 32 NVIDIA GH200 Grace Hopper Superchips connected using the NVLink Switch system, and with TensorRT-LLM improvements, scales to deliver outstanding TTFT for long-context inference using the latest Llama 3.1 70B and 405B models.&nbsp;</p>\n\n\n\n<h2 id=\"time-to-first-token_matters_for_real-time_use_cases\"  class=\"wp-block-heading\">Time-to-first-token matters for real-time use cases<a href=\"#time-to-first-token_matters_for_real-time_use_cases\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Applications such as AI speech bots, digital assistants, AI NPCs in games, and more aim to simulate natural, human-like conversational capabilities. For these use cases, a TTFT in the realm of a few hundred milliseconds is crucial.&nbsp;</p>\n\n\n\n<p>To understand the impact of TTFT on the user experience, consider the following animations. The first represents a TTFT of about half a second, while the second represents a TTFT of about five seconds.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1600\" height=\"1031\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-1.-An-animation-showing-0.5-second-time-to-first-token-TTFT-1.gif\" alt=\"An animation simulating an interactive chat bot that takes 0.5 seconds to begin responding to user queries.\u00a0\" class=\"wp-image-89482\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. An animation showing 0.5 second time-to-first-token (TTFT).&nbsp;</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1600\" height=\"983\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-An-animation-showing-a-5-second-time-to-first-token-TTFT.gif\" alt=\"An animation simulating an interactive chat bot that takes 5 seconds to begin responding to user queries.\" class=\"wp-image-89485\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. An animation showing a 5 second time-to-first-token (TTFT).&nbsp;</em></figcaption></figure></div>\n\n\n<p>Fast TTFT is particularly impactful in services where up-to-date knowledge is important, like in the recent rise of agentic workflows. To build useful agents, <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">Retrieval-Augmented Generation</a> (RAG) \u2013&nbsp; which enhances LLM prompts with relevant data \u2013 is needed for accurate actions and responses. This means that contexts can be very long with tens or hundreds of thousands of tokens. Having a fast TTFT, even at such long contexts, makes these services feel more interactive.</p>\n\n\n\n<p>Below we will show how NVIDIA GH200 NVL32 is able to achieve the fastest published TTFT for the Llama 3.1 models, even at very long contexts.</p>\n\n\n\n<h2 id=\"nvidia_gh200_nvl32_supercharges_ttft_for_long_context_inference\"  class=\"wp-block-heading\">NVIDIA GH200 NVL32 supercharges TTFT for long context inference<a href=\"#nvidia_gh200_nvl32_supercharges_ttft_for_long_context_inference\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To generate the first new token in response to an inference request, the input tokens must be processed by the LLM. This phase of inference, known as prefill, often has a large number of tokens and thus benefits from increased aggregate compute performance. It can be accelerated by splitting the calculations across multiple GPUs using <a href=\"https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/\">parallelism techniques</a>, such as tensor parallelism.&nbsp;</p>\n\n\n\n<p>When computations are split across many GPUs using tensor parallelism, all GPUs involved in the computation must exchange data between all other GPUs in an AllReduce synchronization that happens twice per model layer. As the number of GPUs involved in the calculation increases, the total amount of synchronization traffic grows. Llama 3.1 405B incorporates 126 layers, yielding 252 AllReduce synchronizations per inference step. This means that running Llama 3.1 405B across 32 GPUs with an input sequence of 122,880 tokens, generates 114 TB of aggregate interconnect traffic. A high-bandwidth, low-latency all-to-all GPU-to-GPU fabric is needed to minimize time spent during these synchronizations and maximize time available GPUs spend on compute.&nbsp;</p>\n\n\n\n<p>GH200 NVL32 is a rack scale solution that connects 32 NVIDIA GH200 Grace Hopper Superchips \u2013 each composed of an NVIDIA Grace CPU and an NVIDIA Hopper GPU connected via NVLink-C2C \u2013 using the NVLink Switch System. This allows each Hopper GPU to communicate with any other GPU within the NVLink domain at full 900 GB/s bandwidth, for 28.8 TB/s of aggregate bandwidth. The NVLink Switch System means the combined 32 GH200\u2019s form \u201cone mighty GPU\u201d with up to 127 petaFLOPs of peak FP8 AI compute. This helps to dramatically shorten TTFT, particularly on the most demanding models with long context.&nbsp;</p>\n\n\n\n<p>In scaling from eight NVIDIA H200 Tensor Core GPUs to 32 GH200 Grace Hopper Superchips, TTFT for a 122,880 token Llama 3.1 405B query, TTFT is accelerated by 3x, enabling a real-time experience. And, even for Llama 3.1 70B, TTFT for the same length query sees a 2.6x speedup in TTFT.&nbsp;</p>\n\n\n\n<p>In the following sections, we show how GH200 NVL32 makes responsive, long context Llama 3.1 70B and 405B inference possible.&nbsp;</p>\n\n\n\n<h3 id=\"llama_31_70b\"  class=\"wp-block-heading\">Llama 3.1 70B<a href=\"#llama_31_70b\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>A single GH200 NVL32 system achieves a TTFT of just 472 milliseconds when running Llama 3.1 70B, using an input sequence length of 32,768. In practical terms, this means that Llama 3.1 70B can begin outputting a summary of a 90-page document or coding suggestions on thousands of lines of code, in less than half a second.</p>\n\n\n\n<figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><tbody><tr><td></td><td><strong>Llama 3.1 70B</strong><br><strong>Time to First token (milliseconds)</strong><br><em>(Lower is better)</em></td></tr><tr><td><strong>Input Sequence Length</strong></td><td><strong>GH200 NVL32</strong></td></tr><tr><td>4,096</td><td>64</td></tr><tr><td>32,768</td><td>472</td></tr><tr><td>122,880</td><td>2,197</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 1. Llama 3.1 70B time-to-first-token (TTFT) using the GH200 NVL32 rack-scale system.</em></em><br><br><em>Data measured between 9/6/2024 and 9/10/2024 using an internal TensorRT-LLM development branch. Batch = 1.</em></figcaption></figure>\n\n\n\n<p>And, for an input sequence length of 122,880 \u2013 approximately 15K lines of code or a 330 page book \u2013 GH200 NVL32 can achieve a TTFT of just 2.2 seconds.</p>\n\n\n\n<h3 id=\"llama_31_405b\"  class=\"wp-block-heading\">Llama 3.1 405B<a href=\"#llama_31_405b\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Llama 3.1 405B requires substantially more compute to generate the first token of a response, as the model incorporates nearly 6X the parameter count of Llama 3.1 70B.</p>\n\n\n\n<figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><tbody><tr><td></td><td><strong>Llama 3.1 405B</strong><br><strong>Time to First token (milliseconds)</strong><br><em>(Lower is better)</em></td></tr><tr><td><strong>Input Sequence Length</strong></td><td><strong>GH200 NVL32</strong></td></tr><tr><td>4,096</td><td>208&nbsp;</td></tr><tr><td>32,768</td><td>1,627&nbsp;</td></tr><tr><td>122,880</td><td>7,508&nbsp;</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Llama 3.1 405B time-to-first-token (TTFT) using the GH200 NVL32 rack-scale system.</em><br><br><em>Data measured between 9/6/2024 and 9/10/2024 using an internal TensorRT-LLM development branch. Batch = 1.</em></figcaption></figure>\n\n\n\n<p>GH200 NVL32, running Llama 3.1 405B, is able to provide a TTFT of about 1.6 seconds using a 32,768 token input. And, using a small codebase-sized 122,880 token input, GH200 NVL32 can begin responding in just 7.5 seconds.&nbsp;</p>\n\n\n\n<h2 id=\"inference_continues_to_be_a_hotbed_of_invention\"  class=\"wp-block-heading\">Inference continues to be a hotbed of invention<a href=\"#inference_continues_to_be_a_hotbed_of_invention\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The pace of inference innovation across serving techniques, runtime optimizations, kernels and more has been extraordinary. Advancements like <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\">in-flight batching</a>, <a href=\"https://developer.nvidia.com/blog/low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch/\">speculative decoding</a>, <a href=\"https://developer.nvidia.com/blog/next-generation-of-flashattention/\">FlashAttention</a>, <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\">key-value caching</a>, and more have been developed by both industry and academia. Collectively, these innovations are enabling more capable models and systems to be deployed efficiently and more cost-effectively in production, making powerful AI capabilities more accessible to the entire NVIDIA ecosystem.&nbsp;</p>\n\n\n\n<p>To innovate quickly, researchers need a rich developer ecosystem and a productive tool stack. And, for \u200cinnovations to have the greatest reach, a large platform installed base is required. The NVIDIA accelerated computing platform has more than 5 million developers, with an installed base of several hundred million GPUs across CSPs, on-prem, personal computers, and edge devices&nbsp; \u2013 all compatible with the CUDA programming model. Deep engagement with developers, computing providers, and customers enables and accelerates AI innovation on the NVIDIA platform.</p>\n\n\n\n<h2 id=\"next_up_accelerating_agentic_workflows\"  class=\"wp-block-heading\">Next up: accelerating agentic workflows<a href=\"#next_up_accelerating_agentic_workflows\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Agentic workflows perform tree search, self-reflection, and iterative inferences to reason and produce answers to complex queries. This means that the number of inferences per prompt will grow by orders of magnitude. With each successive inference, we would need to process the aggregate response in the next agent as a new context \u2014 thus fast TTFT becomes even more important as workflows scale.</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/blog/low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch/\">Fast token generation speed</a> is also important for agentic workflows. In a future chapter, we will provide an update on accelerating token generation speed by scaling to many more GPUs on the NVIDIA platform with NVLink and the NVLink Switch system.&nbsp;&nbsp;</p>\n\n\n\n<h2 id=\"nvidia_blackwell_gb200_nvl72_powers_a_new_era_of_computing&nbsp;\"  class=\"wp-block-heading\">NVIDIA Blackwell GB200 NVL72 powers a new era of computing&nbsp;<a href=\"#nvidia_blackwell_gb200_nvl72_powers_a_new_era_of_computing&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Looking ahead, as model sizes continue to grow rapidly, and as models support even longer context lengths, and agentic workflows become more popular, the amount of delivered compute performance required for fast inference continues to rise.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"5823\" height=\"4367\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy.jpg\" alt=\"A GB200 NVL72 compute tray with the key components such as boards, cooling, and so on visible above the tray itself. The exploded view of the rack is in front of the GB200 NVL72 rack. \" class=\"wp-image-88967\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy.jpg 5823w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy-300x225.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy-625x469.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy-153x115.jpg 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy-768x576.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy-1536x1152.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy-2048x1536.jpg 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy-645x484.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy-400x300.jpg 400w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy-120x90.jpg 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy-362x271.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy-147x110.jpg 147w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Exploded-240819_retouch_v2_withRack-copy-1024x768.jpg 1024w\" sizes=\"(max-width: 5823px) 100vw, 5823px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. An exploded view of a GB200 NVL72 tray, with a GB200 NVL72 rack.&nbsp;</em></figcaption></figure></div>\n\n\n<p>The GB200 NVL72, based on the NVIDIA Blackwell platform, delivers the next giant leap for generative AI and accelerated computing. With second-generation Transformer Engine and fifth-generation Tensor Cores, Blackwell delivers up to 20 PFLOPS of FP4 AI compute \u2013 up 5x the AI compute of NVIDIA Hopper. And, fifth-generation NVLink provides 1,800 GB/s of GPU-to-GPU bandwidth \u2013 twice that provided by Hopper \u2013 and expands NVLink domain size to 72 GPUs with the GB200 NVL72 rack-scale system, enabled by the latest NVLink Switch chip.&nbsp;</p>\n\n\n\n<p>NVIDIA continues to innovate at every layer of the technology stack to increase performance, reduce total cost of ownership, and enable the next-generation of AI.</p>\n\n\n\n<p><em>This blog is part of a series &#8211; view <a href=\"https://developer.nvidia.com/blog/low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch/\">Low Latency Inference Chapter 1: Up to 1.9x Higher Llama 3.1 Performance with Medusa on NVIDIA HGX H200 with NVLink Switch</a>.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Many of the most exciting applications of large language models (LLMs), such as interactive speech bots, coding co-pilots, and search, need to begin responding to user queries quickly to deliver positive user experiences. The time that it takes for an LLM to ingest a user prompt (and context, which can be sizable) and begin outputting &hellip; <a href=\"https://developer.nvidia.com/blog/low-latency-inference-chapter-2-blackwell-is-coming-nvidia-gh200-nvl32-with-nvlink-switch-gives-signs-of-big-leap-in-time-to-first-token-performance/\">Continued</a></p>\n", "protected": false}, "author": 1481, "featured_media": 89488, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1492560", "discourse_permalink": "https://forums.developer.nvidia.com/t/low-latency-inference-chapter-2-blackwell-is-coming-nvidia-gh200-nvl32-with-nvlink-switch-gives-signs-of-big-leap-in-time-to-first-token-performance/308030", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 852, 3110, 1903], "tags": [296, 3065, 3266, 453, 4159, 3933, 2932], "coauthors": [2940, 3020, 2732, 1327, 506, 3849], "class_list": ["post-88938", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-data-center-cloud", "category-generative-ai", "category-features", "tag-ai-inference-microservices", "tag-avatar", "tag-chatbot", "tag-featured", "tag-inference-performance", "tag-llama", "tag-large-language-models"], "acf": {"post_industry": ["Cloud Services"], "post_products": ["Blackwell", "Hopper", "NVLink", "NVSwitch", "TensorRT-LLM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-GH200.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-n8u", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88938"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1481"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88938"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88938/revisions"}], "predecessor-version": [{"id": 89502, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88938/revisions/89502"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89488"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88938"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88938"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88938"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88938"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89214, "date": "2024-09-26T09:35:55", "date_gmt": "2024-09-26T16:35:55", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89214"}, "modified": "2024-10-28T14:54:29", "modified_gmt": "2024-10-28T21:54:29", "slug": "harnessing-data-with-ai-to-boost-zero-trust-cyber-defense", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/harnessing-data-with-ai-to-boost-zero-trust-cyber-defense/", "title": {"rendered": "Harnessing Data with AI to Boost Zero Trust Cyber Defense"}, "content": {"rendered": "\n<p>Modern cyber threats have grown increasingly sophisticated, posing significant risks to federal agencies and critical infrastructure. According to <a href=\"http://Deloitte\" data-type=\"link\" data-id=\"http://Deloitte\">Deloitte</a>, cybersecurity is the\u00a0top priority for governments and public sectors, highlighting the need to adapt to an increasingly digital world for efficiency and speed.\u00a0 \u00a0</p>\n\n\n\n<p>Threat examples include insider threats, supply chain vulnerabilities, ransomware attacks, and other complex cyber intrusions that can cause severe disruptions and data breaches. To combat these evolving risks, a zero-trust security strategy is essential for government agencies to protect sensitive data and critical systems. However, more can be done to strengthen zero-trust implementations. &nbsp;</p>\n\n\n\n<p>At its core, cybersecurity is a data problem. &nbsp;</p>\n\n\n\n<p>As the number of connected users and devices expands, organizations are generating more data than they can effectively collect, manage, and analyze.&nbsp; If we cannot observe 100% of the data across the entire enterprise for every user and machine, how can we build a robust model to detect all deviations? &nbsp;</p>\n\n\n\n<p>A <em>zero-trust strategy</em> assumes that no entity is trusted by default and verification is required from everyone to gain access. However, this approach requires increased visibility into every application and user on the network for continuous authentication and monitoring. &nbsp;</p>\n\n\n\n<p>To achieve zero-trust maturity, the vast amount of data must be continuously monitored and analyzed for each user and application to identify anomalous behaviors. Human analysts cannot track all sensitive data being generated nor set policies for every potential vulnerability. Traditional rule-based mechanisms cannot keep pace with the escalating adversary landscape. &nbsp;</p>\n\n\n\n<p>Using AI and generative AI technologies for advanced data analytics and automation is crucial.&nbsp;</p>\n\n\n\n<h2 id=\"bolster_cybersecurity_with_100%_data_visibility&nbsp;&nbsp;\"  class=\"wp-block-heading\">Bolster cybersecurity with 100% data visibility&nbsp;&nbsp;<a href=\"#bolster_cybersecurity_with_100%_data_visibility&nbsp;&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The massive influx of data significantly increases cybersecurity risks, creating an urgent need for advanced solutions like accelerated computing and AI. &nbsp;</p>\n\n\n\n<p>This is where <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/morpheus/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Morpheus</a>, a GPU-accelerated cybersecurity AI framework, can help. Morpheus enables you to build optimized AI pipelines for filtering, processing, and classifying large volumes of real-time data.\u202fWith Morpheus, organizations can harness the power of AI to analyze 100% of their data in real time across entire networks.&nbsp;&nbsp;</p>\n\n\n\n<p>Traditional user behavior analysis relies on rule-based approaches or supervised learning models, which require predefined rules or labeled data to identify usual patterns. However, these methods can miss new or evolving threats that don\u2019t fit the predefined patterns. &nbsp;</p>\n\n\n\n<p>Morpheus uses deep learning and unsupervised learning to overcome these limitations. By analyzing large volumes of unlabeled data, it can identify and learn the normal behavior and detect deviations from these learned patterns, flagging them as potential anomalies. This approach enables the identification of previously unseen or undetectable threats, providing a more robust and adaptive security solution.&nbsp;</p>\n\n\n\n<p>Using GPU acceleration, Morpheus can also process and analyze data at a much faster rate, delivering performance improvements of up to 600x compared to CPU-only solutions. This substantial speed increase reduces the time to detect from weeks to minutes, enabling more timely responses to potential security risks.&nbsp;</p>\n\n\n\n<p>The Morpheus architecture is designed to harness the power of GPUs throughout the entire data processing pipeline, including data ingestion, preprocessing, inference, and post-processing (Figure 1). This enables it to handle vast amounts of telemetry, including raw packet data, efficiently and quickly. &nbsp;</p>\n\n\n\n<p>Morpheus can perform rapid preprocessing, run deep learning models for real-time inference detecting anomaly, and execute post-processing to trigger immediate actions or policy updates. Using deep learning and <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/\" target=\"_blank\" rel=\"noreferrer noopener\">data science tools</a>, this integrated approach enables the development of security applications that can respond swiftly to threats and anomalies.&nbsp;</p>\n\n\n\n<p>When combined with generative AI, Morpheus can unlock a broader range of advanced cybersecurity use cases, extending traditional detection to enhance a human analyst\u2019s capabilities in solving complex problems. &nbsp;</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NIM</a> is a set of easy-to-use microservices for the secure deployment of AI model inferencing and <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NeMo</a> is an end-to-end platform for custom generative AI development. With these services, Morpheus can automate security vulnerability analysis and remediation, generate synthetic data to train AI models for accurate <a href=\"https://www.nvidia.com/en-us/ai-data-science/ai-workflows/spear-phishing/\" target=\"_blank\" rel=\"noreferrer noopener\">spear-phishing detection</a>, and address many more use cases.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"433\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/morpheus-cybersecurity-ai-pipeline-1024x433.png\" alt=\"Diagram shows input from SIEM/SOAR, app logs, cloud logs, and network telemetry moving through preprocessing, modeling, output, and inference. AI and generative AI enable faster anomaly detection, sensitive information detection, phishing detection, vulnerability analysis, ransomware detection, fraud detection, and more.\u00a0\" class=\"wp-image-89222\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/morpheus-cybersecurity-ai-pipeline-1024x433.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/morpheus-cybersecurity-ai-pipeline-300x127.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/morpheus-cybersecurity-ai-pipeline-625x264.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/morpheus-cybersecurity-ai-pipeline-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/morpheus-cybersecurity-ai-pipeline-768x324.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/morpheus-cybersecurity-ai-pipeline-645x272.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/morpheus-cybersecurity-ai-pipeline-500x211.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/morpheus-cybersecurity-ai-pipeline-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/morpheus-cybersecurity-ai-pipeline-362x153.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/morpheus-cybersecurity-ai-pipeline-260x110.png 260w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/morpheus-cybersecurity-ai-pipeline.png 1153w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA Morpheus cybersecurity AI&nbsp;pipeline</em></figcaption></figure></div>\n\n\n<p>Morpheus can be used as an SDK to build fully custom pipelines and models. You can also reference Morpheus cybersecurity AI workflows with pretrained models for various example use cases. NVIDIA collaborates with managed service partners, such as Deloitte and Accenture Federal Services, and leading security providers, such as <a href=\"https://www.crowdstrike.com/press-releases/crowdstrike-aims-to-secure-the-future-of-generative-ai-innovation-in-collaboration-with-nvidia/\" target=\"_blank\" rel=\"noreferrer noopener\">CrowdStrike</a> and <a href=\"https://newsroom.trendmicro.com/2024-08-07-Trend-Micro-Strengthens-AI-Deployments-for-Enterprises-and-Governments-with-NVIDIA-AI-Enterprise\" target=\"_blank\" rel=\"noreferrer noopener\">Trend Micro</a> to assist governments and enterprises in implementing cybersecurity AI applications. &nbsp;</p>\n\n\n\n<p>Here are two common challenges that public organizations are facing, where Morpheus cybersecurity AI workflows can help provide reference examples to address:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Accelerating anomaly detection with digital fingerprinting</li>\n\n\n\n<li>Automating CVE analysis with generative AI at enterprise scale&nbsp;</li>\n</ul>\n\n\n\n<h3 id=\"accelerating_anomaly_detection_with_digital_fingerprinting&nbsp;\"  class=\"wp-block-heading\">Accelerating anomaly detection with digital fingerprinting&nbsp;<a href=\"#accelerating_anomaly_detection_with_digital_fingerprinting&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>One of the common security risks faced by governments is <em>insider threats</em>. Insider threats can originate from employees or contractors who have access to sensitive information and can misuse it, either intentionally or unintentionally. &nbsp;</p>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/ai-data-science/ai-workflows/digital-fingerprinting/\" target=\"_blank\" rel=\"noreferrer noopener\">digital fingerprinting AI workflow</a> in Morpheus was created to address this challenge.&nbsp;This workflow provides a reference example designed to uniquely fingerprint every user, service, account, and machine across the entire enterprise to detect anomalies. &nbsp;</p>\n\n\n\n<p>While conventional user behavior detection depends on large grain models and pattern- or rule-based methods, these approaches are fragile to threats that resemble typical enterprise behavior. &nbsp;</p>\n\n\n\n<p>In contrast, Morpheus\u2019s digital fingerprinting approach offers a more nuanced and precise way for threat detection. Digital fingerprinting involves creating detailed and personalized models for every employee, group, business unit, and organization. This method captures the unique behavior patterns and activities of each user, enabling a more granular analysis of what constitutes normal behavior compared to anomalous behavior.&nbsp;</p>\n\n\n\n<p>With digital fingerprinting, Morpheus can identify anti-patterns with exceptional granularity to detect complex and subtle threats. The system continuously monitors user and machine activities, detecting any deviations from established patterns. When a shift is detected, it generates alerts with actionable information for security analysts so that they can investigate potential threats quickly and respond effectively.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/8rEPkHRvDq0?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1.&nbsp;Enterprise-Scale Cybersecurity Pinpoints Threats Faster</em>&nbsp;</figcaption></figure>\n\n\n\n<h3 id=\"automating_cve_analysis_with_generative_ai_at_enterprise_scale&nbsp;\"  class=\"wp-block-heading\">Automating CVE analysis with generative AI at enterprise scale&nbsp;<a href=\"#automating_cve_analysis_with_generative_ai_at_enterprise_scale&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Patching software security issues is increasingly challenging as the number of vulnerabilities reported in the <a href=\"https://www.cve.org/\" target=\"_blank\" rel=\"noreferrer noopener\">CVE database</a> continues to grow at an unprecedented pace. Legacy systems are vulnerable to evolving cyberthreats, so keeping up with the latest security updates becomes essential for government IT teams to defend against breaches.&nbsp;&nbsp;</p>\n\n\n\n<p>To triage a container for vulnerabilities, hundreds of pieces of information must be retrieved, understood, and synthesized. On average, it takes a human analyst hours or days to assess security issues for one container. With the growing number of vulnerabilities, the traditional manual approach to scanning and patching has become unmanageable.\u202f&nbsp;</p>\n\n\n\n<p>Generative AI unlocks the possibility to enhance vulnerability defense while decreasing workloads on security teams. Organizations have already begun to explore generative AI to help automate this process. However, doing so at an enterprise scale requires a complex AI system to do the collection, comprehension, and synthesis of massive amounts of information.&nbsp;&nbsp;</p>\n\n\n\n<p>Video 2 shows how generative AI and RAG can be used to reduce the time to identify and mitigate CVEs from hours or days to mere seconds.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/CEi_I7FMRV4?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 2. Supercharge Software Delivery with Event-Driven RAG</em>&nbsp;</figcaption></figure>\n\n\n\n<p>To solve this problem, we created&nbsp;the&nbsp;security vulnerability analysis AI workflow (Agent Morpheus for the purpose of this post). Using NVIDIA NIM microservices, NeMo Retriever, and Morpheus, this application enables <a href=\"https://developer.nvidia.com/blog/applying-generative-ai-for-cve-analysis-at-an-enterprise-scale/\" target=\"_blank\" rel=\"noreferrer noopener\">CVE analysis accelerated at enterprise scale</a>, dramatically reducing time to assess from days to just seconds.\u202f&nbsp;</p>\n\n\n\n<p>Agent Morpheus expedites the manual work of a human security analyst by properly and thoroughly researching and investigating a CVE and the scanned software container to confirm vulnerabilities. It generates investigation checklists, executes tasks to retrieve and analyze information, and then assesses if the container is vulnerable and exploitable. This process continues until all checklist items are addressed. &nbsp;</p>\n\n\n\n<p>Finally, the agent summarizes the interaction, generates action justifications, and presents them to a human analyst for final decision-making (Figure 2).&nbsp;&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"785\" height=\"597\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/security-vulnerability-ai-workflow-rag.png\" alt=\"Diagram shows that an LLM agent can cycle through the perceive, reason, and act loop itself, presenting a summary to a human analyst who can then make the final decision faster.\" class=\"wp-image-89221\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/security-vulnerability-ai-workflow-rag.png 785w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/security-vulnerability-ai-workflow-rag-300x228.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/security-vulnerability-ai-workflow-rag-625x475.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/security-vulnerability-ai-workflow-rag-151x115.png 151w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/security-vulnerability-ai-workflow-rag-768x584.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/security-vulnerability-ai-workflow-rag-645x491.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/security-vulnerability-ai-workflow-rag-394x300.png 394w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/security-vulnerability-ai-workflow-rag-118x90.png 118w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/security-vulnerability-ai-workflow-rag-362x275.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/security-vulnerability-ai-workflow-rag-145x110.png 145w\" sizes=\"(max-width: 785px) 100vw, 785px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA AI workflow for security vulnerability and exploitability analysis with event-driven RAG</em></figcaption></figure></div>\n\n\n<p>This event-driven approach uses large language models (LLMs) and retrieval-augmented generation (RAG). It enables security analysts to identify exploitable and vulnerable components in a software package, triggered by the creation of a new software package or a new CVE. It helps reduce noise and identify false positives so that security teams can focus on the most critical security issues.&nbsp;</p>\n\n\n\n<h2 id=\"learn_more\"  class=\"wp-block-heading\">Learn more<a href=\"#learn_more\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The two workflows demonstrate how AI and generative AI can address today\u2019s security challenges, particularly in threat detection and vulnerability management. &nbsp;</p>\n\n\n\n<p>Morpheus can also extend to many other detection use cases, such as spear-phishing, sensitive information, and ransomware. These can be implemented across government agencies to bolster their zero-trust security strategies as the cybersecurity landscape evolves.&nbsp;</p>\n\n\n\n<p>For more information, see the following resources:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://www.nvidia.com/en-us/ai-data-science/products/morpheus/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Morpheus</a> product page</li>\n\n\n\n<li><a href=\"https://github.com/nv-morpheus/Morpheus/tree/branch-24.10/models/model-cards\" target=\"_blank\" rel=\"noreferrer noopener\">/nv-morpheus</a>&nbsp;GitHub repo for models&nbsp;</li>\n\n\n\n<li><a href=\"https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+C-DS-03+V1\" target=\"_blank\" rel=\"noreferrer noopener\">Building AI-Based Cybersecurity Pipelines</a> instructor-led course</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/launchpad/ai/digital-fingerprinting-to-detect-cyber-threats/\" target=\"_blank\" rel=\"noreferrer noopener\">Digital Fingerprinting to Detect Cyber Threats</a> free hands-on lab&nbsp;</li>\n</ul>\n\n\n\n<p>Sign up to be notified about&nbsp;the upcoming <a href=\"https://www.nvidia.com/en-us/ai-data-science/ai-workflows/security-vulnerability-analysis/\" target=\"_blank\" rel=\"noreferrer noopener\">security vulnerability analysis AI workflow</a> It\u2019s free with a 90-day trial of <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA AI Enterprise</a>. To learn more about how you can use NVIDIA Morpheus to address cybersecurity challenges with AI, contact <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/contact-sales/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA AI Enterprise Sales</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Modern cyber threats have grown increasingly sophisticated, posing significant risks to federal agencies and critical infrastructure. According to Deloitte, cybersecurity is the\u00a0top priority for governments and public sectors, highlighting the need to adapt to an increasingly digital world for efficiency and speed.\u00a0 \u00a0 Threat examples include insider threats, supply chain vulnerabilities, ransomware attacks, and other &hellip; <a href=\"https://developer.nvidia.com/blog/harnessing-data-with-ai-to-boost-zero-trust-cyber-defense/\">Continued</a></p>\n", "protected": false}, "author": 2033, "featured_media": 89427, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1492462", "discourse_permalink": "https://forums.developer.nvidia.com/t/harnessing-data-with-ai-to-boost-zero-trust-cyber-defense/308004", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 696, 3110], "tags": [453, 1946, 1511, 2541], "coauthors": [3738], "class_list": ["post-89214", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-cybersecurity", "category-data-science", "category-generative-ai", "tag-featured", "tag-government-national-labs", "tag-security-ai", "tag-zero-trust"], "acf": {"post_industry": ["Public Sector"], "post_products": ["Morpheus", "NeMo", "NeMo Retriever", "NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cybersecurity-ai-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ncW", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Cybersecurity", "link": "https://developer.nvidia.com/blog/category/cybersecurity/", "id": 1464}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89214"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2033"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89214"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89214/revisions"}], "predecessor-version": [{"id": 89472, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89214/revisions/89472"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89427"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89214"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89214"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89214"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89214"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89334, "date": "2024-09-26T09:00:00", "date_gmt": "2024-09-26T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89334"}, "modified": "2024-10-23T16:37:06", "modified_gmt": "2024-10-23T23:37:06", "slug": "spotlight-montai-builds-a-multimodal-ai-platform-for-drug-discovery-using-nvidia-nim-microservices", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/spotlight-montai-builds-a-multimodal-ai-platform-for-drug-discovery-using-nvidia-nim-microservices/", "title": {"rendered": "Spotlight: Montai Builds a Multimodal AI Platform for Drug Discovery Using NVIDIA NIM Microservices"}, "content": {"rendered": "\n<p>Drug discovery aims to develop new therapeutic agents that effectively target diseases while minimizing side effects for patients. Using multimodal data\u2014such as molecular structures, cellular images, sequences, and unstructured data\u2014can be highly valuable in identifying novel and safe drug candidates.&nbsp;</p>\n\n\n\n<p>Yet, creating multimodal AI models for computer-aided drug discovery is challenging. These models must align diverse types of data and handle significant computational complexity. Multimodal AI models often require advanced deep learning techniques, which are computationally demanding. Ensuring that these models use information from all data types effectively without introducing bias is a major difficulty. For more details, see <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10886071/\">Toward Unified AI Drug Discovery with Multimodal Knowledge</a>.</p>\n\n\n\n<p><a href=\"https://www.montai.com/\">Montai</a> Therapeutics, a<a href=\"https://www.flagshippioneering.com/\"> Flagship Pioneering</a> company, is tackling these challenges using the <a href=\"https://www.nvidia.com/en-us/clara/bionemo/\">NVIDIA BioNeMo</a> platform. At the core of Montai&#8217;s innovation is the aggregation and curation of the world&#8217;s largest, fully annotated library of Anthromolecule chemistry. Anthromolecules, a term coined by Montai, refer to the rigorously curated collection of bioactive molecules humans have consumed in foods, supplements, and herbal medicines\u2014a diverse chemical source.</p>\n\n\n\n<p>Anthromolecules and their derivatives, with scaffolds designed to modulate biology, offer far greater chemical structural diversity than traditional synthetic combinatorial chemistry libraries. This diversity is critical for two reasons: Anthromolecule<sup> </sup>chemistry has already been proven to be a source of FDA-approved drugs for various diseases, yet it remains largely untapped for systematic drug development. The rich topological structures across this diverse chemistry offer a far wider range of vectors to engage complex biology with precision and selectivity and can unlock small molecule pill-based solutions for targets that have historically eluded drug developers to transform therapeutic offerings across chronic diseases.</p>\n\n\n\n<h2 id=\"creating_a_multimodal_ai_drug_discovery_platform_using_nvidia_nim\"  class=\"wp-block-heading\">Creating a multimodal AI drug discovery platform using NVIDIA NIM<a href=\"#creating_a_multimodal_ai_drug_discovery_platform_using_nvidia_nim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In a recent collaboration, Montai and the NVIDIA BioNeMo solution team have made significant strides in developing a multimodal model aimed at virtually identifying potential small molecule drugs from Anthromolecule<sup> </sup>sources. The model, built on AWS EC2, is trained on multiple large-scale biological datasets. It incorporates <a href=\"https://build.nvidia.com/mit/diffdock\">NVIDIA BioNeMo DiffDock NIM</a>, a state-of-the-art generative model for blind molecular docking pose estimation. BioNeMo DiffDock NIM is part of <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a>, a set of easy-to-use microservices designed to accelerate deployment of generative AI across cloud, data center, and workstations.</p>\n\n\n\n<p>The collaboration between Montai and NVIDIA has produced notable model architecture optimization on the backbone of a contrastive learning foundation model. Initial results are promising, with the model demonstrating superior performance to traditional machine learning methods for molecular function prediction.</p>\n\n\n\n<p>The multimodal model unifies information across four modalities:&nbsp;</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Chemical structure</li>\n\n\n\n<li>Phenotypic cell data</li>\n\n\n\n<li>Gene expression data</li>\n\n\n\n<li>Information about biological pathways&nbsp;</li>\n</ol>\n\n\n\n<p>The combined use of these four modalities has resulted in a model that outperforms single-modality models, demonstrating the benefits of contrastive learning and foundation model paradigms in the AI for drug discovery space.&nbsp;</p>\n\n\n\n<p>By integrating these diverse modalities, the model will help Montai Therapeutics more effectively identify promising lead compounds for drug development through their CONECTA platform. This innovative drug operating system facilitates the predictable discovery of transformative small molecule drugs from a wide range of untapped human chemistry.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"837\" height=\"182\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/montai-conecta-platform.png\" alt=\"Image showing a representation of Montai's CONECTA platform. The image contains three circles - the one on the left showing phylogenetic relationship trees representing biological databases and pathways. The circle on the right depicts a heatmap containing diverse chemical databases. And the circle in the middle represents the confluence of biological and chemical data on Montai\u2019s Conecta platform.\n\" class=\"wp-image-89340\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/montai-conecta-platform.png 837w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/montai-conecta-platform-300x65.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/montai-conecta-platform-625x136.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/montai-conecta-platform-179x39.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/montai-conecta-platform-768x167.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/montai-conecta-platform-645x140.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/montai-conecta-platform-500x109.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/montai-conecta-platform-160x35.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/montai-conecta-platform-362x79.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/montai-conecta-platform-506x110.png 506w\" sizes=\"(max-width: 837px) 100vw, 837px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The Montai CONECTA platform connects biological pathways (left) to diverse chemical space (right) to rapidly and iteratively discern novel small molecules with the potential to potently and selectively treat chronic<strong> </strong>diseases</em></figcaption></figure>\n\n\n\n<h2 id=\"conclusion&nbsp;\"  class=\"wp-block-heading\">Conclusion&nbsp;<a href=\"#conclusion&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Currently, the collaborative efforts are focused on incorporating a fifth modality, the \u201cdocking fingerprint,\u201d derived from DiffDock predictions. The role of NVIDIA BioNeMo has been instrumental in scaling up the inference process, enabling more efficient computation. For example, DiffDock on the DUD-E dataset, with 40 poses per ligand on eight <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 Tensor Core</a> GPUs, achieves a processing speed of 0.76 seconds per ligand.</p>\n\n\n\n<p>These advancements underscore the importance of efficient GPU utilization in drug screening and highlight the successful use of NVIDIA NIM and a multimodal AI model. The collaboration between Montai and NVIDIA represents a critical step forward in the pursuit of more effective and efficient drug discovery processes.&nbsp;</p>\n\n\n\n<p>Learn more about <a href=\"https://www.nvidia.com/en-us/clara/bionemo/\">NVIDIA BioNeMo</a> and <a href=\"https://build.nvidia.com/mit/diffdock\">NVIDIA BioNeMo DiffDock NIM</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Drug discovery aims to develop new therapeutic agents that effectively target diseases while minimizing side effects for patients. Using multimodal data\u2014such as molecular structures, cellular images, sequences, and unstructured data\u2014can be highly valuable in identifying novel and safe drug candidates.&nbsp; Yet, creating multimodal AI models for computer-aided drug discovery is challenging. These models must align &hellip; <a href=\"https://developer.nvidia.com/blog/spotlight-montai-builds-a-multimodal-ai-platform-for-drug-discovery-using-nvidia-nim-microservices/\">Continued</a></p>\n", "protected": false}, "author": 2313, "featured_media": 89344, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1492449", "discourse_permalink": "https://forums.developer.nvidia.com/t/spotlight-montai-builds-a-multimodal-ai-platform-for-drug-discovery-using-nvidia-nim-microservices/308001", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 503], "tags": [3941, 3262, 453, 3739], "coauthors": [4045, 4046], "class_list": ["post-89334", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "category-simulation-modeling-design", "tag-ai-impact", "tag-bionemo", "tag-featured", "tag-nim"], "acf": {"post_industry": "", "post_products": "", "post_learning_levels": "", "post_content_types": "", "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/molecules-black-background.png", "jetpack_shortlink": "https://wp.me/pcCQAL-neS", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89334"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2313"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89334"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89334/revisions"}], "predecessor-version": [{"id": 89470, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89334/revisions/89470"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89344"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89334"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89334"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89334"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89334"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89345, "date": "2024-09-25T13:30:00", "date_gmt": "2024-09-25T20:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89345"}, "modified": "2024-10-22T13:34:33", "modified_gmt": "2024-10-22T20:34:33", "slug": "build-a-digital-human-interface-for-ai-apps-with-an-nvidia-nim-agent-blueprint", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/build-a-digital-human-interface-for-ai-apps-with-an-nvidia-nim-agent-blueprint/", "title": {"rendered": "Build a Digital Human Interface for AI Apps with an NVIDIA NIM Agent Blueprint"}, "content": {"rendered": "\n<p>Providing customers with quality service remains a top priority for businesses across industries, from answering questions and troubleshooting issues to facilitating online orders. As businesses scale operations and expand offerings globally to compete, the demand for seamless customer service grows exponentially.&nbsp;</p>\n\n\n\n<p>Searching knowledge base articles or navigating complex phone trees can be a useful resource, but the density of information customers must often parse through can contribute to poor retention.</p>\n\n\n\n<p>With the convergence of technologies such as <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/\">generative AI</a>, <a href=\"https://www.nvidia.com/en-in/deep-learning-ai/solutions/conversational-ai/\">conversational AI</a>, and <a href=\"https://www.nvidia.com/en-us/use-cases/visual-ai-agents/\">visual AI</a>, application development teams are seeking ways to increase engagement with self-serve customer applications to boost overall satisfaction and retention.&nbsp;</p>\n\n\n\n<p>Instead of a text-based chatbot, you can now add digital human interfaces to personalize chatbot applications and use <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation (RAG)</a> to offer a smooth, human-like interaction. This post explains how this can be achieved using the <a href=\"https://build.nvidia.com/nvidia/digital-humans-for-customer-service\">digital human NVIDIA NIM Agent Blueprint</a>, which combines <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM microservices</a> with reference code and documentation.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai-data-science/ai-workflows/\">NVIDIA NIM Agent Blueprints</a> are reference workflows that enterprises can use to build and operationalize custom AI applications. <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a> is a set of easy-to-use microservices designed to accelerate deployment of generative AI across cloud, data center, and workstations.&nbsp;</p>\n\n\n\n<h2 id=\"designing_a_human-like_chatbot_for_customer_service\"  class=\"wp-block-heading\">Designing a human-like chatbot for customer service<a href=\"#designing_a_human-like_chatbot_for_customer_service\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>When designing a customer service tool, it\u2019s important to consider your end users and their desired method of interfacing with your application.</p>\n\n\n\n<p>While text-based apps offer several advantages, the need for multilingual language support and accurate information retrieval fuel the demand for digital human interfaces.</p>\n\n\n\n<p>For example, <a href=\"https://build.nvidia.com/nvidia/digital-humans-for-customer-service\">James is an interactive digital human</a> who is knowledgeable about NVIDIA and NVIDIA products. Enterprises looking to provide scalable, engaging, customer service support can leverage a similar avatar to fulfill these needs with higher accuracy using RAG for information retrieval\u2014all while operating with low latency.</p>\n\n\n\n<p>To build better user interfaces that address diverse user needs, you can use the digital human for customer service NVIDIA NIM Agent blueprint, which uses the following:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://build.nvidia.com/nvidia/parakeet-ctc-1_1b-asr/\"><strong>NVIDIA Riva ASR NIM</strong></a><strong>: </strong>State-of-the-art <a href=\"https://build.nvidia.com/nvidia/parakeet-ctc-1_1b-asr\">Parakeet automatic speech recognition (ASR) model</a>, capable of transcribing spoken English with exceptional accuracy.</li>\n\n\n\n<li><a href=\"https://build.nvidia.com/nvidia/fastpitch-hifigan-tts\"><strong>NVIDIA Riva TTS NIM</strong>:</a> FastPitch is a mel-spectrogram generator, designed to be used as the first part of a neural text-to-speech (TTS) system in conjunction with a neural vocoder. This model can output a female or a male voice for US English<a href=\"https://build.nvidia.com/nvidia/fastpitch-hifigan-tts\">.</a></li>\n\n\n\n<li><a href=\"https://build.nvidia.com/nvidia/audio2face\"><strong>NVIDIA Audio2Face NIM</strong></a><strong>: </strong>Animates 3D character\u2019s facial characteristics to match any audio track.\u00a0</li>\n\n\n\n<li><a href=\"https://build.nvidia.com/meta/llama3-8b\"><strong>Llama 3 8B NIM</strong></a>: Advanced state-of-the-art large language model (LLM) with language understanding, superior reasoning, and text generation.</li>\n</ul>\n\n\n\n<p>For more information, visit <a href=\"https://developer.nvidia.com/ace\">NVIDIA ACE</a>.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/8xMeIwBnIpU?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. See how digital humans are transforming industries with NVIDIA ACE</em></figcaption></figure>\n\n\n\n<h2 id=\"a_digital_human_nim_agent_blueprint_for_customer_service_apps&nbsp;\"  class=\"wp-block-heading\">A digital human NIM Agent Blueprint for customer service apps&nbsp;<a href=\"#a_digital_human_nim_agent_blueprint_for_customer_service_apps&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The <a href=\"https://build.nvidia.com/nvidia/digital-humans-for-customer-service\">digital human for customer service NIM Agent Blueprint</a> is powered by <a href=\"https://developer.nvidia.com/ace/tokkio-showcase\">NVIDIA Tokkio</a>, which is a reference workflow of the ACE ecosystem. It brings enterprise applications to life with a 3D animated digital human interface. With an approachable, human-like interface, customer service applications can provide better user experiences with faster resolutions than traditional customer service options.</p>\n\n\n\n<p>This workflow is designed to integrate within your existing generative AI applications built using RAG (Figure 1).&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"899\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service.png\" alt=\"Architecture diagram showing digital human workflow with audio, video, and agent capabilities.\n\" class=\"wp-image-89355\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service-300x135.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service-625x281.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service-179x81.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service-768x345.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service-1536x691.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service-645x290.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service-500x225.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service-160x72.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service-362x163.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service-245x110.png 245w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/diagram-nvidia-nim-agent-blueprint-digital-human-customer-service-1024x461.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The digital human for customer service NVIDIA NIM Agent Blueprint diagram</em></figcaption></figure>\n\n\n\n<p>There are two main steps to evolve your applications to include a full digital human interface, whether they\u2019re running in your data center, in the cloud, or at the edge.</p>\n\n\n\n<h3 id=\"step_1_initiating_user_interaction&nbsp;\"  class=\"wp-block-heading\">Step 1: Initiating user interaction&nbsp;<a href=\"#step_1_initiating_user_interaction&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Walking through the workflow shown in Figure 1, audio from the user comes in through the web front end and is processed by an audio/video engine and passed on to the NVIDIA ACE agent.</p>\n\n\n\n<h3 id=\"step_2_powering_life-like_digital_humans\"  class=\"wp-block-heading\">Step 2: Powering life-like digital humans<a href=\"#step_2_powering_life-like_digital_humans\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The agent uses the audio pipeline to convert audio to text and text to audio as it interacts with the RAG-powered chatbot or copilot using its API. The audio is sent to the 3D animation pipeline to animate the avatar and render its features to be lifelike.&nbsp;</p>\n\n\n\n<p>As users engage the digital human, they can provide feedback on the response (thumbs up or down, for example), which is then fed into the backend RAG application for improvement.</p>\n\n\n\n<p>The digital human for customer service NVIDIA NIM Agent Blueprint includes all the components you need to get started with your own digital human project:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Digital human, named Ben, which can be customized for your applications</li>\n\n\n\n<li>Sample applications\u00a0</li>\n\n\n\n<li>Customization documentation</li>\n\n\n\n<li>Reference code</li>\n\n\n\n<li>Helm chart</li>\n\n\n\n<li>Integration guidelines</li>\n\n\n\n<li>Deployment instructions</li>\n\n\n\n<li>Evaluation metrics</li>\n</ul>\n\n\n\n<p>The comprehensive package is designed to help you quickly create and deploy AI-powered digital humans for customer service applications, while enabling customization to meet your specific business needs.&nbsp;</p>\n\n\n\n<p>By leveraging this NIM Agent Blueprint and its components, you can:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Increase engagement and satisfaction for user-facing applications by enabling natural conversations with enterprise applications and data.</li>\n\n\n\n<li>Use a suite of easy-to-use and performance-optimized NVIDIA NIM microservices, for avatar animation, speech AI, and generative AI.</li>\n\n\n\n<li>Create lifelike digital humans that are rendered with subsurface scattering for accurate skin and hair through the Omniverse RTX microservice, animated with the Audio2Face NIM, and has a responsive speech interface with the NVIDIA Riva Parakeet NIM and ElevenLabs integrations.</li>\n\n\n\n<li>Runs anywhere, in an on-premises data center, in the cloud, or at the edge.</li>\n</ul>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://build.nvidia.com/nvidia/digital-humans-virtual-assistant\">Experience the digital human for customer service interactive demo</a>. Navigate to the Blueprint Card to see customization, compatibility, and deployment information.</p>\n\n\n\n<p>To see the digital human for customer service NIM Agent Blueprint documentation, sample code, and Helm chart, visit the <a href=\"https://github.com/NVIDIA/GenerativeAIExamples\">Generative AI Examples</a> GitHub repository.</p>\n\n\n\n<p>To start building applications using NVIDIA NIM microservices, visit the <a href=\"https://build.nvidia.com/explore/discover\">NVIDIA API Catalog</a>. You\u2019ll be prompted to join the <a href=\"https://developer.nvidia.com/join-nvidia-developer-program\">NVIDIA Developer Program</a> for free access during development. Or request a 90-day trial of <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, including access to NIM with NVIDIA support. For more information, see the <a href=\"https://forums.developer.nvidia.com/t/nim-faq/300317\">NVIDIA NIM FAQ</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Providing customers with quality service remains a top priority for businesses across industries, from answering questions and troubleshooting issues to facilitating online orders. As businesses scale operations and expand offerings globally to compete, the demand for seamless customer service grows exponentially.&nbsp; Searching knowledge base articles or navigating complex phone trees can be a useful resource, &hellip; <a href=\"https://developer.nvidia.com/blog/build-a-digital-human-interface-for-ai-apps-with-an-nvidia-nim-agent-blueprint/\">Continued</a></p>\n", "protected": false}, "author": 2219, "featured_media": 89357, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1491963", "discourse_permalink": "https://forums.developer.nvidia.com/t/build-a-digital-human-interface-for-ai-apps-with-an-nvidia-nim-agent-blueprint/307923", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [3063, 453, 3739, 4134, 3613], "coauthors": [3946, 3962, 3062], "class_list": ["post-89345", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "tag-omniverse-ace", "tag-featured", "tag-nim", "tag-nim-agent-blueprint", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["General"], "post_products": ["AI Enterprise", "Avatar Cloud Engine (ACE)", "NIM", "Riva"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/digital-human-interface-representation.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nf3", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89345"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2219"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89345"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89345/revisions"}], "predecessor-version": [{"id": 89356, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89345/revisions/89356"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89357"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89345"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89345"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89345"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89345"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89436, "date": "2024-09-25T11:39:49", "date_gmt": "2024-09-25T18:39:49", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89436"}, "modified": "2024-11-06T21:08:12", "modified_gmt": "2024-11-07T05:08:12", "slug": "deploying-accelerated-llama-3-2-from-the-edge-to-the-cloud", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/deploying-accelerated-llama-3-2-from-the-edge-to-the-cloud/", "title": {"rendered": "Deploying Accelerated Llama 3.2 from the Edge to the Cloud"}, "content": {"rendered": "\n<p>Expanding the open-source Meta Llama collection of models, the Llama 3.2 collection includes vision language models (VLMs), small language models (SLMs), and an updated Llama Guard model with support for vision. When paired with the NVIDIA accelerated computing platform, Llama 3.2 offers developers, researchers, and enterprises valuable new capabilities and optimizations to realize their generative AI use cases.</p>\n\n\n\n<p>Trained on <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100 Tensor Core GPUs</a>, the SLMs in 1B and 3B sizes are ideal for deploying Llama-based AI assistants across edge devices. The VLMs in 11B and 90B sizes support text and image inputs and output text. With multimodal support, the VLMs help developers build powerful applications requiring visual grounding, reasoning, and understanding. For example, they can build AI agents for image captioning, image-text retrieval, visual Q&amp;A, and document Q&amp;A, among others. The Llama Guard models now also support image input guardrails in addition to text input.&nbsp;</p>\n\n\n\n<p>Llama 3.2 model architecture is an auto-regressive language model that uses an optimized transformer architecture. The instruction tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. All models support a long context length of 128K tokens and are optimized for inference with support for grouped query attention (GQA).</p>\n\n\n\n<p>NVIDIA is optimizing the Llama 3.2 collection of models to deliver high throughput and low latency across millions of GPUs worldwide\u2014from data centers to local workstations with <a href=\"https://www.nvidia.com/en-us/ai-on-rtx/\">NVIDIA RTX</a>, and at the edge with <a href=\"https://developer.nvidia.com/embedded-computing\">NVIDIA Jetson</a>. This post describes the hardware and software optimizations, customizations, and ease-of-deployment capabilities.\u00a0</p>\n\n\n\n<h2 id=\"accelerating_llama_32_performance_with_nvidia_tensorrt&nbsp;\"  class=\"wp-block-heading\">Accelerating Llama 3.2 performance with NVIDIA TensorRT&nbsp;<a href=\"#accelerating_llama_32_performance_with_nvidia_tensorrt&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA is accelerating the Llama 3.2 model collection to reduce cost and latency while delivering unparalleled throughput and providing an optimal end-user experience. <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> includes <a href=\"https://developer.nvidia.com/tensorrt/download\">TensorRT</a> and <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main\">TensorRT-LLM</a> libraries for high-performance deep learning inference.</p>\n\n\n\n<p>The Llama 3.2 1B and Llama 3.2 3B models are being accelerated for long-context support in TensorRT-LLM using the <a href=\"https://arxiv.org/pdf/2310.05209\">scaled rotary position embedding (RoPE)</a> technique and several other <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\">optimizations</a>, including KV caching and in-flight batching.&nbsp;</p>\n\n\n\n<p>The Llama 3.2 11B and Llama 3.2 90B models are multimodal and include a vision encoder with a text decoder. The vision encoder is being accelerated by exporting the model into an ONNX graph and building the TensorRT engine. <a href=\"https://onnx.ai/\">ONNX</a> export creates a standard model definition with built-in operators and standard data types, focused on inferencing. TensorRT uses the ONNX graph to optimize the model for target GPUs by building the <a href=\"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#fit\">TensorRT engine</a>. These engines offer a variety of hardware-level optimizations to maximize NVIDIA GPU utilization through layer and tensor fusion in conjunction with kernel auto-tuning.&nbsp;</p>\n\n\n\n<p>The visual information from the vision encoder is fused into the Llama text decoder with a cross-attention mechanism that is supported in TensorRT-LLM. This enables the VLMs to efficiently generate text by taking into account visual reasoning and understanding in context with text input.&nbsp;</p>\n\n\n\n<h2 id=\"easily_deploy_generative_ai_solutions_using_nvidia_nim\"  class=\"wp-block-heading\">Easily deploy generative AI solutions using NVIDIA NIM<a href=\"#easily_deploy_generative_ai_solutions_using_nvidia_nim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The TensorRT optimizations are available through production-ready deployments using <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a> microservices. NIM microservices accelerate the deployment of generative AI models across NVIDIA-accelerated infrastructure anywhere, including cloud, data center, and workstations.</p>\n\n\n\n<p><a href=\"https://build.nvidia.com/meta/llama-3.2-90b-vision-instruct\">Llama 3.2 90B Vision Instruct</a>, <a href=\"https://build.nvidia.com/meta/llama-3.2-11b-vision-instruct\">Llama 3.2 11B Vision Instruct</a>, <a href=\"https://build.nvidia.com/meta/llama-3.2-3b-instruct\">Llama 3.2 3B Instruct</a>, and <a href=\"https://build.nvidia.com/meta/llama-3.2-1b-instruct\">Llama 3.2 1B Instruct</a> are supported through NIM microservices for production deployments. NIM provides simplified management and orchestration of generative AI workloads, standard application programming interface (APIs), and enterprise support with production-ready containers. Offering strong and growing ecosystem support with over 175 partners integrating their solutions with NVIDIA NIM microservices, developers, researchers and enterprises around the world can maximize their return on investment for generative AI applications.</p>\n\n\n\n<h2 id=\"customize_and_evaluate_llama_32_models_with_nvidia_ai_foundry_and_nvidia_nemo\"  class=\"wp-block-heading\">Customize and evaluate Llama 3.2 models with NVIDIA AI Foundry and NVIDIA NeMo<a href=\"#customize_and_evaluate_llama_32_models_with_nvidia_ai_foundry_and_nvidia_nemo\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai/foundry/\">NVIDIA AI Foundry</a> provides an end-to-end platform for Llama 3.2 model customizations with access to advanced AI tools, computing resources, and AI expertise. Fine-tuned on proprietary data, the custom models enable enterprises to achieve better performance and accuracy in domain-specific tasks, gaining a competitive edge.</p>\n\n\n\n<p>With <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo</a>, developers can curate their training data, leverage advanced tuning techniques including LoRA, SFT, DPO, and RLHF to customize the Llama 3.2 models, evaluate for accuracy, and add guardrails to ensure appropriate responses from the models. AI Foundry provides dedicated capacity on <a href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\">NVIDIA DGX Cloud</a>, and is supported by NVIDIA AI experts. The output is a custom Llama 3.2 model packaged as an NVIDIA NIM inference microservice, which can be deployed anywhere.</p>\n\n\n\n<h2 id=\"scale_local_inference_with_nvidia_rtx_and_nvidia_jetson\"  class=\"wp-block-heading\">Scale local inference with NVIDIA RTX and NVIDIA Jetson<a href=\"#scale_local_inference_with_nvidia_rtx_and_nvidia_jetson\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Today, Llama 3.2 models are optimized on the 100M+ NVIDIA RTX PCs and workstations worldwide. For Windows deployments, NVIDIA has optimized this suite of models to work efficiently using the ONNX-GenAI runtime, with a DirectML backend. Get started with the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/models/meta-llama-3.2-3b-onnx-int4-rtx\">Llama 3.2 3B model on NVIDIA RTX</a>.</p>\n\n\n\n<p>The new VLM and SLM models unlock new capabilities on NVIDIA RTX systems. To demonstrate, we created an example of a multimodal <a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">retrieval-augmented generation (RAG)</a> pipeline that combines text and visual data processing (for images, plots, and charts, for example) for enhanced information retrieval and generation.&nbsp;</p>\n\n\n\n<p><a href=\"https://github.com/jayrodge/Multimodal-RAG-with-Llama-3.2\">Learn how to run this pipeline on NVIDIA RTX Linux systems using the Llama 3.2 SLM and VLM</a>. Note that you\u2019ll need a Linux workstation with an NVIDIA RTX professional GPU with 30+ GB of memory.</p>\n\n\n\n<p>SLMs are tailored for local deployment on edge devices using techniques like distillation, pruning, and quantization to reduce memory, latency, and computational requirements while retaining accuracy for application-focused domains. To download and deploy the Llama 3.2 1B and 3B SLMs onboard your Jetson with optimized GPU inference and INT4/FP8 quantization, see the <a href=\"https://www.jetson-ai-lab.com/tutorial_slm.html#llama-3-2\">SLM Tutorial on NVIDIA Jetson AI Lab</a>.&nbsp;&nbsp;</p>\n\n\n\n<p>Multimodal models are increasingly useful in edge applications for their unique vision capabilities in video analytics and robotics. The <a href=\"https://www.jetson-ai-lab.com/llama_vlm.html\">Llama 3.2 11B VLM is supported on embedded Jetson AGX Orin 64 GB</a>.</p>\n\n\n\n<h2 id=\"advancing_community_ai_models\"  class=\"wp-block-heading\">Advancing community AI models<a href=\"#advancing_community_ai_models\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>An active open-source contributor, NVIDIA is committed to optimizing community software that helps users address their toughest challenges. Open-source AI models also promote transparency and enable users to broadly share work on AI safety and resilience.</p>\n\n\n\n<p>The <a href=\"https://blogs.nvidia.com/blog/hugging-face-inference-nim-microservices-dgx-cloud/\">Hugging Face inference-as-a-service</a> capabilities enable developers to rapidly deploy leading <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models (LLMs)</a> such as the Llama 3 collection with optimization from NVIDIA NIM microservices running on <a href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\">NVIDIA DGX Cloud</a>.&nbsp;</p>\n\n\n\n<p>Get free access to NIM for research, development, and testing through the <a href=\"https://developer.nvidia.com/developer-program\">NVIDIA Developer Program</a>.&nbsp;</p>\n\n\n\n<p>Explore the NVIDIA AI inference platform further, including how <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a>, <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">NVIDIA TensorRT-LLM</a>, <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT,</a> and <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton</a> use state-of-the-art techniques such as <a href=\"https://developer.nvidia.com/blog/tune-and-deploy-lora-llms-with-nvidia-tensorrt-llm/\">LoRA</a> to accelerate the latest LLMs.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Expanding the open-source Meta Llama collection of models, the Llama 3.2 collection includes vision language models (VLMs), small language models (SLMs), and an updated Llama Guard model with support for vision. When paired with the NVIDIA accelerated computing platform, Llama 3.2 offers developers, researchers, and enterprises valuable new capabilities and optimizations to realize their generative &hellip; <a href=\"https://developer.nvidia.com/blog/deploying-accelerated-llama-3-2-from-the-edge-to-the-cloud/\">Continued</a></p>\n", "protected": false}, "author": 1837, "featured_media": 89440, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1491929", "discourse_permalink": "https://forums.developer.nvidia.com/t/deploying-accelerated-llama-3-2-from-the-edge-to-the-cloud/307915", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110, 1903], "tags": [3693, 453, 2932, 3739, 3953], "coauthors": [3461, 2387, 142, 2362, 2367], "class_list": ["post-89436", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "category-features", "tag-ai-foundation-models", "tag-featured", "tag-large-language-models", "tag-nim", "tag-vlms"], "acf": {"post_industry": ["General"], "post_products": ["AI Enterprise", "H100", "Jetson", "NeMo", "NIM", "TensorRT", "TensorRT-LLM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/llama-3.2-graphic.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ngw", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89436"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1837"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89436"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89436/revisions"}], "predecessor-version": [{"id": 89469, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89436/revisions/89469"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89440"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89436"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89436"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89436"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89436"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89454, "date": "2024-09-25T08:53:36", "date_gmt": "2024-09-25T15:53:36", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89454"}, "modified": "2024-10-17T12:07:15", "modified_gmt": "2024-10-17T19:07:15", "slug": "how-ai-and-robotics-are-driving-agricultural-productivity-and-sustainability", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/how-ai-and-robotics-are-driving-agricultural-productivity-and-sustainability/", "title": {"rendered": "How AI and Robotics are Driving Agricultural Productivity and Sustainability"}, "content": {"rendered": "\n<p>By 2030, John Deere aims for fully autonomous farming, addressing global challenges like labor shortages, sustainability, and food security. Their AI and robotics solutions make farming more efficient and profitable, reduce environmental impact, lower carbon footprints, and promote biodiversity.</p>\n\n\n\n<p>In this session, Chris Padwick, director of Machine Learning and Computer Vision at John Deere, outlines how AI and robotics advance agriculture through their autonomous tractor and See &amp; Spray system. Leveraging deep learning and computer vision to optimize resource use and increase productivity, these technologies help farmers by automating tasks like precision herbicide application, reducing chemical usage, conserving resources, and improving crop yields.</p>\n\n\n\n<script src=\"https://api-prod.nvidia.com/search/nvidia-search-library.js\"></script>\n \n\n<div id=\"nvidia-event-details-widget\"></div>\n<style>\n.nvidia-search-widget .cleanslate , .nvidia-search-widget .player-overlay {\ndisplay:none;\n}\n</style>\n \n\n<script>\n \n NvidiaSearchLibrary.EventSessionDetailsWidget.mount({\n          site: 'https://www.nvidia.com',\n          language: 'en-us',\n          sessionId: 'gtc24-s63033',\n          jwtToken: '',\n \u2002\u2002\u2002\u2002voltronApiUrl:  'https://api-prod.nvidia.com/services/nod/api/v1/',\n          apiUrl: 'https://api-prod.nvidia.com/search/graphql',\n           onLogin: () => { },\n          onLogout: () => { },\n       \n          onSeeAllSessions: (speakerName) => {\n            window.location.href =  'https://www.nvidia.com/en-us/on-demand/search/?q=\"' + speakerName+'\"';\n          },\n          searchApiUrl: 'https://api-prod.nvidia.com/search/graphql',\n          searchToken: '',\n          uiConfId: '50468382',\n          showSessionRating: false,\n          anonToken: '',\n        });\n \n</script>\n\n\n\n<p>Follow along with a <a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/S63033.pdf\">PDF of the session</a> as Padwick discusses how the collaboration with NVIDIA, dating to participation in the NVIDIA Inception Program, has been instrumental in accelerating the development of these AI-driven systems.\u00a0Powered by NVIDIA GPUs, John Deere\u2019s connected machines gather and process real-time data on-site, enabling autonomous systems to make real-time decisions every 20\u201380 seconds, even in challenging conditions.\u00a0</p>\n\n\n\n<p>This is a must-watch for anyone interested in the future of agriculture and how cutting-edge AI and robotics are reshaping the industry. You&#8217;ll learn how AI-powered autonomous systems can tackle global challenges while gaining insights into how computer vision, edge computing, and real-time data processing drive sustainable farming.</p>\n\n\n\n<p>Watch the advanced talk on\u00a0<a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s63033/\">Transforming Agriculture with AI and Computer Vision</a>, explore more videos on NVIDIA On-Demand, and gain valuable skills and insights from industry experts by joining the\u00a0<a href=\"https://developer.nvidia.com/developer-program\">NVIDIA Developer Program</a>.</p>\n\n\n\n<p><em>This content was partially crafted with the assistance of generative AI and LLMs. It underwent careful review and was edited by the NVIDIA Technical Blog team to ensure precision, accuracy, and quality.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>By 2030, John Deere aims for fully autonomous farming, addressing global challenges like labor shortages, sustainability, and food security. Their AI and robotics solutions make farming more efficient and profitable, reduce environmental impact, lower carbon footprints, and promote biodiversity. In this session, Chris Padwick, director of Machine Learning and Computer Vision at John Deere, outlines &hellip; <a href=\"https://developer.nvidia.com/blog/how-ai-and-robotics-are-driving-agricultural-productivity-and-sustainability/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 89459, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1491865", "discourse_permalink": "https://forums.developer.nvidia.com/t/how-ai-and-robotics-are-driving-agricultural-productivity-and-sustainability/307897", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 63, 1903], "tags": [2415, 3941, 453, 3986], "coauthors": [2315], "class_list": ["post-89454", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-edge-computing", "category-robotics", "category-features", "tag-agriculture", "tag-ai-impact", "tag-featured", "tag-nvidia-on-demand"], "acf": {"post_industry": ["General"], "post_products": ["General"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Video"], "post_collections": ["GTC March 2024"]}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Agriculture-AI.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-ngO", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89454"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89454"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89454/revisions"}], "predecessor-version": [{"id": 89462, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89454/revisions/89462"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89459"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89454"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89454"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89454"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89454"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88527, "date": "2024-09-24T13:30:00", "date_gmt": "2024-09-24T20:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88527"}, "modified": "2024-10-17T12:07:16", "modified_gmt": "2024-10-17T19:07:16", "slug": "developing-next-gen-wireless-networks-with-nvidia-aerial-omniverse-digital-twin", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/developing-next-gen-wireless-networks-with-nvidia-aerial-omniverse-digital-twin/", "title": {"rendered": "Developing Next-Generation Wireless Networks with NVIDIA Aerial Omniverse Digital Twin"}, "content": {"rendered": "\n<p>The journey to 6G has begun, offering opportunities to deliver a network infrastructure that is performant, efficient, resilient, and adaptable. 6G networks will be significantly more complex than their predecessors and will rely on a variety of new technologies, especially AI and machine learning (ML).&nbsp;</p>\n\n\n\n<p>To advance these new technologies and optimize network performance and efficiency, there is the need for a wireless network digital twin platform for research and development. The network digital twin relies on a digital replica of the mobile network in a realistic physical radio environment, as illustrated in Figure 1.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/reference-architecture-network-digital-twin.jpg\" alt=\"Diagram showing reference architecture for physical network, network digital twin, and network applications.\n\" class=\"wp-image-88533\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/reference-architecture-network-digital-twin.jpg 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/reference-architecture-network-digital-twin-300x169.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/reference-architecture-network-digital-twin-625x352.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/reference-architecture-network-digital-twin-179x101.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/reference-architecture-network-digital-twin-768x432.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/reference-architecture-network-digital-twin-645x363.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/reference-architecture-network-digital-twin-960x540.jpg 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/reference-architecture-network-digital-twin-500x281.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/reference-architecture-network-digital-twin-160x90.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/reference-architecture-network-digital-twin-362x204.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/reference-architecture-network-digital-twin-196x110.jpg 196w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Network digital twin</em></figcaption></figure>\n\n\n\n<p>The <a href=\"https://developer.nvidia.com/aerial-omniverse-digital-twin\">NVIDIA Aerial Omniverse Digital Twin (AODT)</a> provides a network digital twin platform to accelerate 6G research and development. AODT leverages key NVIDIA technologies\u2014including the near real-time ray tracing capabilities of NVIDIA RTX GPUs and <a href=\"https://www.nvidia.com/en-us/omniverse/\">NVIDIA Omniverse</a>\u2014to realize a physically accurate, highly performant, and modular digital twin platform for wireless network.</p>\n\n\n\n<h2 id=\"from_link-level_simulations_to_system-level_simulations&nbsp;\"  class=\"wp-block-heading\">From link-level simulations to system-level simulations&nbsp;<a href=\"#from_link-level_simulations_to_system-level_simulations&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Research and simulation of radio access network (RAN) can generally be subdivided into two major areas:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Link-level simulations are essential for analyzing the performance and characteristics of point-to-point communication. <a href=\"https://developer.nvidia.com/sionna\">NVIDIA Sionna</a> has emerged as a powerful tool in this domain, offering a GPU-accelerated open-source library specifically designed for link-level simulations. This innovative platform enables researchers and engineers to rapidly prototype complex communication algorithms, significantly accelerating the development process.</li>\n\n\n\n<li>System-level simulations study performance and characteristics when several point-to-point links are simultaneously transmitting and receiving, and how they affect each other. NVIDIA AODT brings system-level simulation capabilities to the RAN research and development community, thus enabling the analysis and development of concepts\u2014spanning across layers or involving multiple transmission points, for example.\u00a0\u00a0\u00a0</li>\n</ol>\n\n\n\n<p>NVIDIA AODT enables physically accurate simulations of complete 6G systems, from a single base station to a comprehensive network with a large number of base stations covering an entire city. It incorporates software-defined RAN and user-equipment simulators, along with realistic terrain and properties of the physical world. Using the AODT enables researchers to simulate and build base-station algorithms based on site-specific data and to train models in real time to improve transmission efficiency.\u00a0</p>\n\n\n\n<p>This post provides an overview of the features and benefits of the <a href=\"https://developer.nvidia.com/aerial-omniverse-digital-twin\">NVIDIA Aerial Omniverse Digital Twin (AODT)</a>, as well as a typical workflow for using it.</p>\n\n\n\n<h2 id=\"transforming_wireless_research_and_development&nbsp;\"  class=\"wp-block-heading\">Transforming wireless research and development&nbsp;<a href=\"#transforming_wireless_research_and_development&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA AODT offers researchers an open modular platform, the ability to test before deployment, an advanced physically accurate electromagnetic (EM) solver,&nbsp;and much more.&nbsp;</p>\n\n\n\n<h3 id=\"open_and_customizable_modular_platform_\"  class=\"wp-block-heading\">Open and customizable modular platform <a href=\"#open_and_customizable_modular_platform_\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The AODT is built on NVIDIA Omniverse, which enables the use of custom extensions, a convenient graphical user interface, beautiful visualizations, and remote collaboration for distributed teams. Developers can easily replace AODT modular blocks with ecosystem partners\u2019 solutions using open APIs. Those APIs are well defined and documented in the <a href=\"https://docs.nvidia.com/aerial/aerial-dt\">AODT user guide</a>.&nbsp;&nbsp;</p>\n\n\n\n<p>In addition, customers can replace any module with their own customized model, and replace AODT text-book reference design with innovative layer-1 (L1) or Layer-2 (L2) algorithms in a realistic system-level simulation environment.</p>\n\n\n\n<h3 id=\"3gpp-compliant_platform_for_6g_standardization\"  class=\"wp-block-heading\">3GPP-compliant platform for 6G standardization<a href=\"#3gpp-compliant_platform_for_6g_standardization\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>AODT provides a physically accurate radio environment, user equipment (UE) mobility, and standard-compliant waveforms plus signaling to assess the benefits of new AI use cases such as beamforming, channel state information (CSI) compression, and positioning.\u00a0</p>\n\n\n\n<h3 id=\"testing_before_deployment\"  class=\"wp-block-heading\">Testing before deployment<a href=\"#testing_before_deployment\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The AODT is tightly coupled with the <a href=\"https://developer.nvidia.com/aerial-cuda-accelerated-ran/\">NVIDIA Aerial CUDA-Accelerated RAN</a>, enabling researchers and developers to test the system-level performance and efficiency of their GPU-accelerated algorithms before deploying them in a live network. You can test algorithms such as multiple input multiple output (MIMO) detection, scheduling, and more.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/J5-rkgL2dFA?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Learn about the features and use cases of NVIDIA Aerial Omniverse Digital Twin&nbsp;<br></em></figcaption></figure>\n\n\n\n<h3 id=\"ai_and_ml_in_the_loop_with_digital_twins&nbsp;\"  class=\"wp-block-heading\">AI and ML in the loop with digital twins&nbsp;<a href=\"#ai_and_ml_in_the_loop_with_digital_twins&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>As explained in <a href=\"https://www.delloro.com/ai-what-does-it-mean-for-the-ran-market/\">AI \u2013 What Does It Mean for the RAN Market?</a>, AI and various ML algorithms will be important research areas for the <a href=\"https://developer.nvidia.com/blog/?p=79335&amp;preview=1&amp;_ppp=ae1daf9067\">evolution of 5G and the development of 6G</a>. When AI and ML are part of the future network specifications, NVIDIA AODT can be used to train new algorithms for specific environments. The AODT consists of a digital twin of a RAN, wireless physical world, and user equipment (UE). The AODT is fully connected with <a href=\"https://developer.nvidia.com/aerial-ai-radio-frameworks\">NVIDIA Aerial AI Radio Frameworks</a> &nbsp;to leverage various neural network models and <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a>, and make the AI/ML system-in-loop, as illustrated in Figure 2.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"811\" height=\"423\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-architecture.png\" alt=\"Diagram showing NVIDIA Aerial Omniverse Digital Twin user interface, infrastructure, RAN, physical world and UE digital twins, together with Aerial AI RAN in the loop.\" class=\"wp-image-89117\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-architecture.png 811w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-architecture-300x156.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-architecture-625x326.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-architecture-179x93.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-architecture-768x401.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-architecture-645x336.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-architecture-500x261.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-architecture-160x83.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-architecture-362x189.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-architecture-211x110.png 211w\" sizes=\"(max-width: 811px) 100vw, 811px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA AODT architecture</em></figcaption></figure></div>\n\n\n<p>The platform provides researchers with prebuilt tools to run complex simulations faster and explore AI RAN in new ways to improve the performance and efficiency of future wireless networks.</p>\n\n\n\n<h3 id=\"advanced_physically_accurate_electromagnetic_solver&nbsp;\"  class=\"wp-block-heading\">Advanced physically accurate electromagnetic solver&nbsp;<a href=\"#advanced_physically_accurate_electromagnetic_solver&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Radio propagation\u202fis the behavior of\u202fradio waves as they travel from one point to another. To accurately model radio propagation, the embedded electromagnetic (EM) solver is a state-of-the-art software component based on ray tracing that includes all the different characteristics of physical radio propagation. The EM solver is implemented in CUDA code, making it blazingly fast compared to contemporary solutions.&nbsp;</p>\n\n\n\n<p>The AODT EM solver has a comprehensive ray tracing model, taking into account reflection, diffraction and directional diffuse scattering effects. The antenna patterns can also be defined at each antenna element level. The GPU-accelerated software makes the EM solver more than 100x faster than traditional CPU-based channel simulators, based on early benchmarking tests.</p>\n\n\n\n<h3 id=\"partner_enhancements&nbsp;\"  class=\"wp-block-heading\">Partner enhancements&nbsp;<a href=\"#partner_enhancements&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Thanks to its modular design, each modular block in NVIDIA AODT shown in Figure 2 is replaceable by a partner\u2019s product. It\u2019s also an API-driven open platform. Partners can adopt the EM solver with their own traditional propagation model for a comprehensive suite of wireless channel simulators. It can also integrate with commercially available antenna models for partners to take advantage of system-level simulation with AI/ML agents. For example, Ansys has showcased this modularity with the integration of Perceive EM software and NVIDIA AODT using the EM Solver API. To learn more, see <a href=\"https://www.ansys.com/blog/pioneering-future-radar-systems-wireless-communications\">Pioneering the Future of Radar Systems and Wireless Communications Optimization With Synthetic Data on Demand</a>.</p>\n\n\n\n<p>The Aerial CUDA-Accelerated RAN can work with various 3GPP- and O-RAN-compliant O-DUs, enabling researchers to develop various innovative transceivers. 6G researchers and partners can also take advantage of open interfaces with CUDA-accelerated MAC layer (cuMAC) to generate synthetic data, creating various schedulers and radio resource control to improve network and user performance.</p>\n\n\n\n<p>The AODT platform can be run on-premises or on the cloud (with Azure, for example). For on-premises, the AODT can be installed and run on the users\u2019 own servers and network, giving users full control and ownership of their data and applications. The cloud gives users the convenience and flexibility of accessing the platform from anywhere at any time.</p>\n\n\n\n<h3 id=\"site-specific_city-scale_near_real-time_system-level_simulations&nbsp;\"  class=\"wp-block-heading\">Site-specific, city-scale, near real-time system-level simulations&nbsp;<a href=\"#site-specific_city-scale_near_real-time_system-level_simulations&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>NVIDIA AODT is scalable from the smallest to the largest of environments. It supports everything from the smallest deployments of a single base station with a few users running on a single GPU to large city-scale models with hundreds of base stations and thousands of users running on large GPU clusters. The GPU-based solution takes milliseconds to calculate city-scale radio frequency (RF) environments. Various benchmark tests have demonstrated that EM solver and RAN simulations are magnitudes faster than traditional CPU-based simulations.&nbsp; Further optimization will improve the near real-time simulation experience, moving toward real time.</p>\n\n\n\n<p>The AODT can support the geographic information system (GIS), from level of detail (LOD) 1 and LOD 2 (city maps) to LOD 3 and LOD 4 (architectural details such as roof overhangs, openings, and other fa\u00e7ade details; indoor features like rooms and furniture). This produces city-scale simulations with much finer granularities. Additional enhancements to the UI make the platform more accessible and user friendly.</p>\n\n\n\n<h3 id=\"functionally_full-stack_ran_layers&nbsp;\"  class=\"wp-block-heading\">Functionally full-stack RAN layers&nbsp;<a href=\"#functionally_full-stack_ran_layers&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://developer.nvidia.com/aerial-cuda-accelerated-ran/\">NVIDIA Aerial CUDA-Accelerated RAN</a>, a major component of AODT, consists of 3GPP-compliant software-defined physical (PHY) and medium access control (MAC) layers, both in uplink and downlink. This serves as a reference design for researchers and developers in their work to find new innovative solutions for the RAN. This has strong ecosystem value, as there won\u2019t be any need to invest in separate resources for this implementation work. And as new features are added to the NVIDIA Aerial CUDA-Accelerated RAN, they will also be available in NVIDIA AODT.&nbsp;</p>\n\n\n\n<p>For example, the embedded CUDA-accelerated physical layer (cuPHY) in AODT enables researchers to study various beamforming and neural receivers in a multiple cell and site-specific environment. In addition, having cuMAC embedded enables the development of AI/ML-based Layer 2 and above innovations. For example, this can help drive the development of custom radio resource management methods to improve spectral efficiency and ensure quality of service.&nbsp;&nbsp;\u00a0\u00a0\u00a0</p>\n\n\n\n<h2 id=\"workflow_for_a_simulation_using_nvidia_aodt&nbsp;\"  class=\"wp-block-heading\">Workflow for a simulation using NVIDIA AODT&nbsp;<a href=\"#workflow_for_a_simulation_using_nvidia_aodt&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This section presents a description of a typical workflow. Figure 3 shows how to run a simulation leveraging NVIDIA AODT. Examples and post-scripts are also provided in the <a href=\"https://docs.nvidia.com/aerial/aerial-dt\">AODT User Guide</a>.</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Load the 3D environment (GIS data). AODT supports both CityGML and CityJSON data formats.&nbsp;</li>\n\n\n\n<li>Define specific parameters for embedded models, including:&nbsp;\n<ul class=\"wp-block-list\">\n<li>Electromagnetic elements, which provide key parameters for radio units and terminals plus antenna models (types of polarization, output power, and so on).</li>\n\n\n\n<li>Mobility model to specify parameters for different mobility classes (pedestrians, cars, trains, and so on).</li>\n</ul>\n</li>\n\n\n\n<li>Run the simulation&nbsp;</li>\n\n\n\n<li>Obtain the results, such as:\n<ul class=\"wp-block-list\">\n<li>Channel impulse response (per antenna)&nbsp;</li>\n\n\n\n<li>Downlink and uplink throughput\u00a0</li>\n</ul>\n</li>\n\n\n\n<li>Visualize the results leveraging the graphical capabilities of NVIDIA Omniverse (optional).\u00a0</li>\n</ol>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"559\" height=\"451\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-simulation-workflow-1.png\" alt=\"The workflow of a typical AODT simulation; from top to bottom: load 3D environment scene creation, define specific configurations, run AODT simulations, obtain results, analyses and visualization.\n\" class=\"wp-image-88547\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-simulation-workflow-1.png 559w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-simulation-workflow-1-300x242.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-simulation-workflow-1-143x115.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-simulation-workflow-1-372x300.png 372w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-simulation-workflow-1-112x90.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-simulation-workflow-1-362x292.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-aodt-simulation-workflow-1-136x110.png 136w\" sizes=\"(max-width: 559px) 100vw, 559px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Workflow of a typical AODT simulation</em></figcaption></figure>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\">Summary<a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA AODT is a powerful and versatile platform that provides a large-scale, physically accurate model of radio wave propagation for system-level RAN simulations. The platform is tightly integrated with PyTorch and TensorRT, enabling AI/ML in the loop. GPU-accelerated physics-based modeling, a user-friendly interface, and the incorporation of functionally exact RAN layers makes AODT ready for 6G. It is an ideal platform for researchers, algorithm developers, network engineers, and other professionals who need to simulate and analyze wireless networks in a variety of environments.</p>\n\n\n\n<p>Ready to get started? You can access the NVIDIA Aerial Omniverse Digital Twin by joining the <a href=\"https://developer.nvidia.com/6g-program\">NVIDIA 6G Developer Program</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The journey to 6G has begun, offering opportunities to deliver a network infrastructure that is performant, efficient, resilient, and adaptable. 6G networks will be significantly more complex than their predecessors and will rely on a variety of new technologies, especially AI and machine learning (ML).&nbsp; To advance these new technologies and optimize network performance and &hellip; <a href=\"https://developer.nvidia.com/blog/developing-next-gen-wireless-networks-with-nvidia-aerial-omniverse-digital-twin/\">Continued</a></p>\n", "protected": false}, "author": 2276, "featured_media": 88532, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1490743", "discourse_permalink": "https://forums.developer.nvidia.com/t/developing-next-generation-wireless-networks-with-nvidia-aerial-omniverse-digital-twin/307801", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 2758, 503], "tags": [817, 453, 2375], "coauthors": [4007, 3758, 3759, 3514], "class_list": ["post-88527", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-edge-computing", "category-simulation-modeling-design", "tag-5g", "tag-featured", "tag-industrial-digitalization-digital-twin"], "acf": {"post_industry": ["Telecommunications"], "post_products": ["Aerial", "Omniverse", "Sionna", "TensorRT"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/aerial-landscape-network-overlay-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-n1R", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88527"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2276"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88527"}], "version-history": [{"count": 28, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88527/revisions"}], "predecessor-version": [{"id": 89241, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88527/revisions/89241"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88532"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88527"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88527"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88527"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88527"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89330, "date": "2024-09-24T11:27:35", "date_gmt": "2024-09-24T18:27:35", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89330"}, "modified": "2024-10-17T12:07:17", "modified_gmt": "2024-10-17T19:07:17", "slug": "accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/", "title": {"rendered": "Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo"}, "content": {"rendered": "\n<p><a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo</a> has consistently developed <a href=\"https://www.nvidia.com/en-us/glossary/speech-to-text/\">automatic speech recognition (ASR)</a> models that set the benchmark in the industry, particularly those topping the <a href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\">Hugging Face Open ASR Leaderboard</a>.&nbsp;</p>\n\n\n\n<p>These <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html\">NVIDIA NeMo ASR models</a> that transcribe speech into text offer a range of architectures designed to optimize both speed and accuracy:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>CTC model (</strong><a href=\"https://huggingface.co/nvidia/parakeet-ctc-1.1b\"><strong>nvidia/parakeet-ctc-1.1b</strong></a><strong>)</strong>: This model features a <a href=\"https://arxiv.org/abs/2305.05084\">FastConformer</a> encoder and a softmax prediction head. It\u2019s non-autoregressive, meaning future predictions do not depend on the previous ones, enabling fast and efficient inference.</li>\n\n\n\n<li><strong>RNN-T model (</strong><a href=\"https://huggingface.co/nvidia/parakeet-rnnt-1.1b\"><strong>nvidia/parakeet-rnnt-1.1b</strong></a><strong>)</strong>: This transducer model adds a prediction and joint network to the FastConformer encoder, making it autoregressive\u2014each prediction depends on the previous prediction history. Due to this property, there is a common misconception that RNN-T models are slow for GPU inference and better suited to CPUs.\u00a0</li>\n</ul>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>TDT model (</strong><a href=\"https://huggingface.co/nvidia/parakeet-tdt-1.1b\"><strong>nvidia/parakeet-tdt-1.1b</strong></a><strong>): </strong>Another transducer model, but trained with a refined transducer objective called token-and-duration transducer (TDT). While still autoregressive, it can perform multiple predictions at each step, making it faster at inference.</li>\n\n\n\n<li><strong>TDT-CTC model (</strong><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/parakeet-tdt_ctc-110m\"><strong>parakeet-tdt_ctc-110m</strong></a><strong>)</strong>: This is a hybrid variant of transducer and CTC decoders, bringing both decoders for faster convergence during training. It enables training only one model for two decoders.\u00a0</li>\n\n\n\n<li><strong>AED model (</strong><a href=\"https://huggingface.co/nvidia/canary-1b\"><strong>nvidia/canary-1b</strong></a><strong>):</strong> Attention-encoder-decoder (AED) model, also based on the FastConformer, is autoregressive and offers the highest accuracy (lowest <a href=\"https://en.wikipedia.org/wiki/Word_error_rate\">word error rate</a>, or WER) at the cost of additional computation.</li>\n</ul>\n\n\n\n<p>Previously, these models faced speed performance bottlenecks such as casting overheads, low compute intensity, and divergence performance issues.&nbsp;</p>\n\n\n\n<p>In this post, you\u2019ll discover how NVIDIA boosted the inference speed of NeMo ASR models by up to 10x (Figure 1) through key enhancements like autocasting tensors to <code>bfloat16</code>, the innovative label-looping algorithm, and the introduction of <a href=\"https://developer.nvidia.com/blog/employing-cuda-graphs-in-a-dynamic-environment/\">CUDA Graphs</a> available with <a href=\"https://github.com/NVIDIA/NeMo\">NeMo 2.0.0</a>.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"873\" height=\"428\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/graph-nvidia-nemo-asr-model-speedup.png\" alt=\"Bar graph illustrating NeMo ASR models up to 10x speed improvements with autocasting tensors to bfloat16, the innovative label-looping algorithm, and the introduction of CUDA graphs speed performance enhancements.\n\" class=\"wp-image-89366\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/graph-nvidia-nemo-asr-model-speedup.png 873w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/graph-nvidia-nemo-asr-model-speedup-300x147.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/graph-nvidia-nemo-asr-model-speedup-625x306.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/graph-nvidia-nemo-asr-model-speedup-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/graph-nvidia-nemo-asr-model-speedup-768x377.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/graph-nvidia-nemo-asr-model-speedup-645x316.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/graph-nvidia-nemo-asr-model-speedup-500x245.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/graph-nvidia-nemo-asr-model-speedup-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/graph-nvidia-nemo-asr-model-speedup-362x177.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/graph-nvidia-nemo-asr-model-speedup-224x110.png 224w\" sizes=\"(max-width: 873px) 100vw, 873px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA NeMo ASR models achieve up to 10x speed improvement in inverse real-time factor (RTFx) through recent optimizations</em></figcaption></figure>\n\n\n\n<h2 id=\"overcoming_speed_performance_bottlenecks\"  class=\"wp-block-heading\">Overcoming speed performance bottlenecks<a href=\"#overcoming_speed_performance_bottlenecks\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This section dives into how NVIDIA ASR models overcame various speed performance bottlenecks, from casting overheads to batch processing optimization, low compute intensity, and divergence performance issues.&nbsp;</p>\n\n\n\n<h3 id=\"casting_overheads_from_automatic_mixed_precision\"  class=\"wp-block-heading\">Casting overheads from automatic mixed precision<a href=\"#casting_overheads_from_automatic_mixed_precision\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>From the early days of NeMo, inference has been performed within<em> torch.amp.autocast</em> context manager. This automatically casts <code>float32</code> weights to<em> </em><code>float16</code> or <code>bfloat16</code> before matrix multiplications, enabling the use of half precision tensor core operations. Under the hood, automatic mixed precision (AMP) maintains a \u201ccast cache\u201d that stores these conversions, typically speeding up training. However, several issue arise:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Outdated autocast behavior</strong>: Autocast was developed when operations like <code>softmax</code> and <code>layer norm</code> were rare. In modern models like Conformers and Transformers, these operations are common and cause the <code>float16</code> or <code>bfloat16</code> inputs to be <a href=\"https://pytorch.org/docs/stable/amp.html#cuda-ops-that-can-autocast-to-float32\">cast back to float32</a>, leading to additional casts before every matrix multiplication.</li>\n\n\n\n<li><strong>Parameter handling:</strong> For the AMP cast cache to be effective, parameters need<strong> </strong><code>requires_grad=True</code>. Unfortunately, in the NeMo transcribe API, this flag is set to False (<a href=\"https://github.com/NVIDIA/NeMo/pull/9086/files/7d52560559ec9d4f9c933b995513cafc82d66ee8#r1587085036\"><code>requires_grad=False</code></a>), preventing the cache from working and leading to unnecessary casting overhead.</li>\n\n\n\n<li><strong>Frequent cache clearing</strong>: The cast cache is cleared every time the <code>torch.amp.autocast</code> context manager is exited. Users often wrap single inference calls within the context manager, preventing effective utilization of the cache.</li>\n</ul>\n\n\n\n<p>The overhead of these extra casts is significant. Figure 2 shows how casting before a matrix multiplication in the Parakeet CTC 1.1B model adds 200 microseconds of overhead, while the matrix multiplication itself only takes 200 microseconds\u2014meaning half of the run time is spent on casting. This is captured by <a href=\"https://developer.nvidia.com/nsight-systems\">NVIDIA Nsight Systems</a>, a profiling and analysis tool that visualizes workload metrics on a timeline for performance tuning.&nbsp;</p>\n\n\n\n<p>In the CUDA HW row of Figure 2:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>The first two light blue sections indicate the kernels handling the casting from <code>float32</code> to <code>bfloat16</code>.\u00a0</li>\n\n\n\n<li>The empty white regions indicate the casting overhead taking 200 microseconds of the total 400 microseconds runtime.</li>\n</ul>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1176\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime.png\" alt=\"Screenshot illustrating that half of the total runtime is spent on casting operations instead of matrix multiplication, as captured with NVIDIA Nsight Systems performance analysis tool.\n\" class=\"wp-image-89371\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime-300x176.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime-625x368.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime-179x105.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime-768x452.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime-1536x904.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime-645x379.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime-500x294.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime-153x90.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime-362x213.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime-187x110.png 187w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-screenshot-runtime-1024x602.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Half of the total runtime is spent on casting instead of matrix multiplication, as shown with NVIDIA Nsight Systems</em></figcaption></figure>\n\n\n\n<h3 id=\"resolving_amp_overheads_with_full_half-precision_inference\"  class=\"wp-block-heading\">Resolving AMP overheads with full half-precision inference<a href=\"#resolving_amp_overheads_with_full_half-precision_inference\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To address challenges associated with AMP, we implemented best practices by performing inference fully in half precision (either <code>float16</code> or <code>bfloat16</code>). This approach, described in <a href=\"https://github.com/NVIDIA/NeMo/pull/9198\">NVIDIA/NeMo pull requests</a> on GitHub, eliminates unnecessary casting overhead without compromising accuracy, as precision-sensitive operations like <code>softmax</code><em> </em>and <code>layer norm</code> still use <code>float32</code> under the hood, even when half precision inputs are specified. See the <a href=\"https://github.com/pytorch/pytorch/blob/01fc22056a3d7f3c9c0852826ec5dab17c0d0060/aten/src/ATen/AccumulateType.h#L12-L45\">AccumulateType</a> and <a href=\"https://github.com/pytorch/pytorch/blob/01fc22056a3d7f3c9c0852826ec5dab17c0d0060/aten/src/ATen/native/cuda/SoftMax.cu#L845C29-L845C37\">SoftMax</a> examples.\u00a0\u00a0</p>\n\n\n\n<p>You can enable this in <code>examples/asr/transcribe_speech.py</code> by setting <code>compute_dtype=float16</code> or <code>compute_dtype=bfloat16</code> while ensuring <code>amp=True</code> is not set (default is <code>amp=False</code>). Setting both <code>amp=True</code> and a value for <code>compute_dtype</code> will cause an <a href=\"https://github.com/NVIDIA/NeMo/blob/7bb42715f62faf2c3367b64b28ab03d326cd08ff/examples/asr/transcribe_speech.py#L275-L276\">error</a>. If you are writing your own Python code, simply call <code>model.to(torch.bfloat16)</code> or <code>model.to(torch.float16)</code> to achieve this optimization, as demonstrated at <a href=\"https://github.com/NVIDIA/NeMo/blob/7bb42715f62faf2c3367b64b28ab03d326cd08ff/examples/asr/transcribe_speech.py#L280-L281\">NeMo/examples/asr/transcribe_speech.py</a>.</p>\n\n\n\n<h3 id=\"optimizing_batch_processing_for_enhanced_performance\"  class=\"wp-block-heading\">Optimizing batch processing for enhanced performance<a href=\"#optimizing_batch_processing_for_enhanced_performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In NeMo, certain operations were originally executed sequentially, processing one element of a mini-batch at a time. This approach caused slowdowns, as each kernel operation runs quickly at batch size 1, but the overhead of launching CUDA kernels for each element leads to inefficiencies. By switching to fully batched processing, we take full advantage of the GPU streaming multiprocessor resources.&nbsp;</p>\n\n\n\n<p>Two specific operations, <a href=\"https://github.com/NVIDIA/NeMo/pull/9100\">CTC greedy decoding</a> and <a href=\"https://github.com/NVIDIA/NeMo/pull/8964\">feature normalization</a>, were impacted by this issue. By moving from sequential to fully batched processing, we achieved a 10% increase in throughput for each operation, resulting in an overall speedup of approximately 20%.</p>\n\n\n\n<p><a href=\"https://github.com/NVIDIA/NeMo/pull/9155\">SpecAugment</a> became 8-10x faster after resolving similar issues. (This runs only during training, so is not the focus here.)</p>\n\n\n\n<h3 id=\"low_compute_intensity_in_rnn-t_and_tdt_prediction_networks\"  class=\"wp-block-heading\">Low compute intensity in RNN-T and TDT prediction networks<a href=\"#low_compute_intensity_in_rnn-t_and_tdt_prediction_networks\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>RNN-T and TDT models have long been seen as unsuitable for server-side GPU inference due to their autoregressive prediction and joint networks. For instance, in the Parakeet RNN-T 1.1B model, greedy decoding consumed 67% of the total runtime, even though the prediction and joint networks made up less than 1% of the model\u2019s parameters.&nbsp;</p>\n\n\n\n<p>The reason? These kernels perform so little work that their performance is completely bounded by the kernel launch overhead, leaving the GPU idle most of the time. To illustrate, a CUDA kernel might take only 1 to 3 microseconds to execute, while launching one can take 5 and 10 microseconds. In practice, we found the GPU was idle for about 80% of the time, indicating that eliminating this idle time we could speed up inference 5x.</p>\n\n\n\n<p>Figure 3 shows a snapshot of a few \u201couter time steps\u201d of the RNN-T greedy decoding algorithm. The CUDA HW row contains multiple blank (non-blue) regions, indicating times when no CUDA code is executing. Since each outer time step in the algorithm corresponds to processing a single 80 milliseconds frame of input, running for 1.5 to 4.3 milliseconds is unacceptably slow.</p>\n\n\n\n<p>In the CUDA HW row of Figure 3, regions that are not blue indicate when no CUDA code is executing (GPU is idle).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"867\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding.png\" alt=\"Screenshot illustrating low GPU utilization during RNN-T greedy decoding as captured with NVIDIA Nsight Systems performance analysis tool.\n\" class=\"wp-image-89378\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding-300x130.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding-625x271.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding-768x333.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding-1536x666.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding-645x280.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding-500x217.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding-362x157.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding-254x110.png 254w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nsight-systems-low-gpu-utilization-rnn-t-greedy-decoding-1024x444.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Low GPU utilization during RNN-T greedy decoding, as shown by Nsight Systems</em></figcaption></figure>\n\n\n\n<h3 id=\"eliminating_low_compute_intensity_with_dynamic_control_flow_in_cuda_graphs_conditional_nodes\"  class=\"wp-block-heading\">Eliminating low compute intensity with dynamic control flow in CUDA Graphs conditional nodes<a href=\"#eliminating_low_compute_intensity_with_dynamic_control_flow_in_cuda_graphs_conditional_nodes\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Traditionally, CUDA Graphs are used to eliminate kernel launch overhead. However, they haven\u2019t supported dynamic control flow, such as <code>while</code> loops, making them unsuitable for straightforward use in greedy decoding. <a href=\"https://developer.nvidia.com/cuda-12-4-0-download-archive\">CUDA Toolkit 12.4</a> introduced <a href=\"https://developer.nvidia.com/blog/dynamic-control-flow-in-cuda-graphs-with-conditional-nodes/\">CUDA Graphs conditional nodes</a>, which enable dynamic control flow.\u00a0</p>\n\n\n\n<p>We used these nodes to implement greedy decoding for RNN-T and TDT models, effectively eliminating all kernel launch overhead in the following files:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/asr/parts/submodules/cuda_graph_rnnt_greedy_decoding.py\">cuda_graph_rnnt_greedy_decoding.py</a></li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/asr/parts/submodules/rnnt_loop_labels_computer.py\">nnt_loop_labels_computer.py</a></li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/asr/parts/submodules/tdt_loop_labels_computer.py\">tdt_loop_labels_computer.py</a>\u00a0</li>\n</ul>\n\n\n\n<p>The latter two files implement the label-looping variant of greedy decoding, discussed in the next section.</p>\n\n\n\n<p>For a detailed explanation of the problem and our solution, see our paper, <a href=\"https://arxiv.org/abs/2406.03791\">Speed of Light Exact Greedy Decoding for RNN-T Speech Recognition Models on GPU</a>. Additionally, we have <a href=\"https://github.com/pytorch/pytorch/pull/130386\">submitted a pull request to PyTorch</a> to support conditional nodes in pure PyTorch, without requiring any direct interaction with CUDA APIs, using its new <code>torch.cond</code> and <code>torch.while_loop</code> control flow APIs.</p>\n\n\n\n<h3 id=\"divergence_in_rnn-t_and_tdt_prediction_networks\"  class=\"wp-block-heading\">Divergence in RNN-T and TDT prediction networks<a href=\"#divergence_in_rnn-t_and_tdt_prediction_networks\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>One significant issue with performing batched RNN-T and TDT inference is the divergence in&nbsp; vanilla greedy search algorithms. This divergence can cause some inputs to progress while others stall, leading to increased latency when using larger batch sizes. As a result, many implementations opt for inference with a batch size of 1 to avoid this issue. However, using a batch size of 1 prevents full hardware utilization, which is inefficient and uneconomical.</p>\n\n\n\n<p>The conventional decoding algorithm (Figure 4), commonly used for transducer decoding, involves a nested-loop design:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Outer loop: Iterates over frames (the encoder output)</li>\n\n\n\n<li>Inner loop: Retrieves labels one by one until the special blank symbol is encountered\u00a0</li>\n</ul>\n\n\n\n<p>For each non-blank symbol, both the hidden state and the output of the autoregressive prediction network should be updated. During batched inference, the inner loop can produce a varying number of labels for different utterances in the batch. Consequently, the number of calls to the prediction network is determined by the maximum number of non-blank labels across all utterances for each frame, which is suboptimal.</p>\n\n\n\n<p>Figure 4 shows an example of a conventional frame-looping decoding algorithm with two utterances in a batch, four frames each, and CAT and DOG transcriptions. \u2205 denotes unnecessary computations in batched decoding.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"638\" height=\"404\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/conventional-frame-looping-decoding-algorithm-example-1.png\" alt=\"Conventional frame-looping decoding algorithm example with 2 utterances in a batch, 4 frames each, and \u201cCAT\u201d and \u201cDOG\u201d transcriptions. There is 6 unnecessary computations in batched decoding, denoted with \u2205.\n\" class=\"wp-image-89388\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/conventional-frame-looping-decoding-algorithm-example-1.png 638w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/conventional-frame-looping-decoding-algorithm-example-1-300x190.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/conventional-frame-looping-decoding-algorithm-example-1-625x396.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/conventional-frame-looping-decoding-algorithm-example-1-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/conventional-frame-looping-decoding-algorithm-example-1-474x300.png 474w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/conventional-frame-looping-decoding-algorithm-example-1-142x90.png 142w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/conventional-frame-looping-decoding-algorithm-example-1-362x229.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/conventional-frame-looping-decoding-algorithm-example-1-174x110.png 174w\" sizes=\"(max-width: 638px) 100vw, 638px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Conventional frame-looping decoding algorithm example</em></figcaption></figure>\n\n\n\n<h3 id=\"solving_divergence_with_efficient_new_decoding_algorithm\"  class=\"wp-block-heading\">Solving divergence with efficient new decoding algorithm<a href=\"#solving_divergence_with_efficient_new_decoding_algorithm\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To address conventional frame-looping decoding algorithm issues, we introduced a new<a href=\"https://arxiv.org/abs/2406.06220\"> label-looping algorithm</a> that also uses nested loops but with a key difference: the roles of the loops are swapped (Figure 5).</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Outer loop: Iterates over labels until all frames have been decoded.</li>\n\n\n\n<li>Inner loop: Iterates over frames, identifying the next frame with a non-blank label for each utterance in the batch. This is done by advancing indices pointing to the current encoder frames, which vary for each utterance in the batch.</li>\n</ul>\n\n\n\n<p>Figure 5 shows an example of the new label-looping decoding algorithm with two utterances in a batch, four frames each, and CAT and DOG transcriptions. \u2205 denotes unnecessary computations in batched decoding.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"663\" height=\"522\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/new-label-looping-decoding-algorithm-example-1.png\" alt=\"New Label-looping decoding algorithm example with 2 utterances in a batch, 4 frames each, and \u201cCAT\u201d and \u201cDOG\u201d transcriptions. There is 2 unnecessary computations in batched decoding, denoted with \u2205.\n\" class=\"wp-image-89386\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/new-label-looping-decoding-algorithm-example-1.png 663w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/new-label-looping-decoding-algorithm-example-1-300x236.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/new-label-looping-decoding-algorithm-example-1-625x492.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/new-label-looping-decoding-algorithm-example-1-146x115.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/new-label-looping-decoding-algorithm-example-1-645x508.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/new-label-looping-decoding-algorithm-example-1-381x300.png 381w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/new-label-looping-decoding-algorithm-example-1-114x90.png 114w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/new-label-looping-decoding-algorithm-example-1-362x285.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/new-label-looping-decoding-algorithm-example-1-140x110.png 140w\" sizes=\"(max-width: 663px) 100vw, 663px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. New label-looping decoding algorithm example\u00a0</em></figcaption></figure>\n\n\n\n<p>In this approach, the prediction network is evaluated at each step of the outer loop with the maximum possible batch size. The number of such evaluations is precisely the length of the longest transcription (in number of tokens) across all utterances, making it the minimum number of evaluations required. The inner loop performs operations only using the joint network. To enhance efficiency, encoder and prediction network projections are applied early in the process, minimizing the need for costly recalculations.</p>\n\n\n\n<p>This batched label-looping algorithm significantly increases efficiency for both RNN-T and TDT networks, enabling much faster decoding\u2014even when implemented with pure PyTorch code without additional GPU-specific optimization.</p>\n\n\n\n<h2 id=\"performance_enhancements_up_to_10x_faster_and_up_to_45x_more_cost-effective\"  class=\"wp-block-heading\">Performance enhancements up to 10x faster and up to 4.5x more cost-effective<a href=\"#performance_enhancements_up_to_10x_faster_and_up_to_45x_more_cost-effective\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The label-looping and CUDA Graphs have brought transducer models inverse real time factor i.e. RTFx (duration of audio generated / computation time; higher is better) closer than ever to that of CTC models. This impact is particularly pronounced in smaller models, where reduced kernel launch overheads\u2014especially in operations involving small data sizes like prediction network weights and input tensors\u2014result in even greater performance gains. Additionally, CTC models have seen substantial speed improvements thanks to the newly implemented vectorized feature normalization decoding implementations.&nbsp;</p>\n\n\n\n<p>All these up to 10x speed enhancements (Figure 1) are available in <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html\">NVIDIA NeMo ASR models</a> with <a href=\"https://github.com/NVIDIA/NeMo\">NeMo 2.0.0</a>, which offers a fast and cost-effective alternative to CPUs.&nbsp;</p>\n\n\n\n<p>To better illustrate benefits of ASR GPU-based inference, we estimated the cost of transcribing 1 million hours of speech using both CPUs and NVIDIA GPUs commonly available on cloud platforms like AWS, focusing on compute-optimized instances. For this comparison, we used the NVIDIA Parakeet RNN-T 1.1B model.&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>CPU-based estimation:</strong> For the CPU estimation, we run NeMo ASR with batch size of 1 on a single pinned CPU core. This method, a common industry practice, allows for linear scaling across multiple cores, while maintaining a constant RTFx. We selected AMD EPYC 9454 CPU with a measured RTFx of 4.5, which is available via Amazon EC2 C7a compute-optimized instances.\u00a0</li>\n\n\n\n<li><strong>GPU-based estimation:</strong> For GPU, we used the results from the Hugging Face Open ASR Leaderboard, which were run on NVIDIA A100 80GB. The equivalent AWS instance is <code>p4de.24xlarge</code><em>,</em> featuring 8x NVIDIA A100 80GB GPUs.\u00a0</li>\n\n\n\n<li><strong>Cost calculation</strong>: To calculate the total cost for both CPU and GPU, we:\n<ul class=\"wp-block-list\">\n<li>Divided 1 million hours of speech by the respective RTFx.</li>\n\n\n\n<li>Rounded up to the nearest hour.</li>\n\n\n\n<li>Multiplied the result by the hourly instance cost.\u00a0</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>As shown in Table 1,&nbsp; switching from CPUs to GPUs for RNN-T inference yields up to 4.5x cost savings.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table class=\"has-fixed-layout\"><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>CPU/GPU</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>AWS instance</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Hourly cost&nbsp;</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong># of vCPU/GPU</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Streams per instance*&nbsp;</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>RTFx\u00a0</strong><br><strong>(single unit)</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Total&nbsp;</strong><strong>RTFx&nbsp;</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Cost of 1M hr transcription</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>AMD Epyc 4th Gen</strong></td><td class=\"has-text-align-center\" data-align=\"center\">C7a.48xlarge</td><td class=\"has-text-align-center\" data-align=\"center\">$9.85</td><td class=\"has-text-align-center\" data-align=\"center\">192</td><td class=\"has-text-align-center\" data-align=\"center\">192</td><td class=\"has-text-align-center\" data-align=\"center\">4.5</td><td class=\"has-text-align-center\" data-align=\"center\">864</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>$11,410</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>NVIDIA A100 80GB</strong></td><td class=\"has-text-align-center\" data-align=\"center\">P4de.24xlarge</td><td class=\"has-text-align-center\" data-align=\"center\">$40.97</td><td class=\"has-text-align-center\" data-align=\"center\">8</td><td class=\"has-text-align-center\" data-align=\"center\">512</td><td class=\"has-text-align-center\" data-align=\"center\">2053</td><td class=\"has-text-align-center\" data-align=\"center\">16425</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>$2,499</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. For transcribing 1 million hours of speech, NVIDIA Parakeet RNNT 1.1B model demonstrated up to 4.5x savings when run on GPU versus CPU. Pricing is for on-demand AWS instances as of August 14, 2024</em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">* CPU or global batch size for GPU</p>\n\n\n\n<h2 id=\"accelerate_your_transcriptions_with_nvidia_asr&nbsp;\"  class=\"wp-block-heading\">Accelerate your transcriptions with NVIDIA ASR&nbsp;<a href=\"#accelerate_your_transcriptions_with_nvidia_asr&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo</a> boosts performance speed up to 10x across ASR models that top <a href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\">Hugging Face Open ASR Leaderboard</a>. This leap in speed is powered by innovations such as autocasting tensors to <code>bfloat16</code>, the label-looping algorithm, and <a href=\"https://developer.nvidia.com/blog/constructing-cuda-graphs-with-dynamic-parameters/\">CUDA Graphs</a> optimizations.\u00a0</p>\n\n\n\n<p>These accelerated performances also enable significant cost savings. For example, NeMo GPU-powered inference on the NVIDIA A100 offers up to 4.5x cost savings compared to CPU-based alternatives when transcribing one million hours of speech.&nbsp;</p>\n\n\n\n<p>Continued efforts to optimize models like NVIDIA Canary 1B and Whisper will further reduce the cost of running attention-encoder-decoder and speech LLM-based ASR models. NVIDIA is also advancing its CUDA Graphs conditional nodes and integrating them with compiler frameworks like TorchInductor, which will provide further GPU speedups and efficiency gains. For more details, check out our <a href=\"https://github.com/pytorch/pytorch/pull/130386\">pull request for support conditional nodes in PyTorch</a>.</p>\n\n\n\n<p>We\u2019ve also released a smaller Parakeet hybrid transducer-ctc model, <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/parakeet-tdt_ctc-110m\">Parakeet TDT CTC 10M</a>, that achieves an RTFx of ~4,300 with improved accuracy of average WER of 7.5 on HF ASR Leaderboard test sets, further extending the NeMo ASR capabilities.</p>\n\n\n\n<p>Explore <a href=\"https://build.nvidia.com/explore/speech\">NVIDIA NIM for speech and translation</a> for faster, cost-effective integration of multilingual transcriptions and translations into your production applications running in the cloud, in a data center, or on a workstation.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA NeMo has consistently developed automatic speech recognition (ASR) models that set the benchmark in the industry, particularly those topping the Hugging Face Open ASR Leaderboard.&nbsp; These NVIDIA NeMo ASR models that transcribe speech into text offer a range of architectures designed to optimize both speed and accuracy: Previously, these models faced speed performance bottlenecks &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/\">Continued</a></p>\n", "protected": false}, "author": 2315, "featured_media": 89364, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1490706", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/307795", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [453, 1133, 126], "coauthors": [4047, 3733, 4048, 3680, 2519], "class_list": ["post-89330", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "tag-featured", "tag-nemo", "tag-optimization"], "acf": {"post_industry": ["General"], "post_products": ["CUDA", "NeMo", "NIM"], "post_learning_levels": ["Intermediate Technical", "Advanced Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/asr-model-representation.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-neO", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Conversational AI", "link": "https://developer.nvidia.com/blog/category/conversational-ai/", "id": 1050}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89330"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2315"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89330"}], "version-history": [{"count": 24, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89330/revisions"}], "predecessor-version": [{"id": 89423, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89330/revisions/89423"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89364"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89330"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89330"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89330"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89330"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89401, "date": "2024-09-24T09:36:57", "date_gmt": "2024-09-24T16:36:57", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89401"}, "modified": "2024-11-05T18:27:00", "modified_gmt": "2024-11-06T02:27:00", "slug": "nvidia-gh200-grace-hopper-superchip-delivers-outstanding-performance-in-mlperf-inference-v4-1", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-gh200-grace-hopper-superchip-delivers-outstanding-performance-in-mlperf-inference-v4-1/", "title": {"rendered": "NVIDIA GH200 Grace Hopper Superchip Delivers Outstanding Performance in MLPerf Inference v4.1"}, "content": {"rendered": "\n<p>In the latest round of MLPerf Inference \u2013 a suite of standardized, peer-reviewed inference benchmarks \u2013 the NVIDIA platform delivered outstanding performance across the board. Among the many submissions made using the NVIDIA platform were results using the NVIDIA GH200 Grace Hopper Superchip. GH200 tightly couples an NVIDIA Grace CPU with an NVIDIA Hopper GPU using NVIDIA NVLink-C2C, a high-bandwidth, low-latency interconnect for superchips.&nbsp;</p>\n\n\n\n<p>In this post, we take a closer look at the great performance demonstrated by servers powered by the NVIDIA GH200 in the latest round of MLPerf Inference benchmarks.</p>\n\n\n\n<p>The NVIDIA GH200 Grace Hopper Superchip is a new type of converged CPU and GPU architecture combining the high-performance and power efficient NVIDIA Grace CPU with the powerful Hopper GPU using NVLink-C2C, delivering 900 GB/s of bandwidth to the GPU, 7x faster than todays\u2019 servers. With GH200, the CPU and GPU share a single per-process page table, enabling all CPU and GPU threads to access all system-allocated memory that can reside on physical CPU or GPU memory. When adopted, this architecture removes the need to copy memory back and forth between the CPU and GPU.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1100\" height=\"825\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/GH200-NVL2-Top-Down-blog.jpg\" alt=\"top-down look of an NVIDIA GH200 NVL2 Server\" class=\"wp-image-89407\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/GH200-NVL2-Top-Down-blog.jpg 1100w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/GH200-NVL2-Top-Down-blog-300x225.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/GH200-NVL2-Top-Down-blog-625x469.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/GH200-NVL2-Top-Down-blog-153x115.jpg 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/GH200-NVL2-Top-Down-blog-768x576.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/GH200-NVL2-Top-Down-blog-645x484.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/GH200-NVL2-Top-Down-blog-400x300.jpg 400w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/GH200-NVL2-Top-Down-blog-120x90.jpg 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/GH200-NVL2-Top-Down-blog-362x272.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/GH200-NVL2-Top-Down-blog-147x110.jpg 147w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/GH200-NVL2-Top-Down-blog-1024x768.jpg 1024w\" sizes=\"(max-width: 1100px) 100vw, 1100px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA GH200 NVL2 Server</em></figcaption></figure></div>\n\n\n<p>NVIDIA GH200 NVL2 builds on the successes of the NVIDIA GH200 by connecting two GH200 Superchips with NVLink in a single node, making it easier to deploy, manage, and scale to meet the demands of single-node LLM inference, <a href=\"https://developer.nvidia.com/blog/deploying-retrieval-augmented-generation-applications-on-nvidia-gh200-delivers-accelerated-performance/\" target=\"_blank\" rel=\"noreferrer noopener\">Retrieval Augmented Generation</a> (RAG), recommenders, <a href=\"https://developer.nvidia.com/blog/introduction-to-graph-neural-networks-with-nvidia-cugraph-dgl/\" target=\"_blank\" rel=\"noreferrer noopener\">graph neural networks</a> (GNNs), high-performance computing (HPC) and <a href=\"https://developer.nvidia.com/blog/nvidia-triton-inference-server-achieves-outstanding-performance-in-mlperf-inference-4-1-benchmarks/\" target=\"_blank\" rel=\"noreferrer noopener\">data processing</a>.&nbsp;</p>\n\n\n\n<p>The GH200 NVL2 fuses two Grace CPUs and two Hopper GPUs in an innovative architecture delivering 8 petaflops of AI performance into a single node. The Grace CPUs come with 144 Arm Neoverse cores and up to 960GB of LPDDR5X memory. The Hopper GPUs offer 288GB of the latest HBM3e memory and up to 10TB/s of memory bandwidth, 3.5x and 3x more than the H100 GPU respectively. This simplifies development with the coherent memory, delivers leading performance in a single server and allows customers to scale out to meet demand.&nbsp;</p>\n\n\n\n<h2 id=\"gh200_delivers_world-class_generative_ai_performance\"  class=\"wp-block-heading\">GH200 delivers world-class generative AI performance<a href=\"#gh200_delivers_world-class_generative_ai_performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>On a per-accelerator basis, the NVIDIA GH200 delivered outstanding inference performance across every generative AI benchmark in MLPerf Inference v4.1. Across the two most demanding LLM benchmarks \u2013 Mixtral 8x7B and Llama 2 70B \u2013 as well as DLRMv2, representing recommender systems, GH200 delivered up to 1.4x more performance per accelerator compared to the H100 Tensor Core GPU.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"598\" height=\"512\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Per-accelerator-performance-comparison-of-GH200-and-H100.png\" alt=\"A chart showing the performance uplifts of GH200 compared to H100 on several MLPerf Inference v4.1 benchmarks \u2013 1.2x on Mixtral 8x7B, 1.3x on DLRMv2 99%, and 1.4x on Llama 2 70B\" class=\"wp-image-89410\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Per-accelerator-performance-comparison-of-GH200-and-H100.png 598w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Per-accelerator-performance-comparison-of-GH200-and-H100-300x257.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Per-accelerator-performance-comparison-of-GH200-and-H100-134x115.png 134w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Per-accelerator-performance-comparison-of-GH200-and-H100-350x300.png 350w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Per-accelerator-performance-comparison-of-GH200-and-H100-105x90.png 105w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Per-accelerator-performance-comparison-of-GH200-and-H100-362x310.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Per-accelerator-performance-comparison-of-GH200-and-H100-128x110.png 128w\" sizes=\"(max-width: 598px) 100vw, 598px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Per-accelerator performance comparison of GH200 and H100.&nbsp;</em></figcaption></figure></div>\n\n\n<p>And, compared to the best two-socket, CPU-only submissions using currently-available x86 CPUs, a single GH200 Grace Hopper Superchip delivered up to 22x higher throughput on the GPT-J benchmark. There were no CPU-only submissions on the more challenging Llama 2 70B or Mixtral 8x7B benchmarks.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"585\" height=\"438\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-3.-Single-GH200-performance-comparison-to-two-socket-Xeon-8592.png\" alt=\"A chart showing the performance uplifts of GH200 compared to H100 on several MLPerf Inference v4.1 benchmarks \u2013 1.2x on Mixtral 8x7B, 1.3x on DLRMv2 99%, and 1.4x on Llama 2 70B. \" class=\"wp-image-89411\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-3.-Single-GH200-performance-comparison-to-two-socket-Xeon-8592.png 585w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-3.-Single-GH200-performance-comparison-to-two-socket-Xeon-8592-300x225.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-3.-Single-GH200-performance-comparison-to-two-socket-Xeon-8592-154x115.png 154w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-3.-Single-GH200-performance-comparison-to-two-socket-Xeon-8592-401x300.png 401w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-3.-Single-GH200-performance-comparison-to-two-socket-Xeon-8592-120x90.png 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-3.-Single-GH200-performance-comparison-to-two-socket-Xeon-8592-362x271.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-3.-Single-GH200-performance-comparison-to-two-socket-Xeon-8592-147x110.png 147w\" sizes=\"(max-width: 585px) 100vw, 585px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Single GH200 performance comparison to two-socket Xeon 8592+.&nbsp;</em></figcaption></figure></div>\n\n\n<p>Additionally, many organizations looking to deploy generative AI workloads in production are looking to run real-time, user-facing services. The MLPerf Inference server scenario aims to measure inference throughput under defined latency constraints, better representing popular real-time use cases than the offline scenario.&nbsp;</p>\n\n\n\n<p>Also note that while GH200 delivers server scenario performance within 5% of its offline performance, on the challenging Llama 2 70B benchmark, the best CPU-only submission using x86 sees performance degrade by 55% in the server scenario compared to the offline scenario.</p>\n\n\n\n<p>While many small- and mid-size generative AI models and use cases \u2013 including all MLPerf Inference v4.1 benchmarks \u2013&nbsp; can run optimally on a single GH200 Superchip, there are scenarios where multiple accelerators need to work in tandem to meet latency constraints. For example, while Llama 3.1 70B can fit comfortably in the memory of a single Hopper GPU, deployments with more stringent latency requirements benefit from the combined AI compute performance provided by the <a href=\"https://developer.nvidia.com/blog/nvidia-nvlink-and-nvidia-nvswitch-supercharge-large-language-model-inference/\" target=\"_blank\" rel=\"noreferrer noopener\">two Hopper GPUs</a> connected via NVLink in the GH200 NVL2.</p>\n\n\n\n<h2 id=\"the_ecosystem_is_embracing_nvidia_gh200\"  class=\"wp-block-heading\">The ecosystem is embracing NVIDIA GH200<a href=\"#the_ecosystem_is_embracing_nvidia_gh200\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In this round of MLPerf Inference, many NVIDIA partners made submissions using their NVIDIA GH200 and GH200 NVL2-based server designs, including server makers Hewlett-Packard Enterprise (HPE) that submitted one GH200 NVL2-based design, QCT and Supermicro, as well as cloud service provider Oracle Cloud Infrastructure (OCI).\u00a0</p>\n\n\n\n<h5 class=\"wp-block-heading\">HPE:</h5>\n\n\n\n<p>\u201cWe are already seeing outstanding performance with the HPE ProLiant Compute DL384 Gen12 with a submission based on the NVIDIA GH200 NVL2 design in no small part due to the 144GB HBM3e memory available per Superchip,\u201d said Kenneth Leach, Principal AI Performance Engineer and MLCommons representative at HPE. \u201cAs part of the NVIDIA AI Computing by HPE portfolio, we\u2019ve proven time and again that this platform can deliver high performance for generative AI inference, and appreciate NVIDIA\u2019s continued collaboration. As the first company to submit performance results with this version of the NVIDIA GH200 NVL2, HPE is incredibly proud of the work we continue to pioneer through our long-standing NVIDIA partnership.\u201d</p>\n\n\n\n<h5 class=\"wp-block-heading\">Oracle:</h5>\n\n\n\n<p>\u201cWe validated the combination of Grace CPU and H200 GPU, connected with NVLink interconnect, for AI inference. This demonstrated the outstanding performance of this NVIDIA architecture, and the even greater potential of the upcoming Grace Blackwell. We look forward to supporting customers with the OCI Supercluster based on NVIDIA Grace Blackwell Superchips,\u201d said Sanjay Basu, Senior Director, Cloud Engineering, Oracle Cloud Infrastructure.</p>\n\n\n\n<h5 class=\"wp-block-heading\">QCT:</h5>\n\n\n\n<p>\u201cWith the\u00a0GH200\u2019s groundbreaking architecture, we\u2019re equipped to enhance developer productivity and drive the next wave of AI applications. Our MLPerf results highlight\u00a0GH200\u2019s potential to bring AI into enterprise systems to meet the computational demands of modern data centers,\u201d said Mike Yang, Executive Vice President of Quanta Computer Inc. and President of QCT.</p>\n\n\n\n<h5 class=\"wp-block-heading\">Supermicro:</h5>\n\n\n\n<p>\u201cTo help data centers manage rising power demands from AI workloads, the GH200 offers exceptional efficiency. In addition, our MLPerf test submission showcases the significant performance increase that GH200 unlocks for customers.\u201d</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The GH200 demonstrated great performance and versatility in the latest MLPerf Inference benchmarks. GH200 NVL2, available today, provides up to 3.5x more GPU memory capacity and 3x more bandwidth than the H100 for compute- and memory-intensive workloads. With a balanced CPU-GPU architecture to address a wide variety of enterprise needs, a flexible 2U air-cooled MGX platform design that\u2019s easy to deploy and scale out, and a 1.2 TB pool of fast memory, it is an ideal solution for enterprises looking to run mainstream LLMs and the large and expanding universe of CUDA-accelerated applications.</p>\n\n\n\n<p class=\"has-small-font-size\"><em><strong>Oracle Future Product Disclaimer</strong></em><br><em>The preceding is intended to outline Oracle\u2019s general product direction. It is intended for information purposes only, and may not be incorporated into any contract. It is not a commitment to deliver any material, code, or functionality, and should not be relied upon in making purchasing decisions. The development, release, timing, and pricing of any features or functionality described for Oracle\u2019s products may change and remains at the sole discretion of Oracle Corporation.&nbsp;</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>In the latest round of MLPerf Inference \u2013 a suite of standardized, peer-reviewed inference benchmarks \u2013 the NVIDIA platform delivered outstanding performance across the board. Among the many submissions made using the NVIDIA platform were results using the NVIDIA GH200 Grace Hopper Superchip. GH200 tightly couples an NVIDIA Grace CPU with an NVIDIA Hopper GPU &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-gh200-grace-hopper-superchip-delivers-outstanding-performance-in-mlperf-inference-v4-1/\">Continued</a></p>\n", "protected": false}, "author": 2008, "featured_media": 89405, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1490659", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-gh200-grace-hopper-superchip-delivers-outstanding-performance-in-mlperf-inference-v4-1/307772", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1205, 1903], "tags": [791, 453, 3052, 4159, 2932, 973, 3613], "coauthors": [3708, 2732, 1327, 4049], "class_list": ["post-89401", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "category-networking-communications", "category-features", "tag-data-preprocessing", "tag-featured", "tag-graph-neural-networks", "tag-inference-performance", "tag-large-language-models", "tag-mlperf", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["General", "Consumer Internet", "HPC / Scientific Computing"], "post_products": ["Blackwell", "Grace CPU", "H100", "Hopper", "NVLink"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-GH200-Grace-Hopper-Superchip-Delivers-Outstanding-Performance-in-MLPerf-Inference.png", "jetpack_shortlink": "https://wp.me/pcCQAL-nfX", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89401"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2008"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89401"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89401/revisions"}], "predecessor-version": [{"id": 89450, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89401/revisions/89450"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89405"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89401"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89401"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89401"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89401"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89245, "date": "2024-09-24T08:00:00", "date_gmt": "2024-09-24T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89245"}, "modified": "2024-10-23T16:37:35", "modified_gmt": "2024-10-23T23:37:35", "slug": "spotlight-petrobras-accelerates-linear-solvers-for-reservoir-simulation-using-nvidia-grace-cpu", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/spotlight-petrobras-accelerates-linear-solvers-for-reservoir-simulation-using-nvidia-grace-cpu/", "title": {"rendered": "Spotlight: Petrobras Speeds Up Linear Solvers for Reservoir Simulation Using NVIDIA Grace CPU"}, "content": {"rendered": "\n<p>Reservoir simulation helps reservoir engineers optimize their resource exploration approach by simulating complex scenarios and comparing with real-world field data. This extends to simulation of depleted reservoirs that could be repurposed for carbon storage from operations. Reservoir simulation is crucial for energy companies aiming to enhance operational efficiency in exploration and production.&nbsp;</p>\n\n\n\n<p>This post demonstrates how the <a href=\"https://www.nvidia.com/en-us/data-center/grace-cpu/\">NVIDIA Grace CPU</a> excels in solving linear systems within this workflow, with Petrobras achieving up to 4.5x faster time-to-solution, 4.3x greater <a href=\"https://www.nvidia.com/en-us/glossary/energy-efficiency/\">energy efficiency</a>, and 1.5x higher scalability compared to alternative x86-based CPUs.&nbsp;</p>\n\n\n\n<p>Petrobras is a leading Brazilian energy company transitioning to new energy sources while maintaining its core oil and gas (O&amp;G) exploration and production business. According to the<a href=\"https://top500.org/lists/top500/\"> Top500</a> and<a href=\"https://top500.org/lists/green500/\"> Green500</a> lists, Petrobras has the largest HPC infrastructure in Latin America, powered by the NVIDIA full-stack accelerated computing platform. Their primary workloads are seismic processing and reservoir simulation.&nbsp;</p>\n\n\n\n<p>The company pioneered ultra-deepwater exploration with operations reaching depths up to 7 km. With a single well drill costing up to $100 million, high-performance computing (HPC) helps reduce resource exploration uncertainty and improve production success rates.&nbsp;</p>\n\n\n\n<h2 id=\"reservoir_simulation_and_the_solverbr_project&nbsp;\"  class=\"wp-block-heading\"><strong>Reservoir simulation and the SolverBR project</strong>&nbsp;<a href=\"#reservoir_simulation_and_the_solverbr_project&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Solving linear systems is the most time-consuming task in reservoir simulations (Figure 1). This process can account for up to 70% of the total computational time within the Petrobras simulation pipeline. Therefore, optimizing sparse linear solvers for performance and maintaining high accuracy is crucial for reliable reservoir studies.&nbsp;</p>\n\n\n\n<p>Petrobras collaborated with UFRJ and other innovative research institutes in Brazil to develop SolverBR, a CPU-based linear equations solver that uses novel compute parallelization techniques with an efficient multicore implementation. SolverBR is integrated into proprietary geomechanical and third-party flow simulators, including Computer Modelling Group (CMG) IMEX and GEM, widely used for compositional simulation of pre-salt reserves.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"853\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow.png\" alt=\"Workflow diagram from Petrobras with icons representing reservoir model assets, reservoir simulators, sparse matrices, linear solver configurations, and performance evaluation.\n\" class=\"wp-image-89252\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow-300x128.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow-625x267.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow-768x328.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow-1536x655.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow-645x275.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow-500x213.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow-362x154.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow-258x110.png 258w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/end-to-end-reservoir-simulation-flow-1024x437.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. End-to-end reservoir simulation flow</em></figcaption></figure>\n\n\n\n<h2 id=\"porting_solverbr_from_x86_to_arm&nbsp;\"  class=\"wp-block-heading\"><strong>Porting SolverBR from x86 to Arm</strong>&nbsp;<a href=\"#porting_solverbr_from_x86_to_arm&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Arm-based processors such as the <a href=\"https://www.nvidia.com/en-us/data-center/grace-cpu/\">NVIDIA Grace CPU</a> are gaining momentum in HPC applications, including in the energy industry. Petrobras, NVIDIA, and Brazil\u2019s CESAR Innovation Center are partnering to port and benchmark SolverBR to the NVIDIA Grace CPU in an initiative to measure the main benefits of Arm-based CPUs.&nbsp;&nbsp;</p>\n\n\n\n<p>The NVIDIA Grace CPU has 72-core Arm Neoverse V2, connected by a high-bandwidth NVIDIA scalable coherency fabric and paired with high-bandwidth and low-power double data rate 5x (LPDDR5X) memory.&nbsp;</p>\n\n\n\n<p>Initial results demonstrated that NVIDIA Grace delivers best-in-class performance ratios across time-to-solution (TTS) and estimated energy-to-solution (ETS), compared to x86-based flagship processors available on-premises and in the cloud.&nbsp;</p>\n\n\n\n<p>The project focuses on maintaining a multiplatform build system, enabling a single codebase and compilation scripts to work seamlessly across various platforms. This ensures consistent testing and reliable performance comparisons when porting the x86 codebase to Arm with minimal required effort. The Arm ecosystem robust open-source compilers and debuggers have significantly facilitated this transition.&nbsp;</p>\n\n\n\n<p>GCC 12.3 was chosen as the test compiler due to its excellent performance. GCC 12.3 or above is recommended due to its tuning support for the Arm Neoverse V2 core. The porting to Arm process involved a few simple steps:&nbsp;&nbsp;</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Removal of all x86 architecture-specific flags, such as <code>-maxv</code>, <code>-march</code>, and <code>-mtune</code>, which have different meanings in Arm.&nbsp;</li>\n\n\n\n<li>Use the <code>-O3</code> optimization level to trigger optimization steps such as function inlining, vectorization, loop unrolling, interchange, and fusion. For even more performance, consider <code>-Ofast</code>.&nbsp;</li>\n\n\n\n<li>Append the <code>-mcpu=native</code> to CFLAGS to ensure the compiler auto-detects the build system&#8217;s CPU.</li>\n\n\n\n<li>Finally, use <code>-flto</code> as an optional flag for link-time optimization. Depending on the application, <code>-fsigned-char</code> or <code>-funsigned-char</code> may also be required.&nbsp;</li>\n</ol>\n\n\n\n<p>Minimal effort was taken to address compilation errors by replacing Intel Intrinsics functions with Arm-specific functions using the header-only library sse2neon. Runtime errors were fixed, including memory synchronization issues caused by specific compiler optimizations, which resulted in instruction reordering and subsequent floating-point precision divergencies.&nbsp;</p>\n\n\n\n<p>For this initial experiment, Petrobras used a fixed set of compilation flags for each architecture (x86_64 and aarch64) without implementing processor-specific tuning. The goal is to comprehend the performance behavior out-of-the-box. Table 1 illustrates the compilation flags employed.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table class=\"has-fixed-layout\"><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Architecture&nbsp;</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Compiler flags&nbsp;</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">x86_64<strong>&nbsp;</strong></td><td class=\"has-text-align-center\" data-align=\"center\">-std=c++17 -O3 -lrt -fPIC -m64 -march=native -mtune=native -fopenmp-simd -fopenmp&nbsp;</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">aarch64<strong>&nbsp;</strong></td><td class=\"has-text-align-center\" data-align=\"center\">-std=c++17 -O3 -lrt \u2013fPIC -mcpu=native -fopenmp-simd -fopen&nbsp;</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Compilation flags for x86 and Arm-based architectures</em></figcaption></figure>\n\n\n\n<p> &nbsp;<strong>Measuring performance and energy efficiency</strong>&nbsp;</p>\n\n\n\n<p>Singularity containers were employed to replicate the SolverBR computing stack across various platforms to ensure reproducibility. A single definition file was utilized to generate multiple runtime containers, resulting in unique .sif files, one for each CPU architecture. Table 2 specifies all the tested CPUs, both x86-based and Arm-based, on-premises and in the cloud. NVIDIA conducted the experiments on the Grace architecture, while Petrobras and CESAR executed the benchmarks on the remaining architectures.\u00a0</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table class=\"has-fixed-layout\"><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Environment</strong>&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Processor</strong>&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Architecture</strong>&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Physical cores</strong>&nbsp;</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">On-premises&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">Intel Xeon Gold 6248*&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">x86_64&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">20&nbsp;</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">On-premises&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">NVIDIA Grace CPU**&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">Armv9&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">72&nbsp;</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">AWS EC2 R7g&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">AWS Graviton3&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">Armv8&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">64&nbsp;</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">AWS EC2 R7i&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">Intel Xeon Platinum 8488C (Sapphire Rapids)&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">x86_64&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">48&nbsp;</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">AWS EC2 R7a&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">AMD EPYC 9R14&nbsp;<br>(Genoa)&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">x86_64&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">96&nbsp;</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Specifications across x86-based and ARM-based processors on-premises and in the cloud &nbsp;</em></figcaption></figure>\n\n\n\n<p class=\"has-text-align-left has-small-font-size\"><em><em>*Intel Xeon Gold 6248 is the main on-premises CPU cluster at Petrobras Research Center (CENPES)</em>&nbsp;<br><em>**NVIDIA Grace CPU data computed on a single NVIDIA Grace SoC using the GH200 Superchip platform</em></em></p>\n\n\n\n<p>Testing the linear portion of the reservoir simulation pipeline shown in Figure 1 involved extracting the CMG-generated sparse matrices from pre-salt oil and gas field datasets (B\u00fazios, Proxy 100, Proxy 200, Sapinho\u00e1) and the<a href=\"https://www.spe.org/web/csp/datasets/set02.htm\"> SPE10 benchmark model</a>. The example establishes how CMG simulators can be modularized and extended with third-party components or software. CMG engineers are currently exploring additional opportunities to port components and capabilities of their simulators, aiming to optimize performance across various platforms and hardware architectures.</p>\n\n\n\n<p>Figure 2 displays the linear methods configurations: Adaptive Implicit Method (AIM), Krylov Subspace Projection Generalized Minimal Residual (KSPGMRES), Constrained Pressure Residual (CPR), Domain Decomposition (DD), Incomplete LU Factorization (ILU). Each system of equations was solved 50 times for all models across 3 years, resulting in thousands of executions.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1600\" height=\"579\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations.png\" alt=\"A side-by-side representation of linear system solver configurations for SPE10 matrices and B\u00fazios, Proxy100, Proxy200, and Sapinho\u00e1 matrices.\n\" class=\"wp-image-89262\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations.png 1600w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations-300x109.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations-625x226.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations-179x65.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations-768x278.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations-1536x556.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations-645x233.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations-500x181.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations-160x58.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations-362x131.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations-304x110.png 304w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/linear-system-solvers-configurations-1024x371.png 1024w\" sizes=\"(max-width: 1600px) 100vw, 1600px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Linear system solvers configurations</em></figcaption></figure>\n\n\n\n<p>Figure 3 presents the speedup results for each model, utilizing the maximum available cores in single-socket processors. Petrobras currently uses the Intel Xeon Gold 6248 platform in its on-premises production CPU cluster, which serves as the reference point for results normalization.&nbsp;</p>\n\n\n\n<p>The NVIDIA Grace architecture demonstrates superior performance, achieving the highest performance ratios across all models, including:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Up to 4.5x speedup over Intel Xeon Gold 6248 (Petrobras baseline CPU)&nbsp;</li>\n\n\n\n<li>Up to 2.9x speedup over Intel Xeon Platinum 8488C&nbsp;</li>\n\n\n\n<li>Up to 1.9x speedup over AMD EPYC 9R14&nbsp;</li>\n</ul>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1547\" height=\"567\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/charts-single-socket-speedups-nvidia-grace.png\" alt=\"Bar chart showing single-socket speedups with max core count, showing NVIDIA Grace-72 in green, AMD EPYC 9R14-96 in red, AWS Graviton3-64 in orange, Intel Xeon Platinum 8488C-48 in blue, and Intel Xeon Gold 6248-20 in gray.\n\" class=\"wp-image-89264\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/charts-single-socket-speedups-nvidia-grace.png 1547w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/charts-single-socket-speedups-nvidia-grace-300x110.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/charts-single-socket-speedups-nvidia-grace-625x229.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/charts-single-socket-speedups-nvidia-grace-179x66.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/charts-single-socket-speedups-nvidia-grace-768x281.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/charts-single-socket-speedups-nvidia-grace-1536x563.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/charts-single-socket-speedups-nvidia-grace-645x236.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/charts-single-socket-speedups-nvidia-grace-500x183.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/charts-single-socket-speedups-nvidia-grace-160x59.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/charts-single-socket-speedups-nvidia-grace-362x133.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/charts-single-socket-speedups-nvidia-grace-1024x375.png 1024w\" sizes=\"(max-width: 1547px) 100vw, 1547px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Single-socket speedups across AMD, AWS, Intel, and NVIDIA processors, with NVIDIA Grace achieving the highest performance ratio across all models</em></figcaption></figure>\n\n\n\n<p>Figure 4 shows leading scalability for the NVIDIA Grace CPU when varying the total number of cores up to the maximum for each single-socket processor, up to 53% over the least scalable option. The NVIDIA Grace CPU demonstrates exceptional performance for this specific workload due to its <a href=\"https://developer.nvidia.com/blog/inside-nvidia-grace-cpu-nvidia-amps-up-superchip-engineering-for-hpc-and-ai/\">unique characteristics</a>, which include high effective memory bandwidth, an advanced CPU Scalable Coherent Fabric (NVIDIA SCF), and the adoption of server-class Arm Neoverse V2 cores.\u00a0</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"469\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu-625x469.png\" alt=\"Line chart showing single-socket solver scalability with NVIDIA Grace in green, AMD EPYC 9R14 in red, AWS Graviton3 in orange, Intel Xeon Platinum 8488C in blue, and Intel Xeon Gold 6248 in gray.\n\" class=\"wp-image-89265\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu-625x469.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu-300x225.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu-153x115.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu-768x577.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu-1536x1153.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu-645x484.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu-400x300.png 400w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu-120x90.png 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu-362x272.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu-146x110.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu-1024x769.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/chart-single-socket-solver-scalability-buzios-nvidia-grace-cpu.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Scalability results varying the number of cores across AMD, AWS, Intel, and NVIDIA processors for the B\u00fazios model, with NVIDIA Grace exhibiting the highest scalability</em></figcaption></figure>\n\n\n\n<p>To estimate energy efficiency for the NVIDIA Grace CPU compared to on-premises CPUs, the maximum thermal design power (TDP) of each CPU was evaluated at full load with an estimated memory consumption based on processor capacity and technology generation. For the NVIDIA Grace CPU SoC, CPU memory consumption was approximately 250 W, and the speedups were reported using the AMD EPYC 9R14 as the baseline.&nbsp;</p>\n\n\n\n<p>NVIDIA Grace demonstrated the highest estimated energy efficiency across all linear solver tests at maximum load, with up to 4.3x higher energy efficiency (Figure 5).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1355\" height=\"778\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/estimated-energy-efficiency-comparison-max-load.png\" alt=\"Bar chart showing estimated energy efficiency at max load with NVIDIA Grace-72 in green, AMD EPYC 9R14-96 in red, Intel Xeon Platinum 8488C-48 in blue, and Intel Xeon Gold 6248-20 in gray.\n\" class=\"wp-image-89267\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/estimated-energy-efficiency-comparison-max-load.png 1355w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/estimated-energy-efficiency-comparison-max-load-300x172.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/estimated-energy-efficiency-comparison-max-load-625x359.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/estimated-energy-efficiency-comparison-max-load-179x103.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/estimated-energy-efficiency-comparison-max-load-768x441.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/estimated-energy-efficiency-comparison-max-load-645x370.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/estimated-energy-efficiency-comparison-max-load-500x287.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/estimated-energy-efficiency-comparison-max-load-157x90.png 157w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/estimated-energy-efficiency-comparison-max-load-362x208.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/estimated-energy-efficiency-comparison-max-load-192x110.png 192w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/estimated-energy-efficiency-comparison-max-load-1024x588.png 1024w\" sizes=\"(max-width: 1355px) 100vw, 1355px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Estimated energy efficiency comparison at max load for AMD, Intel, and NVIDIA processors</em></figcaption></figure>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The NVIDIA Grace CPU outperformed all tested x86-based and Arm-based CPUs in TTS, scalability, and energy efficiency. This success is primarily due to the NVIDIA Grace architecture, which focuses on energy-to-compute and high application performance. Key features such as LPDDR5X memory, unified cache coherence design, SCF, and an optimized software stack based on GCC contributed to these results.&nbsp;</p>\n\n\n\n<p>Next, Petrobras plans to port and benchmark their end-to-end geomechanical and reservoir simulators to Arm and explore the full potential of multiple NVIDIA Grace Superchips to further improve their time to solution.&nbsp;</p>\n\n\n\n<p>To learn more about <a href=\"https://www.nvidia.com/en-us/data-center/grace-cpu/\">NVIDIA Grace CPU</a>, watch the on-demand NVIDIA GTC session, <a href=\"https://resources.nvidia.com/en-us-upstream-energies/gtc24-s62529?ncid=no-ncid\">Accelerating Linear Solvers on NVIDIA Grace</a>.</p>\n\n\n\n<h3 id=\"acknowledgments\"  class=\"wp-block-heading\">Acknowledgments<a href=\"#acknowledgments\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><em>This work was executed by the following engineers and analysts: Felipe Portella (Petrobras), Jose Roberto Pereira Rodrigues (Petrobras), Leonardo Gasparini (Petrobras), Vitor Aquino (CESAR), Luigi Marques da Luz (CESAR)</em>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Reservoir simulation helps reservoir engineers optimize their resource exploration approach by simulating complex scenarios and comparing with real-world field data. This extends to simulation of depleted reservoirs that could be repurposed for carbon storage from operations. Reservoir simulation is crucial for energy companies aiming to enhance operational efficiency in exploration and production.&nbsp; This post demonstrates &hellip; <a href=\"https://developer.nvidia.com/blog/spotlight-petrobras-accelerates-linear-solvers-for-reservoir-simulation-using-nvidia-grace-cpu/\">Continued</a></p>\n", "protected": false}, "author": 2308, "featured_media": 89249, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1490627", "discourse_permalink": "https://forums.developer.nvidia.com/t/spotlight-petrobras-accelerates-linear-solvers-for-reservoir-simulation-using-nvidia-grace-cpu/307763", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 503], "tags": [3941, 453, 3099], "coauthors": [4041, 4042, 1327], "class_list": ["post-89245", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-simulation-modeling-design", "tag-ai-impact", "tag-featured", "tag-grace-hopper-superchip"], "acf": {"post_industry": ["Energy"], "post_products": ["Grace CPU"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/image5.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ndr", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89245"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2308"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89245"}], "version-history": [{"count": 20, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89245/revisions"}], "predecessor-version": [{"id": 89520, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89245/revisions/89520"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89249"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89245"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89245"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89245"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89245"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88932, "date": "2024-09-23T13:01:55", "date_gmt": "2024-09-23T20:01:55", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88932"}, "modified": "2024-11-06T21:08:39", "modified_gmt": "2024-11-07T05:08:39", "slug": "using-generative-ai-to-enable-robots-to-reason-and-act-with-remembr", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/using-generative-ai-to-enable-robots-to-reason-and-act-with-remembr/", "title": {"rendered": "Using Generative AI to Enable Robots to Reason and Act with ReMEmbR"}, "content": {"rendered": "\n<p><a href=\"https://developer.nvidia.com/blog/visual-language-models-on-nvidia-hardware-with-vila/\">Vision-language models</a> (VLMs) combine the powerful language understanding of foundational LLMs with the vision capabilities of <a href=\"https://developer.nvidia.com/blog/improve-accuracy-and-robustness-of-vision-ai-apps-with-vision-transformers-and-nvidia-tao/\">vision transformers</a> (ViTs) by projecting text and images into the same embedding space. They can take unstructured multimodal data, reason over it, and return the output in a structured format. Building on a broad base of pretraining, they can be easily adapted for different vision-related tasks by providing new prompts or parameter-efficient fine-tuning.</p>\n\n\n\n<p>They can also be integrated with live data sources and tools, to request more information if they don\u2019t know the answer or take action when they do. LLMs and VLMs can act as agents, reasoning over data to help robots perform meaningful tasks that might be hard to define.</p>\n\n\n\n<p>In a previous post, <a href=\"https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/\">Bringing Generative AI to Life with NVIDIA Jetson</a>, we demonstrated that you can run LLMs and VLMs on NVIDIA Jetson Orin devices, enabling a breadth of new capabilities like zero-shot object detection, video captioning, and text generation on edge devices.&nbsp;</p>\n\n\n\n<p>But how can you apply these advances to perception and autonomy in robotics? What are the challenges you face when deploying these models into the field?</p>\n\n\n\n<p>In this post, we discuss ReMEmbR, a project that combines LLMs, VLMs, and <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation</a> (RAG) to enable robots to reason and take actions over what they see during a long-horizon deployment, on the order of hours to days.&nbsp;</p>\n\n\n\n<p>ReMEmbR\u2019s memory-building phase uses VLMs and <a href=\"https://www.nvidia.com/en-us/glossary/vector-database/\">vector databases</a> to efficiently build a long-horizon semantic memory. Then ReMEmbR\u2019s querying phase uses an <a href=\"https://developer.nvidia.com/blog/introduction-to-llm-agents/\">LLM agent</a> to reason over that memory. It is fully open source and runs on-device.</p>\n\n\n\n<p>ReMEmbR addresses many of the challenges faced when using LLMs and VLMs in a robotics application:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>How to handle large contexts.</li>\n\n\n\n<li>How to reason over a spatial memory.</li>\n\n\n\n<li>How to build a prompt-based agent to query more data until a user\u2019s question is answered.&nbsp;</li>\n</ul>\n\n\n\n<p>To take things a step further, we also built an example of using ReMEmbR on a real robot. We did this using Nova Carter and <a href=\"https://developer.nvidia.com/isaac/ros\">NVIDIA Isaac ROS</a> and we share the code and steps that we took. For more information, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://nvidia-ai-iot.github.io/remembr/\">ReMEmbR</a> website</li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA-AI-IOT/remembr\">/NVIDIA-AI-IOT/remembr</a> GitHub repo</li>\n\n\n\n<li><a href=\"https://arxiv.org/abs/2409.13682\">ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation</a> paper</li>\n</ul>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/JlYVBAQC0tQ?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Enhancing Robot Navigation with LLM Agent ReMEmbR</em></figcaption></figure>\n\n\n\n<h2 id=\"remembr_for_long-horizon_spatial_and_temporal_memory&nbsp;_reasoning_and_action\"  class=\"wp-block-heading\">ReMEmbR for long-horizon spatial and temporal memory,&nbsp; reasoning, and action<a href=\"#remembr_for_long-horizon_spatial_and_temporal_memory&nbsp;_reasoning_and_action\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Robots are increasingly expected to perceive and interact with their environments over extended periods. Robots are deployed for hours, if not days, at a time and they incidentally perceive different objects, events, and locations.&nbsp;</p>\n\n\n\n<p>For robots to understand and respond to questions that require complex multi-step reasoning in scenarios where the robot has been deployed for long periods, we built ReMEmbR, a retrieval-augmented memory for embodied robots.&nbsp;</p>\n\n\n\n<p>ReMEmbR builds scalable long-horizon memory and reasoning systems for robots, which improve their capacity for perceptual question-answering and semantic action-taking. ReMEmbR consists of two phases: memory-building and querying.&nbsp;</p>\n\n\n\n<p>In the memory-building phase, we took advantage of VLMs for constructing a structured memory by using vector databases. During the querying phase, we built an LLM agent that can call different retrieval functions in a loop, ultimately answering the question that the user asked.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"648\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system-1024x648.png\" alt=\"The diagram shows an image and a prompt being fed into a video captioner. The caption is then embedded and fed into a vector database, along with the position and time information. A querying phase consists of a user querying an LLM with the question, \u201cWhere is the nearest elevator?\u201d\u00a0 The LLM reasons over the vector database by using text, position, and time queries of the vector database. When the LLM agent is ready to answer the question, it calls the answering function, generates a response, and answers the user question, including a navigable xyz position.\" class=\"wp-image-88941\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system-1024x648.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system-300x190.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system-625x396.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system-768x486.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system-1536x972.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system-645x408.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system-474x300.png 474w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system-142x90.png 142w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system-362x229.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system-174x110.png 174w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-full-system.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Full ReMEmbR system</em></figcaption></figure></div>\n\n\n<h3 id=\"building_a_smarter_memory\"  class=\"wp-block-heading\">Building a smarter memory<a href=\"#building_a_smarter_memory\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>ReMEmbR&#8217;s memory-building phase is all about making memory work for robots. When your robot has been deployed for hours or days, you need an efficient way of storing this information. Videos are easy to store, but hard to query and understand.&nbsp;</p>\n\n\n\n<p>During memory building, we take short segments of video, caption them with the <a href=\"https://github.com/NVlabs/VILA\">NVIDIA VILA</a> captioning VLM, and then embed them into a MilvusDB vector database. We also store timestamps and coordinate information from the robot in the vector database.&nbsp;</p>\n\n\n\n<p>This setup enabled us to efficiently store and query all kinds of information from the robot\u2019s memory. By capturing video segments with VILA and embedding them into a MilvusDB vector database, the system can remember anything that VILA can capture, from dynamic events such as people walking around and specific small objects, all the way to more general categories.&nbsp;</p>\n\n\n\n<p>Using a vector database makes it easy to add new kinds of information for ReMEmbR to take into consideration.</p>\n\n\n\n<h3 id=\"remembr_agent\"  class=\"wp-block-heading\">ReMEmbR agent<a href=\"#remembr_agent\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Given such a long memory stored in the database, a standard LLM would struggle to reason quickly over the long context.&nbsp;</p>\n\n\n\n<p>The LLM backend for the ReMEmbR agent can be <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM microservices</a>, local on-device LLMs, or other LLM APIs. When a user poses a question, the LLM generates queries to the database, retrieving relevant information iteratively. The LLM can query for text information, time information, or position information depending on what the user is asking. This process repeats until the question is answered.&nbsp;</p>\n\n\n\n<p>Our use of these different tools for the LLM agent enables the robot to go beyond answering questions about how to go to specific places and enables reasoning spatially and temporally. Figure 2 shows how this reasoning phase may look.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"720\" height=\"450\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/example-remembr-flow.gif\" alt=\"GIF shows the LLM agent being asked how to get upstairs. It first determines that it must query the database for stairs, for which it retrieves an outdoor staircase that is not sufficient. Then, it queries and returns an elevator, which may be sufficient. The LLM then queries the database for stairs that are indoors. It finds the elevator as a sufficient response and returns that to the user as an answer to their question.\" class=\"wp-image-89276\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Example ReMEmbR query and reasoning flow</em></figcaption></figure></div>\n\n\n<h2 id=\"deploying_remembr_on_a_real_robot\"  class=\"wp-block-heading\">Deploying ReMEmbR on a real robot<a href=\"#deploying_remembr_on_a_real_robot\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To demonstrate how ReMEmbR can be integrated into a real robot, we built a demo using ReMEmbR with NVIDIA Isaac ROS and Nova Carter. Isaac ROS, built on the open-source <a href=\"https://www.ros.org/\">ROS 2 software framework</a>, is a collection of accelerated computing packages and AI models, bringing NVIDIA acceleration to ROS developers everywhere.</p>\n\n\n\n<p>In the demo, the robot answers questions and guides people around an office environment. To demystify the process of building the application, we wanted to share the steps we took:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Building an occupancy grid map</li>\n\n\n\n<li>Running the memory builder</li>\n\n\n\n<li>Running the ReMEmbR agent</li>\n\n\n\n<li>Adding speech recognition</li>\n</ul>\n\n\n\n<h3 id=\"building_an_occupancy_grid_map\"  class=\"wp-block-heading\">Building an occupancy grid map<a href=\"#building_an_occupancy_grid_map\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The first step we took was to create a map of the environment. To build the vector database, ReMEmbR needs access to the monocular camera images as well as the global location (pose) information.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"624\" height=\"216\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nova-carter-odometry-pipeline-map.png\" alt=\"Picture shows the Nova Carter robot with an arrow pointing at the 3D Lidar + odometry being fed into a Nav2 2D SLAM pipeline, which is used to build a map.\" class=\"wp-image-88944\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nova-carter-odometry-pipeline-map.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nova-carter-odometry-pipeline-map-300x104.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nova-carter-odometry-pipeline-map-179x62.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nova-carter-odometry-pipeline-map-500x173.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nova-carter-odometry-pipeline-map-160x55.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nova-carter-odometry-pipeline-map-362x125.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nova-carter-odometry-pipeline-map-318x110.png 318w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Building an occupancy grid map with Nova Carter</em></figcaption></figure></div>\n\n\n<p>Depending on your environment or platform, obtaining the global pose information can be challenging. Fortunately, this is straightforward when using <a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/landing_pages/nova_carter_landing_page.html\">Nova Carter</a>. Nova Carter, powered by the Nova Orin reference architecture, is a complete robotics development platform that accelerates the development and deployment of next-generation autonomous mobile robots (AMRs). It may be equipped with a 3D LIDAR to generate accurate and globally consistent metric maps.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"600\" height=\"335\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nova-carter-2d-occupancy-map.gif\" alt=\"GIF shows a 2D occupancy grid being built online using Nova Carter. The map fills out over time as the robot moves throughout the environment.\" class=\"wp-image-88945\"/><figcaption class=\"wp-element-caption\"><em>Figure 4. FoxGlove visualization of an occupancy grid map being built with Nova Carter</em></figcaption></figure></div>\n\n\n<p>By following the <a href=\"https://nvidia-isaac-ros.github.io/robots/nova_carter/demo_lidar_mapping.html\">Isaac ROS documentation</a>, we quickly built an occupancy map by teleoperating the robot. This map is later used for localization when building the ReMEmbR database and for path planning and navigation for the final robot deployment.</p>\n\n\n\n<h3 id=\"running_the_memory_builder\"  class=\"wp-block-heading\">Running the memory builder<a href=\"#running_the_memory_builder\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>After we created the map of the environment, the second step was to populate the vector database used by ReMEmbR. For this, we teleoperated the robot, while running <a href=\"https://wiki.ros.org/amcl\">AMCL</a> for global localization. For more information about how to do this with Nova Carter, see <a href=\"https://nvidia-isaac-ros.github.io/reference_workflows/isaac_perceptor/tutorials_on_carter/demo_navigation.html\">Tutorial: Autonomous Navigation with Isaac Perceptor and Nav2</a>.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"623\" height=\"289\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-memory-builder.png\" alt=\"The system diagram shows running the ReMEmBr demo memory builder. The occupancy grid map is used as input. The VILA node captions images from the camera. The captions and localization information are stored in a vector database.\" class=\"wp-image-88946\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-memory-builder.png 623w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-memory-builder-300x139.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-memory-builder-179x83.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-memory-builder-500x232.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-memory-builder-160x74.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-memory-builder-362x168.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-memory-builder-237x110.png 237w\" sizes=\"(max-width: 623px) 100vw, 623px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Running the ReMEmBr memory builder</em></figcaption></figure></div>\n\n\n<p>With the localization running in the background, we launched two additional ROS nodes specific to the memory-building phase.</p>\n\n\n\n<p>The first ROS node runs the <a href=\"https://github.com/NVlabs/VILA\">VILA</a> model to generate captions for the robot camera images. This node runs on the device, so even if the network is intermittent we could still build a reliable database. </p>\n\n\n\n<p>Running this node on Jetson is made easier with <a href=\"https://dusty-nv.github.io/NanoLLM/\">NanoLLM </a>for quantization and inference. This library, along with many others, is featured in the <a href=\"https://www.jetson-ai-lab.com/\">Jetson AI Lab</a>. There is even a recently released ROS package (<a href=\"https://github.com/NVIDIA-AI-IOT/ros2_nanollm\">ros2_nanollm</a>) for easily integrating NanoLLM models with a ROS application.</p>\n\n\n\n<p>The second ROS node subscribes to the captions generated by VILA, as well as the global pose estimated by the AMCL node. It builds text embeddings for the captions and stores the pose, text, embeddings, and timestamps in the vector database.</p>\n\n\n\n<h3 id=\"running_the_remembr_agent\"  class=\"wp-block-heading\">Running the ReMEmbR agent<a href=\"#running_the_remembr_agent\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"623\" height=\"304\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-agent.png\" alt=\"Diagram shows that when the user has a question, the agent node leverages the pose information from AMCL and generates queries for the vector database in a loop. When the LLM has an answer, and if it is a goal position for the robot, a message is sent on the goal pose topic, which navigates the robot using Nav2.\" class=\"wp-image-88948\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-agent.png 623w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-agent-300x146.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-agent-179x87.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-agent-500x244.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-agent-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-agent-362x177.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-agent-225x110.png 225w\" sizes=\"(max-width: 623px) 100vw, 623px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Running the ReMEmbR agent to answer user queries and navigate to goal poses</em></figcaption></figure></div>\n\n\n<p>After we populated the vector database, the ReMEmbR agent had everything it needed to answer user queries and produce meaningful actions.</p>\n\n\n\n<p>The third step was to run the <a href=\"https://www.youtube.com/watch?v=JlYVBAQC0tQ\">live demo</a>. To make the robot\u2019s memory static, we disabled the image captioning and memory-building nodes and enabled the ReMEmbR agent node. As detailed earlier, the ReMEmbR agent is responsible for taking a user query, querying the vector database, and determining the appropriate action the robot should take. In this instance, the action is a destination goal pose corresponding to the user\u2019s query.</p>\n\n\n\n<p>We then tested the system end-to-end by manually typing in user queries:</p>\n\n\n\n<p>\u201cTake me to the nearest elevator\u201d<br>\u201cTake me somewhere I can get a snack\u201d</p>\n\n\n\n<p>The ReMEmbR agent determines the best goal pose and publishes it to the <code>/goal_pose</code> topic. The path planner then generates a global path for the robot to follow to navigate to this goal.</p>\n\n\n\n<h3 id=\"adding_speech_recognition\"  class=\"wp-block-heading\">Adding speech recognition<a href=\"#adding_speech_recognition\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In a real application, users likely won\u2019t have access to a terminal to enter queries and need an intuitive way to interact with the robot. For this, we took the application a step further by integrating speech recognition to generate the queries for the agent.&nbsp;</p>\n\n\n\n<p>On Jetson Orin platforms, integrating speech recognition is straightforward. We accomplished this by writing a ROS node that wraps the recently released <a href=\"https://github.com/NVIDIA-AI-IOT/whisper_trt\">WhisperTRT</a> project. WhisperTRT optimizes OpenAI\u2019s whisper model with <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a>, enabling low-latency inference on NVIDIA Jetson AGX Orin and NVIDIA Jetson Orin Nano.</p>\n\n\n\n<p>The WhisperTRT ROS node directly accesses the microphone using PyAudio and publishes recognized speech on the speech topic.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"624\" height=\"86\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-speech-recognition-whispertrt.png\" alt=\"The diagram shows taking in user input, which is recognized with a WhisperTRT speech recognition node that publishes a speech topic that the ReMEmbR agent node listens to.\" class=\"wp-image-88949\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-speech-recognition-whispertrt.png 624w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-speech-recognition-whispertrt-300x41.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-speech-recognition-whispertrt-179x25.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-speech-recognition-whispertrt-500x69.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-speech-recognition-whispertrt-160x22.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-speech-recognition-whispertrt-362x50.png 362w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Integrating speech recognition with WhisperTRT, for natural user interaction</em></figcaption></figure></div>\n\n\n<h3 id=\"all_together\"  class=\"wp-block-heading\">All together<a href=\"#all_together\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>With all the components combined, we created our <a href=\"https://www.youtube.com/watch?v=JlYVBAQC0tQ\">full demo of the robot</a>.</p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>We hope this post inspires you to explore <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI</a> in robotics. To learn more about the contents presented in this post, try out the ReMEmBr code, and get started building your own generative AI robotics applications, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://nvidia-ai-iot.github.io/remembr/\">ReMEmbR</a> website</li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA-AI-IOT/remembr\">/NVIDIA-AI-IOT/remembr</a> GitHub repo</li>\n\n\n\n<li><a href=\"https://arxiv.org/abs/2409.13682\">ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation</a> paper</li>\n\n\n\n<li><a href=\"https://nvidia-isaac-ros.github.io/\">NVIDIA Isaac ROS</a> documentation</li>\n\n\n\n<li><a href=\"https://robotics.segway.com/nova-carter/\">Nova Carter</a></li>\n\n\n\n<li><a href=\"https://www.jetson-ai-lab.com/\">NVIDIA Jetson AI Lab</a></li>\n</ul>\n\n\n\n<p>Sign up for the <a href=\"https://developer.nvidia.com/developer-program\">NVIDIA Developer Program</a> for updates on additional resources and reference architectures to support your development goals.</p>\n\n\n\n<p>Stay up to date on <a href=\"https://www.linkedin.com/showcase/nvidiarobotics\">LinkedIn</a>, <a href=\"https://www.instagram.com/nvidiarobotics/\">Instagram</a>, <a href=\"https://x.com/NVIDIARobotics\">X</a>, and <a href=\"https://www.facebook.com/NVIDIARobotics\">Facebook</a>.&nbsp;For more information, explore our <a href=\"https://docs.nvidia.com/\">documentation</a> and join the robotics community on our <a href=\"https://forums.developer.nvidia.com/c/agx-autonomous-machines/55\">developer forums</a> and <a href=\"https://www.youtube.com/@NVIDIADeveloper/\">YouTube</a> channels. Follow along with <a href=\"https://learn.nvidia.com/en-us/training/find-training?q=robotics\">self-paced training</a> and webinars (<a href=\"https://gateway.on24.com/wcc/experience/elitenvidiabrill/1407606/3998202/isaac-ros-webinar-series\">Isaac ROS </a>and <a href=\"https://gateway.on24.com/wcc/experience/elitenvidiabrill/1407606/4076607/isaac-sim-series\">Isaac Sim</a>).</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Vision-language models (VLMs) combine the powerful language understanding of foundational LLMs with the vision capabilities of vision transformers (ViTs) by projecting text and images into the same embedding space. They can take unstructured multimodal data, reason over it, and return the output in a structured format. Building on a broad base of pretraining, they can &hellip; <a href=\"https://developer.nvidia.com/blog/using-generative-ai-to-enable-robots-to-reason-and-act-with-remembr/\">Continued</a></p>\n", "protected": false}, "author": 2296, "featured_media": 89277, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1490038", "discourse_permalink": "https://forums.developer.nvidia.com/t/using-generative-ai-to-enable-robots-to-reason-and-act-with-remembr/307677", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2758, 3110, 63], "tags": [453, 2932, 3953], "coauthors": [4029, 2948, 4028], "class_list": ["post-88932", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-edge-computing", "category-generative-ai", "category-robotics", "tag-featured", "tag-large-language-models", "tag-vlms"], "acf": {"post_industry": ["Manufacturing"], "post_products": ["Isaac ROS", "Jetson", "TensorRT"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/remembr-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-n8o", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Robotics", "link": "https://developer.nvidia.com/blog/category/robotics/", "id": 63}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88932"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2296"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88932"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88932/revisions"}], "predecessor-version": [{"id": 89329, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88932/revisions/89329"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89277"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88932"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88932"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88932"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88932"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89283, "date": "2024-09-23T09:41:34", "date_gmt": "2024-09-23T16:41:34", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89283"}, "modified": "2024-10-04T15:08:06", "modified_gmt": "2024-10-04T22:08:06", "slug": "advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/", "title": {"rendered": "Advancing the Accuracy-Efficiency Frontier with Llama-3.1-Nemotron-51B"}, "content": {"rendered": "\n<p>Today, NVIDIA released a unique language model that delivers an unmatched accuracy-efficiency performance. Llama 3.1-Nemotron-51B, derived from Meta\u2019s Llama-3.1-70B, uses a novel neural architecture search (NAS) approach that results in a highly accurate and efficient model. </p>\n\n\n\n<p>The model fits on a single NVIDIA H100 GPU at high workloads, making it much more accessible and affordable. The excellent accuracy-efficiency sweet spot exhibited by the new model stems from changes to the model\u2019s architecture that lead to a significantly lower memory footprint, reduced memory bandwidth, and reduced FLOPs while maintaining excellent accuracy. We demonstrate that this approach can be generalized by creating another smaller and faster variant from the reference model.</p>\n\n\n\n<p>In July 2024, Meta released Llama-3.1-70B, a leading state-of-the-art large language model (LLM). Today we announce <a href=\"https://build.nvidia.com/nvidia/llama-3_1-nemotron-51b-instruct\" target=\"_blank\" rel=\"noreferrer noopener\">Llama 3.1-Nemotron-51B-Instruct</a>, developed using NAS and knowledge distillation derived from the reference model, Llama-3.1-70B.\u00a0</p>\n\n\n\n<h2 id=\"superior_throughput_and_workload_efficiency\"  class=\"wp-block-heading\">Superior throughput and workload efficiency<a href=\"#superior_throughput_and_workload_efficiency\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The Nemotron model yields 2.2x faster inference compared to the reference model while maintaining nearly the same accuracy. The model opens a new set of opportunities with a reduced memory footprint, which enables running 4x larger workloads on a single GPU during inference.</p>\n\n\n\n<figure class=\"wp-block-table is-style-stripes\"><table class=\"has-fixed-layout\"><tbody><tr><td>&nbsp;</td><td colspan=\"2\"><strong>Accuracy</strong></td><td colspan=\"2\"><strong>Efficiency</strong></td></tr><tr><td>&nbsp;</td><td><strong>MT Bench</strong></td><td><strong>MMLU</strong></td><td><strong>Text generation</strong> <strong>(128/1024)</strong></td><td><strong>Summarization/ RAG (2048/128)</strong></td></tr><tr><td><strong>Llama-3.1- Nemotron-51B- Instruct</strong></td><td>8.99</td><td>80.2%</td><td>6472</td><td>653</td></tr><tr><td><strong>Llama 3.1-70B- Instruct</strong></td><td>8.93</td><td>81.66%</td><td>2975</td><td>339</td></tr><tr><td><strong>Llama 3.1-70B- Instruct (single GPU)</strong></td><td>\u2014</td><td>\u2014</td><td>1274</td><td>301</td></tr><tr><td><strong>Llama 3-70B</strong></td><td>8.94</td><td>80.17%</td><td>2975</td><td>339</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Overview of the Llama-3.1-Nemotron-51B-Instruct accuracy and efficiency.</em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">Note: Speed is reported in tokens per second per GPU, Measured on machines equipped with 8 X NVIDIA H100 SXM GPUs, with FP8 quantization using <a href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/\">TRT-LLM</a> as the runtime engine. For each model with the optimal number of GPUs through tensor parallelism (unless otherwise stated). The numbers in the brackets show the (input/output sequence lengths).</p>\n\n\n\n<p>We discuss the detailed performance metrics later in this post.</p>\n\n\n\n<h2 id=\"optimized_accuracy_per_dollar\"  class=\"wp-block-heading\">Optimized accuracy per dollar<a href=\"#optimized_accuracy_per_dollar\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Foundation models display incredible quality in solving complex tasks: reasoning, summarization, and more. However, a major challenge in the adoption of \u200ctop models is their inference cost. </p>\n\n\n\n<p>As the field of generative AI evolves, the balance between accuracy and efficiency (directly impacting cost) will become the decisive factor in model selection. Moreover, the capability to run a model on a single GPU significantly streamlines its deployment, opening opportunities for new applications to run anywhere, from edge systems to data centers to the cloud, as well as facilitating serving multiple models via Kubernetes and <a href=\"https://build.nvidia.com/nim/agent-blueprints\">NIM blueprints</a>.\u00a0</p>\n\n\n\n<p>Consequently, we engineered Llama 3.1-Nemotron-51B-Instruct to achieve this optimal tradeoff (Figure 1). Throughput is inversely proportional to price, so the best tradeoff is obtained by models on the efficient frontier displayed in the chart. Figure 1 shows that the model pushes beyond the current efficient frontier, making it the model that provides the best accuracy per dollar.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"871\" height=\"511\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Accuracy-vs.-Throughput-performance-of-Llama-3.1-Nemotron-51B.png\" alt=\"The graph shows accuracy vs. throughput of various frontier models and the efficient frontier highlights the accuracy Mistral and Llama models can achieve while delivering a given throughput. The Llama-3.1-Nemotron-51B model delivers higher accuracy per throughput and is placed above the Efficient frontier line.\" class=\"wp-image-89313\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Accuracy-vs.-Throughput-performance-of-Llama-3.1-Nemotron-51B.png 871w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Accuracy-vs.-Throughput-performance-of-Llama-3.1-Nemotron-51B-300x176.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Accuracy-vs.-Throughput-performance-of-Llama-3.1-Nemotron-51B-625x367.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Accuracy-vs.-Throughput-performance-of-Llama-3.1-Nemotron-51B-179x105.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Accuracy-vs.-Throughput-performance-of-Llama-3.1-Nemotron-51B-768x451.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Accuracy-vs.-Throughput-performance-of-Llama-3.1-Nemotron-51B-645x378.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Accuracy-vs.-Throughput-performance-of-Llama-3.1-Nemotron-51B-500x293.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Accuracy-vs.-Throughput-performance-of-Llama-3.1-Nemotron-51B-153x90.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Accuracy-vs.-Throughput-performance-of-Llama-3.1-Nemotron-51B-362x212.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Accuracy-vs.-Throughput-performance-of-Llama-3.1-Nemotron-51B-187x110.png 187w\" sizes=\"(max-width: 871px) 100vw, 871px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Accuracy vs. Throughput performance of Llama-3.1-Nemotron-51B compared to frontier models. Throughput was measured through NIM with concurrency 25 (serving throughput).</em></figcaption></figure></div>\n\n\n<p class=\"has-small-font-size\">The model quality is defined as the weighted average of MT-Bench and MMLU (10*MT-Bench + MMLU)/2, plotted compared to model throughput per a single NVIDIA H100 80GB GPU. Gray dots represent state-of-the-art models, while the dashed line represents the \u2018efficient frontier\u2019.</p>\n\n\n\n<h2 id=\"simplifying_inference_with_nvidia_nim\"  class=\"wp-block-heading\">Simplifying inference with NVIDIA NIM<a href=\"#simplifying_inference_with_nvidia_nim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The Nemotron model is optimized with TensorRT-LLM engines for higher inference performance and packaged as an NVIDIA NIM microservice to streamline and accelerate the deployment of generative AI models across NVIDIA accelerated infrastructure anywhere, including cloud, data center, and workstations.</p>\n\n\n\n<p>NIM uses inference optimization engines, industry-standard APIs, and prebuilt containers to provide high-throughput AI inference that scales with demand.</p>\n\n\n\n<p class=\"has-black-color has-text-color has-link-color wp-elements-0e3f785789f409c96fd51cb52a601eca\">Try out <a href=\"https://build.nvidia.com/nvidia/llama-3_1-nemotron-51b-instruct\">Llama-3.1-Nemotron-51B NIM microservice</a> through the API from <a href=\"http://ai.nvidia.com/\">ai.nvidia.com</a> with free NVIDIA credits.</p>\n\n\n\n<h2 id=\"building_the_model_with_nas\"  class=\"wp-block-heading\">Building the model with NAS<a href=\"#building_the_model_with_nas\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Inference and hardware-aware methods for designing neural architectures have been successfully used in many domains. However, LLMs are still constructed as repeated identical blocks, with little regard for inference cost overheads incurred by this simplification. To tackle these challenges, we developed efficient NAS technology and training methods that can be used to create non-standard transformer models designed for efficient inference on specific GPUs.</p>\n\n\n\n<p>Our technology can select neural architectures that optimize various constraints. The range includes enormous design spaces that include a zoo of non-standard transformer models using alternative attention and FFN blocks of varying efficiency degrees, up to a complete block elimination in the extreme case.</p>\n\n\n\n<p>We then use our block-distillation (Figure 2) framework to train all these block variants for all layers of a (large) parent LLM in parallel. In a basic version of block-distillation, training data is passed through the reference model\u00a0 (also known as a teacher). </p>\n\n\n\n<p>For each block, its input is taken from the teacher and injected into the matching block of the student. The outputs of the teacher and student for the block are compared and the student block is trained so that the student block mimics the functionality of the teacher block. A more advanced scenario where a single student block mimics multiple teacher blocks is depicted on the right side in Figure 2.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"936\" height=\"222\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Block-distillation-For-blocks-of-the-reference-model-blue-we-create-multiple-variants-for-the-\u2018student-model-yellow-that-mimic-the-block-wise-teacher-functionality.png\" alt=\"The image displays two diagrams illustrating the block-distillation process. In each diagram, a student block (yellow) is trained to mimic either one or multiple blocks of the teacher model (blue). The training aims to minimize the difference between the input to the teacher block (red curved line) and the output of the teacher block (black curved line). This minimization is achieved using a loss function (depicted in green). The left diagram shows one teacher block being mimicked by the student block, while the right diagram demonstrates the student block mimicking the operation of two consecutive teacher blocks.\" class=\"wp-image-89285\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Block-distillation-For-blocks-of-the-reference-model-blue-we-create-multiple-variants-for-the-\u2018student-model-yellow-that-mimic-the-block-wise-teacher-functionality.png 936w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Block-distillation-For-blocks-of-the-reference-model-blue-we-create-multiple-variants-for-the-\u2018student-model-yellow-that-mimic-the-block-wise-teacher-functionality-300x71.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Block-distillation-For-blocks-of-the-reference-model-blue-we-create-multiple-variants-for-the-\u2018student-model-yellow-that-mimic-the-block-wise-teacher-functionality-625x148.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Block-distillation-For-blocks-of-the-reference-model-blue-we-create-multiple-variants-for-the-\u2018student-model-yellow-that-mimic-the-block-wise-teacher-functionality-179x42.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Block-distillation-For-blocks-of-the-reference-model-blue-we-create-multiple-variants-for-the-\u2018student-model-yellow-that-mimic-the-block-wise-teacher-functionality-768x182.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Block-distillation-For-blocks-of-the-reference-model-blue-we-create-multiple-variants-for-the-\u2018student-model-yellow-that-mimic-the-block-wise-teacher-functionality-645x153.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Block-distillation-For-blocks-of-the-reference-model-blue-we-create-multiple-variants-for-the-\u2018student-model-yellow-that-mimic-the-block-wise-teacher-functionality-500x119.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Block-distillation-For-blocks-of-the-reference-model-blue-we-create-multiple-variants-for-the-\u2018student-model-yellow-that-mimic-the-block-wise-teacher-functionality-160x38.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Block-distillation-For-blocks-of-the-reference-model-blue-we-create-multiple-variants-for-the-\u2018student-model-yellow-that-mimic-the-block-wise-teacher-functionality-362x86.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-2.-Block-distillation-For-blocks-of-the-reference-model-blue-we-create-multiple-variants-for-the-\u2018student-model-yellow-that-mimic-the-block-wise-teacher-functionality-464x110.png 464w\" sizes=\"(max-width: 936px) 100vw, 936px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Block distillation where blue reference model blocks are multiple variants for the yellow student models that mimic the block-wise teacher functionality</em></figcaption></figure></div>\n\n\n<p>Next, we use our Puzzle algorithm to efficiently score each alternative replacement puzzle piece and search our enormous design space for the most accurate models, while adhering to a set of inference constraints, such as memory size and required throughput. </p>\n\n\n\n<p>Finally, by using knowledge distillation (KD) loss for both block scoring and training, we demonstrate the potential to narrow the accuracy gap between our model and the reference model using a much more efficient architecture with a tiny fraction of the reference model training costs. Using our methods on Llama-3.1-70B as the reference model, we built \u200b\u200bLlama-3.1-Nemotron-51B-Instruct, a 51B model that breaks the efficient frontier of LLMs on a single NVIDIA H100 GPU (Figure 1).\u00a0</p>\n\n\n\n<p>The \u200b\u200bLlama-3.1-Nemotron-51B-Instruct architecture is unique in its irregular block structure with many layers in which the attention and FFN are reduced or pruned, resulting in better utilization of H100 and highlighting the importance of optimizing LLMs for inference. Figure 3 schematically depicts the irregular structure of the resulting architecture and highlights the resulting compute saving, which amounts to the green area in the Figure.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1252\" height=\"272\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Runtime-of-Puzzle-chosen-blocks.png\" alt=\"The image shows runtime of puzzle chosen blocks that are depicted as layers. They are color coded, blue blocks attention layers, red blocks for FFN layers, and green blocks represent overall runtime savings. The image shows 80 layers of the reference models.\" class=\"wp-image-89305\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Runtime-of-Puzzle-chosen-blocks.png 1252w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Runtime-of-Puzzle-chosen-blocks-300x65.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Runtime-of-Puzzle-chosen-blocks-625x136.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Runtime-of-Puzzle-chosen-blocks-179x39.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Runtime-of-Puzzle-chosen-blocks-768x167.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Runtime-of-Puzzle-chosen-blocks-645x140.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Runtime-of-Puzzle-chosen-blocks-500x109.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Runtime-of-Puzzle-chosen-blocks-160x35.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Runtime-of-Puzzle-chosen-blocks-362x79.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Runtime-of-Puzzle-chosen-blocks-506x110.png 506w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Runtime-of-Puzzle-chosen-blocks-1024x222.png 1024w\" sizes=\"(max-width: 1252px) 100vw, 1252px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Runtime of Puzzle chosen blocks (layers) for attention layers (blue) and FFN layers (red)&nbsp;across the 80 layers of the reference model. Green areas correspond to overall runtime savings.</em></figcaption></figure></div>\n\n\n<p>Our innovative techniques enable us to develop models that redefine the efficient frontier of LLMs. Crucially, we can cost-effectively design multiple models from a single reference model, each optimized for specific hardware and inference scenarios. This capability empowers us to maintain best-in-class performance for LLM inference across our current and future hardware platforms.</p>\n\n\n\n<h2 id=\"detailed_results\"  class=\"wp-block-heading\">Detailed results<a href=\"#detailed_results\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Here are the model accuracy and performance metrics for our model.</p>\n\n\n\n<h3 id=\"model_accuracy\"  class=\"wp-block-heading\">Model accuracy<a href=\"#model_accuracy\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Table 2 lists all the benchmarks that we evaluated, comparing our model and the reference model Llama-3.1-70B. The <strong>Accuracy preserved</strong> column is the ratio between our model\u2019s score and that of the teacher.\u00a0</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Benchmark</strong></td><td><strong>Llama-3.1 70B-instruct</strong></td><td><strong>Llama-3.1-Nemotron-51B- Instruct</strong></td><td><strong>Accuracy preserved</strong></td></tr><tr><td>winogrande</td><td>85.08%</td><td>84.53%</td><td>99.35%</td></tr><tr><td>arc_challenge</td><td>70.39%</td><td>69.20%</td><td>98.30%</td></tr><tr><td>MMLU</td><td>81.66%</td><td>80.20%</td><td>98.21%</td></tr><tr><td>hellaswag</td><td>86.44%</td><td>85.58%</td><td>99.01%</td></tr><tr><td>gsm8k</td><td>92.04%</td><td>91.43%</td><td>99.34%</td></tr><tr><td>truthfulqa</td><td>59.86%</td><td>58.63%</td><td>97.94%</td></tr><tr><td>xlsum_english</td><td>33.86%</td><td>31.61%</td><td>93.36%</td></tr><tr><td>MMLU Chat</td><td>81.76%</td><td>80.58%</td><td>98.55%</td></tr><tr><td>gsm8k Chat</td><td>81.58%</td><td>81.88%</td><td>100.37%</td></tr><tr><td>Instruct HumanEval (n=20)</td><td>75.85%</td><td>73.84%</td><td>97.35%</td></tr><tr><td>MT Bench</td><td>8.93</td><td>8.99</td><td>100.67%</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Accuracy comparison of the Nemotron model to the Llama-3.1-70B-Instruct model across several industry benchmarks</em></figcaption></figure>\n\n\n\n<h3 id=\"performance\"  class=\"wp-block-heading\">Performance<a href=\"#performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Table 3 shows the number of tokens per second per GPU (NVIDIA H100 80-GB GPU). You can see that for a range of relevant scenarios, short and long inputs as well as outputs, our model doubles the throughput of the teacher model, making it cost-effective across multiple use cases. </p>\n\n\n\n<p>TPX describes the number of GPUs on which the process runs in parallel. We also list the performance of Llama 3.1-70B on a single GPU to demonstrate the value of our model in such a setting.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Scenario</strong></td><td><strong>Input/Output Sequence Length</strong></td><td><strong>Llama-3.1- Nemotron-Instruct</strong></td><td><strong>Llama-3.1-70B-Instruct</strong></td><td><strong>Ratio</strong></td><td><strong>Llama (TP1)</strong></td></tr><tr><td>Chatbot</td><td>128/128</td><td>5478 (TP1)</td><td>2645 (TP1)</td><td>2.07</td><td>2645</td></tr><tr><td>Text generation</td><td>128/1024</td><td>6472 (TP1)</td><td>2975 (TP4)</td><td>2.17</td><td>1274</td></tr><tr><td>Long text generation</td><td>128/2048</td><td>4910 (TP2)</td><td>2786 (TP4)</td><td>1.76</td><td>646</td></tr><tr><td>System 2 reasoning</td><td>128/4096</td><td>3855 (TP2)</td><td>1828 (TP4)</td><td>2.11</td><td>313</td></tr><tr><td>Summarization/ RAG</td><td>2048/128</td><td>653 (TP1)</td><td>339 (TP4)</td><td>1.92</td><td>300</td></tr><tr><td>Stress test 1</td><td>2048/2048</td><td>2622 (TP2)</td><td>1336 (TP4)</td><td>1.96</td><td>319</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 3. Throughput comparison of the number of tokens generated by the models for popular use cases. All numbers are in tokens per second per GPU.</em></figcaption></figure>\n\n\n\n<p>The main factor in determining the cost of running a model is <em>throughput</em>, the total number of tokens that the system can generate in one second. However, in some scenarios (for example, chatbots), the rate at which a single end user receives the response from the model is important for the user experience. This is quantified by the tokens per second per user, termed the user-side throughput. </p>\n\n\n\n<p>Figure 4 shows this user-side throughput plotted against the throughput at different batch sizes. As seen in all batch sizes, our model is superior to Llama-3.1-70B.\u00a0</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"629\" height=\"373\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-4.-Server-throughput-vs.-user-side-throughput-plotted-at-different-batch-size-for-the-Nemotron-model-and-for-Llama-3.1-70B-1.png\" alt=\"The chart shows that the Llama 3.1-70B curve is lower compared to Llama-3.1-Nemotron-51B-Instruct.\" class=\"wp-image-89288\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-4.-Server-throughput-vs.-user-side-throughput-plotted-at-different-batch-size-for-the-Nemotron-model-and-for-Llama-3.1-70B-1.png 629w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-4.-Server-throughput-vs.-user-side-throughput-plotted-at-different-batch-size-for-the-Nemotron-model-and-for-Llama-3.1-70B-1-300x178.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-4.-Server-throughput-vs.-user-side-throughput-plotted-at-different-batch-size-for-the-Nemotron-model-and-for-Llama-3.1-70B-1-625x371.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-4.-Server-throughput-vs.-user-side-throughput-plotted-at-different-batch-size-for-the-Nemotron-model-and-for-Llama-3.1-70B-1-179x106.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-4.-Server-throughput-vs.-user-side-throughput-plotted-at-different-batch-size-for-the-Nemotron-model-and-for-Llama-3.1-70B-1-500x297.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-4.-Server-throughput-vs.-user-side-throughput-plotted-at-different-batch-size-for-the-Nemotron-model-and-for-Llama-3.1-70B-1-152x90.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-4.-Server-throughput-vs.-user-side-throughput-plotted-at-different-batch-size-for-the-Nemotron-model-and-for-Llama-3.1-70B-1-362x215.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Figure-4.-Server-throughput-vs.-user-side-throughput-plotted-at-different-batch-size-for-the-Nemotron-model-and-for-Llama-3.1-70B-1-185x110.png 185w\" sizes=\"(max-width: 629px) 100vw, 629px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Server throughput vs. user-side throughput, plotted at different batch sizes for the Nemotron model and for Llama-3.1-70B</em></figcaption></figure></div>\n\n\n<h2 id=\"tailoring_llms_for_diverse_needs\u00a0\"  class=\"wp-block-heading\">Tailoring LLMs for diverse needs\u00a0<a href=\"#tailoring_llms_for_diverse_needs\u00a0\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The NAS approach offers you flexibility in selecting the optimal balance between accuracy and efficiency. To demonstrate this versatility, we created another variant from the same reference model, this time prioritizing speed and cost. Llama-3.1-Nemotron-40B-Instruct was developed using the same methodology but with a modified speed requirement during the puzzle phase.\u00a0</p>\n\n\n\n<p>This model achieves a 3.2x speed increase compared to the parent model, with a moderate decrease in accuracy. Table 4 shows competitive performance metrics.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table class=\"has-fixed-layout\"><tbody><tr><td></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\"><strong>Accuracy</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\"><strong>Speed</strong></td></tr><tr><td></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>MT bench</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>MMLU</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Text generation </strong><strong>(128/1024)</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Summarization/ RAG (2048/128)</strong></td></tr><tr><td><strong>Llama-3.1- Nemotron-40B-Instruct</strong></td><td class=\"has-text-align-center\" data-align=\"center\">8.69</td><td class=\"has-text-align-center\" data-align=\"center\">77.10%</td><td class=\"has-text-align-center\" data-align=\"center\">9568</td><td class=\"has-text-align-center\" data-align=\"center\">862</td></tr><tr><td><strong>Llama-3.1- Nemotron-51B-Instruct</strong></td><td class=\"has-text-align-center\" data-align=\"center\">8.99</td><td class=\"has-text-align-center\" data-align=\"center\">80.20%</td><td class=\"has-text-align-center\" data-align=\"center\">6472</td><td class=\"has-text-align-center\" data-align=\"center\">653</td></tr><tr><td><strong>Llama 3.1-70B-Instruct</strong></td><td class=\"has-text-align-center\" data-align=\"center\">8.93</td><td class=\"has-text-align-center\" data-align=\"center\">81.72%</td><td class=\"has-text-align-center\" data-align=\"center\">2975</td><td class=\"has-text-align-center\" data-align=\"center\">339</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 4. Overview of the Llama-3.1-Nemotron-40B-Instruct accuracy and efficiency</em></figcaption></figure>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\"><strong>Summary</strong><a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://build.nvidia.com/nvidia/llama-3_1-nemotron-51b-instruct\">Llama 3.1-Nemotron-51B-Instruct</a> provides a new set of opportunities for users and companies that want to use highly accurate foundation models, but do so in a cost-controlled manner. By providing the best tradeoff between accuracy and efficiency, we believe the model is an attractive option for builders. Moreover, these results demonstrate the effectiveness of the NAS approach and intend to extend the method to other models.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Today, NVIDIA released a unique language model that delivers an unmatched accuracy-efficiency performance. Llama 3.1-Nemotron-51B, derived from Meta\u2019s Llama-3.1-70B, uses a novel neural architecture search (NAS) approach that results in a highly accurate and efficient model. The model fits on a single NVIDIA H100 GPU at high workloads, making it much more accessible and affordable. &hellip; <a href=\"https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/\">Continued</a></p>\n", "protected": false}, "author": 2310, "featured_media": 66733, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1489984", "discourse_permalink": "https://forums.developer.nvidia.com/t/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/307664", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [453, 3933, 2932], "coauthors": [4043, 4044], "class_list": ["post-89283", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-llama", "tag-large-language-models"], "acf": {"post_industry": ["General"], "post_products": ["NeMo", "NIM", "TensorRT-LLM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/federated-ml-nvidia-flare-featured-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-ne3", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89283"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2310"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89283"}], "version-history": [{"count": 22, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89283/revisions"}], "predecessor-version": [{"id": 89953, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89283/revisions/89953"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/66733"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89283"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89283"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89283"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89283"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88923, "date": "2024-09-23T09:00:00", "date_gmt": "2024-09-23T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88923"}, "modified": "2024-10-03T11:38:28", "modified_gmt": "2024-10-03T18:38:28", "slug": "just-released-free-openusd-training-courses", "status": "publish", "type": "post", "link": "https://nvda.ws/3XnK9OF", "title": {"rendered": "Just Released: Free OpenUSD Training Courses"}, "content": {"rendered": "\n<p>Accelerate your OpenUSD workflows with this free curriculum for developers and 3D practitioners.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Accelerate your OpenUSD workflows with this free curriculum for developers and 3D practitioners.</p>\n", "protected": false}, "author": 1642, "featured_media": 88924, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1484038", "discourse_permalink": "https://forums.developer.nvidia.com/t/just-released-free-openusd-training-courses/306368", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3XnK9OF", "_links_to_target": "_blank"}, "categories": [1235, 503], "tags": [3268, 453, 1958, 3700], "coauthors": [3181], "class_list": ["post-88923", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-graphics", "category-simulation-modeling-design", "tag-generative-ai-3d", "tag-featured", "tag-news", "tag-openusd"], "acf": {"post_industry": ["General", "Architecture / Engineering / Construction", "Gaming", "Manufacturing", "Media & Entertainment"], "post_products": ["Omniverse"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ov-dli-learn-usd-CamB_4400x2475_v15_Mixed.png", "jetpack_shortlink": "https://wp.me/pcCQAL-n8f", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88923"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1642"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88923"}], "version-history": [{"count": 1, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88923/revisions"}], "predecessor-version": [{"id": 88926, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88923/revisions/88926"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88924"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88923"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88923"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88923"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88923"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]