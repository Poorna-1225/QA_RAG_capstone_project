[{"id": 89206, "date": "2024-09-20T08:32:09", "date_gmt": "2024-09-20T15:32:09", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89206"}, "modified": "2024-10-03T11:38:29", "modified_gmt": "2024-10-03T18:38:29", "slug": "new-ai-powered-3d-printing-can-help-surgeons-rehearse-procedures", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/new-ai-powered-3d-printing-can-help-surgeons-rehearse-procedures/", "title": {"rendered": "New AI-Powered 3D Printing Can Help Surgeons Rehearse Procedures"}, "content": {"rendered": "\n<p>Researchers at Washington State University (WSU) unveiled a new AI-guided 3D printing technique that can help physicians print intricate replicas of human organs. Surgeons can then use these organ models to practice before performing the actual surgery, which gives doctors more tools to improve surgical results.&nbsp;</p>\n\n\n\n<p>The AI algorithm was trained on images and key attributes of human kidneys and prostates, including characteristics like weight, size, porosity, and vascular architecture. The algorithm works with 3D printers in a process of improvement. It helps find the best settings for three important parts of 3D printing: how accurate the model is, how light it is, and how long it takes to print.&nbsp;</p>\n\n\n\n<p>One of the co-authors of the <a href=\"https://onlinelibrary.wiley.com/doi/10.1002/admt.202400037\" target=\"_blank\" rel=\"noreferrer noopener\">study</a>, Kaiyan Qiu, an assistant professor of mechanical and materials engineering at WSU, said that AI optimizations can materially shorten the time it takes to create viable 3D models. The algorithm adjusts\u00a0 key 3D printing variables, including a printer\u2019s nozzle size and travel speed, the pressure printing materials are dispensed at, and the height of each printed layer.. It then guides the printer in creating an appropriate model for a specific use-case.</p>\n\n\n\n<p>\u201cFor pre-surgical organ models, we know surgeons will need high fidelity models that can be printed out quickly and with low labor intensity,\u201d Prof. Qiu said. \u201cWe imagine a scenario where a surgeon receives an MRI and CT scan [of a patient] in the morning. She has two hours to prepare everything for surgery. The AI can optimize the parameters, and print out a model organ in half-an-hour, and the surgeon can then spend the remaining time practicing [on the organ replica].\u201d&nbsp;</p>\n\n\n\n<p>Qiu and his co-author on the paper, WSU computer science professor, Jana Doppa, used a multi-objective Bayesian Optimization (BO) approach using <a href=\"https://botorch.org/\" target=\"_blank\" rel=\"noreferrer noopener\">BoTorch</a> to improve the efficiency and precision of the 3D printing process. The BO algorithm uses a probabilistic surrogate model to approximate the relationship between printing parameters and the quality of the printed organ models. This process captures uncertainties in the printing process, allowing for more robust optimization.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1000\" height=\"483\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/admt202400037-fig-0001-m.jpg\" alt=\"Flow-chart schematic of multi-objectiveBO assisted 3D-printing of presurgical organs models with three input parameters in tangent with four output parameters. The cycle starts with generating input values based on the current dataset of inputs and corresponding outputs through BO, which are used to produce printing pathways for direct-ink-writing (DIW). After the model is 3D-printed via DIW, image processing is applied to the model to reconstruct a mesh object. The mesh object is then adjusted for comparisons with the ideal model for measurements regarding positive and negative geometrical precisions. The time of model printing and porosity measurements are also calculated. Once all the output measurements are completed, their individual values are re-entered into the BO algorithm to yield new input parameters.\" class=\"wp-image-89211\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/admt202400037-fig-0001-m.jpg 1000w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/admt202400037-fig-0001-m-300x145.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/admt202400037-fig-0001-m-625x302.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/admt202400037-fig-0001-m-179x86.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/admt202400037-fig-0001-m-768x371.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/admt202400037-fig-0001-m-645x312.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/admt202400037-fig-0001-m-500x242.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/admt202400037-fig-0001-m-160x77.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/admt202400037-fig-0001-m-362x175.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/admt202400037-fig-0001-m-228x110.jpg 228w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The methodology for machine learning assisted 3D-printing is a four-step recursive process</em>.</figcaption></figure></div>\n\n\n<p>The researchers used <a href=\"https://www.nvidia.com/en-us/data-center/a40/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA A40</a> GPUs to train their AI model, and <a href=\"https://developer.nvidia.com/blog/getting-started-with-nvidia-instant-nerfs/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NGP Instant NeRF</a> to reconstruct a mesh object of the 3D printed model.</p>\n\n\n\n<p>The AI process the researchers have pioneered is also broadly generalizable. Besides printing model organs, the algorithm can guide printers to create prototypes of implantable medical devices, like pacemakers or stents. The technology can also be used to make models of airplane and robot parts, batteries, or even shoes customized for you.&nbsp;</p>\n\n\n\n<p>To learn more about this research, you can read this <a href=\"https://news.wsu.edu/press-release/2024/08/22/self-improving-ai-method-increases-3d-printing-efficiency/\" target=\"_blank\" rel=\"noreferrer noopener\">article</a>, or this <a href=\"https://onlinelibrary.wiley.com/doi/10.1002/admt.202400037\" target=\"_blank\" rel=\"noreferrer noopener\">research paper</a>.</p>\n\n\n\n<p><em>Featured image credit: <a href=\"https://news.wsu.edu/press-release/2024/08/22/self-improving-ai-method-increases-3d-printing-efficiency/\">Washington State University</a></em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Researchers at Washington State University (WSU) unveiled a new AI-guided 3D printing technique that can help physicians print intricate replicas of human organs. Surgeons can then use these organ models to practice before performing the actual surgery, which gives doctors more tools to improve surgical results.&nbsp; The AI algorithm was trained on images and key &hellip; <a href=\"https://developer.nvidia.com/blog/new-ai-powered-3d-printing-can-help-surgeons-rehearse-procedures/\">Continued</a></p>\n", "protected": false}, "author": 2156, "featured_media": 89210, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1489011", "discourse_permalink": "https://forums.developer.nvidia.com/t/new-ai-powered-3d-printing-can-help-surgeons-rehearse-procedures/307392", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [503], "tags": [3941, 3528, 453, 3102], "coauthors": [3876], "class_list": ["post-89206", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-simulation-modeling-design", "tag-ai-impact", "tag-computational-imaging", "tag-featured", "tag-nerf"], "acf": {"post_industry": ["Academia / Education", "Healthcare & Life Sciences", "HPC / Scientific Computing", "Retail / Consumer Packaged Goods"], "post_products": ["Ampere"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/AI-3D-Printing-WSU.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ncO", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89206"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2156"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89206"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89206/revisions"}], "predecessor-version": [{"id": 89274, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89206/revisions/89274"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89210"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89206"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89206"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89206"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89206"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89229, "date": "2024-09-19T10:50:46", "date_gmt": "2024-09-19T17:50:46", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89229"}, "modified": "2024-09-19T10:50:49", "modified_gmt": "2024-09-19T17:50:49", "slug": "just-released-torch-tensorrt-v2-4-0", "status": "publish", "type": "post", "link": "https://nvda.ws/3Xxr9gH", "title": {"rendered": "Just Released: Torch-TensorRT v2.4.0"}, "content": {"rendered": "\n<p>Includes C++ runtime support in Windows Support, Enhanced Dynamic Shape support in Converters, PyTorch 2.4, CUDA 12.4, TensorRT 10.1, Python 3.12.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Includes C++ runtime support in Windows Support, Enhanced Dynamic Shape support in Converters, PyTorch 2.4, CUDA 12.4, TensorRT 10.1, Python 3.12.</p>\n", "protected": false}, "author": 2102, "featured_media": 89230, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1488446", "discourse_permalink": "https://forums.developer.nvidia.com/t/just-released-torch-tensorrt-v2-4-0/307296", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3Xxr9gH", "_links_to_target": "_blank"}, "categories": [3110], "tags": [296, 453, 1958, 61, 369], "coauthors": [3838], "class_list": ["post-89229", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-ai-inference-microservices", "tag-featured", "tag-news", "tag-python", "tag-pytorch"], "acf": {"post_industry": ["General"], "post_products": ["CUDA", "TensorRT"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Torch-TensorRT-v2.4.0.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ndb", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89229"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2102"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89229"}], "version-history": [{"count": 1, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89229/revisions"}], "predecessor-version": [{"id": 89232, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89229/revisions/89232"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89230"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89229"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89229"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89229"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89229"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89133, "date": "2024-09-19T09:00:00", "date_gmt": "2024-09-19T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89133"}, "modified": "2024-10-09T12:59:39", "modified_gmt": "2024-10-09T19:59:39", "slug": "spotlight-slb-and-nvidia-collaborate-on-gen-ai-solutions-for-energy", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/spotlight-slb-and-nvidia-collaborate-on-gen-ai-solutions-for-energy/", "title": {"rendered": "Spotlight: SLB and NVIDIA Collaborate on Generative AI Solutions for Energy"}, "content": {"rendered": "\n<p>Global energy technology company<strong> </strong><a href=\"https://www.slb.com/news-and-insights/newsroom/press-release/2024/slb-and-nvidia-collaborate-to-develop-generative-ai-solutions-for-the-energy-sector\">SLB has announced</a> the next milestone in its long-standing collaboration with NVIDIA to develop and scale generative AI solutions for the energy industry.</p>\n\n\n\n<p>The collaboration accelerates the development and deployment of energy industry-specific <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI</a> <a href=\"https://blogs.nvidia.com/blog/what-are-foundation-models/\">foundation models</a> across SLB global platforms, including its Delfi digital platform and SLB&#8217;s new Lumi data and AI platform. This work leverages <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo</a>, part of the<a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\"> NVIDIA AI Enterprise</a> software platform, to develop custom generative AI that can be run in the data center, in any cloud, or at the edge.\u00a0</p>\n\n\n\n<p>SLB and NVIDIA are working together to build and optimize generative AI models for the specific needs and requirements of the data-intensive energy industry, including subsurface exploration, production operations, and data management. This will help unlock the full potential of generative AI for energy domain experts\u2015including researchers, scientists, engineers, and IT teams\u2015enabling them to interact with complex technical processes in new ways to drive higher value and lower carbon outcomes across the energy value chain.\u00a0</p>\n\n\n\n<h2 id=\"extending_more_than_15_years_of_collaboration_in_ai_and_hpc\"  class=\"wp-block-heading\">Extending more than 15 years of collaboration in AI and HPC<a href=\"#extending_more_than_15_years_of_collaboration_in_ai_and_hpc\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>SLB and NVIDIA first collaborated in 2008 with the innovative use of NVIDIA GPUs for subsurface imaging and reservoir simulation. The companies have worked closely over the years to optimize every generation of SLB&#8217;s high-performance compute technologies, available on its cloud-based Delfi platform with the latest NVIDIA accelerated computing platform.\u00a0</p>\n\n\n\n<p>Additionally, SLB and NVIDIA have worked with Dell Technologies to deliver both HPC and AI solutions to energy customers. These include solutions for SLB\u2019s Intersect high-resolution reservoir simulator, Omega geophysical data processing software, and the Delfi platform.\u00a0\u00a0</p>\n\n\n\n<h2 id=\"building_seismic_foundation_models_and_generative_ai_copilots\"  class=\"wp-block-heading\">Building seismic foundation models and generative AI copilots<a href=\"#building_seismic_foundation_models_and_generative_ai_copilots\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Energy companies are turning to generative AI for current and future energy systems, improving the balance of energy production with decarbonization goals. Industry-specific generative AI solutions help provide new insights from enterprise data to quickly solve complex challenges.</p>\n\n\n\n<p>SLB is an industry leader in seismic imaging and processing, providing advanced tools and applications for seismic interpretation and integration to enhance subsurface understanding. The company has\u00a0 developed a seismic foundation model based on (ViTs), trained from scratch using SLB-curated data, to be employed for various downstream interpretation tasks.</p>\n\n\n\n<p>In addition, SLB has created a coding co-pilot by fine-tuning a large language model (LLM) on proprietary internal data, significantly enhancing the customer experience of their software tools. By integrating this co-pilot across their software suite, SLB aims to facilitate ease of use for reservoir engineers and geoscientists. The incorporation of generative AI accelerates decision-making by enabling multiple AI models to run concurrently, providing business decision-makers with accurate, real-time insights. </p>\n\n\n\n<p>SLB and NVIDIA are collaborating on these foundational models to optimize performance and efficiency for enterprise-scale generative AI deployments by leveraging <a href=\"https://www.nvidia.com/en-us/ai/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NIM</a>. In addition to optimizing foundational models, SLB is offering its customers seamless access to NeMo for use in their technical workflows, improving performance, optimizing processes and driving innovation.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With generative AI, energy companies can improve the exploration, production, and delivery of energy resources with sustainable operations. SLB is building custom generative AI models, using NVIDIA NeMo and NIM, to help scientists and engineers in the energy industry leverage enterprise data and produce intelligence for subsurface understanding. This AI factory enables breakthroughs in customer engagement, operational efficiency, and revenue growth.</p>\n\n\n\n<p>Learn more about the <a href=\"https://www.slb.com/resource-library/features/2024/how-slb-and-nvidia-are-connecting-the-dots-between-ai-and-energy\">SLB and NVIDIA collaboration</a> and <a href=\"https://www.nvidia.com/en-us/industries/energy/oil-gas-operations/\">AI and HPC for subsurface operations</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Global energy technology company SLB has announced the next milestone in its long-standing collaboration with NVIDIA to develop and scale generative AI solutions for the energy industry. The collaboration accelerates the development and deployment of energy industry-specific generative AI foundation models across SLB global platforms, including its Delfi digital platform and SLB&#8217;s new Lumi data &hellip; <a href=\"https://developer.nvidia.com/blog/spotlight-slb-and-nvidia-collaborate-on-gen-ai-solutions-for-energy/\">Continued</a></p>\n", "protected": false}, "author": 2306, "featured_media": 89203, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1488419", "discourse_permalink": "https://forums.developer.nvidia.com/t/spotlight-slb-and-nvidia-collaborate-on-gen-ai-solutions-for-energy/307288", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 503, 1903], "tags": [1913, 453, 2932], "coauthors": [4039], "class_list": ["post-89133", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "category-simulation-modeling-design", "category-features", "tag-climate-weather-ocean-modeling", "tag-featured", "tag-large-language-models"], "acf": {"post_industry": ["Energy"], "post_products": ["AI Enterprise", "NeMo Microservices", "NIM"], "post_learning_levels": ["General Interest"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/professionals-viewing-flowchart.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nbD", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89133"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2306"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89133"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89133/revisions"}], "predecessor-version": [{"id": 89421, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89133/revisions/89421"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89203"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89133"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89133"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89133"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89133"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89142, "date": "2024-09-18T15:48:43", "date_gmt": "2024-09-18T22:48:43", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89142"}, "modified": "2024-09-19T13:17:19", "modified_gmt": "2024-09-19T20:17:19", "slug": "quickly-voice-your-apps-with-nvidia-nim-microservices-for-speech-and-translation", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/quickly-voice-your-apps-with-nvidia-nim-microservices-for-speech-and-translation/", "title": {"rendered": "Quickly Voice Your Apps with NVIDIA NIM Microservices for Speech and Translation"}, "content": {"rendered": "\n<p><a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a>, part of <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, provides containers to self-host GPU-accelerated inferencing microservices for pretrained and customized AI models across clouds, data centers, and workstations. NIM microservices for <a href=\"https://build.nvidia.com/explore/speech\">speech and translation</a> are now available.&nbsp;</p>\n\n\n\n<p>The new speech and translation microservices leverage <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/riva/\">NVIDIA Riva</a> and provide <a href=\"https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.html\">automatic speech recognition (ASR)</a>, <a href=\"https://docs.nvidia.com/deeplearning/riva/user-guide/docs/translation/translation-overview.html\">neural machine translation (NMT)</a>, and <a href=\"https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-overview.html\">text-to-speech (TTS)</a> services.&nbsp;</p>\n\n\n\n<p>Integrating multilingual voice capabilities into your applications with NVIDIA speech and translation NIM microservices offers beyond just advanced ASR, NMT, and TTS for enhanced global user experience and accessibility. Whether you\u2019re building customer service bots, interactive voice assistants, or multilingual content platforms, these NIM microservices are optimized for high-performance AI inference at scale, and provide accuracy and flexibility to voice-enable your applications with minimal development effort.&nbsp;</p>\n\n\n\n<p>In this post, you\u2019ll learn how to perform basic inference tasks\u2014such as transcribing speech, translating text, and generating synthetic voices\u2014directly through your browser by using interactive speech and translation model interfaces in the <a href=\"https://build.nvidia.com/explore/discover\">NVIDIA API catalog</a>. You\u2019ll also learn how to run these flexible microservices on your infrastructure, access them through APIs, and seamlessly integrate them into your applications.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/-g8wE3zBmCQ?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Watch a demo on deploying speech NIM microservices and connecting them to a simple retrieval-augmented generation pipeline for voice-to-voice interaction</em></figcaption></figure>\n\n\n\n<h2 id=\"quick_inference_with_speech_and_translation_nim&nbsp;\"  class=\"wp-block-heading\">Quick inference with speech and translation NIM&nbsp;<a href=\"#quick_inference_with_speech_and_translation_nim&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Speech NIM microservices are available in the <a href=\"https://build.nvidia.com/explore/discover\">API catalog</a>, where you can easily perform inference tasks using the interactive browser UI. With the click of a button, you can transcribe English speech, translate text between over 30 languages, or convert text into natural-sounding speech. The API catalog provides a convenient starting point for exploring the basic capabilities of speech and translation NIM microservices.</p>\n\n\n\n<p>The true power of these tools lies in their flexibility to be deployed wherever your data resides\u2014anywhere. You can run these microservices on any compatible NVIDIA GPU, access them through APIs, and seamlessly integrate them into your applications. This versatility enables you to deploy speech NIM microservices in environments ranging from local workstations to cloud and data center infrastructure, providing scalable options tailored to your needs.</p>\n\n\n\n<h2 id=\"running_speech_and_translation_nim_microservices_with_nvidia_riva_python_clients\"  class=\"wp-block-heading\">Running speech and translation NIM microservices with NVIDIA Riva Python clients<a href=\"#running_speech_and_translation_nim_microservices_with_nvidia_riva_python_clients\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This section guides you through cloning the <a href=\"https://github.com/nvidia-riva/python-clients\">nvidia-riva / python-clients</a> GitHub repo and using provided scripts to run simple inference tasks on the NVIDIA API catalog Riva endpoint, located at <code>grpc.nvcf.nvidia.com:443</code>.&nbsp;</p>\n\n\n\n<p>To quickly test the speech NIM microservices, navigate to a NIM landing page from the <a href=\"https://build.nvidia.com/explore/speech\">API catalog</a> and click the Try API tab. Note that to use these commands, you\u2019ll need an NVIDIA API key. If you don&#8217;t already have one, simply click the Get API Key button in the far right corner under the Try API tab. Read on for some examples of what you can do.</p>\n\n\n\n<h3 id=\"transcribing_audio_in_streaming_mode\"  class=\"wp-block-heading\">Transcribing audio in streaming mode<a href=\"#transcribing_audio_in_streaming_mode\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Run the following command to transcribe an audio file in real time.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\npython python-clients/scripts/asr/transcribe_file.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id &quot;1598d209-5e27-4d3c-8079-4751568b1081&quot; \\\n    --metadata &quot;authorization&quot; &quot;Bearer \n$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC&quot; \\\n    --language-code en-US \\\n    --input-file &lt;path_to_audio_file&gt;\n</pre></div>\n\n\n<h3 id=\"translating_text_from_english_to_german\"  class=\"wp-block-heading\">Translating text from English to German<a href=\"#translating_text_from_english_to_german\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The following command translates the English sentence, \u201cThis is an example text for Riva text translation\u201d into German, \u201cDies ist ein Beispieltext f\u00fcr Riva Text\u00fcbersetzung.\u201d</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\npython python-clients/scripts/nmt/nmt.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id &quot;647147c1-9c23-496c-8304-2e29e7574510&quot; \\\n    --metadata &quot;authorization&quot; &quot;Bearer \n$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC&quot; \\\n    --text &quot;This is an example text for Riva text translation&quot; \\\n    --source-language-code en \\\n    --target-language-code de\n</pre></div>\n\n\n<h3 id=\"generating_synthetic_speech\"  class=\"wp-block-heading\">Generating synthetic speech<a href=\"#generating_synthetic_speech\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The following command converts the text, \u201cThis audio is generated from NVIDIA\u2019s text-to-speech model\u201d into speech and saves the audio output as <code>audio.wav</code>. This is particularly useful if you\u2019re working in a terminal on a remote system and the audio output cannot easily be routed to your local microphone.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\npython python-clients/scripts/tts/talk.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id &quot;0149dedb-2be8-4195-b9a0-e57e0e14f972&quot;  \\\n    --metadata authorization &quot;Bearer \n$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC&quot; \\\n    --text &quot;this audio is generated from nvidia&#039;s text-to-speech model&quot; \\\n    --voice &quot;English-US.Female-1&quot; \\\n    --output audio.wav\n</pre></div>\n\n\n<h2 id=\"running_speech_nim_locally_with_docker\"  class=\"wp-block-heading\">Running speech NIM locally with Docker<a href=\"#running_speech_nim_locally_with_docker\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>If you have access to <a href=\"https://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix.html\">advanced NVIDIA data center GPUs</a>, you can run the speech NIM microservices locally by following the instructions provided under the Docker tabs of the <a href=\"https://build.nvidia.com/nvidia/parakeet-ctc-1_1b-asr/docker\">ASR</a>, <a href=\"https://build.nvidia.com/nvidia/megatron-1b-nmt/docker\">NMT</a>, and <a href=\"https://build.nvidia.com/nvidia/fastpitch-hifigan-tts/docker\">TTS</a> NIM landing pages. Alternatively, you can refer to the more detailed Getting Started documentation for <a href=\"https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html\">ASR</a>, <a href=\"https://docs.nvidia.com/nim/riva/nmt/latest/getting-started.html\">NMT</a>, and <a href=\"https://docs.nvidia.com/nim/riva/tts/latest/getting-started.html\">TTS</a> that explains each <code>docker run</code> parameter and guides you through the setup process.&nbsp;</p>\n\n\n\n<p>An NGC API key is required to pull NIM microservices from the NVIDIA container registry (<code>nvcr.io</code>) and run them on your own system. The NVIDIA API key that you generated in the previous section should work for this purpose. Alternatively, navigate to an <a href=\"https://build.nvidia.com/nvidia/parakeet-ctc-1_1b-asr/docker\">ASR</a>, <a href=\"https://build.nvidia.com/nvidia/megatron-1b-nmt/docker\">NMT</a>, or <a href=\"https://build.nvidia.com/nvidia/fastpitch-hifigan-tts/docker\">TTS</a> NIM landing page, select the Docker tab, and click on Get API Key.&nbsp;</p>\n\n\n\n<h2 id=\"integrating_speech_nim_microservices_with_a_rag_pipeline\"  class=\"wp-block-heading\">Integrating speech NIM microservices with a RAG pipeline<a href=\"#integrating_speech_nim_microservices_with_a_rag_pipeline\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This section covers launching the ASR and TTS NIM microservices on your system and connecting them to a <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG/examples/basic_rag/langchain\">basic retrieval-augmented generation (RAG) pipeline</a> from the NVIDIA <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/tree/main\">Generative AI Examples</a> GitHub repo. This setup enables uploading documents into a knowledge base, asking questions about them verbally, and obtaining answers in synthesized, natural-sounding voices.</p>\n\n\n\n<h3 id=\"set_up_the_environment\"  class=\"wp-block-heading\">Set up the environment<a href=\"#set_up_the_environment\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Before launching the NIM microservices, export your NGC API key to your system as <code>NGC_API_KEY</code>, either directly in the terminal or through your environmental source file.&nbsp;</p>\n\n\n\n<p>Next, log into the NVIDIA Docker container registry:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\necho &quot;$NGC_API_KEY&quot; | docker login nvcr.io --username &#039;$oauthtoken&#039; \n--password-stdin\n</pre></div>\n\n\n<p>Then, create a <code>LOCAL_NIM_CACHE</code> directory:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nexport LOCAL_NIM_CACHE=&lt;path/to/nim_cache&gt;\nmkdir -p &quot;$LOCAL_NIM_CACHE&quot;\nchmod 777 $LOCAL_NIM_CACHE\n</pre></div>\n\n\n<p>You\u2019ll store your models in this directory and mount it to the NIM containers. Make sure not to skip the <code>chmod 777</code> command. Otherwise, the NIM Docker container will lack permission to download the model files to the <code>LOCAL_NIM_CACHE</code> directory.&nbsp;</p>\n\n\n\n<p>By default, the speech NIM microservices download the model files to a location that\u2019s only accessible inside a running container. If you intend to run only one of the Riva services at a time, you should either leave <code>LOCAL_NIM_CACHE</code> unspecified, clear your <code>LOCAL_NIM_CACHE</code> directory between stopping one NIM container and starting another one, or specify a different <code>LOCAL_NIM_CACHE</code> directory for each speech NIM. Here the same <code>LOCAL_NIM_CACHE</code> is used for both ASR and TTS so as to run both services simultaneously.&nbsp;</p>\n\n\n\n<h3 id=\"launch_the_asr_nim\"  class=\"wp-block-heading\">Launch the ASR NIM<a href=\"#launch_the_asr_nim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Use the following script to start the ASR NIM:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nexport CONTAINER_NAME=parakeet-ctc-1.1b-asr\n\ndocker run -it --rm --name=$CONTAINER_NAME \\\n  --runtime=nvidia \\\n  --gpus &#039;&quot;device=0&quot;&#039; \\\n  --shm-size=8GB \\\n  -e NGC_API_KEY=$NGC_API_KEY \\\n  -e NIM_MANIFEST_PROFILE=9136dd64-4777-11ef-9f27-37cfd56fa6ee \\\n  -e NIM_HTTP_API_PORT=9000 \\\n  -e NIM_GRPC_API_PORT=50051 \\\n  -p 9000:9000 \\\n  -p 50051:50051 \\\n  -v &quot;$LOCAL_NIM_CACHE:/home/nvs/.cache/nim&quot; \\\n  nvcr.io/nim/nvidia/parakeet-ctc-1.1b-asr:1.0.0\n</pre></div>\n\n\n<p>If the <code>LOCAL_NIM_CACHE</code> directory is empty (such as the first time you execute this command), it may take 20-30 minutes to complete. During this time, NIM will download the acoustic (offline, streaming optimized for latency, and streaming optimized for throughput) and punctuation models as .tar.gz files, then uncompress them within the container.&nbsp;</p>\n\n\n\n<p>You might want to change the <code>NIM_MANIFEST_PROFILE</code> parameter, depending on your GPU type. By default, the NIM container downloads ONNX-formatted model files that run on any sufficiently advanced NVIDIA GPU. However, if you change this parameter appropriately when starting the ASR or TTS NIM, you&#8217;ll instead download NVIDIA TensorRT-formatted model files that have been optimized for NVIDIA H100, A100, or L40 GPUs. This results in faster inference on one of the supported GPUs. Optimized NIM support for additional GPU types is forthcoming.&nbsp;</p>\n\n\n\n<p>Table 1 shows the supported <code>NIM_MANIFEST_PROFILE</code> values for the ASR NIM for the Parakeet CTC Riva 1.1B En-US model:&nbsp;&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table class=\"has-fixed-layout\"><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>GPU</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>NIM_MANIFEST_PROFILE</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Generic</td><td class=\"has-text-align-center\" data-align=\"center\">9136dd64-4777-11ef-9f27-37cfd56fa6ee</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">NVIDIA H100</td><td class=\"has-text-align-center\" data-align=\"center\">7f0287aa-35d0-11ef-9bba-57fc54315ba3</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">NVIDIA A100</td><td class=\"has-text-align-center\" data-align=\"center\">32397eba-43f4-11ef-b63c-1b565d7d9a02</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">NVIDIA L40</td><td class=\"has-text-align-center\" data-align=\"center\">40d7e326-43f4-11ef-87a2-239b5c506ca7</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Supported <code>NIM_MANIFEST_PROFILE</code> values for the ASR NIM for the Parakeet CTC Riva 1.1B En-US model</em></figcaption></figure>\n\n\n\n<p>You can also find this table in the <a href=\"https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models\">Supported Models</a> section of the Getting Started page in the ASR NIM documentation. Note the slight difference between the model name and the container name. Unlike some other NIM microservices, the speech and translation NIM microservices do not support the <a href=\"https://docs.nvidia.com/nim/large-language-models/latest/utilities.html#list-available-model-profiles\">list-model-profiles</a> utility, meaning you can\u2019t access the list of valid <code>NIM_MANIFEST_PROFILE</code> values through the <code>docker</code> CLI.&nbsp;</p>\n\n\n\n<h3 id=\"launch_the_tts_nim\"  class=\"wp-block-heading\">Launch the TTS NIM<a href=\"#launch_the_tts_nim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>After stopping the ASR NIM, either with Ctrl+C in the same terminal or with <code>docker stop</code> or <code>docker container stop</code> in a different terminal, launch the TTS NIM:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nexport CONTAINER_NAME=fastpitch-hifigan-tts\n\ndocker run -it --rm --name=$CONTAINER_NAME \\\n  --runtime=nvidia \\\n  --gpus &#039;&quot;device=0&quot;&#039; \\\n  --shm-size=8GB \\\n  -e NGC_API_KEY=$NGC_API_KEY \\\n  -e NIM_MANIFEST_PROFILE=3c8ee3ee-477f-11ef-aa12-1b4e6406fad5 \\\n  -e NIM_HTTP_API_PORT=9000 \\\n  -e NIM_GRPC_API_PORT=50051 \\\n  -p 9000:9000 \\\n  -p 50051:50051 \\\n  -v &quot;$LOCAL_NIM_CACHE:/home/nvs/.cache/nim&quot; \\\n  nvcr.io/nim/nvidia/fastpitch-hifigan-tts:1.0.0\n</pre></div>\n\n\n<p>From Table 2, choose a suitable <code>NIM_MANIFEST_PROFILE</code> value for the FastPitch HifiGAN Riva En-US model:&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table class=\"has-fixed-layout\"><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>GPU</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>NIM_MANIFEST_PROFILE</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Generic</td><td class=\"has-text-align-center\" data-align=\"center\">3c8ee3ee-477f-11ef-aa12-1b4e6406fad5</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">NVIDIA H100</td><td class=\"has-text-align-center\" data-align=\"center\">bbce2a3a-4337-11ef-84fe-e7f5af9cc9af</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">NVIDIA A100</td><td class=\"has-text-align-center\" data-align=\"center\">5ae1da8e-43f3-11ef-9505-e752a24fdc67</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">NVIDIA L40</td><td class=\"has-text-align-center\" data-align=\"center\">713858f8-43f3-11ef-86ee-4f6374fce1aa</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Supported NIM_MANIFEST_PROFILE values for TTS NIM for the FastPitch HifiGAN Riva En-US model</em></figcaption></figure>\n\n\n\n<p>You can also find this table in the <a href=\"https://docs.nvidia.com/nim/riva/tts/latest/getting-started.html#supported-models\">Supported Models</a> section of the Getting Started page in the TTS NIM documentation. Note the slight difference between the model name and the container name.&nbsp;</p>\n\n\n\n<p>Starting the TTS NIM should be much faster than starting the ASR NIM, because the constituent models take up far less space in total. However, because we\u2019re using the same <code>LOCAL_NIM_CACHE</code> directory for both the ASR and TTS NIMs, the TTS NIM will launch both the ASR and TTS models.</p>\n\n\n\n<h3 id=\"connect_the_speech_nim_microservices_to_the_rag_pipeline\"  class=\"wp-block-heading\">Connect the speech NIM microservices to the RAG pipeline<a href=\"#connect_the_speech_nim_microservices_to_the_rag_pipeline\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The RAG web app is part of the NVIDIA <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/tree/main\">Generative AI Examples</a> GitHub repo. After cloning the repo, the main thing you need to do is edit&nbsp; <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/blob/main/RAG/examples/basic_rag/langchain/docker-compose.yaml\">RAG/examples/basic_rag/langchain/docker-compose.yaml</a>. Set the <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/blob/main/RAG/examples/basic_rag/langchain/docker-compose.yaml#L76\"><code>PLAYGROUND_MODE</code></a> for the <code>rag-playground</code> service to <code>speech</code> and add the following <code>environment</code> variables to that service:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nservices:\n  ...\n  rag-playground:\n    ...\n    environment: \n      ...\n      RIVA_API_URI: &lt;riva-ip-address&gt;:50051\n      TTS_SAMPLE_RATE: 48000\n</pre></div>\n\n\n<p>If you wish to set <code>RIVA_API_URI</code> with an overridable default value in the <code>docker-compose</code> file (in the format shown below), do not put quotation marks around the default value. If you do, the Python <code>os</code> module will include them in the string defining the Riva URI, which will cause problems.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nRIVA_API_URI: ${RIVA_API_URI:-&lt;riva-ip-address&gt;:50051}\n</pre></div>\n\n\n<p>Even if you&#8217;re running the RAG example and the speech NIM microservices on the same machine, you need to specify the IP address or permanent hostname; <code>localhost</code> won&#8217;t work here.</p>\n\n\n\n<p>See more detailed instructions on <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/blob/main/docs/riva-asr-tts.md\">adding ASR and TTS capabilities to a RAG pipeline</a> in the Generative AI Examples repo.&nbsp;</p>\n\n\n\n<p>Once the <code>docker-compose</code> file is edited appropriately, run the following command in the same directory to start the container network.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\ndocker compose up -d --build\n</pre></div>\n\n\n<p>Also verify that each container within the network is running:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\ndocker ps --format &quot;table {{.ID}}\\t{{.Names}}\\t{{.Status}}&quot;\n</pre></div>\n\n\n<h3 id=\"test_the_speech_nim_and_rag_integration\"  class=\"wp-block-heading\">Test the speech NIM and RAG integration<a href=\"#test_the_speech_nim_and_rag_integration\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To test your setup, navigate to <code>localhost:8090</code> in your browser. If you\u2019re running the RAG app on a remote system without port forwarding, use <code>&lt;remote-IP-address&gt;:8090</code> instead. The interface (Figures 1 and 2) enables you to query the <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language model (LLM)</a> by text or voice, and receive a spoken response. If the models are available, you can change the ASR and TTS languages using menu options.&nbsp;</p>\n\n\n\n<p>For document-based queries, click on the Knowledge Base tab near the top right of the page. Here, you can upload PDF, plain text, or markdown files. The contents are embedded as multidimensional vectors and indexed in a vector database, enabling the LLM to answer questions based on this new information.&nbsp;</p>\n\n\n\n<p>To provide an example, upload a PDF version of the recent press release, <a href=\"https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing\">NVIDIA Blackwell Platform Arrives to Power a New Era of Computing</a>. Despite being published over five months ago, the default LLM for this sample web app is not pretrained of this information.</p>\n\n\n\n<p>Next, return to the Converse tab, click the microphone button, and ask the app, \u201cHow many transistors does the NVIDIA Blackwell GPU contain?\u201d Without the knowledge base, the LLM is unable to provide the correct answer (Figure 1).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"987\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1.png\" alt=\"Screenshot showing the testing of the speech NIM and RAG pipeline without an knowledge base.\" class=\"wp-image-89191\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1-300x148.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1-625x309.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1-768x379.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1-1536x758.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1-645x318.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1-500x247.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1-160x79.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1-362x179.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1-223x110.png 223w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-without-knowledge-base-1-1024x506.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Testing the NIM and RAG pipeline without a knowledge base</em></figcaption></figure></div>\n\n\n<p>Now, with the knowledge base active, ask the same question again. This time, leveraging the full RAG pipeline, the LLM answers correctly based on the newly embedded information (Figure 2).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"987\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1.png\" alt=\"Screenshot showing the testing of the speech NIM and RAG pipeline using an active knowledge base.\" class=\"wp-image-89194\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1-300x148.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1-625x309.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1-768x379.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1-1536x758.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1-645x318.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1-500x247.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1-160x79.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1-362x179.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1-223x110.png 223w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/testing-nvidia-nim-rag-pipeline-with-active-knowledge-base-1-1024x506.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Testing the NIM and RAG pipeline with an active knowledge base</em></figcaption></figure></div>\n\n\n<h2 id=\"get_started_adding_multilingual_speech_ai_to_your_apps\"  class=\"wp-block-heading\">Get started adding multilingual speech AI to your apps<a href=\"#get_started_adding_multilingual_speech_ai_to_your_apps\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In this post, you\u2019ve learned to set up NVIDIA speech and translation NIM microservices and test them directly through your browser by using the interactive speech and translation model interfaces. You have dived into the flexibility of deploying NIM speech and translation microservices and integrating them into a RAG pipeline for document-based knowledge retrieval with synthesized voice responses.&nbsp;</p>\n\n\n\n<p>Ready to add powerful multilingual speech AI to your own applications? Try <a href=\"https://build.nvidia.com/explore/speech\">speech NIM microservices</a> to experience the ease of integrating ASR, NMT, and TTS into your pipelines. Explore the APIs and see how these NIM microservices can transform your applications to scalable, real-time voice services for worldwide users.</p>\n\n\n\n<p></p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA NIM, part of NVIDIA AI Enterprise, provides containers to self-host GPU-accelerated inferencing microservices for pretrained and customized AI models across clouds, data centers, and workstations. NIM microservices for speech and translation are now available.&nbsp; The new speech and translation microservices leverage NVIDIA Riva and provide automatic speech recognition (ASR), neural machine translation (NMT), and &hellip; <a href=\"https://developer.nvidia.com/blog/quickly-voice-your-apps-with-nvidia-nim-microservices-for-speech-and-translation/\">Continued</a></p>\n", "protected": false}, "author": 1553, "featured_media": 89147, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1487853", "discourse_permalink": "https://forums.developer.nvidia.com/t/quickly-voice-your-apps-with-nvidia-nim-microservices-for-speech-and-translation/307192", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110, 1903], "tags": [296, 453, 3166], "coauthors": [3057, 2519], "class_list": ["post-89142", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "category-features", "tag-ai-inference-microservices", "tag-featured", "tag-speech-ai"], "acf": {"post_industry": ["General"], "post_products": ["NIM", "Riva"], "post_learning_levels": ["Intermediate Technical", "Advanced Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/conversational-ai-graphic-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nbM", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Conversational AI", "link": "https://developer.nvidia.com/blog/category/conversational-ai/", "id": 1050}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89142"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1553"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89142"}], "version-history": [{"count": 18, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89142/revisions"}], "predecessor-version": [{"id": 89238, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89142/revisions/89238"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89147"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89142"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89142"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89142"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89142"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89179, "date": "2024-09-18T11:06:44", "date_gmt": "2024-09-18T18:06:44", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89179"}, "modified": "2024-09-19T12:28:59", "modified_gmt": "2024-09-19T19:28:59", "slug": "event-developer-day-for-financial-services", "status": "publish", "type": "post", "link": "https://nvda.ws/47wJ2R8", "title": {"rendered": "Event: Developer Day for Financial Services"}, "content": {"rendered": "\n<p>Join this virtual developer day to learn how AI and Machine Learning can revolutionize fraud detection and financial crime prevention.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Join this virtual developer day to learn how AI and Machine Learning can revolutionize fraud detection and financial crime prevention.</p>\n", "protected": false}, "author": 1071, "featured_media": 89181, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1487789", "discourse_permalink": "https://forums.developer.nvidia.com/t/event-developer-day-for-financial-services/307176", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/47wJ2R8", "_links_to_target": "_blank"}, "categories": [1464, 696], "tags": [453, 3550, 276, 1958], "coauthors": [2240], "class_list": ["post-89179", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-cybersecurity", "category-data-science", "tag-featured", "tag-fraud-detection", "tag-graph-analytics", "tag-news"], "acf": {"post_industry": ["Financial Services"], "post_products": ["cuGraph", "RAPIDS", "Triton Inference Server"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Financial-Services-Developer-Day-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-ncn", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89179"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1071"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89179"}], "version-history": [{"count": 1, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89179/revisions"}], "predecessor-version": [{"id": 89183, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89179/revisions/89183"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89181"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89179"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89179"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89179"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89179"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89054, "date": "2024-09-18T10:03:46", "date_gmt": "2024-09-18T17:03:46", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89054"}, "modified": "2024-09-19T12:29:43", "modified_gmt": "2024-09-19T19:29:43", "slug": "nvidia-presents-ai-security-expertise-at-leading-cybersecurity-conferences", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-presents-ai-security-expertise-at-leading-cybersecurity-conferences/", "title": {"rendered": "NVIDIA Presents AI Security Expertise at Leading Cybersecurity Conferences"}, "content": {"rendered": "\n<p>Each August, tens of thousands of security professionals attend the cutting-edge security conferences Black Hat USA and DEF CON. This year, NVIDIA AI security experts joined these events to share our work and learn from other members of the community.</p>\n\n\n\n<p>This post provides an overview of these contributions, including a keynote on the rapidly evolving AI landscape, adversarial machine learning training, presentations on LLM security, and more. This work helps to provide the security community with the knowledge necessary to effectively deploy AI systems with a security mindset.&nbsp;</p>\n\n\n\n<h2 id=\"nvidia_at_black_hat_usa_2024\"  class=\"wp-block-heading\">NVIDIA at Black Hat USA 2024<a href=\"#nvidia_at_black_hat_usa_2024\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.blackhat.com/us-24/\">Black Hat</a> is an internationally recognized cybersecurity event that provides technical, relevant information security research. This year, there was a growing buzz around both the possible applications of generative AI tools in security ecosystems, as well as the security of AI deployments themselves.&nbsp;</p>\n\n\n\n<p>At the AI Summit keynote, Bartley Richardson, director of Cybersecurity AI at NVIDIA, joined WWT CEO Jim Kavanaugh to share insights on the rapidly evolving AI landscape, particularly how AI and automation can transform how to tackle today\u2019s cyber challenges. In other sessions, experts from NVIDIA and its partners discussed both how AI has revolutionized security postures and techniques around securing AI systems.</p>\n\n\n\n<p>Many Black Hat briefings echoed a common sentiment: the deployment of AI tools and systems inherently requires a measured approach to security, and implementing effective trust boundaries and access controls remains as important as ever.&nbsp;</p>\n\n\n\n<p>In a panel on AI Safety, NVIDIA Senior Director of AI and Legal Ethics Nikki Pope joined practitioners from Microsoft and Google to discuss the complex landscape of AI safety, common myths and pitfalls, and the responsibilities of anyone charged with deploying safe and responsible AI. NVIDIA VP of Software Product Security Daniel Rohrer shared NVIDIA\u2019s perspective on the unique challenges that come with securing AI data centers in a session hosted by Trend Micro.&nbsp;</p>\n\n\n\n<h2 id=\"nvidia_at_def_con_32\"  class=\"wp-block-heading\">NVIDIA at DEF CON 32<a href=\"#nvidia_at_def_con_32\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://defcon.org/\">DEF CON</a> is the world\u2019s largest hacker conference, with dozens of villages where people discuss security\u2014and compete doing real-time hacking\u2014within focused contexts such as network data, social engineering, cars, and satellites. Many NVIDIA researchers have supported the DEF CON AI Village, which for the past 2 years has hosted popular live <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language model (LLM)</a> red-teaming events.&nbsp;</p>\n\n\n\n<p>This year, AI remained a central theme both in the <a href=\"https://aivillage.org/\">AI Village</a> and in the <a href=\"https://aicyberchallenge.com/\">AI Cyber Challenge (AIxCC)</a>. The AI Village once again hosted a Generative Red Team challenge, where participants attacked an LLM, which led to real-time improvements to the model\u2019s safety guardrails and model card. Nikki Pope delivered a keynote emphasizing the critical role of algorithmic fairness and safety in AI systems.&nbsp;</p>\n\n\n\n<p>At the AIxCC, hosted by the Defense Advanced Research Projects Agency (DARPA), red and blue teams alike convened to build autonomous agents that scanned code bases to identify vulnerabilities and implement exploits. The challenge was built on the premise that there are more security vulnerabilities than there are people able to identify them, and that AI-powered tools in this space can continue to supplement and accelerate security research.</p>\n\n\n\n<p>The <a href=\"https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/\">NVIDIA AI Red Team</a> brought our own expertise to these important events, sharing our knowledge with the community through trainings, AI security talks, and demo labs of our open source tooling.</p>\n\n\n\n<h2 id=\"adversarial_machine_learning_training\"  class=\"wp-block-heading\">Adversarial machine learning training<a href=\"#adversarial_machine_learning_training\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This year at Black Hat, NVIDIA and Dreadnode delivered a two-day training on machine learning (ML). The training covered the techniques of assessing security risks against ML models, as well as the implementation and execution of specific attacks.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"692\" height=\"339\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/becca-lynch-black-hat-ml-session.png\" alt=\"Students seated at tables with laptops listen to remarks from the instructor at the front of the room, delivering remarks on a slide titled \u2018Gradient Descent\u2019.\n\" class=\"wp-image-89082\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/becca-lynch-black-hat-ml-session.png 692w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/becca-lynch-black-hat-ml-session-300x147.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/becca-lynch-black-hat-ml-session-625x306.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/becca-lynch-black-hat-ml-session-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/becca-lynch-black-hat-ml-session-645x316.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/becca-lynch-black-hat-ml-session-500x245.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/becca-lynch-black-hat-ml-session-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/becca-lynch-black-hat-ml-session-362x177.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/becca-lynch-black-hat-ml-session-225x110.png 225w\" sizes=\"(max-width: 692px) 100vw, 692px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. NVIDIA Security Researcher Becca Lynch instructing attendees on machine learning at Black Hat</em></em></figcaption></figure>\n\n\n\n<p>Participants received instruction on the foundations of ML models and the attacks against them before moving to self-paced labs where they practiced executing these attacks. The topics were broken down into the following sections:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Introduction:</strong> Learning the basics of PyTorch and ML models</li>\n\n\n\n<li><strong>Evasion:</strong> Crafting specific inputs designed to deceive a model into making incorrect predictions or classifications</li>\n\n\n\n<li><strong>Extraction:</strong> Reverse engineering a model\u2019s underlying parameters and architecture by exploiting access to the model outputs</li>\n\n\n\n<li><strong>Assessments:</strong> Understanding tools and frameworks available for executing attacks and standardized methods of assessing model security</li>\n\n\n\n<li><strong>Inversion:</strong> Exploiting model endpoints to reconstruct or infer potentially sensitive input / training data</li>\n\n\n\n<li><strong>Poisoning:</strong> Injecting malicious input into the training dataset to corrupt the model\u2019s learning process</li>\n\n\n\n<li><strong>LLMs:</strong> Learning about prompt injection and how many of the previously mentioned attacks can be applied against LLMs</li>\n</ul>\n\n\n\n<p>The practical labs helped students gain experience executing attacks, including crafting images that led to misclassifications against convolutional neural networks, membership inference attacks to extract model training data, poisoning model training data to generate misclassifications at test time, prompt injection against LLMs, and more.&nbsp;</p>\n\n\n\n<p>Participants in the course ranged from data scientists and security engineers to CISOs. They left armed with both a grounded knowledge of ML and attacks against ML systems, and a framework for applying an adversarial mindset within their organizations. These are crucial components to shaping effective defensive strategies.</p>\n\n\n\n<p>Check out the self-guided version of this course, <a href=\"https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-DS-03+V1\">Exploring Adversarial Machine Learning</a>, available through the NVIDIA Deep Learning Institute.</p>\n\n\n\n<h2 id=\"focus_on_llm_security\"  class=\"wp-block-heading\">Focus on LLM security<a href=\"#focus_on_llm_security\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA Principal Security Architect <a href=\"https://www.blackhat.com/us-24/briefings/schedule/speakers.html#richard-harang-35501\">Rich Harang</a> presented his talk, <a href=\"https://www.blackhat.com/us-24/briefings/schedule/#practical-llm-security-takeaways-from-a-year-in-the-trenches-39468\">Practical LLM Security: Takeaways From a Year in the Trenches</a> to a keen Black Hat audience. The focus was on grounding LLM security in a familiar application security framework and leaving audience members with a foundational understanding of the full threat topology around LLM applications.\u00a0</p>\n\n\n\n<p>The talk centered on the security issues that arise with <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation (RAG)</a> LLM architectures. As many enterprises are adopting LLM applications in their environment, RAG systems provide the model with the most up-to-date data and context available by retrieving data from a document store at the time of each query.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"755\" height=\"382\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/rich-harang-black-hat-session.png\" alt=\"Speaker Rich Harang delivers remarks behind a Black Hat podium. The screen reads \u201cCase study: Info leaks via guardrails\u201d and shows an architectural diagram of a RAG system showing the potential for data leakage from sources provided to the system.\" class=\"wp-image-89083\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/rich-harang-black-hat-session.png 755w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/rich-harang-black-hat-session-300x152.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/rich-harang-black-hat-session-625x316.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/rich-harang-black-hat-session-179x91.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/rich-harang-black-hat-session-645x326.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/rich-harang-black-hat-session-500x253.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/rich-harang-black-hat-session-160x81.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/rich-harang-black-hat-session-362x183.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/rich-harang-black-hat-session-217x110.png 217w\" sizes=\"(max-width: 755px) 100vw, 755px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. NVIDIA Principal Security Architect Rich Harang delivers his talk at Black Hat&nbsp;</em></em></figcaption></figure></div>\n\n\n<p>While RAG systems can help LLMs stay updated without the need for constant retraining, they also significantly expand the attack surface of the overall architecture. Without fine-grained access control to the RAG data store, there is potential for third-party or attacker-controlled data to enter the RAG data and therefore control the output of the model.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1128\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors.png\" alt=\"Architectural diagram of a RAG system, with red arrows indicating the potential input vectors through external data sources, user input, poisoned training data, attacker controlled output, and third party / insider threat input in external data sources.\n\" class=\"wp-image-89084\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors-625x353.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors-768x433.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors-1536x867.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors-645x364.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/architecture-rag-system-potential-threat-vectors-1024x578.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. Architecture of a RAG system shown with potential input threat vectors</em></em></figcaption></figure>\n\n\n\n<p>The overall message was straightforward: If your model can see the data, someone can get the model to output that data. Attendees ultimately were left with three core takeaways to bring back to their organizations:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Identify and analyze trust and security boundaries.</li>\n\n\n\n<li>Trace data flows, particularly any data that can enter or exit an application.</li>\n\n\n\n<li>Principles of least privilege (especially for plug-ins) and output minimization (error messages and intermediate results) still apply.</li>\n</ol>\n\n\n\n<p>Traditional approaches to security still apply across the board: know where your data is coming from, know where it\u2019s going, and know exactly who and what can control it.</p>\n\n\n\n<h2 id=\"democratizing_llm_security_assessments&nbsp;\"  class=\"wp-block-heading\">Democratizing LLM security assessments&nbsp;<a href=\"#democratizing_llm_security_assessments&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Later in the week at DEF CON, NVIDIA AI Security Researchers Leon Derczynski and Erick Galinkin presented the open-source tool garak as both a demo lab and a talk at the AI Village.&nbsp;</p>\n\n\n\n<p><a href=\"https://arxiv.org/abs/2406.11036\">garak</a>, an acronym for Generative AI Red-Teaming and Assessment Kit, is a platform that enables practitioners to take potential LLM exploits from academic research and quickly test them against their models, automating a portion of what has come to be known as LLM red-teaming.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"778\" height=\"340\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/erick-galinkin-leon-derczynski-garak-def-con.png\" alt=\"Erick Galinkin and Leon Derczynski deliver a talk from behind a podium to the DEF CON audience. The screen reads, \u201cgarak is a platform for LLM red-teaming so you don\u2019t have to read papers on arxiv (unless you want to.\u201d\n\" class=\"wp-image-89085\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/erick-galinkin-leon-derczynski-garak-def-con.png 778w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/erick-galinkin-leon-derczynski-garak-def-con-300x131.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/erick-galinkin-leon-derczynski-garak-def-con-625x273.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/erick-galinkin-leon-derczynski-garak-def-con-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/erick-galinkin-leon-derczynski-garak-def-con-768x336.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/erick-galinkin-leon-derczynski-garak-def-con-645x282.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/erick-galinkin-leon-derczynski-garak-def-con-500x219.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/erick-galinkin-leon-derczynski-garak-def-con-160x70.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/erick-galinkin-leon-derczynski-garak-def-con-362x158.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/erick-galinkin-leon-derczynski-garak-def-con-252x110.png 252w\" sizes=\"(max-width: 778px) 100vw, 778px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. NVIDIA AI Security Researchers Erick Galinkin and Leon Derczynski sharing garak at the DEF CON AI Village&nbsp;</em></em></figcaption></figure>\n\n\n\n<p>garak works by probing your choice of model for a constantly growing list of known vulnerabilities, including attacks on the underlying system itself such as XSS attacks, potentially malicious source files, various prompt injection attacks, and suffix attacks, as well as a number of clever safety jailbreaks. Once a probe is complete, garak generates a report of successful prompts and outcomes for each attack category, as well as overall metrics of the model\u2019s security against the chosen attack vectors.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1123\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components.png\" alt=\"A diagram of the three core components in garak, reading \u2018Probe\u2019, \u2018Generator\u2019, and \u2018Detector\u2019.\n\" class=\"wp-image-89086\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components-625x351.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components-768x431.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components-1536x863.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components-645x362.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components-362x203.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/garak-llm-vulnerability-scanner-core-components-1024x575.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 5. Core components in garak LLM vulnerability scanner</em></em></figcaption></figure>\n\n\n\n<p>garak currently supports just under 120 unique attack probes. At DEF CON, Leon Derczynski and Erick Galinkin demonstrated attacks on models from a number of sources, including NVIDIA. These attacks included:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Generating new adversarial suffixes for jailbreaking</li>\n\n\n\n<li>Forcing aligned models to output otherwise disallowed content</li>\n\n\n\n<li>Forcing a model to generate malware</li>\n\n\n\n<li>Getting a model to regurgitate its training data</li>\n</ul>\n\n\n\n<p>Both the AI Village garak presentation and the demo lab were heavily attended. Many attendees found it to be a huge leap forward for the community in standardizing definitions of security for LLMs.&nbsp;</p>\n\n\n\n<p>garak is available through <a href=\"https://github.com/leondz/garak\">leondz/garak</a> on GitHub, enabling researchers, developers, and security practitioners to concisely quantify the security of various models and compare model performance against various attacks. To learn more, see <a href=\"https://arxiv.org/abs/2406.11036\">garak: A Framework for Security Probing Large Language Models</a>.</p>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\">Summary<a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The team of researchers and practitioners at NVIDIA brought grounded expertise to leading cybersecurity conferences buzzing with excitement and advancements in both AI and security. Our focus remains on providing the security community with the knowledge necessary to effectively threat model, red team, assess, and deploy AI systems with a security mindset.&nbsp;</p>\n\n\n\n<p>If you\u2019re interested in better understanding the fundamentals of adversarial machine learning, enroll in the self-paced online NVIDIA DLI training, <a href=\"https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-DS-03+V1\">Exploring Adversarial Machine Learning.</a>&nbsp;</p>\n\n\n\n<p>To learn more about our ongoing work in this space, browse other <a href=\"https://developer.nvidia.com/blog/search-posts/?categories=Cybersecurity&amp;tags=AI+Security\">NVIDIA Technical Blog posts on cybersecurity and AI security</a>. And catch our team at the <a href=\"https://www.camlis.org/\">Conference on Applied Machine Learning in Information Security (CAMLIS)</a> this October.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Each August, tens of thousands of security professionals attend the cutting-edge security conferences Black Hat USA and DEF CON. This year, NVIDIA AI security experts joined these events to share our work and learn from other members of the community. This post provides an overview of these contributions, including a keynote on the rapidly evolving &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-presents-ai-security-expertise-at-leading-cybersecurity-conferences/\">Continued</a></p>\n", "protected": false}, "author": 2304, "featured_media": 89177, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1487760", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-presents-ai-security-expertise-at-leading-cybersecurity-conferences/307170", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 3110], "tags": [3520, 453, 3650, 3613, 1511], "coauthors": [4035, 3116, 4036, 4037], "class_list": ["post-89054", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-cybersecurity", "category-generative-ai", "tag-ai-red-team", "tag-featured", "tag-llm-techniques", "tag-retrieval-augmented-generation-rag", "tag-security-ai"], "acf": {"post_industry": ["General"], "post_products": ["General"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-security-graphic-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nam", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Cybersecurity", "link": "https://developer.nvidia.com/blog/category/cybersecurity/", "id": 1464}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89054"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2304"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89054"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89054/revisions"}], "predecessor-version": [{"id": 89178, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89054/revisions/89178"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89177"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89054"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89054"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89054"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89054"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88963, "date": "2024-09-17T12:04:16", "date_gmt": "2024-09-17T19:04:16", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88963"}, "modified": "2024-10-28T14:54:43", "modified_gmt": "2024-10-28T21:54:43", "slug": "accelerating-oracle-database-gen-ai-workloads-with-nvidia-nim-and-nvidia-cuvs", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-oracle-database-gen-ai-workloads-with-nvidia-nim-and-nvidia-cuvs/", "title": {"rendered": "Accelerating Oracle Database Generative AI Workloads with NVIDIA NIM and NVIDIA cuVS"}, "content": {"rendered": "\n<p>The vast majority of the world&#8217;s data remains untapped, and enterprises are looking to generate value from this data by creating the next wave of <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI</a> applications that will make a transformative business impact. <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">Retrieval-augmented generation (RAG)</a> pipelines are a key part of this, enabling users to have conversations with large corpuses of data and turning manuals, policy documents, and more into interactive generative AI applications.&nbsp;</p>\n\n\n\n<p>However, there are some common challenges that enterprises face when implementing RAG pipelines. It\u2019s difficult to handle both structured and unstructured data and it\u2019s computationally intensive to process and retrieve data. It\u2019s also important to build privacy and security into RAG pipelines.&nbsp;</p>\n\n\n\n<p>To solve this, NVIDIA and Oracle have worked together to demonstrate how multiple portions of the RAG pipeline can take advantage of the NVIDIA accelerated computing platform on Oracle Cloud Infrastructure (OCI). This approach helps enterprises leverage their structured and unstructured data more effectively, enhancing both the quality and reliability of generative AI outputs.</p>\n\n\n\n<p>This post dives into each component of the RAG pipeline recently demonstrated at Oracle CloudWorld 2024:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>NVIDIA GPUs for accelerated bulk generation of vector embeddings from large datasets in Oracle Autonomous Database</li>\n\n\n\n<li>Accelerated generation of vector indexes for Oracle Database 23ai AI Vector Search with NVIDIA <a href=\"https://rapids.ai/cuvs\">cuVS</a> library</li>\n\n\n\n<li>Performant LLM inference using <a href=\"https://www.nvidia.com/en-us/ai/#referrer=ai-subdomain\">NVIDIA NIM</a> on OCI</li>\n</ul>\n\n\n\n<p>We also explain how you can get started using some of these exciting capabilities.</p>\n\n\n\n<h2 id=\"embedding_generation_with_nvidia_gpus_and_oracle_autonomous_database\"  class=\"wp-block-heading\">Embedding generation with NVIDIA GPUs and Oracle Autonomous Database<a href=\"#embedding_generation_with_nvidia_gpus_and_oracle_autonomous_database\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In today&#8217;s data-rich enterprise environments, effectively harnessing large amounts of text data for generative AI is key to enhancing efficiency, reducing costs, and ultimately driving productivity.&nbsp;</p>\n\n\n\n<p>NVIDIA has partnered with Oracle to demonstrate how customers can get integrated access to <a href=\"https://blogs.oracle.com/machinelearning/post/announcing-gpu-support-for-oml-notebooks-on-adb\">NVIDIA GPUs through Oracle Machine Learning (OML) Notebooks in Autonomous Database</a>. This newly announced capability enables OML users to employ Python to load data directly from an Oracle Database table into an OCI NVIDIA GPU-accelerated virtual machine (VM) instance, generate vector embeddings using the GPU, and store those vectors in Oracle Database where they can be efficiently searched using <a href=\"https://www.oracle.com/database/ai-vector-search/\">AI Vector Search</a>. Provisioning the GPU instance and transferring data to and from it is done automatically for users, enabling seamless access for Autonomous Database users.</p>\n\n\n\n<h2 id=\"accelerated_vector_search_indexes_and_oracle_database_23ai\"  class=\"wp-block-heading\">Accelerated vector search indexes and Oracle Database 23ai<a href=\"#accelerated_vector_search_indexes_and_oracle_database_23ai\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://rapids.ai/cuvs/\">NVIDIA cuVS</a> is an open-source library for GPU-accelerated vector search and clustering. One of the key capabilities of cuVS is its ability to dramatically improve index build time, a key component of vector search.</p>\n\n\n\n<p>NVIDIA has partnered with Oracle to demonstrate a proof of concept that <a href=\"https://blogs.oracle.com/database/post/using-nvidia-gpus-to-accelerate-ai-vector-search-in-oracle-database-23ai\">accelerates vector index builds</a> for the Hierarchical Navigable Small World (HNSW) algorithm. This shows how cuVS constructs a graph-based index which is optimized for speed on the GPU, and then converts the graph to an HNSW-compatible index on the Oracle Database. The end result of pairing GPUs with CPUs results in faster overall index generation than with CPUs alone. The ability to offload index creation to GPUs and deploy to CPU is a key feature of the cuVS library.&nbsp;</p>\n\n\n\n<p>The fast creation of vector indexes is essential for supporting high-volume AI vector workloads, especially when large amounts of enterprise data must be processed and refreshed to keep out-of-the-box LLMs updated with the latest information. Building HNSW indexes on GPUs and deploying them to the Oracle database can increase the performance and lower the cost of AI workloads.</p>\n\n\n\n<h2 id=\"performant_llm_inference_with_nim_on_oci\"  class=\"wp-block-heading\">Performant LLM inference with NIM on OCI<a href=\"#performant_llm_inference_with_nim_on_oci\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a> provides containers to self-host GPU-accelerated inferencing microservices for pretrained and customized AI models across clouds, data centers, and workstations. NIM microservices are especially useful for enterprises to deploy generative AI models efficiently and securely. NIM delivers optimized microservices designed for NVIDIA-accelerated infrastructure, enabling smooth integration with existing tools and applications.&nbsp;</p>\n\n\n\n<p>Developers can quickly deploy LLMs with minimal code, whether on-premises or in Kubernetes-managed cloud environments. NIM also offers top-tier performance out of the box, reducing latency and boosting throughput, which simplifies real-time AI deployment while ensuring secure operations.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1908\" height=\"1073\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack.png\" alt=\"Diagram showing the components of NVIDIA NIM inference microservices, including pre-configured containers for simplified deployment, industry-standard APIs, optimized generative AI models, and GPU-accelerated infrastructure for scalable deployment.\n\" class=\"wp-image-89049\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack.png 1908w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack-625x351.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-optimized-stack-1024x576.png 1024w\" sizes=\"(max-width: 1908px) 100vw, 1908px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. NVIDIA NIM optimized stack</em></em></figcaption></figure>\n\n\n\n<p>Deploying NVIDIA NIM on Oracle Cloud Infrastructure provides enterprises with several key benefits, including:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Improving TCO with low-latency, high-throughput inference that scales</li>\n\n\n\n<li>Speeding time to market with prebuilt, cloud-native microservices</li>\n\n\n\n<li>Maintaining security and control of applications and data with self-hosted model deployment&nbsp;</li>\n</ul>\n\n\n\n<p>NIM can be deployed on OCI in two ways. The first option uses an NVIDIA GPU-accelerated bare-metal instance or on a VM, which provides dedicated server access for strong isolation and the highest performance. To get started, simply sign in, create an NVIDIA GPU-accelerated instance, and securely connect with SSH. Then download a NIM from the <a href=\"http://build.nvidia.com/\">NVIDIA API catalog</a>, launch the Docker container, and call the model directly from your compute instance.&nbsp;</p>\n\n\n\n<p>The second option uses the Oracle Container Engine for Kubernetes (OKE), which makes it easy to deploy, manage, and rapidly scale containerized applications, like NIM, on OCI using <a href=\"https://github.com/NVIDIA/nim-deploy/tree/main/helm\">Helm charts available through NVIDIA/nim-deploy</a> on GitHub.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/c36H4Sz0kBM?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 1. Learn how to deploy generative AI with NVIDIA NIM on OCI</em></em></figcaption></figure>\n\n\n\n<p>For the Oracle CloudWorld demonstration, we worked with the OCI team to show how using NIM for LLMs can enable customers to achieve higher throughput compared to off-the-shelf open-source alternatives when using high levels of concurrency (batch size). This performance boost is particularly evident in text generation and translation use cases.</p>\n\n\n\n<p>Video 2 shows this holistic approach in a sample end-to-end RAG pipeline querying Oracle CloudWorld 2024 session data stored in Oracle Database 23ai. This is an example of how out-of-the-box LLMs can benefit from a RAG pipeline with the NVIDIA accelerated computing platform hosted on OCI. It brings together NeMo Retriever NIM microservices with embedding and reranking model types, the Llama 3.1 405B NIM microservice, and <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100 Tensor Core GPUs</a>.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/IXTHOYEzg-M?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 2. Demo of a Q&amp;A chatbot powered by an end-to-end NVIDIA-accelerated RAG pipeline on Oracle CloudWorld 2024 session catalog data stored in Oracle Database 23ai</em></em></figcaption></figure>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>There are many elements involved in implementing an end-to-end RAG pipeline. NVIDIA has partnered with the OCI and Oracle Database teams to demonstrate how bulk generation of vector embeddings, HNSW index creation, and inferencing elements can be accelerated using NVIDIA GPUs and software. By focusing on some of the most computationally intensive portions of the RAG pipeline, we\u2019ve shown how organizations using Oracle Database 23ai and OCI can increasingly leverage the performance gains available from the NVIDIA accelerated computing platform. This holistic approach will help customers use AI to leverage the immense amount of data stored in Oracle databases.</p>\n\n\n\n<p>Learn more about <a href=\"https://rapids.ai/cuvs/\">cuVS</a>. To try NVIDIA NIM, visit <a href=\"http://ai.nvidia.com\">ai.nvidia.com</a> and sign up for the <a href=\"https://developer.nvidia.com/developer-program\">NVIDIA Developer Program</a> to gain instant access to the microservices. You can also start using <a href=\"https://blogs.oracle.com/machinelearning/post/announcing-gpu-support-for-oml-notebooks-on-adb\">NVIDIA GPU-enabled notebooks on Autonomous Database</a>, the Oracle Database 23ai AI Vector Search with <a href=\"https://www.oracle.com/database/free/\">Oracle Database 23ai Free.</a><br></p>\n", "protected": false}, "excerpt": {"rendered": "<p>The vast majority of the world&#8217;s data remains untapped, and enterprises are looking to generate value from this data by creating the next wave of generative AI applications that will make a transformative business impact. Retrieval-augmented generation (RAG) pipelines are a key part of this, enabling users to have conversations with large corpuses of data &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-oracle-database-gen-ai-workloads-with-nvidia-nim-and-nvidia-cuvs/\">Continued</a></p>\n", "protected": false}, "author": 2298, "featured_media": 89783, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1487047", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-oracle-database-gen-ai-workloads-with-nvidia-nim-and-nvidia-cuvs/307067", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110], "tags": [453, 3739, 3613], "coauthors": [4031, 4032, 4033, 3263, 4034, 2944], "class_list": ["post-88963", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "tag-featured", "tag-nim", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["General"], "post_products": ["CUDA", "cuVS", "Fleet Command", "NeMo Retriever", "NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/oracle-nvidia-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-n8T", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88963"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2298"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88963"}], "version-history": [{"count": 16, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88963/revisions"}], "predecessor-version": [{"id": 89857, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88963/revisions/89857"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89783"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88963"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88963"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88963"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88963"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88729, "date": "2024-09-17T07:30:00", "date_gmt": "2024-09-17T14:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88729"}, "modified": "2024-10-28T14:54:56", "modified_gmt": "2024-10-28T21:54:56", "slug": "optimizing-data-center-performance-with-ai-agents-and-the-ooda-loop-strategy", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/optimizing-data-center-performance-with-ai-agents-and-the-ooda-loop-strategy/", "title": {"rendered": "Optimizing Data Center Performance with AI Agents and the OODA Loop Strategy"}, "content": {"rendered": "\n<p>For any data center, operating large, complex GPU clusters is not for the faint of heart! There is a tremendous amount of complexity. Cooling, power, networking, and even such benign things like fan replacement cycles all must be managed effectively and governed well in accelerated computing data centers. Managing all of this requires an accelerated understanding of the petabytes of telemetry data from every layer of the compute stack.&nbsp;</p>\n\n\n\n<div class='stb-container stb-style-info stb-no-caption'><div class='stb-caption'><div class='stb-logo'><img class='stb-logo__image' src='data:image/png;base64,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' alt='img'/></div><div class='stb-caption-content'></div><div class='stb-tool'></div></div><div class='stb-content'>This post is part of the <a style=\"color: #0000ff;\" href=\"https://developer.nvidia.com/blog/tag/chat-labs/\">NVIDIA Chat Labs series</a>, which shares insights and best practices developed from the internal generative AI projects that we create to help others navigate AI adoption.</div></div>\n\n\n\n<p>Now imagine being able to chat with your data center directly to check on GPU cluster reliability. Consider a question such as, \u201cWhich of the top 5 most frequently replaced parts in our data center have the most supply chain risk?\u201d Maybe you have a more complex task, such as, \u201cExamine each GPU cluster and assign the most relevant technician to resolve the 5% of clusters most at risk for failure.\u201d</p>\n\n\n\n<p>To answer these types of questions and more, our team at NVIDIA embarked on a project that we dubbed LLo11yPop (LLM + Observability), to develop an observability AI agent framework that uses the <a href=\"https://en.wikipedia.org/wiki/OODA_loop\">OODA loop</a> (observation, orientation, decision, action).&nbsp;</p>\n\n\n\n<p>In this post, I provide an overview of how we built an observability agent framework for GPU fleet management, using a multi-LLM compound model in our architecture design. I also describe various agent roles, such as orchestration and task execution. Lastly, I share lessons learned for future experimentation with agentic observability frameworks.</p>\n\n\n\n<h2 id=\"&nbsp;an_observability_agent_framework_to_chat_with_your_gpu_cluster\"  class=\"wp-block-heading\"><em>&nbsp;</em>An observability agent framework to chat with your GPU cluster<a href=\"#&nbsp;an_observability_agent_framework_to_chat_with_your_gpu_cluster\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The NVIDIA DGX Cloud team governs a global GPU fleet that spans all major cloud service providers, as well as our own data centers. As the global buildout of accelerated data centers continues, we had to invent entirely new ways to observe the fleet in a manner that enables us to provide accelerated capabilities to the world in the most efficient, timely manner possible.</p>\n\n\n\n<h3 id=\"monitoring_accelerated_data_centers\"  class=\"wp-block-heading\">Monitoring accelerated data centers<a href=\"#monitoring_accelerated_data_centers\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>With each successive iteration of GPUs, the amount of observability needed expands. Standard data center metrics like utilization, errors, and throughput are the baseline.&nbsp;</p>\n\n\n\n<p>To really understand what\u2019s happening in this next generation of data centers, you must consider everything possible about the physical environment around it:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Temperature</li>\n\n\n\n<li>Humidity&nbsp;</li>\n\n\n\n<li>Power stability</li>\n\n\n\n<li>Latency</li>\n</ul>\n\n\n\n<p>There are dozens of metrics critical to profiling accelerated AI workloads, as they enable you to address data center incidents more quickly. With the additional complexity of GPU clusters designed for training foundation models, it is essential to apply whatever technology we can to help meet the challenge.</p>\n\n\n\n<p>To that end, our team\u2019s first goal was to build a system that enables you to have a conversation with your GPU cluster. Inspired by <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM microservices</a>, we\u2019ve already established the ability for users to have conversations with a database. If you can do that, why not open up the conversation to the kind of high-dimensional data you get from observability systems, enabling data center operators to have this capability?&nbsp;</p>\n\n\n\n<p>At NVIDIA, we have many observability systems across our fleet that we can access using <a href=\"https://en.wikipedia.org/wiki/Elasticsearch\">Elasticsearch</a>. So, we decided to use NIM microservices that enable us to converse with Elasticsearch in human language. That way, the agent system could answer questions such as, \u201cWhich clusters across the fleet have had the most issues with fan failures?\u201d and get accurate, actionable results.</p>\n\n\n\n<h2 id=\"model_architecture\"  class=\"wp-block-heading\">Model architecture<a href=\"#model_architecture\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"434\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types-1024x434.png\" alt=\"Diagram showing 5 types of agents. The orchestrator agent sets goals and routes queries. Analyst agents interpret domain-specific data. Retrieval agents access and retrieve ground truth information. Action agents trigger workflows based on observations. Task execution agents carry out specific steps in a predefined workflow.\" class=\"wp-image-89007\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types-1024x434.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types-300x127.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types-625x265.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types-179x76.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types-768x326.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types-1536x652.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types-645x274.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types-500x212.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types-160x68.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types-362x154.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types-259x110.png 259w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/framework-agent-types.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. High-level overview of the types of agents in the overall framework</em></figcaption></figure></div>\n\n\n<p>Figure 1 shows the types of agents:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Director\n<ul class=\"wp-block-list\">\n<li><strong>Orchestrator agents:</strong> Route questions to the correct analyst and, as a consequence, choose the best action as a response.</li>\n</ul>\n</li>\n\n\n\n<li>Manager\n<ul class=\"wp-block-list\">\n<li><strong>Analyst agents: </strong>Are trained on a specific functional domain. They understand what data they have available and convert broad questions into specific questions that are answered by retrieval agents.</li>\n\n\n\n<li><strong>Action agents:</strong> Coordinate action in response to something observed by the orchestrator, such as notifying an SRE when something needs human attention in the data center.</li>\n</ul>\n</li>\n\n\n\n<li>Worker\n<ul class=\"wp-block-list\">\n<li><strong>Retrieval agents: </strong>Convert questions in a given topic into code that runs against a data source or service endpoint using NVIDIA NeMo Retriever NIM microservices.</li>\n\n\n\n<li><strong>Task execution agents:</strong> Carry out a specific task, often through a workflow engine.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>All agents are modeled on the same kind of organizational hierarchy that you might find in any organization of humans doing knowledge work. Directors coordinate efforts toward the achievement of a mission, managers use specialized domain knowledge to allocate work, and worker agents are optimized toward specific tasks.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1662\" height=\"1206\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1.png\" alt=\"The diagram shows how a classification part of the orchestrator agent selects the correct analyst agent based on the task, informing the supervisor part of orchestrator which agent to use. The orchestrator then directs the retrieval agents to query structured databases, gather relevant data, and use Python code to further refinement or preparation of data for presentation.\" class=\"wp-image-89005\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1.png 1662w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1-300x218.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1-625x454.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1-158x115.png 158w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1-768x557.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1-1536x1115.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1-645x468.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1-413x300.png 413w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1-124x90.png 124w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1-362x263.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1-152x110.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/proof-concept-interactions-1-1024x743.png 1024w\" sizes=\"(max-width: 1662px) 100vw, 1662px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Interactions for the first proof of concept</em></figcaption></figure></div>\n\n\n<p>Figure 2 shows specific agents that we built to accommodate the initial use case where we can analyze various types of telemetry from heterogeneous sources and then use Python for more detailed analysis.&nbsp;</p>\n\n\n\n<p>Another key discovery was the need for a specific type of agent to detect off-topic questions. We learned early on that without such guardrails, the model would hallucinate more frequently about areas beyond the scope of the system.</p>\n\n\n\n<h2 id=\"moving_towards_a_multi-llm_compound_model\"  class=\"wp-block-heading\">Moving towards a multi-LLM compound model<a href=\"#moving_towards_a_multi-llm_compound_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To make this work, we realized that we would need more than one <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language&nbsp;model</a> (LLM) to address all the different types of telemetry for effectively managing clusters. While GPU metrics are required, they aren\u2019t sufficient to understand relevant layers of the stack: everything from the GPU layer up to orchestration layers like Slurm and <a href=\"https://www.nvidia.com/en-us/glossary/kubernetes/\">Kubernetes</a>.&nbsp;</p>\n\n\n\n<p>For more information, see <a href=\"https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/\">The Shift from Models to Compound AI Systems</a>.</p>\n\n\n\n<h3 id=\"using_the_mixture_of_agents_technique\"  class=\"wp-block-heading\">Using the mixture of agents technique<a href=\"#using_the_mixture_of_agents_technique\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To address these diverse needs, we went with a mixture of agents (MoA) approach initially. We developed a series of analyst agents that were experts in their domain, such as GPU cluster operating parameters, Slurm job data, and system log patterns. We then built a supervisor model whose job it is to build a plan and assign tasks to analyst agents, who in turn ask questions to query agents.&nbsp;</p>\n\n\n\n<p>We did all this using <a href=\"https://developer.nvidia.com/blog/an-introduction-to-large-language-models-prompt-engineering-and-p-tuning/#p-tuning_to_customize_llms%C2%A0\">prompt engineering</a>, without fine-tuning or other elaborate techniques, in the lead-up to the first version being put into use. Figure 3 shows us asking a question about Slurm job data and getting an answer and a supporting graph back.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"1080\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/swarm-ai-agent-data-center-interface-demo-1.gif\" alt=\"GIF of the UI answering a question about Slurm job failures that have occurred over a given time period, showing code generated by the LLM as the intermediate step prior to producing a full graph of the answer.\" class=\"wp-image-89004\"/><figcaption class=\"wp-element-caption\"><em>Figure 3. Asking a question about Slurm job failures to Llo11yPop</em></figcaption></figure></div>\n\n\n<p>In Figure 3, we show a common case where our team in resource governance needs to understand trends around Slurm job failures on all clusters we manage within a specific cloud provider.</p>\n\n\n\n<p>The question is passed to the supervisor agent who selects the right agent to answer the question. In this case, it\u2019s the Elasticsearch Slurm analyst. The agent then, based on the need reflected in the question, gathers data from one or more query agents. In this case, it\u2019s the Elasticsearch query tool that converts the question into the correct dialect of SQL to be understood by the Elasticsearch REST interface.&nbsp;</p>\n\n\n\n<p>We typically use this as an initial query to understand a trend, then ask more questions and use the same model to drill down. Perhaps we ask in the chat session to explore why May had more failures than normal, or perhaps we ask other analysts to go into more depth.&nbsp;</p>\n\n\n\n<p>By providing the capability to get an immediate answer, look at a graph, and quickly move on to the next question, we can quickly diagnose issues that drive anything from how we allocate GPUs to where we must do further diagnosis in GPU clusters that may be experiencing issues.</p>\n\n\n\n<p>Using a <a href=\"https://developer.nvidia.com/blog/introduction-to-llm-agents/#swarm_of_agents\">swarm-of-agents technique</a>, we chained together a series of small, focused models and performed several interesting optimizations. For example, we can fine-tune models built for passing SQL in the syntax that Elasticsearch understands from a base model built for code generation and use different, larger base LLM for planning tasks that <a href=\"https://developer.nvidia.com/blog/build-an-llm-powered-api-agent-for-task-execution/#what_is_an_api_agent\">direct execution agents</a>.</p>\n\n\n\n<h2 id=\"from_answers_and_actions_to_autonomous_agents_with_ooda_loops\"  class=\"wp-block-heading\">From answers and actions to autonomous agents with OODA loops<a href=\"#from_answers_and_actions_to_autonomous_agents_with_ooda_loops\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>When we demonstrated that we could get reliable answers from our analyst agents, the next obvious step was to close the loop. This led to the idea of having an autonomous supervisor agent that works towards a mission. If the supervisor agent were an AI engineering director, we would give it a metric to work towards and assign it a mission to improve that metric over time. As with an organization of humans, you would expect your AI director to operate in an OODA loop.</p>\n\n\n\n<h3 id=\"solving_for_gpu_cluster_reliability\"  class=\"wp-block-heading\">Solving for GPU cluster reliability<a href=\"#solving_for_gpu_cluster_reliability\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>For the supervisor AI agent to achieve greater reliability, it should do the following:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li><strong>Observe</strong> to understand data from the observability system.</li>\n\n\n\n<li><strong>Orient</strong> to choose the correct agents to conduct analysis.</li>\n\n\n\n<li><strong>Decide</strong> to make a decision on an action to take.</li>\n\n\n\n<li><strong>Invoke</strong> the action.</li>\n</ol>\n\n\n\n<p>Of course, to gain confidence that the AI director is making relevant decisions, most of the initial actions are the creation of tickets for a site reliability engineer (SRE) to analyze and, if relevant, act on. This is similar to how a human would operate.</p>\n\n\n\n<p>Much like self-driving cars, automation of data center ops exists on a spectrum from human-assisted driving to fully autonomous. In the early stages of adoption, humans are always in the loop.&nbsp;</p>\n\n\n\n<p>When we receive a cluster optimization recommendation, our SRE team analyzes the request, validates it, performs the task if relevant, and provides feedback about what is wrong with the recommendation if not. This forms the <a href=\"https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/#:~:text=Reinforcement%20learning%20with%20human%20feedback%3A\">reinforcement learning from human feedback</a> (RLHF) loop that enables us to improve the system over time, which we combine with other techniques where we use the telemetry from the system itself.</p>\n\n\n\n<h2 id=\"test_pyramid_for_llm_hierarchies\"  class=\"wp-block-heading\">Test pyramid for LLM hierarchies<a href=\"#test_pyramid_for_llm_hierarchies\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"460\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid-1024x460.png\" alt=\"Diagram shows a large number of individual agent evaluations, medium number of analyst agent tests, and few end-to-end tests. ests near the bottom are cheap, fast, and better for individual agent troubleshooting, while tests near the top are expensive, slower, more comprehensive, and test the entire system.\" class=\"wp-image-88744\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid-1024x460.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid-300x135.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid-625x281.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid-179x80.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid-768x345.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid-1536x691.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid-645x290.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid-500x225.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid-160x72.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid-362x163.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid-245x110.png 245w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/test-pyramid.png 1717w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Test pyramid concept adapted to agent hierarchies</em></figcaption></figure></div>\n\n\n<p>In classical microservice architecture, it is common to have tests for specific services, often with their own unit, functional, or end-to-end tests. As with classical microservices, it is most efficient to have more tests on individual components, with fewer yet more comprehensive tests as you move up the hierarchy (Figure 4). This enables you to balance the competing objectives of fast feedback at the lower level, comprehensiveness at the higher level, and the ability to diagnose problems more quickly.&nbsp;</p>\n\n\n\n<p>This last benefit is a key differentiator because if you try to do everything with a large monolithic model, it becomes much more challenging to diagnose why a given topic area might be hallucinating.</p>\n\n\n\n<p>While LLM testing is a large topic area that goes beyond the scope of a single post, it is important to point out that the framework of classical software unit testing changes significantly.&nbsp;</p>\n\n\n\n<p>For example, you might ask the LLM, \u201cHow many GPUs exceeded their normal temperature range?\u201d You would get back responses such as, \u201cThree,\u201d \u201cThree GPUs exceeded,\u201d \u201c3,\u201d \u201c3.00,\u201d or other ways of expressing the same idea. To that end, we used a second, usually stronger LLM, to validate conceptual equivalence of answers in a test suite of over 200 different tests at various levels of the system that run on each build of the system.</p>\n\n\n\n<h2 id=\"lessons_learned_from_building_an_observability_ai_agent\"  class=\"wp-block-heading\">Lessons learned from building an observability AI agent<a href=\"#lessons_learned_from_building_an_observability_ai_agent\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>First, you don\u2019t need to and frankly should not jump to training or tuning models as you are getting started. We achieved a functional prototype by doing prompt engineering and hooking a series of NIM microservices together using LangChain.&nbsp;</p>\n\n\n\n<p>Notable models used in the system include Mixtral 8x7b and more recently, the new Llama 3.1 405b NIM model from <a href=\"https://ai.nvidia.com\">ai.nvidia.com</a>. This enables you to get started and gives you the freedom to choose which model to use in each node of your graph without the sunk cost of model training. After you have something that is working well for the 90% case, then fine-tune specific models to increase accuracy to the required threshold for your use case.</p>\n\n\n\n<p>Second, choose the right model for the right job. Coding models work great as a base for human-to-SQL or other tools that do things like formatting output. Smaller models work great for simpler domains and can help increase speed and save money on tokens. Use larger models for the hardest tasks, often at the orchestrator-agent level where more context is required to understand the whole picture.</p>\n\n\n\n<p>Finally, don\u2019t fully automate without a human in the loop until you have strong evidence that the actions taken by the agentic system are accurate, useful, and safe. Much like you would not start with full autonomy when implementing self-driving cars, to gain the trust of the people operating the system, walk before you run to fully autonomous systems in production.</p>\n\n\n\n<h2 id=\"begin_building_your_ai_agent_application\"  class=\"wp-block-heading\">Begin building your AI agent application<a href=\"#begin_building_your_ai_agent_application\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>For more information about how you can use NVIDIA <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI</a> technologies and tools to build your own AI agents and applications, see <a href=\"http://ai.nvidia.com\">ai.nvidia.com</a> or try out <a href=\"https://build.nvidia.com/\">NVIDIA NIM APIs</a>.</p>\n\n\n\n<p>If you\u2019re just getting started, see <a href=\"https://developer.nvidia.com/blog/building-your-first-llm-agent-application/\">Building Your First LLM Agent Application</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>For any data center, operating large, complex GPU clusters is not for the faint of heart! There is a tremendous amount of complexity. Cooling, power, networking, and even such benign things like fan replacement cycles all must be managed effectively and governed well in accelerated computing data centers. Managing all of this requires an accelerated &hellip; <a href=\"https://developer.nvidia.com/blog/optimizing-data-center-performance-with-ai-agents-and-the-ooda-loop-strategy/\">Continued</a></p>\n", "protected": false}, "author": 2287, "featured_media": 88912, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1486947", "discourse_permalink": "https://forums.developer.nvidia.com/t/optimizing-data-center-performance-with-ai-agents-and-the-ooda-loop-strategy/307046", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 852, 3110], "tags": [3965, 4113, 453, 2932], "coauthors": [4019], "class_list": ["post-88729", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-data-center-cloud", "category-generative-ai", "tag-ai-agent", "tag-chat-labs", "tag-featured", "tag-large-language-models"], "acf": {"post_industry": ["Cloud Services"], "post_products": ["NeMo Retriever", "NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-platform-blog-chat-labs-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-n57", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88729"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2287"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88729"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88729/revisions"}], "predecessor-version": [{"id": 90856, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88729/revisions/90856"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88912"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88729"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88729"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88729"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88729"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89052, "date": "2024-09-17T07:00:00", "date_gmt": "2024-09-17T14:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89052"}, "modified": "2024-09-19T12:30:22", "modified_gmt": "2024-09-19T19:30:22", "slug": "polars-gpu-engine-powered-by-rapids-cudf-now-available-in-open-beta", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/polars-gpu-engine-powered-by-rapids-cudf-now-available-in-open-beta/", "title": {"rendered": "Polars GPU Engine Powered by RAPIDS cuDF Now Available in Open Beta"}, "content": {"rendered": "\n<p>Today, Polars released a new GPU engine powered by RAPIDS cuDF that accelerates Polars workflows up to 13x on NVIDIA GPUs, allowing data scientists to process hundreds of millions of rows of data in seconds on a single machine.</p>\n\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/AoKeit2Fbmw?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div></figure>\n\n\n\n<h2 id=\"growing_data_challenges&nbsp;\"  class=\"wp-block-heading\">Growing data challenges&nbsp;<a href=\"#growing_data_challenges&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Traditional data processing libraries like pandas are single-threaded and become impractical to use beyond a few million rows of data. Distributed data processing systems can handle billions of rows but add complexity and overhead for processing small-to-medium size datasets.&nbsp;</p>\n\n\n\n<p>There has been a gap in tools that process data efficiently for tens of millions up to a few hundred million rows of data. Such workloads are common for model development, demand forecasting, and logistics in industries like finance, retail, and manufacturing.</p>\n\n\n\n<p>Polars is one of the fastest growing Python libraries for data scientists and engineers, and was designed from the ground up to address these challenges. It uses advanced query optimizations to reduce unnecessary data movement and processing, allowing data scientists to smoothly handle workloads of hundreds of millions of rows in scale on a single machine. Polars bridges the gap where single-threaded solutions are too slow, and distributed systems add unnecessary complexity, offering an appealing &#8220;medium-scale&#8221; data processing solution.</p>\n\n\n\n<h2 id=\"bringing_nvidia_accelerated_computing_to_polars\"  class=\"wp-block-heading\">Bringing NVIDIA accelerated computing to Polars<a href=\"#bringing_nvidia_accelerated_computing_to_polars\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Polars leverages multi-threaded execution, advanced memory optimizations, and lazy evaluation to deliver significant acceleration out of the box compared to other CPU-only data manipulation tools.</p>\n\n\n\n<p>However, as organizations across industries face growing data processing demands &#8211; from analyzing billions of financial transactions to managing complex inventory systems &#8211; even higher performance is required. This is where accelerated computing comes into play:</p>\n\n\n\n<p>cuDF is part of the <a href=\"https://rapids.ai/\">NVIDIA RAPIDS</a> suite of <a href=\"https://www.nvidia.com/en-us/technologies/cuda-x/\">CUDA-X</a> libraries. It\u2019s a GPU-accelerated DataFrame library that harnesses the massive parallelism of GPUs to significantly enhance data processing performance.</p>\n\n\n\n<p>The Polars team partnered with NVIDIA to add the speed of cuDF to the efficiency of Polars for an additional performance boost, up to 13x compared to Polars on CPU. This allows users to maintain an interactive experience as their data processing workloads grow to hundreds of millions and even billions of rows of data.</p>\n\n\n\n<figure class=\"wp-block-image size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1050\" height=\"552\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Polars-benchmark-image.png\" alt=\"A bar chart comparing query execution times between Polars CPU and Polars GPU engines across 22 queries. The y-axis shows execution time in seconds from 0 to 45. Most GPU bars are significantly shorter than their CPU counterparts, indicating faster performance. The title states &quot;Accelerate Polars workflows up to 13x\u201d. Additional notes provide benchmark details including scale factor, hardware specs, and a disclaimer about comparability to TPC-H results.\" class=\"wp-image-89074\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Polars-benchmark-image.png 1050w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Polars-benchmark-image-300x158.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Polars-benchmark-image-625x329.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Polars-benchmark-image-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Polars-benchmark-image-768x404.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Polars-benchmark-image-645x339.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Polars-benchmark-image-500x263.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Polars-benchmark-image-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Polars-benchmark-image-362x190.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Polars-benchmark-image-209x110.png 209w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Polars-benchmark-image-1024x538.png 1024w\" sizes=\"(max-width: 1050px) 100vw, 1050px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. These are the four best speedups across a set of 22 queries from the PDS-H benchmark. The Polars GPU engine powered by RAPIDS cuDF offers up to 13x speedup compared to CPU on queries with many complex groupby and join operations.</em><br><br><a href=\"https://github.com/pola-rs/polars-benchmark\"><em>PDS-H benchmark</em></a><em> scale factor 80 | GPU: NVIDIA H100 | CPU: Intel Xeon W9-3495X (Sapphire Rapids) | Storage: Local NVMe</em>. <em>Note: PDS-H is derived from TPC-H but these results are not comparable to TPC-H results.</em></figcaption></figure>\n\n\n\n<p>Built directly into the Polars Lazy API, users can access GPU acceleration for their workflows by simply installing <code>polars[gpu]</code> via pip and passing <code>[engine=\u201dgpu\u201d]</code> to the <em>collect</em> operation. Under the hood, PoIars will attempt to execute operations on the GPU first and fall back to the CPU if necessary. This approach ensures:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Efficient execution and minimal memory usage by using Polars\u2019 query optimizer</li>\n\n\n\n<li>Users can access the GPU engine with zero changes to existing Polars code</li>\n\n\n\n<li>Full compatibility with Polars\u2019 growing ecosystem of data visualization, I/O, and machine learning libraries</li>\n</ul>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\npip install polars&#x5B;gpu] --extra-index-url=https://pypi.nvidia.com\n\nimport polars as pl\n\n(transactions\n .group_by(&quot;CUST_ID&quot;)\n .agg(pl.col(&quot;AMOUNT&quot;).sum())\n .sort(by=&quot;AMOUNT&quot;, descending=True)\n .head()\n .collect(engine=&quot;gpu&quot;))\n</pre></div>\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The Polars GPU engine powered by RAPIDS cuDF is now available in open beta, offering data scientists and engineers in every industry a powerful tool for medium-scale data processing. It accelerates Polars workflows up to 13x on NVIDIA GPUs, efficiently handling datasets of hundreds of millions of rows without the overhead of distributed systems. The Polars GPU engine is built directly into the Polars API, making it easily accessible to every user.</p>\n\n\n\n<h2 id=\"getting_started_with_the_polars_gpu_engine\"  class=\"wp-block-heading\">Getting Started with the Polars GPU Engine<a href=\"#getting_started_with_the_polars_gpu_engine\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Check out these resources to learn more and get started with the Polars GPU engine:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Introductory notebook available on <a href=\"https://nvda.ws/gpu-polars-xl\" target=\"_blank\" rel=\"noreferrer noopener\">GitHub</a> and <a href=\"https://nvda.ws/gpu-polars-xq\" target=\"_blank\" rel=\"noreferrer noopener\">Colab</a></li>\n\n\n\n<li><a href=\"https://pola.rs/posts/gpu-engine-release/\" target=\"_blank\" rel=\"noreferrer noopener\">Polars Release Blog</a></li>\n\n\n\n<li><a href=\"https://docs.pola.rs/user-guide/gpu-support/\" target=\"_blank\" rel=\"noreferrer noopener\">Polars User Guide</a></li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>Today, Polars released a new GPU engine powered by RAPIDS cuDF that accelerates Polars workflows up to 13x on NVIDIA GPUs, allowing data scientists to process hundreds of millions of rows of data in seconds on a single machine. Growing data challenges&nbsp; Traditional data processing libraries like pandas are single-threaded and become impractical to use &hellip; <a href=\"https://developer.nvidia.com/blog/polars-gpu-engine-powered-by-rapids-cudf-now-available-in-open-beta/\">Continued</a></p>\n", "protected": false}, "author": 2305, "featured_media": 89070, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1486933", "discourse_permalink": "https://forums.developer.nvidia.com/t/polars-gpu-engine-powered-by-rapids-cudf-now-available-in-open-beta/307042", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 1903], "tags": [3273, 453, 1958], "coauthors": [4038], "class_list": ["post-89052", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-features", "tag-accelerated-data-analytics", "tag-featured", "tag-news"], "acf": {"post_industry": ["General"], "post_products": ["cuDF", "RAPIDS"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/rapids-ai-day-announcement-tech-blog-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-nak", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89052"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2305"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89052"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89052/revisions"}], "predecessor-version": [{"id": 89108, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89052/revisions/89108"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89070"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89052"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89052"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89052"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89052"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 89091, "date": "2024-09-16T17:50:04", "date_gmt": "2024-09-17T00:50:04", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=89091"}, "modified": "2024-09-16T17:50:07", "modified_gmt": "2024-09-17T00:50:07", "slug": "generate-code-with-abacus-ais-dracarys-large-language-model", "status": "publish", "type": "post", "link": "https://nvda.ws/4gv0n1c", "title": {"rendered": "Generate code with Abacus AI\u2019s Dracarys Large Language Model"}, "content": {"rendered": "\n<p>Dracarys, fine-tuned from Llama 3.1 70B and available from NVIDIA NIM microservice, supports a variety of applications, including data analysis, text summarization, and multi-language support.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Dracarys, fine-tuned from Llama 3.1 70B and available from NVIDIA NIM microservice, supports a variety of applications, including data analysis, text summarization, and multi-language support.</p>\n", "protected": false}, "author": 492, "featured_media": 89092, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1486727", "discourse_permalink": "https://forums.developer.nvidia.com/t/generate-code-with-abacus-ai-s-dracarys-large-language-model/306994", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/4gv0n1c", "_links_to_target": "_blank"}, "categories": [3110], "tags": [3561, 453, 2932, 1958], "coauthors": [610, 3872], "class_list": ["post-89091", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-code-software-generation", "tag-featured", "tag-large-language-models", "tag-news"], "acf": {"post_industry": ["Consumer Internet", "Retail / Consumer Packaged Goods"], "post_products": ["AI Foundation Models", "NIM"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Generate-code-with-Abacus-AIs-Dracarys-Large-Language-Model.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-naX", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89091"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/492"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=89091"}], "version-history": [{"count": 1, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89091/revisions"}], "predecessor-version": [{"id": 89094, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/89091/revisions/89094"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89092"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=89091"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=89091"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=89091"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=89091"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87077, "date": "2024-09-16T17:31:08", "date_gmt": "2024-09-17T00:31:08", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87077"}, "modified": "2024-09-19T12:30:36", "modified_gmt": "2024-09-19T19:30:36", "slug": "memory-efficiency-faster-initialization-and-cost-estimation-with-nvidia-collective-communications-library-2-22", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/memory-efficiency-faster-initialization-and-cost-estimation-with-nvidia-collective-communications-library-2-22/", "title": {"rendered": "Memory Efficiency, Faster Initialization, and Cost Estimation with NVIDIA Collective Communications Library 2.22"}, "content": {"rendered": "\n<p>For the past few months, the NVIDIA Collective Communications Library (NCCL) developers have been working hard on a set of new library features and bug fixes. In this post, we discuss the details of the <a href=\"https://developer.nvidia.com/nccl/nccl-download\">NCCL 2.22</a> release and the pain points addressed.</p>\n\n\n\n<h2 id=\"release_highlights\"  class=\"wp-block-heading\">Release highlights<a href=\"#release_highlights\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA Magnum IO NCCL is a library designed to optimize inter-GPU and multi-node communication, crucial for efficient parallel computing in AI and HPC applications. The value of this release lies in its new features:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Lazy connection establishment for GPU memory saving: </strong>Delays the creation of connections until they are needed, reducing the GPU memory overhead.</li>\n\n\n\n<li><strong>New API for cost estimation and workload balancing:</strong> Exposes a new API to help you optimize compute and communication overlap or research the NCCL cost model.</li>\n\n\n\n<li><strong>Optimizations and instrumentation for <code>ncclCommInitRank</code></strong>: Eliminates redundant topology queries, accelerating initialization by up to&nbsp; 90% for applications that create many communicators.</li>\n\n\n\n<li><strong>Support for multiple subnets with IB Router: </strong>Adds support for communication in jobs spanning multiple InfiniBand subnets, which enables DL training jobs to run on InfiniBand networks that are larger than 40K endpoints.</li>\n</ul>\n\n\n\n<h2 id=\"features\"  class=\"wp-block-heading\">Features<a href=\"#features\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In this section, we dive deeper into the details of each new feature:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Lazy connection establishment</li>\n\n\n\n<li>New cost model API</li>\n\n\n\n<li>Initialization optimizations and instrumentation</li>\n\n\n\n<li>New tuner plugin interface</li>\n\n\n\n<li>Static plugin linking</li>\n\n\n\n<li>Group semantics for abort or destroy</li>\n\n\n\n<li>IB Router support</li>\n</ul>\n\n\n\n<h3 id=\"lazy_connection_establishment\"  class=\"wp-block-heading\">Lazy connection establishment<a href=\"#lazy_connection_establishment\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>NCCL works with a set of persistent, statically allocated connections and buffers for the operation of its eager data transfer protocol. For every given algorithm and protocol that NCCL supports, it creates a separate set of connections and buffers, each requiring multiple megabytes of GPU memory.</p>\n\n\n\n<p>For reference, an <em>algorithm</em> defines the high-level movement of data among participants for a given collective, and a <em>protocol</em> defines the way NCCL sends data. A given algorithm and protocol are chosen based on the operation, message size, scale, and topology to achieve optimal performance.</p>\n\n\n\n<p>Before 2.22, NCCL would form connections between peers for every combination<strong> </strong>of these, potentially wasting megabytes of GPU memory for algorithms and protocols that would never be used.&nbsp;</p>\n\n\n\n<p>Now, NCCL waits to form connections for a given algorithm until the first time it&#8217;s needed. This decreases the NCCL memory overhead by a good margin, especially when NCCL is used in a narrow scope. For instance, if you only ever run <code>ncclAllReduce</code> at the same message size over and over, you should only be using one algorithm on a given system.</p>\n\n\n\n<p>The feature is enabled by default but can be disabled by setting the env <code>NCCL_RUNTIME_CONNECT=0</code>.</p>\n\n\n\n<p>In the previous scenario on a single node DGX-H100, we saw a 3.5x reduction in GPU memory usage by NCCL only using the Ring algorithm, and a 1.47x reduction running only NVSwitch<strong>&#8211;</strong>based reductions.</p>\n\n\n\n<h3 id=\"new_cost_model_api\"  class=\"wp-block-heading\">New cost model API<a href=\"#new_cost_model_api\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Application developers want to take full advantage of the compute, memory, and bandwidth resources available on NVIDIA systems. </p>\n\n\n\n<p>Ideally, compute and communication are perfectly overlapped, both perfectly fed with work, and the full capabilities of your hardware are pushed to the max. Doing this is hard when running large-scale HPC and AI applications, especially with one codebase running on multiple platforms.</p>\n\n\n\n<p>To help solve this problem, NCCL added a new API to enable you to see<strong> </strong>how long it thinks a given operation will take. This API is called <code>ncclGroupSimulateEnd</code><strong>.</strong> It is used the same way as <code>ncclGroupEnd</code><strong>, </strong>which makes it easy for anyone familiar with writing NCCL code.&nbsp;</p>\n\n\n\n<p>The difference is it launches no communications operations. Instead, NCCL calculates how long it thinks the operation will take and sets this in the supplied <code>ncclSimInfo_t</code> structure.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nncclGroupStart()\nncclAllReduce()\nncclGroupSimulateEnd(sim_t)\nprintf(&quot;Estimated completion time=%f microseconds\\n&quot;, sim.time);\nconfigureComputeAmount(sim.time, &amp;computeIters, &amp;workItemSize);\n</pre></div>\n\n\n<p>However,&nbsp; the values returned by this API do not perfectly align with reality. It\u2019s an estimate based on the NCCL internal model. As of 2.22, this API only returns the estimated time of the last operation in the group.</p>\n\n\n\n<h3 id=\"initialization_optimizations_and_instrumentation\"  class=\"wp-block-heading\">Initialization optimizations and instrumentation<a href=\"#initialization_optimizations_and_instrumentation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>With the varied and ever-increasing scale of customer workloads, reducing overhead from NCCL initialization is an increasing priority for the NCCL team.&nbsp;</p>\n\n\n\n<p>Even with single-node jobs, the increased number of NVLink interconnects on NVIDIA Hopper GPUs that must be individually discovered and connected has resulted in a considerable initialization time increase.</p>\n\n\n\n<p>We wanted to improve the initialization time, and we had to study the overhead of each initialization step. We started with instrumenting each phase within <code>ncclCommInitRank</code> and studied the timing of each at various scales. You now see this whenever you collect standard NCCL logs (<code>NCCL_DEBUG=INFO</code>).</p>\n\n\n\n<p>There\u2019s also a new <code>NCCL_PROFILE</code> debug subsystem that gives just the instrumentation information if you don\u2019t care about the rest of the NCCL initialization logs (<code>NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=PROFILE</code>).</p>\n\n\n\n<p>One promising area for improvement was the previously discussed connection establishment. Switching to lazy establishment can save memory and also reduce initialization time.</p>\n\n\n\n<p>Another area was topology discovery, which is an initialization step where every NCCL rank determines the hardware available on the node. This includes which GPUs and NICs are on the system, how many NVLink interconnects are present, as well as the PCI topology and NUMA affinities.&nbsp;</p>\n\n\n\n<p>As it turned out, the way NCCL performed NVLink discovery was suboptimal, because every rank was discovering all the links on its own, leading to redundancy and congestion.</p>\n\n\n\n<p>To address this issue, we reused topology fusion code, first introduced in NCCL 2.21 as part of the Multi-Node NVLink (MNNVL) support, where partial information available on each node was being combined during bootstrap using inter-node communication, resulting in a complete picture of the NVLink topology.&nbsp;</p>\n\n\n\n<p>For 2.22, we extended this feature to work within each node. Now every rank discovers information about its own GPU only and then combines these results with its peers using intra-node topology fusion.</p>\n\n\n\n<p>Together, lazy connection establishment and intra-node topology fusion can shave 90% (~6 seconds) off <code>ncclCommInitRank</code> execution time on a single 8x H100 GPU system. What formerly took ~6.7 seconds now takes ~0.7. For applications that create many communicators during their execution, this can reduce initialization time by a huge amount.</p>\n\n\n\n<h3 id=\"new_tuner_plugin_interface\"  class=\"wp-block-heading\">New tuner plugin interface<a href=\"#new_tuner_plugin_interface\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>With the new tuner plugin interface (v3), NCCL supplies the plugin with a per-collective 2D cost table, reporting the estimated time needed to carry out the operation for every combination of algorithm and protocol.&nbsp;</p>\n\n\n\n<p>NCCL sets the table entries that are not compatible with the detected topology to <code>-1</code>, to indicate to external tuners that these combinations are not supported/allowed to be overwritten.&nbsp;</p>\n\n\n\n<p>To select a specific combination, the external tuner updates the value for the desired algorithm or protocol combination to <code>0</code> or the minimum value across the whole table. After the plugin has updated the cost table, NCCL can use it to select the final configuration for the given collective.</p>\n\n\n\n<h3 id=\"static_plugin_linking\"  class=\"wp-block-heading\">Static plugin linking<a href=\"#static_plugin_linking\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The NCCL team exposes a plugin model for partners to provide their own tuning or network backend in place of the NCCL internal model and InfiniBand plugin. Some partners want to statically link these plugins against their application binaries for convenience&#8217;s sake and to avoid mishaps in loading the wrong one.&nbsp;</p>\n\n\n\n<p>If an application has statically linked either a network or tuner plugin, specify it by setting <code>NCCL_NET_PLUGIN</code> or <code>NCCL_TUNER_PLUGIN</code> to <code>STATIC_PLUGIN</code>.</p>\n\n\n\n<h3 id=\"group_semantics_for_abort_or_destroy\"  class=\"wp-block-heading\">Group semantics for abort or destroy<a href=\"#group_semantics_for_abort_or_destroy\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Previously, <code>ncclCommDestroy</code> and <code>ncclCommAbort</code> would block the calling thread until completed.&nbsp;</p>\n\n\n\n<p>With multi-dimensional parallel ML workloads, one process manages multiple NCCL communicators, and each must be eventually torn down using these APIs. We provided semantics for these applications to destroy more than one communicator at a time in a grouped fashion to avoid deadlocks and provide a better user experience.</p>\n\n\n\n<h3 id=\"ib_router_support\"  class=\"wp-block-heading\">IB Router support<a href=\"#ib_router_support\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>With this feature, NCCL can operate across different Infiniband subnets, connected by one or more routers. NCCL automatically detects when two communicating end-points are on different subnets of an InfiniBand network and exchanges GID information required to establish a connection and communicate.&nbsp;</p>\n\n\n\n<p>When routing between subnets, FLID can be used to identify a group of routers for forwarding and enable higher performance and adaptive routing between subnets. NCCL 2.22 automatically detects the presence of FLID and uses it for connections between endpoints on different subnets.</p>\n\n\n\n<h2 id=\"bug_fixes_and_minor_features\"  class=\"wp-block-heading\">Bug fixes and minor features<a href=\"#bug_fixes_and_minor_features\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NCCL 2.22 provides the following additional updates:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Added support for the <code>allreduce</code> tree algorithm on DGX Google Cloud.</li>\n\n\n\n<li>Logged the NIC name in IB async errors.</li>\n\n\n\n<li>Fixed aggregated collective performance.</li>\n\n\n\n<li>Fixed the performance of registered send and receive operations.</li>\n\n\n\n<li>Added infrastructure code for NVIDIA Trusted Computing Solutions.</li>\n\n\n\n<li>Added separate traffic class for IB and RoCE control messages to enable advanced QoS (set with <code>NCCL_IB_FIFO_TC</code>).</li>\n\n\n\n<li>Added support for PCI peer-to-peer communications across sub-parts of partitioned Broadcom PCI switches.</li>\n</ul>\n\n\n\n<h2 id=\"&nbsp;summary\"  class=\"wp-block-heading\">&nbsp;Summary<a href=\"#&nbsp;summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The NCCL 2.22 release introduces several significant features and optimizations aimed at improving performance and efficiency for high-performance computing (HPC) and AI applications. Improvements also include a new tuner plugin interface, support for static linking of plugins, and enhanced group semantics to prevent deadlocks.</p>\n\n\n\n<p>For more information, see <a href=\"https://developer.nvidia.com/magnum-io\">Magnum IO</a> and <a href=\"https://developer.nvidia.com/NCCL\">NCCL</a>. Provide feedback on the <a href=\"https://forums.developer.nvidia.com/c/accelerated-computing/gpu-accelerated-libraries/12\">GPU-Accelerated Libraries</a> forum.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>For the past few months, the NVIDIA Collective Communications Library (NCCL) developers have been working hard on a set of new library features and bug fixes. In this post, we discuss the details of the NCCL 2.22 release and the pain points addressed. Release highlights NVIDIA Magnum IO NCCL is a library designed to optimize &hellip; <a href=\"https://developer.nvidia.com/blog/memory-efficiency-faster-initialization-and-cost-estimation-with-nvidia-collective-communications-library-2-22/\">Continued</a></p>\n", "protected": false}, "author": 2249, "featured_media": 87126, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1486725", "discourse_permalink": "https://forums.developer.nvidia.com/t/memory-efficiency-faster-initialization-and-cost-estimation-with-nvidia-collective-communications-library-2-22/306993", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 1205, 503], "tags": [1932, 453, 1951], "coauthors": [3974, 3975, 3976, 660, 2613, 2319], "class_list": ["post-87077", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-networking-communications", "category-simulation-modeling-design", "tag-development-tools-and-libraries", "tag-featured", "tag-internet-communications"], "acf": {"post_industry": ["Cloud Services"], "post_products": ["Magnum IO", "NCCL", "NVLink"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cube-of-light-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mEt", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Networking / Communications", "link": "https://developer.nvidia.com/blog/category/networking-communications/", "id": 1205}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87077"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2249"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87077"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87077/revisions"}], "predecessor-version": [{"id": 89090, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87077/revisions/89090"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87126"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87077"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87077"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87077"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87077"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87546, "date": "2024-09-16T14:19:41", "date_gmt": "2024-09-16T21:19:41", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87546"}, "modified": "2024-09-19T12:30:48", "modified_gmt": "2024-09-19T19:30:48", "slug": "orchestrating-innovation-at-scale-with-nvidia-maxine-and-texel", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/orchestrating-innovation-at-scale-with-nvidia-maxine-and-texel/", "title": {"rendered": "Orchestrating Innovation at Scale with NVIDIA Maxine and Texel"}, "content": {"rendered": "\n<p>The <a href=\"https://developer.nvidia.com/maxine\">NVIDIA Maxine</a> AI developer platform is a suite of NVIDIA NIM microservices, cloud-accelerated microservices, and SDKs that offer state-of-the-art features for enhancing real-time video and audio. NVIDIA partners use Maxine features to create better virtual interaction experiences and improve human connections with their applications.&nbsp;</p>\n\n\n\n<p>Making and maintaining eye contact are rare in virtual settings because it is often difficult to align your gaze with the camera while holding a meeting or producing a video. Distractions, scripts, notes off to the side, and other factors add to the challenge of keeping eye contact. </p>\n\n\n\n<p>Maxine Eye Contact solves this problem by aligning users\u2019 gaze with the camera to simulate eye contact and increase engagement and connection. For more information, see <a href=\"https://developer.nvidia.com/blog/nvidia-maxine-elevates-video-conferencing-in-the-cloud/\">NVIDIA Maxine Elevates Video Conferencing in the Cloud</a>.</p>\n\n\n\n<h2 id=\"flexible_options_with_maxine\"  class=\"wp-block-heading\"><strong>Flexible options with Maxine</strong><a href=\"#flexible_options_with_maxine\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>There are several options, outlined later in this post, for integrating Maxine features into applications. Texel is an AI platform that provides cloud-native APIs to help you scale and optimize workflows for image and video processing. It makes integration easier and more cost-efficient for smaller developers using the cloud.&nbsp;</p>\n\n\n\n<p>Co-founders Rahul Sheth, Texel CEO, and Eli Semory, Texel CTO, are enablers of scale. According to Sheth, \u201cThe Maxine Eye Contact SDK from NVIDIA is a state-of-the-art model, but the smaller shops and developers may have challenges in adopting the feature at scale out of the box. Our video pipeline API makes it easy to use any number of models on video in a highly optimized and cost-effective fashion.\u201d</p>\n\n\n\n<p>The collaboration with NVIDIA has saved Texel\u2019s customers valuable development time and made this technology accessible to a broader range of developers and users.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/9Fx5tPONQjU?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Automatic Eye Contact with the Texel API</em></figcaption></figure>\n\n\n\n<p>In Video 1, see how AI enhances video communication by naturally redirecting the eyes toward the camera to enhance engagement and output.</p>\n\n\n\n<h2 id=\"benefits_of_integration\"  class=\"wp-block-heading\"><strong>Benefits of integration</strong><a href=\"#benefits_of_integration\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Maxine allows for flexible, fast deployment and updates across any platform.</p>\n\n\n\n<h3 id=\"nvidia_nim_microservices\"  class=\"wp-block-heading\">NVIDIA NIM Microservices<a href=\"#nvidia_nim_microservices\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Here are the benefits of using NVIDIA NIM microservices to integrate Maxine features into your applications:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Efficiently scales your applications, ensuring optimal performance and resource usage.</li>\n\n\n\n<li>Integrates easily with Kubernetes platforms.</li>\n\n\n\n<li>Enables deploying NVIDIA Triton at scale.</li>\n\n\n\n<li>Provides one-click deployment options, such as for NVIDIA Triton Inference Server.</li>\n</ul>\n\n\n\n<h3 id=\"nvidia_sdks\"  class=\"wp-block-heading\">NVIDIA SDKs<a href=\"#nvidia_sdks\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Here are the benefits of using NVIDIA SDKs to integrate Maxine features into your applications:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Includes NVIDIA Triton Inference Server support for scalable AI model deployment.</li>\n\n\n\n<li>Enables seamless scaling across different cloud environments.&nbsp;</li>\n\n\n\n<li>Supports multi-stream scaling with improved throughput.</li>\n\n\n\n<li>Standardizes model deployment and execution across every workload to simplify AI infrastructure.</li>\n\n\n\n<li>Offers concurrent model execution, maximizing GPU utilization and throughput.</li>\n\n\n\n<li>Provides dynamic batching to improve inference performance.</li>\n\n\n\n<li>Enables model ensembles and business logic scripting for complex AI pipelines.</li>\n\n\n\n<li>Supports cloud, data center, and edge deployments.</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"995\" height=\"560\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-Triton-eye-contact-bar-chart.png\" alt=\"bar chart representing the improvements for eye contact across concurrent streams.\" class=\"wp-image-89022\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-Triton-eye-contact-bar-chart.png 995w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-Triton-eye-contact-bar-chart-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-Triton-eye-contact-bar-chart-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-Triton-eye-contact-bar-chart-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-Triton-eye-contact-bar-chart-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-Triton-eye-contact-bar-chart-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-Triton-eye-contact-bar-chart-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-Triton-eye-contact-bar-chart-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-Triton-eye-contact-bar-chart-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-Triton-eye-contact-bar-chart-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/NVIDIA-Triton-eye-contact-bar-chart-195x110.png 195w\" sizes=\"(max-width: 995px) 100vw, 995px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA Triton support enhances the throughput of concurrent streams</em></figcaption></figure></div>\n\n\n<p>Using the Maxine SDK on Triton Inference Server provides a comprehensive, optimized, and scalable solution for AI model deployment and inference, using multi-framework support, advanced features, and robust model management capabilities.&nbsp;</p>\n\n\n\n<h3 id=\"texel_for_simplified_scaling\"  class=\"wp-block-heading\">Texel for simplified scaling<a href=\"#texel_for_simplified_scaling\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>One of the key strategies for scaling with Maxine is through the integration and optimization of implementers, like Texel, which enable the service to cater to millions of users.&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Simplified API integration:</strong> Control the features you want to activate without having to manage complex backend processes.&nbsp;This simplifies the integration of Maxine features into applications.</li>\n\n\n\n<li><strong>End-to-end pipeline optimization:</strong> Focus on using the features rather than infrastructure. The entire pipeline is optimized from input to inference to output, streamlining data flow and scaling behind the scenes. </li>\n\n\n\n<li><strong>Custom model optimization:</strong> Bring your own custom models to optimize, reducing inference time and NVIDIA GPU memory usage. This makes it easier to scale custom AI solutions built on Maxine.</li>\n\n\n\n<li><strong>Hardware abstraction:</strong> Use the latest NVIDIA GPUs without having to become a hardware expert, lowering the barrier to adopting advanced hardware acceleration.&nbsp;</li>\n\n\n\n<li><strong>Efficient resource utilization:</strong> Run on as few GPUs as possible, potentially reducing costs and making scaling more economical.</li>\n\n\n\n<li><strong>Real-time performance: </strong>Build more responsive applications that can edit images and videos with AI in real time.</li>\n\n\n\n<li><strong>Flexible deployment:</strong> Choose hosted or on-premise deployment options for the scaling approach that best fits your needs.</li>\n</ul>\n\n\n\n<p>The Texel team&#8217;s background in running large GPU fleets at scale (for example, at Snapchat) informs their approach to making NVIDIA-accelerated AI more accessible and scalable. You can effectively address the complexities of scaling and handle the demands of millions of users with the flexible integration options and the help of Texel.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The NVIDIA Maxine AI developer platform, coupled with Texel\u2019s scalable integration solutions, offers you a powerful toolkit for creating cutting-edge video applications with advanced AI features. By using the flexible integration options provided by NVIDIA, you can efficiently implement sophisticated capabilities, like Eye Contact, for real-time enhancements.&nbsp;</p>\n\n\n\n<p>The seamless scalability offered by Texel enables applications to grow from prototype to production-ready systems capable of serving millions of users. Leave the complexities of AI deployment and scaling to experts, and focus on building unique user experiences.&nbsp;</p>\n\n\n\n<p>From enhancing day-to-day video conferencing to integrating AI technology, <a href=\"https://developer.nvidia.com/maxine\">NVIDIA Maxine</a> offers high-quality video communications for all professionals.</p>\n\n\n\n<p>The latest Maxine production release is included exclusively with <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, which enables you to tap into production-ready features such as NVIDIA Triton Inference Server, enterprise support, and more.</p>\n\n\n\n<p>If you\u2019re interested in early access, with non-production access to production and soon-to-be-released features, see the <a href=\"https://developer.nvidia.com/maxine-microservice-early-access\">Maxine Early Access program</a>.</p>\n\n\n\n<p>To help improve features in upcoming releases, provide feedback on the <a href=\"https://crowdsource.nvidia.com/en-us/maxine/\">NVIDIA Maxine and NVIDIA Broadcast App</a> survey.</p>\n\n\n\n<p>For more information, see <a href=\"https://texel.ai\">Texel&#8217;s video APIs</a> and contact the <a href=\"mailto:sales@texel.ai\">Sales team</a> with any questions.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The NVIDIA Maxine AI developer platform is a suite of NVIDIA NIM microservices, cloud-accelerated microservices, and SDKs that offer state-of-the-art features for enhancing real-time video and audio. NVIDIA partners use Maxine features to create better virtual interaction experiences and improve human connections with their applications.&nbsp; Making and maintaining eye contact are rare in virtual settings &hellip; <a href=\"https://developer.nvidia.com/blog/orchestrating-innovation-at-scale-with-nvidia-maxine-and-texel/\">Continued</a></p>\n", "protected": false}, "author": 2233, "featured_media": 87559, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1486681", "discourse_permalink": "https://forums.developer.nvidia.com/t/orchestrating-innovation-at-scale-with-nvidia-maxine-and-texel/306980", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235], "tags": [453, 1950, 2057], "coauthors": [3962, 3961], "class_list": ["post-87546", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-graphics", "tag-featured", "tag-image-recognition", "tag-maxine"], "acf": {"post_industry": ["Media & Entertainment"], "post_products": ["Maxine"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/maxine-eye-contact-texel-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mM2", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Content Creation / Rendering", "link": "https://developer.nvidia.com/blog/category/graphics/", "id": 1235}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87546"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2233"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87546"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87546/revisions"}], "predecessor-version": [{"id": 89034, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87546/revisions/89034"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87559"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87546"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87546"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87546"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87546"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88657, "date": "2024-09-13T09:00:00", "date_gmt": "2024-09-13T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88657"}, "modified": "2024-09-19T12:30:59", "modified_gmt": "2024-09-19T19:30:59", "slug": "improved-data-loading-with-threads", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/improved-data-loading-with-threads/", "title": {"rendered": "Improved Data Loading with Threads"}, "content": {"rendered": "\n<p>Data loading is a critical aspect of deep learning workflows, whether you&#8217;re focused on training or inference. However, it often presents a paradox: the need for a highly convenient solution that is simultaneously customizable. These two goals are notoriously difficult to reconcile.&nbsp;</p>\n\n\n\n<p>One of the traditional solutions to this problem is to scale out the processing and parallelize the user-written function.&nbsp; In this approach, the user creates a custom algorithm, while the system takes on the responsibility of scaling up its execution across multiple workers that simultaneously compute the task. This is where <code>torch.DataLoader</code> comes into play.</p>\n\n\n\n<p>This post documents an experiment we conducted on optimizing <code>torch.DataLoader</code> by switching from processes to threads. This exploration was made possible due to Python&#8217;s ongoing effort to remove the GIL, enabling us to rethink parallelism in deep learning workflows and explore new performance optimizations.&nbsp;</p>\n\n\n\n<h2 id=\"what_is_torchdataloader_and_how_does_it_work\"  class=\"wp-block-heading\">What is torch.DataLoader and how does it work?<a href=\"#what_is_torchdataloader_and_how_does_it_work\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><code>torch.DataLoader</code> is a fundamental tool in PyTorch that facilitates the loading of data in deep learning applications. It plays a pivotal role in managing how data is fed into the model, ensuring that the process is both efficient and effective.&nbsp;</p>\n\n\n\n<p>The important feature of <code>torch.DataLoader</code> is its ability to parallelize the loading process, which is crucial when dealing with large datasets.</p>\n\n\n\n<p>This parallelization is typically achieved by creating multiple worker processes, each responsible for loading a portion of the data. These processes run in parallel, enabling data to be loaded and preprocessed concurrently with model training.&nbsp;</p>\n\n\n\n<p>The parallelism is particularly important for maintaining a steady flow of data to the GPU, minimizing idle time, and maximizing resource utilization.</p>\n\n\n\n<h2 id=\"the_dreaded_gil\"  class=\"wp-block-heading\">The dreaded GIL<a href=\"#the_dreaded_gil\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><code>torch.DataLoader</code> uses processes to parallelize data-loading tasks, and this approach stems directly from a fundamental aspect of Python architecture known as the <em>global interpreter lock</em> (GIL).</p>\n\n\n\n<p>The GIL is a mutex that prevents multiple native threads from executing Python bytecodes simultaneously in CPython, the most widely used Python implementation. This lock was introduced to simplify memory management and ensure thread safety by preventing race conditions when multiple threads try to access or modify Python objects at the same time.</p>\n\n\n\n<p>While the GIL makes Python\u2019s memory management straightforward and helps avoid complex concurrency bugs, it also imposes a significant limitation: Python threads are not truly parallel.&nbsp;</p>\n\n\n\n<p>In CPU-bound tasks, where processing power is the bottleneck, threads are forced to take turns running, leading to suboptimal performance. This is why <code>torch.DataLoader</code> uses processes instead of threads. Each process operates in its own memory space, bypassing the GIL entirely and allowing true parallel execution on multi-core processors.</p>\n\n\n\n<p>Naturally, the GIL\u2019s influence is not all negative. It simplifies the development of Python programs by making thread safety less of a concern for developers, which is one of the reasons Python is so popular.&nbsp;</p>\n\n\n\n<p>On the flip side, the GIL can be a bottleneck in CPU-bound and multi-threaded applications, as it hinders the full utilization of multi-core systems. This trade-off has sparked ongoing debates in the Python community about its merits and drawbacks.</p>\n\n\n\n<h2 id=\"swapping_processes_for_threads\"  class=\"wp-block-heading\">Swapping processes for threads<a href=\"#swapping_processes_for_threads\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With recent developments, the GIL is being removed in upcoming versions of Python. This opens up new possibilities for parallelism in Python applications, including deep learning.&nbsp;</p>\n\n\n\n<p>One of our key ideas was to experiment with swapping the process-based parallelism in <code>torch.DataLoader</code> with thread-based parallelism (Figure 1).</p>\n\n\n\n<figure class=\"wp-block-gallery has-nested-images columns-2 is-cropped wp-block-gallery-2 is-layout-flex wp-block-gallery-is-layout-flex\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"273\" data-id=\"88674\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism-625x273.png\" alt=\"A diagram compares two processes using the PyTorch logo. One side shows three instances of the PyTorch logo with arrows labeled 'spawn' leading from a central logo, indicating process spawning. The other side shows a similar setup, but the arrows are labeled 'pipe' and 'pickle,' suggesting data transfer methods.\" class=\"wp-image-88674\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism-625x273.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism-300x131.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism-768x335.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism-1536x671.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism-645x282.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism-500x218.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism-160x70.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism-362x158.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism-252x110.png 252w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism-1024x447.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/process-parallelism.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>a) Process-based parallelism</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"285\" data-id=\"88673\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism-625x285.png\" alt=\"A diagram compares two processes involving the PyTorch logo. One side shows three gray instances of the PyTorch logo with arrows labeled 'create' leading from a red logo, indicating process creation. The right side shows a similar setup, but the arrows are labeled 'direct access,' suggesting a different method of interaction or data handling.\" class=\"wp-image-88673\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism-625x285.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism-300x137.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism-179x82.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism-768x351.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism-1536x702.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism-645x295.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism-500x228.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism-160x73.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism-362x165.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism-241x110.png 241w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism-1024x468.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/thread-parallelism.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>b) Thread-based parallelism</em></figcaption></figure>\n<figcaption class=\"blocks-gallery-caption wp-element-caption\"><em>Figure 1. Parallelism comparison</em></figcaption></figure>\n\n\n\n<ol class=\"wp-block-list\">\n<li></li>\n</ol>\n\n\n\n<p>Using threads instead of processes has several potential advantages. Threads are generally lighter weight than processes, enabling quicker context switches and lower memory overhead.&nbsp;</p>\n\n\n\n<p>However, threading also comes with its own set of challenges, particularly in ensuring thread safety and avoiding issues like deadlocks.</p>\n\n\n\n<p>We implemented a thread-based version of <code>torch.DataLoader</code> to explore these possibilities. The results were intriguing and demonstrated that threads could be a viable alternative to processes in certain scenarios.</p>\n\n\n\n<h2 id=\"results_of_thread-based_data_loading\"  class=\"wp-block-heading\">Results of thread-based data loading<a href=\"#results_of_thread-based_data_loading\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To assess the performance impact of replacing processes with threads in <code>torch.DataLoader</code>, we conducted a series of experiments across different data processing scenarios. The results highlighted both the potential and the limitations of thread-based parallelism.</p>\n\n\n\n<h3 id=\"image_decoding_with_nvimagecodec\"  class=\"wp-block-heading\">Image decoding with nvImageCodec<a href=\"#image_decoding_with_nvimagecodec\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>One of the most compelling cases for using threads emerged in the image decoding scenario using <a href=\"https://docs.nvidia.com/cuda/nvimagecodec/getting_started.html\">nvImageCodec</a>. In this scenario, the use of threads led to a substantial speedup compared to the traditional process-based approach.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"523\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding-1024x523.png\" alt=\"Bar chart shows throughput in kilobytes of images per second for different numbers of workers between the regular and improved torch.DataLoader. The improved torch.DataLoader consistently achieves higher throughput across all worker counts from 1 to 9.\" class=\"wp-image-88675\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding-1024x523.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding-300x153.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding-625x319.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding-179x91.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding-768x393.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding-1536x785.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding-645x330.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding-500x256.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding-160x82.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding-362x185.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding-215x110.png 215w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvimagecodec-image-decoding.png 1794w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Throughput of </em>nvImageCodec<em> in two scenarios, higher is better</em></figcaption></figure></div>\n\n\n<p class=\"has-text-align-center has-small-font-size\">Benchmark details: EPYC 9654 | H100 | Batch size: 512 | Image size: 640 x 408 (JPEG)</p>\n\n\n\n<p>The primary reason for this improvement is the reduction in CUDA context switching. Processes introduce a heavier overhead when switching contexts, which can cause significant delays, especially in GPU-accelerated workloads.&nbsp;</p>\n\n\n\n<p>Threads, on the other hand, mitigate this overhead, enabling faster, more efficient execution.</p>\n\n\n\n<h3 id=\"image_decoding_with_pillow\"  class=\"wp-block-heading\">Image decoding with Pillow<a href=\"#image_decoding_with_pillow\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In contrast to <code>nvImageCodec</code>, our experiments with Pillow, a widely used Python imaging library, showed that the threaded approach was slightly slower than the process-based method.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"526\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding-1024x526.png\" alt=\"Bar chart shows throughput in kilobytes of images per second for different numbers of workers between the regular and improved torch.DataLoader. There is similar performance between the two, with minor variations, across worker counts from 1 to 9.\" class=\"wp-image-88676\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding-1024x526.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding-300x154.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding-625x321.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding-179x92.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding-768x395.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding-1536x789.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding-645x331.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding-500x257.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding-160x82.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding-362x186.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding-214x110.png 214w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/pillow-throughput-image-decoding.png 1787w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Throughput of Pillow in the two scenarios, higher is better</em></figcaption></figure></div>\n\n\n<p class=\"has-text-align-center has-small-font-size\">Benchmark details: EPYC 9654 | Batch size: 512 | Image size: 640 x 408 (JPEG)</p>\n\n\n\n<p>The key difference here lies in how the global state is managed. Pillow\u2019s operations involve frequent access to global state data stored in dictionaries. When multiple threads access this shared data concurrently, the current implementation relies on atomics to manage these operations safely.&nbsp;</p>\n\n\n\n<p>However, atomics can become a bottleneck under contention, leading to slower performance when compared to separate processes, where each worker has its own isolated state.&nbsp;</p>\n\n\n\n<p>Due to this bottleneck, we initiated a <a href=\"http://discuss.python.org\">discussion on discuss.python.org</a> about revisiting the idea of freezing the data type, which could help mitigate these performance issues by enabling more efficient read access without the need for costly atomics.</p>\n\n\n\n<h3 id=\"combined_results_nvimagecodec_vs_pillow\"  class=\"wp-block-heading\">Combined results: nvImageCodec vs. Pillow<a href=\"#combined_results_nvimagecodec_vs_pillow\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To better show the performance differences, we combined the results from the <code>nvImageCodec</code> and Pillow scenarios into a single chart (Figure 4).&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"525\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput-1024x525.png\" alt=\"Bar chart shows throughput in kilobytes of images per second for different numbers of workers between nvImageCodec in processes, Pillow, and nvImageCodec in threads. nvImageCodec in threads generally achieves the highest throughput, especially as the number of workers increases, followed by Pillow and nvImageCodec in processes.\" class=\"wp-image-88677\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput-1024x525.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput-300x154.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput-625x321.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput-179x92.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput-768x394.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput-1536x788.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput-645x331.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput-500x257.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput-160x82.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput-362x186.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput-214x110.png 214w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/combined-nvimagecodec-pillow-throughput.png 1791w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Combined throughput of Pillow and </em>nvImageCodec<em> using both thread- and process-based </em>torch.DataLoader</figcaption></figure></div>\n\n\n<p class=\"has-text-align-center has-small-font-size\">Benchmark details: EPYC 9654 | H100 | Batch size: 512 | Image size: 640 x 408 (JPEG)</p>\n\n\n\n<p>This comparison clearly demonstrates the stark contrast between the two approaches:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong><code>nvImageCodec</code></strong>: Threads significantly outperform processes, showing that in GPU-heavy tasks with CUDA dependencies, the threaded approach is highly advantageous.</li>\n\n\n\n<li><strong>Pillow</strong>: Processes still hold a slight edge, emphasizing that tasks involving shared state might not benefit as much from threading.</li>\n</ul>\n\n\n\n<p>These findings underscore that removing the GIL can immediately offer significant speedups in GPU-based scenarios. However, as Python takes its first steps into the free-threaded universe, we should put more effort into introducing new tools and concepts that fully leverage hardware capabilities and unlock the language\u2019s full potential.</p>\n\n\n\n<h2 id=\"pros_and_cons_of_thread-based_torchdataloader\"  class=\"wp-block-heading\">Pros and cons of thread-based torch.DataLoader<a href=\"#pros_and_cons_of_thread-based_torchdataloader\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>While our thread-based <code>torch.DataLoader</code> demonstrated clear advantages in certain scenarios, it&#8217;s important to consider the trade-offs.</p>\n\n\n\n<p>The advantages are clear:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Lower overhead</strong>: Threads are less resource-intensive than processes, leading to lower memory usage and faster context switches.</li>\n\n\n\n<li><strong>Better performance in certain scenarios</strong>: As demonstrated in the <code>nvImageCodec</code> experiments, threads can reduce synchronization overhead, improving overall performance.</li>\n</ul>\n\n\n\n<p>The disadvantages are as follows:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Thread safety</strong>: Ensuring that the code is thread-safe can be challenging, especially in complex data pipelines. With threads, there\u2019s also always a higher risk of deadlocks, which can halt the entire data-loading process.</li>\n\n\n\n<li><strong>Extensive synchronization</strong>: Typically, threads must synchronize more often than processes. Implementing thread-based execution needs more scrutiny in the development process.</li>\n\n\n\n<li><strong>Migrating existing implementations</strong>: Free-threaded Python ecosystem is in the early stages of development. It will take some time to adjust the vast amount of dependencies that the deep learning projects have.</li>\n</ul>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The removal of the GIL presents new opportunities for optimizing deep-learning workflows in Python. Our exploration of a thread-based <code>torch.DataLoader</code> demonstrated that it is a beneficial approach whenever the worker implementation involves GPU processing.&nbsp;</p>\n\n\n\n<p>For CPU operations, however, the performance tends to bottleneck due to inefficient parallel read access to data structures, which we hope will be addressed in the future.&nbsp;</p>\n\n\n\n<p>As Python continues to evolve, the landscape of data loading in deep learning is set to change, and we&#8217;re excited to be at the forefront of these developments.</p>\n\n\n\n<p>If you\u2019re interested in learning more about our experiments with free-threaded Python, refer to our <a href=\"https://github.com/NVIDIA/free-threaded-python\">free-threaded Docker environment</a>. Don\u2019t hesitate to post your question in the issues section and try out the free-threaded Python in your use case!</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Data loading is a critical aspect of deep learning workflows, whether you&#8217;re focused on training or inference. However, it often presents a paradox: the need for a highly convenient solution that is simultaneously customizable. These two goals are notoriously difficult to reconcile.&nbsp; One of the traditional solutions to this problem is to scale out the &hellip; <a href=\"https://developer.nvidia.com/blog/improved-data-loading-with-threads/\">Continued</a></p>\n", "protected": false}, "author": 559, "featured_media": 46705, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1485648", "discourse_permalink": "https://forums.developer.nvidia.com/t/improved-data-loading-with-threads/306708", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 696, 3110], "tags": [453, 369], "coauthors": [788, 4015], "class_list": ["post-88657", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-data-science", "category-generative-ai", "tag-featured", "tag-pytorch"], "acf": {"post_industry": ["General"], "post_products": ["General"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2022/04/OptimizeAccess_Featured-Image.jpeg", "jetpack_shortlink": "https://wp.me/pcCQAL-n3X", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88657"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/559"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88657"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88657/revisions"}], "predecessor-version": [{"id": 89017, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88657/revisions/89017"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/46705"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88657"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88657"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88657"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88657"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88870, "date": "2024-09-11T16:01:24", "date_gmt": "2024-09-11T23:01:24", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88870"}, "modified": "2024-09-19T12:31:10", "modified_gmt": "2024-09-19T19:31:10", "slug": "enabling-customizable-gpu-accelerated-video-transcoding-pipelines", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/enabling-customizable-gpu-accelerated-video-transcoding-pipelines/", "title": {"rendered": "Enabling Customizable GPU-Accelerated Video Transcoding Pipelines"}, "content": {"rendered": "\n<p>Today, over 80% of internet traffic is video. This content is generated by and consumed across various devices, including IoT gadgets, smartphones, computers, and TVs. As pixel density and the number of connected devices grow, continued investment in fast, efficient, high-quality video encoding and decoding is essential.</p>\n\n\n\n<p>The latest NVIDIA data center GPUs, such as the <a href=\"https://www.nvidia.com/en-us/data-center/l40s/\">NVIDIA L40S</a> and <a href=\"https://www.nvidia.com/en-us/data-center/l4/\">NVIDIA L4 Tensor Core</a>, address demanding use cases, including AI training, inference, visual computing, cloud gaming, and video transcoding. By combining multiple NVIDIA video decoding (NVDEC) and video encoding (NVENC) video engines with advanced computing capabilities, these GPUs help partners accelerate and customize their transcoding pipelines.</p>\n\n\n\n<p><a href=\"https://www.v-nova.com/london-based-ip-and-software-company/\">V-Nova</a> has ported their implementation of the MPEG-5 Part 2 Low-Complexity Enhancement Video Coding (LCEVC) standard to NVIDIA GPUs. LCEVC enhances existing video coding standards, leveraging NVENC video engines and the computational power of <a href=\"https://www.nvidia.com/en-us/geforce/ada-lovelace-architecture/\">NVIDIA Ada architecture</a> GPUs. This results in improved visual quality and spatial scalability, enabling video providers to build efficient transcoding ladders. These ladders are crucial for maintaining the best video quality across varying network conditions and diverse end devices.</p>\n\n\n\n<p>This post demonstrates how NVIDIA technology enables highly efficient and customizable video transcoding pipelines. First, it provides an overview of LCEVC and its use of NVENC and the <a href=\"https://developer.nvidia.com/video-codec-sdk\">NVIDIA Video Codec SDK</a>. A comparison of visual quality and performance against CPU-based implementations follows. Last, we highlight the cost-effectiveness of a joint transcoding solution using cloud computing instances.</p>\n\n\n\n<h2 id=\"enhancing_video_coding_standards_with_mpeg-5_part_2_lcevc\"  class=\"wp-block-heading\">Enhancing video coding standards with MPEG-5 Part 2 LCEVC<a href=\"#enhancing_video_coding_standards_with_mpeg-5_part_2_lcevc\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>MPEG-5 LCEVC is a &#8220;codec enhancer&#8221; that boosts compression efficiency of any video codec, delivering higher quality at up to <a href=\"https://www.mpeg.org/standards/MPEG-5/2/\">40% lower bitrates</a> while reducing the overall computational complexity compared to encoding at full resolution with the base codec.</p>\n\n\n\n<p>LCEVC employs a hybrid-coding approach with two layers: a base layer and an enhancement layer (Figure 1). The base layer is compressed at a lower resolution, typically a quarter of the target resolution. The enhancement layer refines the video quality by encoding the residual information, that is, the difference between the upscaled base layer and the original frame.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"940\" height=\"378\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/lcevc-encode-decode-workflow.png\" alt=\"LCEVC workflow, including (left to right) encoding, muxing, demuxing, and decoding.\n\" class=\"wp-image-88878\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/lcevc-encode-decode-workflow.png 940w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/lcevc-encode-decode-workflow-300x121.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/lcevc-encode-decode-workflow-625x251.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/lcevc-encode-decode-workflow-179x72.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/lcevc-encode-decode-workflow-768x309.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/lcevc-encode-decode-workflow-645x259.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/lcevc-encode-decode-workflow-500x201.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/lcevc-encode-decode-workflow-160x64.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/lcevc-encode-decode-workflow-362x146.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/lcevc-encode-decode-workflow-274x110.png 274w\" sizes=\"(max-width: 940px) 100vw, 940px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. LCEVC encode and decode flow</em></em></figcaption></figure>\n\n\n\n<p>The low complexity of LCEVC enables efficient, high-performance hardware and software encoder and decoder implementations. V-Nova, in cooperation with NVIDIA, developed an LCEVC encoder for NVIDIA GPUs. Tight integration with NVENC ensures optimal video compression performance with minimal GPU resource usage. Key use cases include ultra-low-latency pixel streaming for VR/XR, cloud gaming, and dense transcoding for video streaming. In these scenarios, LCEVC reduces bandwidth and delivery costs while significantly improving service quality. Although this post focuses on LCEVC-enhanced HEVC, H.264/AVC is also supported and AV1 is on the roadmap.</p>\n\n\n\n<h2 id=\"integrating_nvenc_and_lcevc_for_low-latency_and_latency-tolerant_encoding\"  class=\"wp-block-heading\">Integrating NVENC and LCEVC for low-latency and latency-tolerant encoding<a href=\"#integrating_nvenc_and_lcevc_for_low-latency_and_latency-tolerant_encoding\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://developer.nvidia.com/video-codec-sdk\">NVIDIA Video Codec SDK</a> releases 12.1 and 12.2 introduce several new low-level APIs to enable higher degrees of control and flexibility for NVIDIA customers. These APIs add support for reconstructed frame output and access to encoder statistics, designed to maximize performance and ease of integration. Additionally, new encoding tools have been added for both low-latency (LL) and latency-tolerant encoding use cases that enable improved visual quality for NVENC.</p>\n\n\n\n<p>Figure 2 illustrates the NVENC and LCEVC encoder integration.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1977\" height=\"1083\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc.png\" alt=\"Diagram shows NVENC and LCEVC encoder integration, with source video (left); downsampling, first-level corrections, normative upsampling, top-level corrections, mux (center); LCEVC and base compressed video (right).\n\" class=\"wp-image-88880\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc.png 1977w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc-300x164.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc-625x342.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc-179x98.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc-768x421.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc-1536x841.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc-645x353.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc-500x274.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc-160x88.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc-362x198.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc-201x110.png 201w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/integration-nvenc-lcevc-1024x561.png 1024w\" sizes=\"(max-width: 1977px) 100vw, 1977px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. Integration of NVENC with LCEVC encoder</em></em></figcaption></figure>\n\n\n\n<p>The following functionalities were essential to combine LCEVC GPU implementation with NVENC video encoding engines:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://docs.nvidia.com/video-technologies/video-codec-sdk/12.2/nvenc-video-encoder-api-prog-guide/index.html#nvenc-reconstructed-output\"><strong>Reconstructed frame output API</strong></a>: During frame encoding, the NVENCODE API provides both the compressed frame and the reconstructed frame available in device memory. This enables LCEVC to upsample and encode the reconstructed frame directly on the GPU, eliminating the need for memory copies over PCI Express or decoding the encoded bitstream to obtain reconstruction for further processing.</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/video-technologies/video-codec-sdk/12.2/nvenc-video-encoder-api-prog-guide/index.html#nvenc-row-block-level-stats\"><strong>Encoder statistics API</strong></a>: Rate control poses a significant challenge for highly efficient LCEVC encoder implementations. Managing two layers instead of one, the LCEVC rate control requires detailed information about the base layer encoding process. LCEVC leverages the encoder statistics API to access per block QP and bit count.</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/improving-video-quality-with-nvidia-video-codec-sdk-12-2-for-hevc/\"><strong>New video coding tools</strong></a>: Among other additions, <a href=\"https://docs.nvidia.com/video-technologies/video-codec-sdk/12.2/nvenc-video-encoder-api-prog-guide/index.html#unidirectional-b-frames\">unidirectional B-frames</a> and <a href=\"https://docs.nvidia.com/video-technologies/video-codec-sdk/12.2/nvenc-video-encoder-api-prog-guide/index.html#encoder-tuning-info-and-preset-configurations\">ultra high quality (UHQ)</a> tuning information improve the compression efficiency for low-latency and latency-tolerant use cases, respectively. These new tools are beneficial not only when using NVENC alone but also in combination with LCEVC\u2014that is, as its base layer.</li>\n</ul>\n\n\n\n<h2 id=\"benchmarking_cpu_versus_gpu_video_encoding&nbsp;\"  class=\"wp-block-heading\">Benchmarking CPU versus GPU video encoding&nbsp;<a href=\"#benchmarking_cpu_versus_gpu_video_encoding&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This section benchmarks CPU and GPU transcoding pipeline implementations, focusing on video compression and encoding speed. We examine two use cases: ultra-high-quality (UHQ) natural video transcoding and low-latency encoding for cloud gaming and pixel streaming. The tests include Full HD and UHD encoding at various streaming-relevant bitrates, comparing native HEVC encoders (NVENC HEVC and x265) with their LCEVC-enhanced versions. The comparison assesses quality and cost for both CPU and GPU pipelines.</p>\n\n\n\n<h3 id=\"methodology\"  class=\"wp-block-heading\">Methodology<a href=\"#methodology\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The tests run on comparable cloud computing instances with and without an NVIDIA GPU. Table 1 lists the hardware and encoding configurations for both cloud computing instance types. Table 2 shows the testing content per use case.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"3\"><strong>Hardware and CPU/GPU encoder configuration</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Cloud computing instance</strong></td><td class=\"has-text-align-center\" data-align=\"center\">CPU: AMD EPYC 9R14 &#8211; 16 vCPU<br>GPU: N/A</td><td class=\"has-text-align-center\" data-align=\"center\">CPU: AMD EPYC 7R13 &#8211; 8 vCPU<br>GPU: NVIDIA L4 &#8211; 2x NVENC</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Cost/h</strong></td><td class=\"has-text-align-center\" data-align=\"center\">$0.88</td><td class=\"has-text-align-center\" data-align=\"center\">$0.98</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Encoders</strong></td><td class=\"has-text-align-center\" data-align=\"center\">x265, LCEVC (CPU) x265</td><td class=\"has-text-align-center\" data-align=\"center\">NVENC HEVC, LCEVC (GPU) NVENC HEVC</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Preset</strong></td><td class=\"has-text-align-center\" data-align=\"center\">Medium</td><td class=\"has-text-align-center\" data-align=\"center\">P4</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 1. Hardware and CPU/GPU encoder configurations (cost/h at time of writing)</em></em></figcaption></figure>\n\n\n\n<p></p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"3\"><strong>Testing content and use cases</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Use cases/Tune</strong></td><td class=\"has-text-align-center\" data-align=\"center\">Low Latency (LL)</td><td class=\"has-text-align-center\" data-align=\"center\">Latency-tolerant/Ultra High Quality (UHQ)</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Content type</strong></td><td class=\"has-text-align-center\" data-align=\"center\">Gaming and Natural</td><td class=\"has-text-align-center\" data-align=\"center\">Natural</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Input videos</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">11 videos</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Resolution/Bitrate</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">1080p60 (4, 7, 12, and 15 Mbps) and 2160p60 (12, 15, 22, and 30 Mbps)</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Testing content and use cases</em></figcaption></figure>\n\n\n\n<p>All encoders were tested using FFmpeg version 6.1. To ensure fair comparison, NVENC HEVC and x265 configurations closely match, aligning GOP size, number of B frames, and lookahead depth. Furthermore, to maximize hardware utilization, FFmpeg encodes multiple streams in parallel. See the <a href=\"https://docs-vnova-com.s3.eu-west-1.amazonaws.com/NVIDIA_LCEVC_Devblog_Appendix.pdf\">full command lines</a> used for testing.</p>\n\n\n\n<p>The average per-stream encoding frames per second (FPS) as reported by FFmpeg has to be multiplied by the number of streams to derive the total FPS. To then determine the cost of encoding one hour of video, we combine this FPS with the respective instance cost. For LCEVC x265 and LCEVC NVENC HEVC, the same process was repeated.</p>\n\n\n\n<p>While the latest version of LCEVC NVENC HEVC already shows strong performance, we expect further improvements as development continues. Therefore the presented performance evaluation should be considered conservative.&nbsp;</p>\n\n\n\n<h3 id=\"visual_quality_results\"  class=\"wp-block-heading\">Visual quality results<a href=\"#visual_quality_results\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To evaluate visual quality (VQ), we used a diverse set of video sequences, including natural scenes (for UHQ and LL) and gaming content (for LL), ensuring comprehensive testing. The collection of videos, which includes five in Full HD (1080p) and six in UHD (2160p) resolution. We assessed visual quality across various encoding scenarios with target bitrates of 4-15 Mbps for 1080p and 12-30 Mbps for UHD videos.</p>\n\n\n\n<p>We used the Video Multi-Method Assessment Fusion (VMAF, using libvmaf 3.0.0) metric for its high correlation with MOS scores (subjective quality assessments). Additionally, VMAF No Enhancement Gain (VMAF-NEG) was calculated to account for potential image enhancement biases. LCEVC aims to maximize subjective visual quality. Several subjective quality assessments have been conducted by independent third parties for example during the MPEG standardization process. These <a href=\"https://www.mpeg.org/wp-content/uploads/mpeg_meetings/137_OnLine/w21273.zip\">results</a> include BD-RATE (MOS) versus AVC, HEVC, and VVC.</p>\n\n\n\n<p>Figures 3 through 6 present the rate-distortion (RD) curves, averaged across test content, separated by resolution, tune, and objective metric. These results demonstrate that:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>NVENC HEVC (red solid line) consistently achieves higher coding efficiency than x265 (red dashed line).</li>\n\n\n\n<li>Both NVENC HEVC and x265, when enhanced with LCEVC (green lines), outperform their native counterparts.&nbsp;</li>\n\n\n\n<li>LCEVC consistently achieves bitrate savings for VMAF and VMAF-NEG.</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"720\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves.png\" alt=\"Graphs showing 2160p60 tune LL rate-distortion curves comparing x265, LCEVC x265, NVENC HEVC and LCEVC NVENC HEVC.\" class=\"wp-image-88892\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves-300x108.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves-625x225.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves-179x64.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves-768x277.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves-1536x553.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves-645x232.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves-500x180.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves-160x58.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves-362x130.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves-305x110.png 305w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-ll-rate-distortion-curves-1024x369.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. 2160p60 tune LL rate-distortion curves comparing x265, LCEVC x265, NVENC HEVC, and LCEVC NVENC HEVC</em></em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"777\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves.png\" alt=\"Graphs showing 1080p60 tune LL rate-distortion curves comparing x265, LCEVC x265, NVENC HEVC, and LCEVC NVENC HEVC.\" class=\"wp-image-88894\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves-300x117.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves-625x243.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves-179x70.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves-768x299.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves-1536x597.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves-645x251.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves-500x194.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves-160x62.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves-362x141.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves-283x110.png 283w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-ll-rate-distortion-curves-1024x398.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. 1080p60 tune LL rate-distortion curves comparing x265, LCEVC x265, NVENC HEVC, and LCEVC NVENC HEVC</em></em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"697\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves.png\" alt=\"Graph showing 2160p60 tune UHQ rate-distortion curves comparing x265, LCEVC x265, NVENC HEVC, and LCEVC NVENC HEVC.\" class=\"wp-image-88897\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves-300x105.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves-625x218.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves-179x62.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves-768x268.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves-1536x536.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves-645x225.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves-500x174.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves-160x56.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves-362x126.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves-315x110.png 315w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/2160p60-tune-uhq-rate-distortion-curves-1024x357.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 5. 2160p60 tune UHQ rate-distortion curves comparing x265, LCEVC x265, NVENC HEVC, and LCEVC NVENC HEVC</em></em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"745\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves.png\" alt=\"Figure 6. 1080p60 tune UHQ rate-distortion curves comparing x265, LCEVC x265, NVENC HEVC and LCEVC NVENC HEVC.\" class=\"wp-image-88899\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves-300x112.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves-625x233.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves-179x67.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves-768x286.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves-1536x572.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves-645x240.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves-500x186.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves-160x60.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves-362x135.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves-295x110.png 295w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/1080p60-tune-uhq-rate-distortion-curves-1024x382.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 6. 1080p60 tune UHQ rate-distortion curves comparing x265, LCEVC x265, NVENC HEVC and LCEVC NVENC HEVC</em></em></figcaption></figure></div>\n\n\n<p>Table 3 shows a comparison between the bitrate savings achieved when adding LCEVC to NVENC HEVC and to x265. From this table, we reach the following conclusions:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>BD-rate improvements of LCEVC NVENC HEVC versus NVENC HEVC are similar or higher to those of LCEVC x265 versus x265.</li>\n\n\n\n<li>This efficiency is achieved through the V-Nova tight integration of LCEVC with NVENC, using NVENCODE APIs to provide reconstructed frames and encoder statistics for LCEVC\u2019s rate control.</li>\n</ul>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\">Resolution</td><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\">Tune</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">BD-RATE LCEVC NVENC HEVC versus NVENC HEVC</td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\">BD-RATE LCEVC x265 versus x265</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">VMAF</td><td class=\"has-text-align-center\" data-align=\"center\">VMAF-NEG</td><td class=\"has-text-align-center\" data-align=\"center\">VMAF</td><td class=\"has-text-align-center\" data-align=\"center\">VMAF-NEG</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\">2160p60</td><td class=\"has-text-align-center\" data-align=\"center\">UHQ</td><td class=\"has-text-align-center\" data-align=\"center\">-35.30%</td><td class=\"has-text-align-center\" data-align=\"center\">-18.74%</td><td class=\"has-text-align-center\" data-align=\"center\">-33.21%</td><td class=\"has-text-align-center\" data-align=\"center\">-20.04%</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">LL</td><td class=\"has-text-align-center\" data-align=\"center\">-30.65%</td><td class=\"has-text-align-center\" data-align=\"center\">-16.53%</td><td class=\"has-text-align-center\" data-align=\"center\">-19.85%</td><td class=\"has-text-align-center\" data-align=\"center\">-7.92%</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\">1080p60</td><td class=\"has-text-align-center\" data-align=\"center\">UHQ</td><td class=\"has-text-align-center\" data-align=\"center\">-22.39%</td><td class=\"has-text-align-center\" data-align=\"center\">-4.42%</td><td class=\"has-text-align-center\" data-align=\"center\">-23.61%</td><td class=\"has-text-align-center\" data-align=\"center\">-7.29%</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">LL</td><td class=\"has-text-align-center\" data-align=\"center\">-25.04%</td><td class=\"has-text-align-center\" data-align=\"center\">-11.44%</td><td class=\"has-text-align-center\" data-align=\"center\">-19.82%</td><td class=\"has-text-align-center\" data-align=\"center\">-9.06%</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 3. BD-RATE (VMAF/VMAF-NEG) between GPU encoders (NVENC HEVC and LCEVC NVENC HEVC) and CPU encoders (x265 and LCEVC x265)</em></figcaption></figure>\n\n\n\n<h3 id=\"performance_and_cost_results\"  class=\"wp-block-heading\">Performance and cost results<a href=\"#performance_and_cost_results\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Using the same command lines as the visual quality tests, we measured the encoding speed of CPU and GPU pipelines for low-latency and ultra high quality tunes, both for native HEVC encoders and their LCEVC-enhanced versions. From that, we derive the instance cost to encode 1 hour of video.</p>\n\n\n\n<p>The results in Figure 7 show:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>GPU-accelerated HEVC encoding is 2x-4x cheaper than CPU-based x265 for LL and UHQ tunes across resolutions with and without LCEVC.</li>\n\n\n\n<li>LCEVC accelerates both CPU and GPU implementations, especially at higher resolutions and higher quality settings.</li>\n\n\n\n<li>Combining NVIDIA NVENC with the LCEVC GPU encoder provides the highest throughput and thus the lowest cost of service at significantly improved visual quality.</li>\n</ul>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1167\" height=\"570\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cost-per-hour-gpu-encoded-video-ll-uhq.png\" alt=\"Cost per hour of encoded video (USD) for Low-Latency (LL) and Ultra-High-Quality (UHQ) use cases between the GPU video encoders, NVENC HEVC and LCEVC NVENC HEVC, and CPU encoders x265 and LCEVC x265.\n\" class=\"wp-image-88902\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cost-per-hour-gpu-encoded-video-ll-uhq.png 1167w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cost-per-hour-gpu-encoded-video-ll-uhq-300x147.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cost-per-hour-gpu-encoded-video-ll-uhq-625x305.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cost-per-hour-gpu-encoded-video-ll-uhq-179x87.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cost-per-hour-gpu-encoded-video-ll-uhq-768x375.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cost-per-hour-gpu-encoded-video-ll-uhq-645x315.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cost-per-hour-gpu-encoded-video-ll-uhq-500x244.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cost-per-hour-gpu-encoded-video-ll-uhq-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cost-per-hour-gpu-encoded-video-ll-uhq-362x177.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cost-per-hour-gpu-encoded-video-ll-uhq-225x110.png 225w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cost-per-hour-gpu-encoded-video-ll-uhq-1024x500.png 1024w\" sizes=\"(max-width: 1167px) 100vw, 1167px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 7. Cost per hour of encoded video (USD) for LL and UHQ use cases for GPU video encoders (NVENC HEVC and LCEVC NVENC HEVC) and CPU encoders (x265 and LCEVC x265)</em></em></figcaption></figure>\n\n\n\n<p>NVIDIA and V-Nova both have provided enhancements to performance and visual quality for their solutions through software updates. Because of its low-complexity, software-enhanced nature, V-Nova intends to further improve LCEVC encoding performance for existing hardware and thus cost.</p>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\">Summary<a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The joint NVENC plus LCEVC transcoding solution, developed through the collaboration between NVIDIA and V-Nova, highlights how NVENCODE APIs enable tight integration with the LCEVC GPU encoder. This enables customers to combine the hardware-accelerated NVENC from NVIDIA with the LCEVC multi-layered codec enhancement from V-Nova. This integration improves visual quality, increases throughput, and reduces costs compared to CPU-based solutions.</p>\n\n\n\n<p>Ready to get started? Download the <a href=\"https://developer.nvidia.com/nvidia-video-codec-sdk/download\">NVIDIA Video Codec SDK</a> and the <a href=\"https://download.v-nova.com/\">LCEVC GPU encoder</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Today, over 80% of internet traffic is video. This content is generated by and consumed across various devices, including IoT gadgets, smartphones, computers, and TVs. As pixel density and the number of connected devices grow, continued investment in fast, efficient, high-quality video encoding and decoding is essential. The latest NVIDIA data center GPUs, such as &hellip; <a href=\"https://developer.nvidia.com/blog/enabling-customizable-gpu-accelerated-video-transcoding-pipelines/\">Continued</a></p>\n", "protected": false}, "author": 1964, "featured_media": 89001, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1485034", "discourse_permalink": "https://forums.developer.nvidia.com/t/enabling-customizable-gpu-accelerated-video-transcoding-pipelines/306528", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 1235, 852], "tags": [3139, 453, 1281, 796], "coauthors": [3659, 4022, 4023, 2359, 4024, 4025, 4026], "class_list": ["post-88870", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-graphics", "category-data-center-cloud", "tag-ada-lovelace", "tag-featured", "tag-video-codec-sdk", "tag-video-encoding"], "acf": {"post_industry": ["Gaming"], "post_products": "", "post_learning_levels": "", "post_content_types": "", "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/gpus-graphic-data-center-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-n7o", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88870"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1964"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88870"}], "version-history": [{"count": 21, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88870/revisions"}], "predecessor-version": [{"id": 88918, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88870/revisions/88918"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/89001"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88870"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88870"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88870"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88870"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88694, "date": "2024-09-11T10:21:53", "date_gmt": "2024-09-11T17:21:53", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88694"}, "modified": "2024-09-19T12:31:22", "modified_gmt": "2024-09-19T19:31:22", "slug": "spotlight-xpander-ai-equips-nvidia-nim-applications-with-agentic-tools", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/spotlight-xpander-ai-equips-nvidia-nim-applications-with-agentic-tools/", "title": {"rendered": "Spotlight: xpander AI Equips NVIDIA NIM Applications with Agentic Tools"}, "content": {"rendered": "\n<p>Equipping agentic AI applications with tools will usher in the next phase of AI. By enabling autonomous agents and other AI applications to fetch real-time data, perform actions, and interact with external systems, developers can bridge the gap to new, real-world use cases that significantly enhance productivity and the user experience.</p>\n\n\n\n<p><a href=\"https://xpander.ai/\">xpander AI</a>, a member of the <a href=\"https://www.nvidia.com/en-us/startups/\">NVIDIA Inception</a> program for startups, has developed unique AI-ready connector technology, which offers connectivity between AI apps and the complementary systems that make powerful new use cases a reality. xpander helps AI engineers build applications with advanced tool calling use cases, without the need for significant integration work or improving the accuracy of tool calls, both of which represent common hurdles on the path to production.&nbsp;</p>\n\n\n\n<p>xpander AI connectors help developers overcome these challenges with a multi-agent AI pipeline that ingests data related to a target system, creating an enriched and adjusted connector to that system, ready for use by agentic applications.</p>\n\n\n\n<p>Enterprises that leverage <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a> microservices for improved inference performance and use case-specific optimization can now use xpander AI to equip their NIM applications with agentic tools. This enables companies to build advanced, use case-specific AI apps while minimizing the challenges of integration with external systems and APIs.</p>\n\n\n\n<p>This post explains agentic tool calling in <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models (LLMs)</a>, AI-ready connectors, and how you can use xpander AI to equip your NVIDIA NIM application with agentic tools.</p>\n\n\n\n<h2 id=\"what_is_nvidia_nim\"  class=\"wp-block-heading\">What is NVIDIA NIM?<a href=\"#what_is_nvidia_nim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-eu/ai/\">NVIDIA NIM</a>, part of <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, is a set of intuitive inference microservices designed to accelerate generative AI deployment in enterprises. Supporting a wide range of AI models\u2014including NVIDIA AI foundation, community, and custom\u2014NIM ensures seamless, scalable AI inferencing, on-premises or in the cloud, all while leveraging industry-standard APIs.</p>\n\n\n\n<p>NIM microservices provide interactive APIs to run inference on AI models. They are packaged as container images on a per model or model family basis (Figure 1). Each NIM is its own Docker container with a model and includes a runtime that\u2019s compatible with any NVIDIA GPU with sufficient memory. Under the hood, NIM uses <a href=\"https://docs.nvidia.com/tensorrt-llm/index.html\">NVIDIA TensorRT-LLM</a> to optimize the models, with specialized accelerated profiles optimally selected for NVIDIA H100 Tensor Core GPUs, NVIDIA A100 Tensor Core GPUs, NVIDIA A10 Tensor Core GPUs, and NVIDIA L40S GPUs. </p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/blog/access-to-nvidia-nim-now-available-free-to-developer-program-members/\">NIM microservices are now available to NVIDIA Developer Program members for free</a>.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"575\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-architecture-625x575.png\" alt=\"Diagram showing NVIDIA NIM architecture with a common base (bottom) and config layer for Llama 3.1 8B Instruct, and single command for serving (top).\n\" class=\"wp-image-88698\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-architecture-625x575.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-architecture-300x276.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-architecture-125x115.png 125w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-architecture-768x706.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-architecture-645x593.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-architecture-326x300.png 326w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-architecture-98x90.png 98w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-architecture-362x333.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-architecture-120x110.png 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-architecture-1024x942.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-architecture.png 1435w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. NVIDIA NIM architecture with a common base, config layer for Llama 3.1 8B Instruct, and single command for serving</em></em></figcaption></figure>\n\n\n\n<h2 id=\"what_is_llm_tool_calling\"  class=\"wp-block-heading\">What is LLM tool calling?<a href=\"#what_is_llm_tool_calling\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Tool calling, also known as function calling, refers to the ability of LLMs to interact with external tools, APIs, or functions to perform tasks beyond text generation. This process involves defining tools, prompting the LLM, generating tool calls, executing tools, and incorporating results into an LLM response. It enhances LLM capabilities by providing access to real-time data, executing more complex tasks, and improving accuracy.\u00a0</p>\n\n\n\n<p>Tools are interfaces that accept input, execute an action, and then return a result of that action in a structured output according to a predefined schema. Tools often encompass external API calls, which the agent can use to perform tasks that go beyond the capabilities of the LLM, but aren\u2019t limited to external API calls. For example, to get the current temperature for your city, a weather tool might be used. Or, to find out how your favorite sports team is doing in today\u2019s game, a generic web search tool or ESPN tool might be used.</p>\n\n\n\n<p>Tool calling in LLMs offers several significant benefits to developers by significantly extending the versatility and power of LLM functionality. It provides access to real-time data, enabling models to fetch up-to-date information from external sources, such as your sales team\u2019s most urgent CRM updates or the latest system logging. This, in turn, enhances the accuracy and relevance of responses.&nbsp;</p>\n\n\n\n<p>Additionally, tool calling enables LLMs to execute complex tasks that require specialized skills, like performing advanced mathematical calculations, or access information outside of their training data by querying databases. This capability also facilitates workflow automation by integrating LLMs with various APIs and systems, thereby improving efficiency and enabling the development of sophisticated AI applications that can perform multiple, parallel, and complex functions.&nbsp;</p>\n\n\n\n<p>To support tool calling, an LLM is trained to detect when a specific function should be called, and then output a structured response that contains that function and its arguments. NIM supports tool calling for models that have the aforementioned capability. The LLM is packaged as a NIM, which delivers optimal deployability, usability, and performance on the NVIDIA accelerated infrastructure. The microservice packaging also uses APIs that are compatible with OpenAI, so developers can build powerful, world-class generative AI agents and mitigate some of their most common pain points.&nbsp;</p>\n\n\n\n<p>For more examples using Llama 3.1 8B Instruct NIM, LangChain, and LangGraph, reference <a href=\"https://developer.nvidia.com/blog/building-ai-agents-with-nvidia-nim-microservices-and-langchain/\">Building AI Agents with NVIDIA NIM Microservices and LangChain</a> and the <a href=\"https://github.com/langchain-ai/langchain-nvidia/blob/main/cookbook/nvidia_nim_agents_llama3.1.ipynb\">langchain-ai / langchain-nvidia notebook</a> on GitHub. You can also experiment with tools available as part of a demo on the <a href=\"https://build.nvidia.com/explore/discover#llama-3_1-8b-instruct\">NVIDIA NIM API catalog</a> (Figure 2).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1755\" height=\"807\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog.png\" alt=\"A screenshot showing tools available as part of the  NVIDIA NIM API Catalog demo.\n\" class=\"wp-image-88702\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog.png 1755w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog-300x138.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog-625x287.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog-179x82.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog-768x353.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog-1536x706.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog-645x297.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog-500x230.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog-160x74.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog-362x166.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog-239x110.png 239w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/tools-nvidia-nim-api-catalog-1024x471.png 1024w\" sizes=\"(max-width: 1755px) 100vw, 1755px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. Tools available as part of the NVIDIA NIM API catalog demo</em></em></figcaption></figure>\n\n\n\n<h2 id=\"using_xpander_ai_to_enhance_tool_calling_in_nim_applications\"  class=\"wp-block-heading\">Using xpander AI to enhance tool calling in NIM applications<a href=\"#using_xpander_ai_to_enhance_tool_calling_in_nim_applications\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With NIM, you can integrate AI applications into your organization\u2019s tech stack because your application isn\u2019t decoupled from the internal tools, data, and business logic that are unique to your company. This facilitates powerful new ways to address real-world enterprise challenges.&nbsp;</p>\n\n\n\n<p>However, integration with enterprise tech stacks (both to modern APIs and legacy systems) can require significant effort to prepare the target systems for AI-native integrations. Further, implementation of tool calling can present challenges, especially for calling systems like CRM and ERP with complex APIs. These systems can have highly customizable data structures and hundreds of API endpoints. In some cases, especially with homegrown systems and APIs, target systems lack API documentation and OpenAPI specifications entirely, both of which are often used as a starting point for writing tool calls and describing the tools for the model to use.</p>\n\n\n\n<p>Additionally, the more details you give the model about the target API in your tool calling code, the more accurate the model will be at tool selection and generation of structured responses. Operation IDs, parameters, and descriptions that adhere to organizational best practices are critical for accurate and reliable tool calls.</p>\n\n\n\n<h3 id=\"prebuilt_tools_for_nim_applications\"  class=\"wp-block-heading\">Prebuilt tools for NIM applications<a href=\"#prebuilt_tools_for_nim_applications\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>By providing prebuilt tools for your NIM applications, xpander AI simplifies the process of building with tool calling and increases the accuracy of tool calling at runtime. These tools were generated by the xpander AI connector generator, a unique enrichment pipeline that incorporates data such as API reference, docs, and browsing examples, and outputs an AI-ready connector for any target system.&nbsp;</p>\n\n\n\n<p>Supplying models with highly detailed and automatically enriched tools can be especially beneficial in cases where a target system requires a sequence of interdependent API calls to achieve a task. This could be as simple as sending a message on Slack, or as complex as an agentic task to fulfill a user order, each of which depends on gathering data and submitting multiple data points to several API endpoints. xpander AI connectors use several techniques, including setting up dependencies within an OpenAPI specification, to increase the success rate of agentic applications when required to follow a sequence of APIs.</p>\n\n\n\n<h3 id=\"custom_ai-ready_tools\"  class=\"wp-block-heading\">Custom AI-ready tools<a href=\"#custom_ai-ready_tools\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>If an AI-ready connector doesn\u2019t already exist for your NIM application target system, you can also use xpander AI to build a custom connector. By supplying details like API reference, docs, and others referenced previously, xpander AI can automatically generate an enriched AI-ready tool that is adjusted for your application.</p>\n\n\n\n<p>This section walks you through an example of how to use xpander AI to equip your NIM application with tools, using AI-ready connectors.</p>\n\n\n\n<p>Step 1. In the xpander AI Console, enable any prebuilt connector you want to use in your NIM application, or build a new connector to your homegrown enterprise system (Figure 3).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter\"><img decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXewGEL2psyojXQTsckn2Uo5E7auE4B275QEESgwcLZ1FpFjO2xsMTRz8aIENdFHYYUSMMopfzG0DOzffv1KvpSrsy720RCuSyzVs2NioP9O_Iom0ZUp7xUkcxoF2hiZwPtDWkd6JzILGDoLnBEEZSxlYjA?key=CAGF1-DwKsQxiw5y2zBbYw\" alt=\"A screenshot showing xpander AI Console connectors enablement.\n\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. xpander AI Console connector enablement</em></em></figcaption></figure>\n\n\n\n<p>Step 2. Next, select the exact API operations that are made available to your NIM application (Figure 4).&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"561\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-application-connector-selection-xpander-ai-platform-625x561.png\" alt=\"A screenshot showing connector selection for a NIM application on the xpander AI platform.\" class=\"wp-image-88704\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-application-connector-selection-xpander-ai-platform-625x561.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-application-connector-selection-xpander-ai-platform-300x269.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-application-connector-selection-xpander-ai-platform-128x115.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-application-connector-selection-xpander-ai-platform-768x689.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-application-connector-selection-xpander-ai-platform-645x579.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-application-connector-selection-xpander-ai-platform-334x300.png 334w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-application-connector-selection-xpander-ai-platform-100x90.png 100w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-application-connector-selection-xpander-ai-platform-362x325.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-application-connector-selection-xpander-ai-platform-123x110.png 123w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nim-application-connector-selection-xpander-ai-platform.png 880w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. NIM application connector selection on the xpander AI platform</em></em></figcaption></figure></div>\n\n\n<p>Step 3. You\u2019ll then receive integration details to be used with the xpander SDK within your NIM application (Figure 5).</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"510\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-agent-integration-details-xpander-nim-625x510.png\" alt=\"A screenshot showing AI agent integration details displayed once generated on the xpander AI platform.\n\" class=\"wp-image-88708\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-agent-integration-details-xpander-nim-625x510.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-agent-integration-details-xpander-nim-300x245.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-agent-integration-details-xpander-nim-141x115.png 141w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-agent-integration-details-xpander-nim-768x626.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-agent-integration-details-xpander-nim-645x526.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-agent-integration-details-xpander-nim-368x300.png 368w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-agent-integration-details-xpander-nim-110x90.png 110w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-agent-integration-details-xpander-nim-362x295.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-agent-integration-details-xpander-nim-135x110.png 135w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/ai-agent-integration-details-xpander-nim.png 1002w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>&nbsp;<em>Figure 5. AI agent integration details displayed once generated</em></em></figcaption></figure>\n\n\n\n<p>Step 4. As the code sample below shows, by using the xpander SDK within the NIM application, tool calling will use tools provided by xpander. Specifically, the code sample uses chat completions to call the Llama 3.1 70B Instruct model hosted on the NVIDIA platform. The tools that were selected on the xpander AI side (in this case, Notion, Asana, and HubSpot) will be supplied to the model as tools. When the model determines that a tool should be invoked, the structured response will be returned to the application, and executed using the xpander SDK.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom xpander_sdk import XpanderClient, LLMProvider # pip install https://assets.xpanderai.io/xpander-sdk.tar.gz\nfrom openai import OpenAI\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\n# Get all models that supports tools calling\ntool_models = &#x5B;model for model in ChatNVIDIA.get_available_models() if model.supports_tools]\n\n# Define xpander client \n# Set agent key and URL in https://app.xpander.ai\nxpander_client = XpanderClient(\nagent_key=&quot;$&lt;CHANGE_ME&gt;&quot;, agent_url=&quot;https://inbound.xpander.ai/agent/&lt;CHANGE_ME&gt;&quot;,\nllm_provider=LLMProvider.NVIDIA_NIM\n)\n\n# Get the tools in the specific Nvidia NIM format\nxpander_tools = xpander_client.tools()\n\n# Open an OpenAI API client. Key can be achived as part of NVIDIA AI Enterprise \nopenai_client = OpenAI(api_key=&quot;$&lt;CHANGE_ME&gt;&quot;, \n                       base_url=&quot;https://integrate.api.nvidia.com/v1&quot;)\n\n# Send messages to the LLM + The tools in the correct format of function calling\nllm_response = openai_client.chat.completions.create(\n    model=tool_models&#x5B;0].id, # randomly select first model that supports tool calling\n    messages=&#x5B;{ &quot;role&quot;: &quot;user&quot;,&quot;content&quot;: &quot;Israel medals olympics 2024&quot;}],\n    tools=xpander_tools,\n    tool_choice=&quot;required&quot;,\n    max_tokens=1024,\n)\n\n# Process the chat completion response with the Xpander client\ntool_response = xpander_client.xpander_tool_call(tool_selector_response=llm_response.model_dump())\n\n\nfor tool_response in tool_response:\nprint(tool_response.response_message)\n</pre></div>\n\n\n<h2 id=\"ai_agent_success_rate_with_xpander_ai\"  class=\"wp-block-heading\">AI agent success rate with xpander AI<a href=\"#ai_agent_success_rate_with_xpander_ai\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Ensuring that AI agents reliably execute tasks in production is crucial. It can be evaluated by calculating an AI agent\u2019s success rate\u2014the proportion of successful task executions out of all attempts. xpander AI enhances this metric by converting existing APIs into agentic interfaces, enabling AI agents to interact with and control various systems with consistency and reliability.&nbsp;</p>\n\n\n\n<p>To illustrate this, we compared the task execution success rates of xpander AI\u2019s HubSpot Agentic Interface against HubSpot\u2019s standard OpenAPI Specs using a synthetically generated dataset designed to simulate tasks of varying complexity\u2014from entering a simple deal to generating a comprehensive company record with specific attributes and linking it to a campaign.&nbsp;</p>\n\n\n\n<p>The impact on task execution reliability was significant. The HubSpot standard OpenAPI Specs delivered a success rate of 29.92%, while xpander AI\u2019s HubSpot Agentic Interface achieved an 85.65% rate of success. This comparison demonstrates that xpander AI doesn\u2019t just ensure that your agents function properly. Its tools also significantly enhance agent performance across real-world deployments and enable greater reliability in production environments.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"469\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/agent-success-rate-comparison-xpander-ai-nim-625x469.png\" alt=\"Graphs showing agent success rate comparison with and without xpander AI based on synthetically generated test set of HubSpot tasks of varying complexity. Model with xpander AI outperforms the alternative by over 40%.\n\" class=\"wp-image-88713\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/agent-success-rate-comparison-xpander-ai-nim-625x469.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/agent-success-rate-comparison-xpander-ai-nim-300x225.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/agent-success-rate-comparison-xpander-ai-nim-153x115.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/agent-success-rate-comparison-xpander-ai-nim-768x576.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/agent-success-rate-comparison-xpander-ai-nim-645x484.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/agent-success-rate-comparison-xpander-ai-nim-400x300.png 400w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/agent-success-rate-comparison-xpander-ai-nim-120x90.png 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/agent-success-rate-comparison-xpander-ai-nim-362x272.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/agent-success-rate-comparison-xpander-ai-nim-147x110.png 147w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/agent-success-rate-comparison-xpander-ai-nim.png 1024w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 6. Agent success rate comparison with and without xpander AI based on a synthetically generated test set of HubSpot tasks of varying complexity</em></em></figcaption></figure>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The introduction of tool calling in <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a> represents a significant advancement for agentic capabilities. NIM microservices can now enhance larger automation systems, which enables LLMs to select and leverage the most relevant tools to solve problems. With these enhancements, NIM applications can be equipped with function calling.</p>\n\n\n\n<p>You can use xpander AI-ready connectors to equip your NIM applications with reliable agentic tools, as explained in this post. NIM offers compound advantages that improve runtime inference efficiencies, and xpander AI accelerates development and integration time while solutioning for real-world production use cases.</p>\n\n\n\n<p>xpander AI looks forward to having tangible impact on scientific applications. As more use case-specific NIM microservices are released for biology, chemistry, drug discovery, and more, scientists can use xpander AI to integrate APIs into NIM applications without relying on backend and platform engineers.</p>\n\n\n\n<p>Check out the latest <a href=\"https://docs.nvidia.com/nim/large-language-models/latest/introduction.html\">NIM microservices documentation</a> and visit <a href=\"https://github.com/xpander-ai/xpander-sdk\">xpander-ai / xpander-sdk</a> on GitHub to learn more. Dive deeper into NIM in the NVIDIA <a href=\"https://forums.developer.nvidia.com/c/ai-data-science/ai-foundation-models/669\">AI Foundation Models and Endpoints Developer forum</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Equipping agentic AI applications with tools will usher in the next phase of AI. By enabling autonomous agents and other AI applications to fetch real-time data, perform actions, and interact with external systems, developers can bridge the gap to new, real-world use cases that significantly enhance productivity and the user experience. xpander AI, a member &hellip; <a href=\"https://developer.nvidia.com/blog/spotlight-xpander-ai-equips-nvidia-nim-applications-with-agentic-tools/\">Continued</a></p>\n", "protected": false}, "author": 1992, "featured_media": 88697, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1484904", "discourse_permalink": "https://forums.developer.nvidia.com/t/spotlight-xpander-ai-equips-nvidia-nim-applications-with-agentic-tools/306500", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [453, 3650, 1961], "coauthors": [3689, 4016, 4017, 4018], "class_list": ["post-88694", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-llm-techniques", "tag-nvidia-inception"], "acf": {"post_industry": ["General"], "post_products": ["AI Enterprise", "NIM", "TensorRT-LLM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/graphic-representation-nvidia-nim-microservices.png", "jetpack_shortlink": "https://wp.me/pcCQAL-n4y", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88694"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1992"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88694"}], "version-history": [{"count": 22, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88694/revisions"}], "predecessor-version": [{"id": 88995, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88694/revisions/88995"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88697"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88694"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88694"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88694"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88694"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88761, "date": "2024-09-11T09:54:53", "date_gmt": "2024-09-11T16:54:53", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88761"}, "modified": "2024-09-25T10:26:00", "modified_gmt": "2024-09-25T17:26:00", "slug": "processing-one-billion-rows-of-data-with-rapids-cudf-pandas-accelerator-mode", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/processing-one-billion-rows-of-data-with-rapids-cudf-pandas-accelerator-mode/", "title": {"rendered": "Scaling Up to One Billion Rows of Data in pandas using RAPIDS cuDF"}, "content": {"rendered": "\n<p>The <a href=\"https://github.com/gunnarmorling/1brc\">One Billion Row Challenge</a> is a fun benchmark to showcase basic data processing operations. It was originally launched as a <a href=\"https://www.morling.dev/blog/one-billion-row-challenge/\">pure-Java competition</a>, and has gathered a <a href=\"https://github.com/gunnarmorling/1brc/discussions\">community of developers</a> in other languages, including Python, Rust, Go, Swift, and more. The challenge has been useful for many software engineers with an interest in exploring the details of text file reading, hash-based algorithms, and CPU optimizations. As of mid-2024, the <a href=\"https://github.com/gunnarmorling/1brc\">One Billion Row Challenge GitHub repo</a> has so far attracted more than 1.8K forks, earned more than 6K stars, and inspired dozens of <a href=\"https://github.com/gunnarmorling/1brc?tab=readme-ov-file#1brc-on-the-web\">blog posts and videos</a>.</p>\n\n\n\n<p>This post showcases using <a href=\"https://rapids.ai/cudf-pandas/\">RAPIDS cuDF pandas accelerator mode</a> to complete the challenge of processing one billion rows of data. Specifically, we demonstrate how two new features in cuDF pandas accelerator version 24.08\u2014large string support and managed memory with prefetching\u2014enable improved performance of large data sizes with GPU-accelerated data processing workflows.&nbsp;</p>\n\n\n\n<h2 id=\"data_processing_with_rapids_cudf_pandas_accelerator_mode\"  class=\"wp-block-heading\">Data processing with RAPIDS cuDF pandas accelerator mode<a href=\"#data_processing_with_rapids_cudf_pandas_accelerator_mode\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/glossary/pandas-python/\">pandas</a> is an open-source software library built on top of Python specifically for data manipulation and analysis. It\u2019s a flexible tool for data processing that supports the operations needed to complete the One Billion Row Challenge, including parsing a text file, aggregating numeric data by group, and sorting a table.</p>\n\n\n\n<p>RAPIDS cuDF is a GPU DataFrame library that provides a pandas-like API for loading, filtering, and manipulating data. And <a href=\"https://rapids.ai/cudf-pandas/\">RAPIDS cuDF pandas accelerator mode</a> brings accelerated computing to pandas workflows with zero code changes through a unified CPU/GPU user experience. To learn more, see <a href=\"https://developer.nvidia.com/blog/rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes/\">RAPIDS cuDF Accelerates pandas Nearly 150x with Zero Code Changes</a>.</p>\n\n\n\n<p>The following pandas script is sufficient to complete the One Billion Row Challenge:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nimport pandas as pd\n\ndf = pd.read_csv(\n    \u201cmeasurements.txt\u201d, \n    sep=&#039;;&#039;,\n    header=None,\n    names=&#x5B;&quot;station&quot;, &quot;measure&quot;]\n)\ndf = df.groupby(&quot;station&quot;).agg(&#x5B;&quot;min&quot;, &quot;max&quot;, &quot;mean&quot;])\ndf = df.sort_values(&quot;station&quot;)\n</pre></div>\n\n\n<p>Running this script with cuDF pandas accelerator mode involves adding a single-line command before importing pandas in Python or Jupyter Notebooks. The command is <code>%load_ext cudf.pandas</code>.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n%load_ext cudf.pandas\nimport pandas as pd\n</pre></div>\n\n\n<p>Learn more about <a href=\"https://rapids.ai/cudf-pandas/\">RAPIDS cuDF pandas accelerator mode</a>, including different ways of using this mode in Python, such as the Python module flag, or explicitly enabling through import.\u00a0</p>\n\n\n\n<h2 id=\"new_large_data_processing_features_in_rapids_cudf_pandas_accelerator_mode_2408\"  class=\"wp-block-heading\">New large data processing features in RAPIDS cuDF pandas accelerator mode 24.08<a href=\"#new_large_data_processing_features_in_rapids_cudf_pandas_accelerator_mode_2408\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The 24.08 version of RAPIDS cuDF pandas accelerator mode includes two key features for more efficient data processing: large string support and managed memory pool with prefetching. Together, these features work to enable large DataFrame processing\u2014up to 2.1 billion rows of data, with good performance even at 2-3x total GPU memory. Note that Windows Subsystem for Linux (WSL2) has limited support for GPU oversubscription, and the results featured in this post were collected on Ubuntu 22.04.</p>\n\n\n\n<h3 id=\"large_string_support\"  class=\"wp-block-heading\">Large string support<a href=\"#large_string_support\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Large string support enables RAPIDS cuDF to dynamically switch between 32-bit and 64-bit indices. Rather than supporting two types of strings explicitly, the way <a href=\"https://arrow.apache.org/docs/python/generated/pyarrow.string.html#pyarrow.string\">string</a> and <a href=\"https://arrow.apache.org/docs/python/generated/pyarrow.large_string.html\">large string</a> types exist in PyArrow, strings in cuDF switch to 64-bit indices only when the column data exceeds 2.1 billion characters. This enables cuDF to keep a lower memory footprint and higher processing speed for columns with fewer than 2.1 billion characters, and still support efficient processing for large string columns.\u00a0</p>\n\n\n\n<p>Previously, in version 24.06, string overflow would occur when a string column on the GPU had more than 2.1 billion characters, and the resulting overflow error would cause the data to copy back to the host and fall back to pandas processing. Now, with version 24.08, DataFrames may have a mix of large and small string columns, and each column processes correctly as the string column type in cuDF.&nbsp;</p>\n\n\n\n<h3 id=\"managed_memory_pool_with_prefetching\"  class=\"wp-block-heading\">Managed memory pool with prefetching<a href=\"#managed_memory_pool_with_prefetching\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The <a href=\"https://docs.rapids.ai/api/rmm/stable/python_api/#rmm.mr.ManagedMemoryResource\">managed memory pool</a> with prefetching enables cuDF to use both GPU and host memory to store data and avoid out-of-memory errors. Managed memory, also known as <a href=\"https://developer.nvidia.com/blog/unified-memory-cuda-beginners/\">CUDA Unified Virtual Memory,</a> maintains a single address space backed by GPU and host memory. When a GPU kernel launches, any data that is not accessible by the GPU is paged over (migrates) from host memory to GPU memory. Using a memory pool with managed memory reduces the overhead from each allocation and reduces overall execution time. Prefetching is also important for observing good performance with managed memory, because it helps ensure that data is available for GPU kernels without needing to page the data in at the time of compute, which might be \u201cjust too late.\u201d&nbsp;</p>\n\n\n\n<p>Previously, in version 24.06, larger data sets were more likely to exhaust total GPU memory, and the resulting out-of-memory error would also cause the data to copy back to the host and fall back to pandas processing. Now, with version 24.08, cuDF pandas accelerator mode uses a managed memory pool with prefetching enabled. Note that the best performance with large data sizes can be data and workflow dependent. <a href=\"https://www.surveymonkey.com/r/TX3QQQR\">We welcome your feedback</a>.</p>\n\n\n\n<h2 id=\"running_the_one_billion_row_challenge_using_nvidia_gpus\"  class=\"wp-block-heading\">Running the One Billion Row Challenge using NVIDIA GPUs<a href=\"#running_the_one_billion_row_challenge_using_nvidia_gpus\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>You can run the challenge both on high and low memory GPUs, which shows the performance impact of large data features in RAPDIS cuDF 24.08. At a row count of one billion, the challenge begins with a 13.1 GB text file. In cuDF, this turns into 16 GB of string data and 8 GB of float64 data. The <code>read_csv</code> operation tops out at ~76 GB peak memory footprint and the <code>groupby</code> aggregate operation tops out at ~56 GB peak memory footprint.&nbsp;</p>\n\n\n\n<p>Note that the input files were generated according to the One Billion Row Challenge GitHub repo, stored on local NVMe SSD, and cached by the OS during benchmarking. Wall time was measured as the time to execute a complete the <code>subprocess.run</code> command in Python, including all initialization and loading steps.</p>\n\n\n\n<h3 id=\"nvidia_a100_tensore_core_gpu&nbsp;\"  class=\"wp-block-heading\">NVIDIA A100 Tensore Core GPU&nbsp;<a href=\"#nvidia_a100_tensore_core_gpu&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>When running the challenge with RAPIDS cuDF pandas accelerator mode and a GPU with sufficient memory, large string support is critical to good performance. The first hardware set uses an <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 Tensore Core</a> 80 GB PCIe GPU, an Arm Neoverse-N1 CPU with 500 GiB of RAM, and a Samsung MZ1L23T8HBLA SSD. For row counts of 200 million, cuDF shows ~6 seconds runtime, compared to the pandas CPU-only runtime of ~50 seconds. However, for 300 million rows, the string overflow in cuDF 24.06 causes pandas to fall back and increases runtime to ~240 seconds.\u00a0</p>\n\n\n\n<p>With large string support in cuDF 24.08, we observe a one billion row runtime of 17 seconds, which is much faster than the pandas runtime of 260 seconds and cuDF 24.06 runtime of 800 seconds. This is shown in Figure 1 in green. Note that graceful fallback to the CPU due to the GPU running out of memory leads to higher runtime on cuDF 24.06.</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"536\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-a100-gpu-625x536.png\" alt=\"Graph showing runtime for the 1BRC as a function of row count, with cuDF 24.06, cuDF 24.08, and pandas 2.2.2, on an NVIDIA A100 GPU node.\" class=\"wp-image-88773\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-a100-gpu-625x536.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-a100-gpu-300x257.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-a100-gpu-134x115.png 134w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-a100-gpu-768x658.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-a100-gpu-645x553.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-a100-gpu-350x300.png 350w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-a100-gpu-105x90.png 105w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-a100-gpu-362x310.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-a100-gpu-128x110.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-a100-gpu.png 834w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Runtime for the One Billion Row Challenge as a function of row count, with cuDF 24.06, cuDF 24.08, and pandas 2.2.2, on an NVIDIA A100 GPU node</em></figcaption></figure></div>\n\n\n\n<h3 id=\"nvidia_tesla_t4_gpu\"  class=\"wp-block-heading\">NVIDIA Tesla T4 GPU<a href=\"#nvidia_tesla_t4_gpu\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>We can also evaluate performance on an older generation GPU from 2018 that is widely available on notebook platforms like Colab and Kaggle. In this case, the managed memory pool with prefetching becomes critical for good performance. The second hardware set uses an <a href=\"https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-t4/t4-tensor-core-product-brief.pdf\">NVIDIA Tesla T4</a> 14 GB PCIe GPU, an Intel Xeon Gold 6130 CPU with 376 GiB of RAM, and a Dell Express Flash PM1725a SSD. For row counts of 200 million, RAPIDS cuDF pandas accelerator mode shows ~10 seconds runtime compared to a pandas runtime of ~130 seconds. When we scale to one billion rows, the T4 GPU is operating at about 5x oversubscription and still completing with a runtime of 200 seconds, compared to a pandas runtime of 660 seconds (Figure 2).</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"531\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-t4-gpu-625x531.png\" alt=\"Graph showing runtime for the 1BRC as a function of row count, with cuDF 24.06, cuDF 24.08 and pandas 2.2.2, on an NVIDIA T4 GPU node.\" class=\"wp-image-88776\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-t4-gpu-625x531.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-t4-gpu-300x255.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-t4-gpu-135x115.png 135w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-t4-gpu-768x652.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-t4-gpu-645x548.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-t4-gpu-353x300.png 353w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-t4-gpu-106x90.png 106w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-t4-gpu-362x307.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-t4-gpu-130x110.png 130w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-nvidia-t4-gpu.png 841w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Runtime for the One Billion Row Challenge as a function of row count, with cuDF 24.06, cuDF 24.08 and pandas 2.2.2, on an NVIDIA T4 GPU node</em></figcaption></figure></div>\n\n\n\n<p>Overall, the combination of large string support and managed memory pool with prefetching in RAPIDS cuDF pandas accelerator mode 24.08 removes the data size limits that held back performance in version 24.06. In 24.08, the larger memory capacity of the NVIDIA A100 GPU results in a faster runtime and less host-to-GPU data movement than what we observe for the NVIDIA T4 GPU. You can decide which GPU makes sense for your workflows based on cost efficiency and performance. Scaling your data size with cuDF now comes with fewer barriers and more predictable runtimes.</p>\n\n\n\n<h2 id=\"optimizing_the_challenge_in_libcudf\"  class=\"wp-block-heading\">Optimizing the challenge in libcudf<a href=\"#optimizing_the_challenge_in_libcudf\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>If you\u2019re building GPU-accelerated data processing applications and need lower overhead and faster runtimes, we recommend using RAPIDS <a href=\"https://docs.rapids.ai/api/libcudf/stable/\">libcudf</a>, the CUDA C++ computational core of cuDF. libcudf accelerates database and DataFrame operations, from ingestion and parsing, to joins, aggregations, and more.&nbsp;</p>\n\n\n\n<p>We\u2019ve published a module of new <a href=\"https://github.com/rapidsai/cudf/tree/HEAD/cpp/examples/\">C++ examples</a> named <code>billion_rows</code>. These examples demonstrate single-threaded data chunking and multithreaded data pipelining with libcudf. The <code>brc</code> example shows the simple, single-batch processing of the 13 GB One Billion Row Challenge input file. The <code>brc_chunks</code> example shows a chunking pattern that reads byte ranges from the input file, computes partial aggregations, and then combines the final result. The <code>brc_pipeline</code> example shows a pipelining pattern that uses multiple host threads and device CUDA streams to complete the chunked work while saturating copy bandwidth and compute capacity.</p>\n\n\n\n<p>When comparing these methods on an NVIDIA A100 GPU, we find that <code>brc_pipeline</code> achieves the fastest runtime, with ~5.2 seconds runtime using 256 chunks and four threads (Figure 3). With 80 GB of GPU memory on the A100 GPU, all three of the methods can complete the challenge at one billion rows. Chunking and pipelining provide faster runtimes as well as dramatically lowered peak memory footprint. For one billion rows, <code>brc</code> uses 55 GB peak memory, while <code>brc_chunks</code> and <code>brc_pipeline</code> use &lt;1 GB peak memory. The techniques in the libcudf <code>billion_rows</code> example module show how to complete large workflows while efficiently saturating GPU resources and PCIe bandwidth.</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"607\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-a100-gpu-625x607.png\" alt=\"Graph showing runtime for the One Billion Row Challenge on an NVIDIA A100 GPU node, as a function of row count, with cuDF 24.08 and libcudf 24.08 examples from the billion_rows C++ module.\" class=\"wp-image-88782\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-a100-gpu-625x607.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-a100-gpu-300x291.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-a100-gpu-118x115.png 118w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-a100-gpu-645x626.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-a100-gpu-309x300.png 309w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-a100-gpu-93x90.png 93w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-a100-gpu-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-a100-gpu-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-a100-gpu-362x352.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-a100-gpu-113x110.png 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-a100-gpu.png 732w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Runtime for the 1BRC on an NVIDIA A100 GPU node, as a function of row count, with cuDF 24.08 and libcudf 24.08 examples from the billion_rows C++ module</em></figcaption></figure></div>\n\n\n\n<p>Shifting over to the NVIDIA T4 GPU, we find that the <code>brc_pipeline</code> method also achieves the fastest runtime, with ~5.7 seconds runtime using 256 chunks and four threads (Figure 4). For the optimized <code>brc_pipeline</code> case, results for both the T4 and A100 GPUs look similar due to the limits of data transfer from host to GPU over PCIe. With the 16 GB memory capacity of T4, the <code>brc</code> example runs out of memory after 200 million rows and can\u2019t complete the challenge. Chunking and pipelining are effective methods for efficiently completing the challenge, even with lower memory capacity GPUs like T4.</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"611\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu-625x611.png\" alt=\"Graph showing runtime for the 1BRC on an NVIDIA T4 GPU node, as a function of row count, with cuDF 24.08 and libcudf 24.08 examples from the billion_rows C++ module.\" class=\"wp-image-88785\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu-625x611.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu-300x293.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu-118x115.png 118w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu-645x630.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu-307x300.png 307w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu-92x90.png 92w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu-362x354.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu-113x110.png 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/runtime-1brc-row-count-cudf-libcudf-nvidia-t4-gpu.png 735w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Runtime for the One Billion Row Challenge on an NVIDIA T4 GPU node, as a function of row count, with cuDF 24.08 and libcudf 24.08 examples from the billion_rows C++ module</em></figcaption></figure></div>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Processing large datasets is now easier than ever with RAPIDS cuDF pandas accelerator mode. With large string support, your string processing workflow can scale beyond the previous 2.1 billion character limit. With the new managed memory pool, your data processing memory footprint can extend beyond the GPU memory limit.\u00a0To get started with cuDF pandas accelerator mode, check out <a href=\"https://developer.nvidia.com/blog/rapids-cudf-instantly-accelerates-pandas-up-to-50x-on-google-colab/\">RAPIDS cuDF Instantly Accelerates pandas up to 50x on Google Colab</a>.</p>\n\n\n\n<p>RAPIDS cuDF pandas accelerator mode is built using RAPIDS libcudf, the CUDA C++ library for GPU data processing. To get started with RAPIDS libcudf, try building and running a few <a href=\"https://github.com/rapidsai/cudf/tree/HEAD/cpp/examples/\">C++ examples</a>. <a href=\"https://hub.docker.com/r/rapidsai/rapidsai/\">RAPIDS Docker containers</a> are also available for releases and nightly builds to enable easier testing and deployment.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The One Billion Row Challenge is a fun benchmark to showcase basic data processing operations. It was originally launched as a pure-Java competition, and has gathered a community of developers in other languages, including Python, Rust, Go, Swift, and more. The challenge has been useful for many software engineers with an interest in exploring the &hellip; <a href=\"https://developer.nvidia.com/blog/processing-one-billion-rows-of-data-with-rapids-cudf-pandas-accelerator-mode/\">Continued</a></p>\n", "protected": false}, "author": 1438, "featured_media": 56140, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1484893", "discourse_permalink": "https://forums.developer.nvidia.com/t/processing-one-billion-rows-of-data-with-rapids-cudf-pandas-accelerator-mode/306493", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1464, 696], "tags": [453, 1731], "coauthors": [2891, 3078], "class_list": ["post-88761", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-cybersecurity", "category-data-science", "tag-featured", "tag-pandas"], "acf": {"post_industry": ["General"], "post_products": ["A100", "cuDF", "RAPIDS"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Benchmark", "Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2022/10/image3-2.png", "jetpack_shortlink": "https://wp.me/pcCQAL-n5D", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88761"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1438"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88761"}], "version-history": [{"count": 29, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88761/revisions"}], "predecessor-version": [{"id": 89465, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88761/revisions/89465"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/56140"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88761"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88761"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88761"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88761"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88957, "date": "2024-09-11T09:28:27", "date_gmt": "2024-09-11T16:28:27", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88957"}, "modified": "2024-10-09T12:43:46", "modified_gmt": "2024-10-09T19:43:46", "slug": "ai-tool-helps-farmers-combat-crop-loss-and-climate-change", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/ai-tool-helps-farmers-combat-crop-loss-and-climate-change/", "title": {"rendered": "AI Tool Helps Farmers Combat Crop Loss and Climate Change"}, "content": {"rendered": "\n<p>Machine Learning algorithms are beginning to revolutionize modern agriculture. Enabling farmers to combat pests and diseases in real time, the technology is improving crop production and profits, while reducing waste, greenhouse gas emissions, and pesticide use.</p>\n\n\n\n<p>Around 6% of the world\u2019s CO2 emissions come from farming. And every year, up to 40% of crops are lost due to pests and disease. For farmers already operating in a low-margin industry, critical resources wasted on unused crops makes surviving, let alone thriving, that much harder.&nbsp;</p>\n\n\n\n<p>But a new AI-powered platform from startup <a href=\"https://www.fermata.tech/\">Fermata</a> offers farmers a way to mitigate the impact of pests and crop diseases while also making farming more sustainable, and worker-friendly.&nbsp;</p>\n\n\n\n<p>The new ML-powered computer vision system, named Croptimus, continuously scans crops 24/7. When it detects pests or early signs of crop disease, the platform immediately alerts farmers, enabling them to rush resources to impacted crops and keep the threat localized.&nbsp;</p>\n\n\n\n<p>The platform is trained on high-quality data, enabling its computer vision software to accurately differentiate between healthy and at-risk crops and quickly identify pests or diseases. The system gives farmers real-time analytics and provides annotated, 360-degree, augmented reality maps showing up-to-the-minute reports on the health of crops.</p>\n\n\n\n<p>Fermata trains its models using <a href=\"https://developer.nvidia.com/cudnn\">PyTorch with NVIDIA cuDNN</a> on on-prem devices. For inferencing, it uses a combination of both cloud and on-prem compute, including <a href=\"https://www.nvidia.com/en-us/data-center/tesla-t4/\">NVIDIA T4 GPUs </a>running on AWS cloud, and <a href=\"https://developer.nvidia.com/embedded/jetson-nano\">NVIDIA Jetson Nano</a> code optimized with <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> for accelerated performance.\u00a0</p>\n\n\n\n<p>The Croptimus system is deployable in a variety of configurations in large greenhouses and in outdoor farms. Cameras to scan and analyze crops can be mounted on tall poles, greenhouse ceilings, integrated into aerial drones, or attached to mobile robots that regularly traverse rows of crops.&nbsp;&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1324\" height=\"724\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Croptimus-AI-tool.png\" alt=\"An image of a greenhouse.\" class=\"wp-image-88958\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Croptimus-AI-tool.png 1324w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Croptimus-AI-tool-300x164.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Croptimus-AI-tool-625x342.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Croptimus-AI-tool-179x98.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Croptimus-AI-tool-768x420.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Croptimus-AI-tool-645x353.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Croptimus-AI-tool-500x273.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Croptimus-AI-tool-160x87.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Croptimus-AI-tool-362x198.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Croptimus-AI-tool-201x110.png 201w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Croptimus-AI-tool-1024x560.png 1024w\" sizes=\"(max-width: 1324px) 100vw, 1324px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Fermata\u2019s 24-7 crop monitors are deployable in greenhouses like the one pictured and can detect pests or diseases quickly, before they can spread and widely impact yields</em></figcaption></figure></div>\n\n\n<p><a href=\"https://www.youtube.com/watch?v=COCqZqikhgM\"></a>The AI-powered model doesn\u2019t replace a farmer\u2019s existing routines\u2014it augments traditional workflows and makes them more targeted.\u00a0</p>\n\n\n\n<p>For instance, farms typically rely on trained scouters to manually inspect crops. But high-quality scouters are increasingly in short supply, and when they are available, they\u2019re expensive. Additionally, humans get tired or make mistakes. When scouters fail to detect crop diseases or pests, they can quickly proliferate, which can lead to widespread crop spoilage and waste.\u00a0</p>\n\n\n\n<p>By contrast, AI systems like Croptimus are always scanning for issues. When the system flags a potential problem, it sends an alert to farmers who can then direct humans to inspect crops and, when necessary, intervene before pests or diseases can spread. Early intervention reduces crop loss and helps farmers use human labor in a more targeted way.</p>\n\n\n\n<p>Another upside to the system is that farms can detect and mitigate pests and diseases early and have less need for pesticides. Which not only saves farmers money, but also reduces the negative effects of pesticides leaking into the environment.\u00a0</p>\n\n\n\n<p>Read the full story about this ML-powered farming system in <a href=\"https://www.futurefarming.com/smart-farming/fermata-and-agre-tech-join-forces-to-integrate-pest-monitoring-and-robotic-systems/\">Future Farming</a>.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Machine Learning algorithms are beginning to revolutionize modern agriculture. Enabling farmers to combat pests and diseases in real time, the technology is improving crop production and profits, while reducing waste, greenhouse gas emissions, and pesticide use. Around 6% of the world\u2019s CO2 emissions come from farming. And every year, up to 40% of crops are &hellip; <a href=\"https://developer.nvidia.com/blog/ai-tool-helps-farmers-combat-crop-loss-and-climate-change/\">Continued</a></p>\n", "protected": false}, "author": 2156, "featured_media": 88973, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1484879", "discourse_permalink": "https://forums.developer.nvidia.com/t/ai-tool-helps-farmers-combat-crop-loss-and-climate-change/306490", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758, 1903], "tags": [2415, 3941, 1913, 453], "coauthors": [3876], "class_list": ["post-88957", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-edge-computing", "category-features", "tag-agriculture", "tag-ai-impact", "tag-climate-weather-ocean-modeling", "tag-featured"], "acf": {"post_industry": ["General"], "post_products": ["cuDNN", "Jetson", "TensorRT"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Fermata-greenhouse-deployment.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-n8N", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Edge Computing", "link": "https://developer.nvidia.com/blog/category/edge-computing/", "id": 2758}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88957"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2156"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88957"}], "version-history": [{"count": 3, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88957/revisions"}], "predecessor-version": [{"id": 88962, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88957/revisions/88962"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88973"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88957"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88957"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88957"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88957"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88069, "date": "2024-09-11T09:25:00", "date_gmt": "2024-09-11T16:25:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88069"}, "modified": "2024-09-19T12:31:59", "modified_gmt": "2024-09-19T19:31:59", "slug": "advanced-strategies-for-high-performance-gpu-programming-with-nvidia-cuda", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/advanced-strategies-for-high-performance-gpu-programming-with-nvidia-cuda/", "title": {"rendered": "Advanced Strategies for High-Performance GPU Programming with NVIDIA CUDA"}, "content": {"rendered": "\n<p>Stephen Jones, a leading expert and distinguished NVIDIA CUDA architect, offers his guidance and insights with a deep dive into the complexities of mapping applications onto massively parallel machines. Going beyond the basics to explore the intricacies of GPU programming, he focuses on practical techniques such as parallel program design and specific details of GPU optimization for improving the efficiency and performance of your application.</p>\n\n\n\n<p>As part of an ongoing series, this session builds on previous talks. While there&#8217;s no requirement to have seen earlier sessions, you can explore foundational topics like <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31151/\">how GPU computing works</a>, <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41487/\">how CUDA programming works</a>, and <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51210/\">how to write a CUDA program</a>.</p>\n\n\n\n<p>Whether you&#8217;re new to CUDA or looking to enhance your GPU programming skills, this session offers both the theoretical knowledge and actionable strategies needed to excel in high-performance computing.</p>\n\n\n\n<script src=\"https://api-prod.nvidia.com/search/nvidia-search-library.js\"></script>\n \n\n<div id=\"nvidia-event-details-widget\"></div>\n<style>\n.nvidia-search-widget .cleanslate , .nvidia-search-widget .player-overlay {\ndisplay:none;\n}\n</style>\n \n\n<script>\n \n NvidiaSearchLibrary.EventSessionDetailsWidget.mount({\n          site: 'https://www.nvidia.com',\n          language: 'en-us',\n          sessionId: 'gtc24-s62401',\n          jwtToken: '',\n \u2002\u2002\u2002\u2002voltronApiUrl:  'https://api-prod.nvidia.com/services/nod/api/v1/',\n          apiUrl: 'https://api-prod.nvidia.com/search/graphql',\n           onLogin: () => { },\n          onLogout: () => { },\n       \n          onSeeAllSessions: (speakerName) => {\n            window.location.href =  'https://www.nvidia.com/en-us/on-demand/search/?q=\"' + speakerName+'\"';\n          },\n          searchApiUrl: 'https://api-prod.nvidia.com/search/graphql',\n          searchToken: '',\n          uiConfId: '50468382',\n          showSessionRating: false,\n          anonToken: '',\n        });\n \n</script>\n\n\n\n<p>Follow along with a&nbsp;<a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/How-To-Write-A-CUDA-Program.pdf\">PDF of the session</a>, which will equip you with advanced skills and insights to write highly efficient CUDA programs, helping you get the most out of your GPUs. You&#8217;ll dive into:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>GPU architecture</strong>: Key differences between CPU and GPU approaches, with a focus on the NVIDIA Hopper H100 GPU and its implications for parallel processing.</li>\n\n\n\n<li><strong>Parallelism</strong>: Distinction and effective use of data and task parallelism in CUDA programming.</li>\n\n\n\n<li><strong>CUDA execution model</strong>: Understanding how CUDA manages threads and blocks to maximize performance.</li>\n\n\n\n<li><strong>Optimizing data parallelism</strong>: Strategies for running bulk data parallelism and mitigating wave quantization issues.</li>\n\n\n\n<li><strong>Single-wave kernels</strong>: Benefits of mapping data to threads for better load balancing and efficiency.</li>\n\n\n\n<li><strong>Task parallelism</strong>: Enhancing efficiency using CUDA streams and managing dependencies between streams.</li>\n\n\n\n<li><strong>Pipeline parallelism</strong>: Optimizing complex algorithms like sorting with data splitting and dependency management.</li>\n\n\n\n<li><strong>Cache optimization</strong>: Techniques for tiling execution in cache and running tasks in series to boost performance.</li>\n\n\n\n<li><strong>Advanced CUDA techniques</strong>: Avoiding cache thrashing, task-based cache tiling, and minimizing inter-task dependencies.</li>\n</ul>\n\n\n\n<p>Watch the advanced talk on\u00a0<a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s62401/\">How To Write A CUDA Program</a>, explore more videos on NVIDIA On-Demand, and gain valuable skills and insights from industry experts by joining the\u00a0<a href=\"https://developer.nvidia.com/developer-program\">NVIDIA Developer Program</a>.</p>\n\n\n\n<p><em>This content was partially crafted with the assistance of generative AI and LLMs. It underwent careful review and was edited by the NVIDIA Technical Blog team to ensure precision, accuracy, and quality.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Stephen Jones, a leading expert and distinguished NVIDIA CUDA architect, offers his guidance and insights with a deep dive into the complexities of mapping applications onto massively parallel machines. Going beyond the basics to explore the intricacies of GPU programming, he focuses on practical techniques such as parallel program design and specific details of GPU &hellip; <a href=\"https://developer.nvidia.com/blog/advanced-strategies-for-high-performance-gpu-programming-with-nvidia-cuda/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 87828, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1484875", "discourse_permalink": "https://forums.developer.nvidia.com/t/advanced-strategies-for-high-performance-gpu-programming-with-nvidia-cuda/306489", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 1903], "tags": [453, 3986, 527], "coauthors": [2315], "class_list": ["post-88069", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-features", "tag-featured", "tag-nvidia-on-demand", "tag-programming-languages-and-compilers"], "acf": {"post_industry": ["General", "HPC / Scientific Computing"], "post_products": ["CUDA"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Video"], "post_collections": ["GTC March 2024"]}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/CUDA-efficiency.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mUt", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88069"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88069"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88069/revisions"}], "predecessor-version": [{"id": 88795, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88069/revisions/88795"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87828"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88069"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88069"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88069"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88069"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88631, "date": "2024-09-11T09:00:00", "date_gmt": "2024-09-11T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88631"}, "modified": "2024-09-19T12:32:10", "modified_gmt": "2024-09-19T19:32:10", "slug": "constant-time-launch-for-straight-line-cuda-graphs-and-other-performance-enhancements", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/constant-time-launch-for-straight-line-cuda-graphs-and-other-performance-enhancements/", "title": {"rendered": "Constant Time Launch for Straight-Line CUDA Graphs and Other Performance Enhancements"}, "content": {"rendered": "\n<p><a href=\"https://developer.nvidia.com/blog/cuda-graphs/\">CUDA Graphs</a> are a way to define and batch GPU operations as a graph rather than a sequence of stream launches. A CUDA Graph <a href=\"https://developer.nvidia.com/blog/constructing-cuda-graphs-with-dynamic-parameters/\">groups</a> a set of CUDA kernels and other CUDA operations together and executes them with a specified dependency tree. It speeds up the workflow by combining the driver activities associated with CUDA kernel launches and CUDA API calls. It also enforces the dependencies with hardware accelerations, instead of relying solely on CUDA streams and events, when possible.</p>\n\n\n\n<p>CUDA Graphs are particularly important for AI frameworks, as they enable you to capture and replay a sequence of CUDA operations, reducing CPU overhead and improving performance. With the latest improvements, you can now take even greater advantage of CUDA Graphs to accelerate AI workloads.</p>\n\n\n\n<p>NVIDIA has improved the performance of CUDA Graphs in several respects between CUDA Toolkit 11.8 and CUDA Toolkit 12.6 and the accompanying driver builds:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Graph construction and instantiation time</li>\n\n\n\n<li>CPU launch overhead&nbsp;</li>\n\n\n\n<li>General performance in several edge cases</li>\n</ul>\n\n\n\n<p>In this post, we present some microbenchmark numbers for various graphs and discuss the improvements since the initial release. The benchmark code is included. As always, performance characteristics depend on the CPU, GPU, and clock settings. For our testing, we used an Intel Xeon Silver 4208 Processor @ 2.10GHz and an NVIDIA GeForce RTX 3060 GPU. The OS was Ubuntu 22.04. The results in this post were collected using default clock settings.</p>\n\n\n\n<p>This is a snapshot of performance improvements as of CUDA 12.6, so future CUDA releases may not demonstrate the same performance as documented in this post. This snapshot is presented assuming familiarity with CUDA Graphs and how their performance characteristics impact applications. For more information, see <a href=\"https://developer.nvidia.com/blog/cuda-graphs/\">Getting Started with CUDA Graphs</a>.&nbsp;</p>\n\n\n\n<h2 id=\"definitions\"  class=\"wp-block-heading\">Definitions<a href=\"#definitions\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Here are some graph topology terms and performance characteristics discussed later in this post.</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Straight line kernel graph:</strong> A CUDA Graph made completely out of kernel nodes, where every node has a single dependent node except for the last node.</li>\n\n\n\n<li><strong>Parallel straight line:</strong> A CUDA Graph that has width as the number of entry points. Each entry point is followed by its own set of lengths as the number of nodes arrayed in a straight-line formation. This is also referred to as <em>parallel chain</em> in the charts and source code.</li>\n\n\n\n<li><strong>First launch:</strong> The first launch of a graph, which also includes uploading the instantiated graph to the device.</li>\n\n\n\n<li><strong>Repeat launch:</strong> Launching a graph that has already been uploaded to the device.</li>\n\n\n\n<li><strong>CPU overhead for a launch:</strong> CPU time between invoking <code>CUDAGraphLaunch</code> and the completion of the function.&nbsp;</li>\n\n\n\n<li><strong>Device side runtime: </strong>The amount of time for the operation to run on the device.</li>\n\n\n\n<li><strong>Repeat launch device side runtime:</strong> The amount of time it takes for a graph that was previously uploaded to run on the device.</li>\n\n\n\n<li><strong>Time to solution:</strong> Time from the launch call to the completion of the graph running on the device.</li>\n</ul>\n\n\n\n<h2 id=\"constant_time_cpu_overhead_for_repeat_launch\"  class=\"wp-block-heading\">Constant time CPU overhead for repeat launch<a href=\"#constant_time_cpu_overhead_for_repeat_launch\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>One notable improvement highlighted is related to CPU overhead for repeat launch on the NVIDIA Ampere architecture. For straight-line kernel graphs, there has been a significant reduction in time taken during repeat launches. Specifically, the time decreased from 2\u03bcs + 200 ns extra time per node to a nearly constant time of 2.5us + (~1ns per node), showcasing consistent speed enhancements for graphs with 10 nodes or more.</p>\n\n\n\n<p>Launch still takes <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=O%28n%29&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"O(n)\" class=\"latex\" /> time on pre-NVIDIA Ampere architecture hardware.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"609\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-small-straight-line-1024x609.png\" alt=\"Line chart shows that repeat launch CPU overhead had a linear dependency on the graph length in CUDA Toolkit 11.8.\u00a0 It has a near-constant behavior in CUDA Toolkit 12.6. The line chart covers small straight line graph topologies up to 250 nodes in length where the overhead reaches 25 uS in 11.8 and is about 2 uS in 12.6.\" class=\"wp-image-88927\" title=\"Chart\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-small-straight-line-1024x609.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-small-straight-line-300x178.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-small-straight-line-625x372.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-small-straight-line-179x106.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-small-straight-line-768x457.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-small-straight-line-645x383.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-small-straight-line-500x297.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-small-straight-line-151x90.png 151w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-small-straight-line-362x215.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-small-straight-line-185x110.png 185w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-small-straight-line.png 1248w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. CPU overhead of repeat launch of straight line graphs for small straight line topology</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"609\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-large-straight-line-1024x609.png\" alt=\"Line chart shows that the relationships from Figure 1 extend to larger straight-line graphs. The chart extends the relationship to graphs of 2000 nodes in size, where the overhead was 300 uS in 11.8.\" class=\"wp-image-88929\" title=\"Chart\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-large-straight-line-1024x609.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-large-straight-line-300x178.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-large-straight-line-625x372.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-large-straight-line-179x106.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-large-straight-line-768x457.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-large-straight-line-645x383.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-large-straight-line-500x297.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-large-straight-line-151x90.png 151w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-large-straight-line-362x215.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-large-straight-line-185x110.png 185w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-large-straight-line.png 1248w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. CPU overhead of repeat launch of straight-line graphs for large straight-line topology</em></figcaption></figure></div>\n\n\n<p>Because the parallel straight-line graphs measured here have four root nodes, the repeat launch with one node takes 6\u03bcs instead of 2\u03bcs. The impact of chain length on the CPU launch overhead is still negligible for this topology. The exact scaling of having multiple root nodes is not analyzed here, although the impact is expected to be linear with the number of root nodes. Similarly, graphs with non-kernel nodes would have different performance characteristics.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"609\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-four-parallel-chain-1024x609.png\" alt=\"Line chart shows that the relationships from Figure 1 extend to parallel chain topologies. The chart shows that a parallel chain topology with four chains and a length of 250 nodes has a launch overhead of &gt;60 uS in 11.8 and takes close to 6 uS in 12.6.\" class=\"wp-image-88930\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-four-parallel-chain-1024x609.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-four-parallel-chain-300x178.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-four-parallel-chain-625x372.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-four-parallel-chain-179x106.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-four-parallel-chain-768x457.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-four-parallel-chain-645x383.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-four-parallel-chain-500x297.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-four-parallel-chain-151x90.png 151w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-four-parallel-chain-362x215.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-four-parallel-chain-185x110.png 185w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cpu-overhead-four-parallel-chain.png 1248w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. CPU overhead of repeat launch of a graph with four parallel task chains</em></figcaption></figure></div>\n\n\n<h2 id=\"other_measurable_performance_gains\"  class=\"wp-block-heading\">Other measurable performance gains<a href=\"#other_measurable_performance_gains\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Several other aspects of performance can matter for applications benefitting from graphs:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Instantiation time</li>\n\n\n\n<li>First launch CPU overhead</li>\n\n\n\n<li>Repeat launch device runtime</li>\n\n\n\n<li>End-to-end time for repeat graph launch into an empty stream</li>\n</ul>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>metric</strong></td><td><strong>topology</strong></td><td><strong>Topology length</strong></td><td><strong>11.8 (r520.61.05)</strong></td><td><strong>12.6 (r560.28.03)</strong></td><td><strong>%speedup</strong></td></tr><tr><td rowspan=\"5\"><strong>Instantiation</strong></td><td rowspan=\"3\"><strong>Straight line</strong></td><td><strong>10</strong></td><td>20 uS</td><td>16 uS</td><td>25%</td></tr><tr><td><strong>100</strong></td><td>168 uS</td><td>127 uS</td><td>32%</td></tr><tr><td><strong>1025</strong></td><td>2143 uS</td><td>1526 uS</td><td>40%</td></tr><tr><td rowspan=\"2\"><strong>4 parallel chains</strong></td><td><strong>10</strong></td><td>71 uS</td><td>58 uS</td><td>22%</td></tr><tr><td><strong>100</strong></td><td>695 uS</td><td>552 uS</td><td>26%</td></tr><tr><td rowspan=\"4\"><strong>First Launch Cpu Overhead</strong></td><td rowspan=\"3\"><strong>Straight line</strong></td><td><strong>10</strong></td><td>4 uS</td><td>4uS</td><td>0%</td></tr><tr><td><strong>100</strong></td><td>25 uS</td><td>15 uS</td><td>66%</td></tr><tr><td><strong>1025</strong></td><td>278 uS</td><td>175 uS</td><td>59%</td></tr><tr><td><strong>4 parallel chains</strong></td><td><strong>100</strong></td><td>73 uS</td><td>67 uS</td><td>9%</td></tr><tr><td rowspan=\"3\"><strong>Repeat Launch Device Runtime</strong></td><td rowspan=\"3\"><strong>Straight line</strong></td><td><strong>10</strong></td><td>7 uS</td><td>7 uS</td><td>0% better</td></tr><tr><td><strong>100</strong></td><td>61 uS</td><td>53 uS</td><td>15% better</td></tr><tr><td><strong>1025</strong></td><td>629 uS</td><td>567 uS</td><td>11% better</td></tr><tr><td rowspan=\"3\"><strong>End to end time for repeat graph launch into an empty stream</strong></td><td rowspan=\"3\"><strong>Straight line</strong></td><td><strong>10</strong></td><td>12 uS</td><td>9 uS</td><td>30% better</td></tr><tr><td><strong>100</strong></td><td>69 uS</td><td>55 uS</td><td>25% better</td></tr><tr><td><strong>1025</strong></td><td>628 uS</td><td>567 uS</td><td>11% better</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Metric performance on CUDA Toolkit 11.8 and 12.6</em></figcaption></figure>\n\n\n\n<h3 id=\"instantiation_time\"  class=\"wp-block-heading\">Instantiation time<a href=\"#instantiation_time\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The amount of time to instantiate a graph dominates the runtime cost of adopting a graph. This is a one-time cost per graph instantiation. Graph creation time often appears on the critical path of application startup or for workflows that need to periodically create new graphs at times they do not have work running on the GPU.<br><br>Improvements here can impact application startup time for applications that prepare many graphs at program start and minimize the end-to-end latency penalty to other applications that file to hide graph construction latency behind other work execution.</p>\n\n\n\n<h3 id=\"first_launch_cpu_overhead\"  class=\"wp-block-heading\">First launch CPU overhead<a href=\"#first_launch_cpu_overhead\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>As first launch is responsible for uploading the work descriptions for a CUDA Graph to the GPU, it still has an <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=O%28N%29&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"O(N)\" class=\"latex\" /> CPU cost. This cost is paid the first time and doesn\u2019t need to be repaid except partially when the graph is updated. Most of this cost can be paid by performing a separate upload operation, which is not discussed in this post because it\u2019s already available to CUDA Graphs users.</p>\n\n\n\n<h3 id=\"repeat_launch_device_runtime\"  class=\"wp-block-heading\">Repeat launch device runtime<a href=\"#repeat_launch_device_runtime\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Running even empty kernels takes time. Optimizing some inter-kernel latency for straight-line graphs reduces device times by up to a noticeable 60 ns/node.</p>\n\n\n\n<h3 id=\"end-to-end_time_for_repeat_graph_launch_into_an_empty_stream\"  class=\"wp-block-heading\">End-to-end time for repeat graph launch into an empty stream<a href=\"#end-to-end_time_for_repeat_graph_launch_into_an_empty_stream\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>CUDA Graphs contains logic that may allow execution to begin before the launch operation processes all the nodes in a graph, hiding part of the launch cost. Due to this, the launch overhead benefits aren\u2019t fully seen in the time to work completion.&nbsp;</p>\n\n\n\n<p>Instead, you see a more modest improvement of about 60ns/node benefit in time to solution for straight line graphs.</p>\n\n\n\n<h2 id=\"scheduling_based_on_node_creation_order\"  class=\"wp-block-heading\">Scheduling based on node creation order<a href=\"#scheduling_based_on_node_creation_order\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In CUDA Toolkit 12.0, CUDA started paying attention to node creation order as a heuristic to scheduling decisions. CUDA started loosely preferring to schedule node operations for nodes created earlier over nodes created later.&nbsp;</p>\n\n\n\n<p>Specifically, the order of the root nodes was fixed and a pass that ordered the nodes by a breadth-first traversal was removed. The logic originally was designed to keep root nodes at the start of the graph\u2019s node list and to prioritize making parallel work available for the GPU to run.&nbsp;</p>\n\n\n\n<p>In cases where memcopy operations would be assigned to the same copy engine and falsely serialize, the old heuristic would often wind up unintentionally serializing the memcopy operations needed for the first compute items after memcopy operations that were only needed for later compute tasks. This caused the graph to have bubbles in the work available to the compute engines. So, developers often created the graph nodes in a deliberate order: nodes needed early in the graph were created first and nodes needed later were created later.&nbsp;&nbsp;</p>\n\n\n\n<p>By paying attention to node creation order when making some scheduling decisions, CUDA can give you scheduling that better matches your intuitive expectations.&nbsp;</p>\n\n\n\n<h2 id=\"microbenchmark_source_code\"  class=\"wp-block-heading\">Microbenchmark source code<a href=\"#microbenchmark_source_code\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The microbenchmarks code has been added to the <a href=\"https://github.com/NVIDIA/cuda-samples/tree/master/Samples/6_Performance/cudaGraphsPerfScaling\">/NVIDIA/cuda-samples</a> as the <code>cudaGraphsPerfScaling</code> application. Compile the app and run the <code>dataCollection.bash</code> script with your GPU name and CUDA driver version to generate the data.</p>\n\n\n\n<h1 class=\"wp-block-heading\">Summary</h1>\n\n\n\n<p>New optimizations to CUDA graphs make a compelling argument for seeing if they translate to real performance improvements in applications using graphs.</p>\n\n\n\n<p>In this post, we discussed CUDA Graphs performance improvements after CUDA 11.8. These improvements include enhancements in graph construction and instantiation time, CPU launch overhead, and overall performance in various scenarios.&nbsp;</p>\n\n\n\n<p>For more information, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://developer.nvidia.com/blog/cuda-graphs/\">Getting Started with CUDA Graphs</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs\">CUDA C++ Programming Guide: CUDA Graphs</a>\n<ul class=\"wp-block-list\">\n<li><a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#whole-graph-update\">Whole Graph Update</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#individual-node-update\">Individual Node Update</a></li>\n</ul>\n</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html\">CUDA Runtime API: Graph Management</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/a-guide-to-cuda-graphs-in-gromacs-2023/\">A Guide to CUDA Graphs in GROMACS</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/employing-cuda-graphs-in-a-dynamic-environment/\">Employing CUDA Graphs in a Dynamic Environment</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/constructing-cuda-graphs-with-dynamic-parameters/\">Constructing CUDA Graphs with Dynamic Parameters</a></li>\n\n\n\n<li><a href=\"https://www.olcf.ornl.gov/wp-content/uploads/2021/10/013_CUDA_Graphs.pdf\">CUDA Graphs (OLCF presentation)</a></li>\n\n\n\n<li><a href=\"https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/\">Accelerating PyTorch with CUDA Graphs</a></li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>CUDA Graphs are a way to define and batch GPU operations as a graph rather than a sequence of stream launches. A CUDA Graph groups a set of CUDA kernels and other CUDA operations together and executes them with a specified dependency tree. It speeds up the workflow by combining the driver activities associated with &hellip; <a href=\"https://developer.nvidia.com/blog/constant-time-launch-for-straight-line-cuda-graphs-and-other-performance-enhancements/\">Continued</a></p>\n", "protected": false}, "author": 2111, "featured_media": 75549, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1484861", "discourse_permalink": "https://forums.developer.nvidia.com/t/constant-time-launch-for-straight-line-graphs-and-other-performance-enhancements/306488", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 503], "tags": [453, 145, 126, 1914], "coauthors": [3830, 2319], "class_list": ["post-88631", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "category-simulation-modeling-design", "tag-featured", "tag-graph-algorithms", "tag-optimization", "tag-cluster-supercomputing"], "acf": {"post_industry": ["HPC / Scientific Computing"], "post_products": ["CUDA", "HPC SDK"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hpc-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-n3x", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88631"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2111"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88631"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88631/revisions"}], "predecessor-version": [{"id": 88998, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88631/revisions/88998"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75549"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88631"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88631"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88631"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88631"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88566, "date": "2024-09-10T09:30:00", "date_gmt": "2024-09-10T16:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88566"}, "modified": "2024-09-19T12:32:22", "modified_gmt": "2024-09-19T19:32:22", "slug": "accelerating-the-hpcg-benchmark-with-nvidia-math-sparse-libraries", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-the-hpcg-benchmark-with-nvidia-math-sparse-libraries/", "title": {"rendered": "Accelerating the HPCG Benchmark with NVIDIA Math Sparse Libraries"}, "content": {"rendered": "\n<p>In the realm of high-performance computing (HPC), NVIDIA has continually advanced HPC by offering its highly optimized NVIDIA High-Performance Conjugate Gradient (HPCG) benchmark program as part of the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/hpc-benchmarks\">NVIDIA HPC benchmark program collection</a>.\u00a0</p>\n\n\n\n<p>We now provide the NVIDIA HPCG benchmark program in the <a href=\"https://github.com/NVIDIA/nvidia-hpcg\">/NVIDIA/nvidia-hpcg</a> GitHub repo, using its high-performance math libraries, <a href=\"https://docs.nvidia.com/cuda/cusparse/\">cuSPARSE</a>, and the <a href=\"https://developer.nvidia.com/nvpl\">NVIDIA Performance Libraries</a> (NVPL) to achieve optimal performance for sparse matrix-vector multiplication (SpMV) and sparse matrix triangular solvers (SpSV) on NVIDIA GPUs and NVIDIA CPUs.</p>\n\n\n\n<h2 id=\"understanding_the_hpcg_benchmark\"  class=\"wp-block-heading\">Understanding the HPCG benchmark<a href=\"#understanding_the_hpcg_benchmark\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The <a href=\"https://www.hpcg-benchmark.org/\">HPCG benchmark</a> complements the High-Performance LINPACK (HPL) benchmark, traditionally used to rank supercomputers in the <a href=\"https://top500.org/lists/top500/\">TOP500 list</a>. While HPL measures peak floating-point performance, it does not fully represent the performance of real-world applications that rely on memory access patterns and data locality.&nbsp;</p>\n\n\n\n<p>This is where HPCG comes in, offering a more comprehensive assessment by simulating a variety of computation and data access patterns common in scientific computing. The HPCG benchmark is used to rank supercomputers in the TOP500 list on par with the HPL benchmark.&nbsp;&nbsp;</p>\n\n\n\n<p>HPCG evaluates the preconditioned conjugate gradient (PCG) method, crucial for solving large, sparse linear systems in fields like computational fluid dynamics, structural analysis, and material science. It assesses HPC systems&#8217; memory bandwidth and latency handling real-world applications by constructing a globally distributed sparse linear system using a 27-point stencil per grid point in a 3D domain.&nbsp;</p>\n\n\n\n<p>Managed by MPI processes, this setup enables scalable computations across processors. Key operations include the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Vector inner product</li>\n\n\n\n<li>Updates</li>\n\n\n\n<li>SpSV for symmetric Gauss-Seidel (SYMGS) smoothing</li>\n\n\n\n<li>SpMV</li>\n</ul>\n\n\n\n<p>For more information about the benchmark, see the <a href=\"https://www.osti.gov/biblio/1113870\">HPCG Benchmark Technical Specification</a>.</p>\n\n\n\n<p>The efficiency of HPCG heavily relies on the performance of SpMV and SpSV operations. These computations form the backbone of the iterative PCG method, directly influencing the benchmark&#8217;s ability to simulate real-world workloads accurately. High-performance implementations of SpMV and SpSV can significantly enhance the overall performance of HPC systems, providing valuable insights into their capabilities in handling complex scientific computations.</p>\n\n\n\n<h2 id=\"enhancing_performance_on_hpcg\"  class=\"wp-block-heading\">Enhancing performance on HPCG<a href=\"#enhancing_performance_on_hpcg\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The performance of the NVIDIA HPCG benchmark program is significantly enhanced through its specialized math libraries: <a href=\"https://developer.nvidia.com/cusparse\">cuSPARSE</a> for GPUs and <a href=\"https://docs.nvidia.com/nvpl/_static/sparse/index.html\">NVPL Sparse</a> for aarch64 architectures such as the NVIDIA Grace CPU. These libraries are integral to accelerating sparse linear algebra operations essential for iterative algorithms like the PCG method.\u00a0</p>\n\n\n\n<p>cuSPARSE, optimized for NVIDIA GPU architectures, supports a wide range of functionalities including SpMV, sparse matrix-matrix multiplication (SpMM), and SpSV. These operations are critical for many scientific computations due to their ability to handle large-scale sparse matrices efficiently.</p>\n\n\n\n<p>NVPL Sparse, on the other hand, targets Arm 64-bit architectures such as the NVIDIA Grace CPU. It is part of NVPL, designed to maximize performance and efficiency on these platforms. This library includes functionalities tailored to sparse linear algebra, enabling applications to fully exploit the capabilities of Arm-based architectures.&nbsp;</p>\n\n\n\n<p>The recent beta release of NVPL Sparse boosts performance on Grace CPUs and provides a competitive edge in heterogeneous computing environments.</p>\n\n\n\n<p>The cuSPARSE and NVPL Sparse generic APIs offer flexibility in configuring data layouts, supporting mixed data types, and selecting algorithms tailored to specific computational tasks, ensuring compatibility across different applications and optimization opportunities:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Configurable storage formats such as CSR and ELLPACK</li>\n\n\n\n<li>Flexible data types for input, output, and compute</li>\n\n\n\n<li>Various indexing options such as 32-bit and 64-bit&nbsp;</li>\n</ul>\n\n\n\n<p>Memory management features and extensive consistency checks further enhance reliability and performance consistency across diverse computing scenarios.</p>\n\n\n\n<p>By integrating cuSPARSE and NVPL Sparse into the NVIDIA HPCG benchmark program, it enhances critical operations such as SpMV and SpSV. These libraries empower you to achieve peak performance on NVIDIA GPU and Grace CPU architectures, advancing supercomputing capabilities to effectively manage intricate scientific simulations and computations.</p>\n\n\n\n<h2 id=\"optimizations_and_innovations\"  class=\"wp-block-heading\">Optimizations and innovations<a href=\"#optimizations_and_innovations\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The symmetric Gauss-Seidel smoother involves forward and backward SpSV of the sparse matrix, where the triangular solves are inherently sequential due to row dependencies. To address this, the NVIDIA HPCG benchmark program employs graph coloring to reorder the sparse matrix rows, enabling parallel processing of independent rows grouped by color. For more information, see <a href=\"https://www.researchgate.net/publication/283864782_A_CUDA_Implementation_of_the_High_Performance_Conjugate_Gradient_Benchmark\">A CUDA Implementation of the High Performance Conjugate Gradient Benchmark</a>.\u00a0</p>\n\n\n\n<p>NVIDIA has also introduced the sliced-ELLPACK storage format for sparse matrices within the NVIDIA HPCG benchmark program. This format minimizes computational overhead by reducing zero-padding in lower and upper triangular matrices compared to traditional CSR formats. The adoption of sliced-ELLPACK has demonstrated notable performance improvements, including 1.2x faster SpMV and 1.7x faster SpSV on the NVIDIA DGX H100 platform.\u00a0</p>\n\n\n\n<p>The NVIDIA HPCG benchmark program implements specific strategies to minimize unnecessary memory accesses and computational redundancies. For instance, by restricting matrix operations to relevant portions during iterative steps and leveraging specialized APIs like <code>cusparseSpSV_UpdateMatrix</code> and <code>nvpl_sparse_update_matrix</code>, the NVIDIA HPCG benchmark program achieves efficient sparse matrix updates and computations.</p>\n\n\n\n<p>Beyond using the NVIDIA math API, the NVIDIA HPCG benchmark program incorporates several optimizations to enhance the efficiency of the PCG execution phase: overlap of computation and communication and support for different point-to-point (P2P) communication modes.</p>\n\n\n\n<h3 id=\"overlap_of_computation_and_communication\"  class=\"wp-block-heading\">Overlap of computation and communication<a href=\"#overlap_of_computation_and_communication\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>During critical operations such as SpMV and SYMGS computations, the NVIDIA HPCG benchmark program overlaps computation with communication tasks. This strategy involves transferring boundary data from the GPU to the CPU, performing MPI send/receive operations with neighboring processes, and transferring the results back to the GPU.\u00a0</p>\n\n\n\n<p>The NVIDIA HPCG benchmark program achieves this overlap with CUDA streams, which enable concurrent execution of communication copies in separate streams from computation kernels, thereby reducing idle time and improving overall throughput.</p>\n\n\n\n<h3 id=\"support_for_different_p2p_communication_modes\"  class=\"wp-block-heading\">Support for different P2P communication modes<a href=\"#support_for_different_p2p_communication_modes\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The NVIDIA HPCG benchmark program supports various P2P communication modes to evaluate performance across diverse cluster configurations:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Host-MPI</li>\n\n\n\n<li>CUDA-AWARE-MPI</li>\n\n\n\n<li>Host-All2allv</li>\n\n\n\n<li>CUDA-AWARE-All2allv</li>\n\n\n\n<li>NVIDIA Collective Communication Library (NCCL)</li>\n</ul>\n\n\n\n<p>Each mode is optimized for specific network architectures and communication patterns, enabling HPCG to assess and optimize performance in different cluster environments effectively. This flexibility ensures that the NVIDIA HPCG benchmark program can adapt to varying communication infrastructures and maximize efficiency in large-scale parallel computations.</p>\n\n\n\n<h2 id=\"heterogeneous_computing_capabilities\"  class=\"wp-block-heading\">Heterogeneous computing capabilities<a href=\"#heterogeneous_computing_capabilities\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The NVIDIA HPCG benchmark program extends its optimization strategies to heterogeneous computing environments, seamlessly integrating GPUs and NVIDIA Grace CPUs. This approach involves assigning an MPI rank to each GPU and one or more MPI ranks to the Grace CPU.\u00a0</p>\n\n\n\n<p>To fully maximize the utilization of every aspect of the system, the strategy is to allocate a larger local problem size to the GPU compared to the Grace CPU. This ensures that the computational strengths of both the GPU and the CPU are fully leveraged. During MPI blocking communication steps like <code>MPI_Allreduce</code>, this approach helps maintain balanced workloads across the system components, optimizing overall performance and minimizing idle time for any part of the system.</p>\n\n\n\n<p>In the NVIDIA HPCG benchmark program, the GPU and Grace CPU local problems are configured to differ in only one dimension while keeping the other dimensions identical. This design enables proper halo exchange operations across the dimensions that remain identical between the GPU and Grace ranks.&nbsp;</p>\n\n\n\n<p>Figure 1 shows an example of this design. The GPU and Grace CPU ranks have the same <code>y</code> and <code>z</code> dimensions. The <code>x</code> dimension is different, which enables assigning different local problems for the GPU and Grace ranks. The NVIDIA HPCG benchmark program offers you the flexibility to choose the 3D shape of ranks, select the different dimensions, and configure the sizes of the GPU and Grace ranks.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-1024x576.png\" alt=\"A 3D problem processed using NVIDIA HPCG\u2019s heterogeneous execution configuration of NVIDIA GPUs and Grace CPUs. The GPU and Grace CPU local problems have different x-dimensions but identical y and z dimensions. A 1-point halo region is shared between local problems.\" class=\"wp-image-88605\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-1024x576.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-1536x864.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-hpcg-heterogeneous-execution.png 1600w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA HPCG heterogeneous execution involving both GPU and Grace CPU</em></figcaption></figure></div>\n\n\n<p>The performance of the heterogeneous execution mode depends on the shape of the 3D grid, the ratio between the GPU and Grace CPU problem sizes, and the hardware capabilities of the GPU and CPU.&nbsp;</p>\n\n\n\n<p>On the Grace Hopper Superchip (GH200), the GPU problem size should be 16x larger than the Grace CPU problem size to achieve a 5% performance improvement over the GPU-only execution mode. The <a href=\"https://github.com/NVIDIA/nvidia-hpcg/blob/master/bin/sample-docker/hpcg-aarch64-gh4-gpu-cpu.sh\">command-line parameters</a> offer great flexibility in selecting the different dimensions and configuring the 3D grid dimensions accordingly.</p>\n\n\n\n<h2 id=\"hpcg_performance\"  class=\"wp-block-heading\">HPCG performance<a href=\"#hpcg_performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The <a href=\"https://www.hpcg-benchmark.org/faq/index.html#360\">HPCG runtime guidelines</a> state, \u201cThe problem size should reflect what would be reasonable for a real sparse iterative solver.\u201d Typically, this means the HPCG problem size should be at least 25% of the available system memory.\u00a0\u00a0</p>\n\n\n\n<p>These guidelines create an interesting challenge for benchmarking HPCG on the NVIDIA Grace Hopper Superchip (GH200) as real-world applications relying on sparse iterative solvers may perform their calculations entirely in CPU memory, entirely in GPU memory, or a combination of both.&nbsp;&nbsp;</p>\n\n\n\n<p>To most accurately represent sparse iterative solver performance on GH200, we\u2019ve chosen to present the performance of the following problem sizes:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>25% of CPU memory</li>\n\n\n\n<li>25% of GPU memory</li>\n\n\n\n<li>25% of the combined CPU and GPU memory</li>\n</ul>\n\n\n\n<p>Figure 2 shows the performance comparison between the NVIDIA HPCG benchmark program and the official HPCG benchmark on the NVIDIA GH200-480GB. To optimize parallelism for the official HPCG benchmark, we used 72 MPI ranks to manage smaller local tasks.&nbsp;</p>\n\n\n\n<p>The NVIDIA HPCG CPU-only configuration demonstrated a 20% performance improvement over the official HPCG benchmark due to CPU software optimization.&nbsp;</p>\n\n\n\n<p>Figure 2 also shows the performance of the NVIDIA HPCG GPU-only configuration, alongside the heterogeneous GPU and Grace CPU implementation, which has a 5% performance boost compared to the GPU-only setup when the Grace CPU handles a smaller problem that can overlap with the GPU workload. The Grace CPU MPI rank handles an additional 1/16 of the GPU problem size, producing the highest speedup of 17.4x compared to the official HPCG benchmark executing on the CPU.\u00a0</p>\n\n\n\n<p>Finally, when the HPCG problem is 25% of the total system memory (CPU+GPU), the total speedup is 4.2x.&nbsp; In this case, there is an advantageous trade-off between runtime performance and total problem size.&nbsp; The problem size is similar to the large CPU-only case; however, runtime performance has increased by over 4x.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"898\" height=\"357\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/hpcg-benchmark-performance-comparison.png\" alt=\"Comparison of NVIDIA HPCG performance: Grace-only, GPU-only, and heterogeneous configurations against the official HPCG benchmark, tested on NVIDIA GH200-480GB.\" class=\"wp-image-88606\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/hpcg-benchmark-performance-comparison.png 898w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/hpcg-benchmark-performance-comparison-300x119.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/hpcg-benchmark-performance-comparison-625x248.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/hpcg-benchmark-performance-comparison-179x71.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/hpcg-benchmark-performance-comparison-768x305.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/hpcg-benchmark-performance-comparison-645x256.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/hpcg-benchmark-performance-comparison-500x199.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/hpcg-benchmark-performance-comparison-160x64.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/hpcg-benchmark-performance-comparison-362x144.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/hpcg-benchmark-performance-comparison-277x110.png 277w\" sizes=\"(max-width: 898px) 100vw, 898px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Performance compared to the official HPCG benchmark</em></figcaption></figure></div>\n\n\n<p class=\"has-text-align-center has-small-font-size\">The experiments were performed on an NVIDIA GH200 GPU with a 480-GB memory capacity (GH200-480GB).</p>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\">Summary<a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The open-source <a href=\"https://github.com/NVIDIA/nvidia-hpcg\">NVIDIA HPCG benchmark program</a> uses high-performance math libraries, cuSPARSE, and NVPL Sparse, for optimal performance on GPUs and Grace CPUs. It supports GPU-only, Grace-only, and heterogeneous execution, offering flexibility to configure execution modes.&nbsp;</p>\n\n\n\n<p>The NVIDIA HPCG benchmark program uses a unique design where the GPU and Grace CPU problem sizes differ in one dimension while keeping others identical, facilitating efficient halo exchange operations.</p>\n\n\n\n<p>For more information, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://github.com/NVIDIA/nvidia-hpcg\">/NVIDIA/nvidia-hpcg</a> GitHub repo</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/cusparse\">NVIDIA cuSPARSE</a> developer page</li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/nvpl\">NVIDIA Performance Libraries (NVPL)</a> developer page</li>\n\n\n\n<li><a href=\"https://forums.developer.nvidia.com/c/accelerated-computing/5\">Accelerated Computing</a> developer forum</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>In the realm of high-performance computing (HPC), NVIDIA has continually advanced HPC by offering its highly optimized NVIDIA High-Performance Conjugate Gradient (HPCG) benchmark program as part of the NVIDIA HPC benchmark program collection.\u00a0 We now provide the NVIDIA HPCG benchmark program in the /NVIDIA/nvidia-hpcg GitHub repo, using its high-performance math libraries, cuSPARSE, and the NVIDIA &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-the-hpcg-benchmark-with-nvidia-math-sparse-libraries/\">Continued</a></p>\n", "protected": false}, "author": 2278, "featured_media": 75549, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1483993", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-the-hpcg-benchmark-with-nvidia-math-sparse-libraries/306359", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852], "tags": [453, 48, 49], "coauthors": [4009, 4010, 3031], "class_list": ["post-88566", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "tag-featured", "tag-mpi", "tag-multi-gpu"], "acf": {"post_industry": ["HPC / Scientific Computing"], "post_products": ["CUDA", "cuSPARSE"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hpc-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-n2u", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88566"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2278"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88566"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88566/revisions"}], "predecessor-version": [{"id": 88809, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88566/revisions/88809"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75549"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88566"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88566"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88566"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88566"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87876, "date": "2024-09-10T09:30:00", "date_gmt": "2024-09-10T16:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87876"}, "modified": "2024-10-18T13:11:21", "modified_gmt": "2024-10-18T20:11:21", "slug": "streamlining-data-processing-for-domain-adaptive-pretraining-with-nvidia-nemo-curator", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/streamlining-data-processing-for-domain-adaptive-pretraining-with-nvidia-nemo-curator/", "title": {"rendered": "Streamlining Data Processing for Domain Adaptive Pretraining with NVIDIA NeMo Curator"}, "content": {"rendered": "\n<p>Domain-adaptive pretraining (DAPT) of <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models</a> (LLMs) is an important step towards building domain-specific models. These models demonstrate greater capabilities in domain-specific tasks compared to their off-the-shelf open or commercial counterparts.&nbsp;</p>\n\n\n\n<p>Recently, NVIDIA published a paper about <a href=\"https://arxiv.org/pdf/2311.00176\">ChipNeMo</a>, a family of foundation models that are geared toward industrial chip design applications. ChipNeMo models are the result of the continued pretraining of the Llama 2 family of models on a corpus of proprietary, as well as publicly available domain-specific data.</p>\n\n\n\n<p>This post walks you through the process of curating a training dataset, using ChipNeMo dataset as an example, from a variety of publicly available sources using <a href=\"https://developer.nvidia.com/blog/scale-and-curate-high-quality-datasets-for-llm-training-with-nemo-curator/\">NVIDIA NeMo Curator</a>.&nbsp;</p>\n\n\n\n<h2 id=\"nemo_curator\"  class=\"wp-block-heading\">NeMo Curator<a href=\"#nemo_curator\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NeMo Curator is a GPU-accelerated data-curation library that improves generative AI model performance by preparing large-scale, high-quality datasets for pretraining and customization.&nbsp;</p>\n\n\n\n<p>NeMo Curator lowers the data processing time by scaling to multi-node multi-GPU (MNMG) and enables the preparation of large pretraining datasets. It offers workflows to download and curate data from various public sources out of the box such as Common Crawl, Wikipedia, and arXiv.&nbsp;</p>\n\n\n\n<p>It also provides flexibility for you to customize data curation pipelines to address their unique requirements and create custom datasets.</p>\n\n\n\n<p>For more information about the basic building blocks, see the <a href=\"https://developer.nvidia.com/blog/curating-custom-datasets-for-llm-training-with-nvidia-nemo-curator\">Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator</a> tutorial.</p>\n\n\n\n<h2 id=\"chipnemo\"  class=\"wp-block-heading\">ChipNeMo<a href=\"#chipnemo\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>A large portion of ChipNeMo\u2019s training corpus consists of data from Wikipedia, open-source GitHub repositories as well as arXiv publications.&nbsp;</p>\n\n\n\n<p>Figure 1 shows that the data curation pipeline involves the following high-level steps:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Acquiring data:\n<ul class=\"wp-block-list\">\n<li>Download relevant Wikipedia articles and convert them to JSONL files.</li>\n\n\n\n<li>Clone relevant GitHub repositories, determine all relevant source code files, and convert them to JSONL files.</li>\n\n\n\n<li>Download papers from arXiv in PDF format and convert them into JSONL files.</li>\n</ul>\n</li>\n\n\n\n<li>Using existing tools to unify the Unicode representation and special characters.</li>\n\n\n\n<li>Defining custom filters to remove too short, too long, duplicate, or irrelevant records.</li>\n\n\n\n<li>Redacting all personally identifiable information (PII) from the dataset.</li>\n\n\n\n<li>Organizing the data based on the metadata and writing the results to disk.</li>\n\n\n\n<li>(Optional) Blending and shuffling the data.</li>\n</ol>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"270\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms-1024x270.png\" alt=\"Diagram shows the different steps in processing the data for training domain-specific LLMs, including downloading, extracting, cleaning, blending, and shuffling.\u00a0\" class=\"wp-image-88591\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms-1024x270.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms-300x79.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms-625x165.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms-179x47.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms-768x203.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms-1536x405.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms-645x170.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms-500x132.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms-160x42.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms-362x95.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms-417x110.png 417w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/data-processing-training-domain-specific-llms.png 1691w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Processing the data for training domain-specific LLMs</em></figcaption></figure></div>\n\n\n<p>To access the complete code for this tutorial, see the <a href=\"https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/dapt-curation\">/NVIDIA/NeMo-Curator</a> GitHub repo.</p>\n\n\n\n<h2 id=\"prerequisites\"  class=\"wp-block-heading\">Prerequisites<a href=\"#prerequisites\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Before starting, install NeMo Curator by following the instructions in the <a href=\"https://github.com/NVIDIA/NeMo-Curator/blob/main/README.md\">NeMo Curator GitHub README file</a>.&nbsp;&nbsp;</p>\n\n\n\n<p>This tutorial also relies on the <a href=\"https://tesseract-ocr.github.io/\">Tesseract library</a> to enable PDF parsing functionality, which can be installed by obtaining the binaries, or your operating system\u2019s package manager.&nbsp;</p>\n\n\n\n<p>After that, run the following commands from the terminal to verify the installation. Also, install the dependencies needed for following along:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: bash; title: ; notranslate\" title=\"\">\n$ sudo apt install tesseract-ocr  # For Debian-based Linux distros\n$ pip install nemo-curator\n$ python -c &quot;import nemo_curator; print(nemo_curator);&quot;\n$ pip3 install -r requirements.txt\n</pre></div>\n\n\n<h2 id=\"data_acquisition\"  class=\"wp-block-heading\">Data acquisition<a href=\"#data_acquisition\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>We provide a list of Wikipedia articles, GitHub repositories, and arXiv publications used in the ChipNeMo training corpus, and demonstrate how to convert this data into JSONL.&nbsp;</p>\n\n\n\n<p>The conversion process varies by data source:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>For Wikipedia articles, parse the web pages to extract the main content.&nbsp;</li>\n\n\n\n<li>For arXiv publications, parse the PDF files into plain text.&nbsp;</li>\n\n\n\n<li>For GitHub repositories, identify relevant source code files and ignore irrelevant data.&nbsp;</li>\n</ul>\n\n\n\n<p>As discussed in our <a href=\"https://developer.nvidia.com/blog/curating-custom-datasets-for-llm-training-with-nvidia-nemo-curator\">previous tutorial</a>, the first step of curating a dataset is to implement the <a href=\"https://github.com/NVIDIA/NeMo-Curator/blob/fc167a6edffd38a55c333742972a5a25b901cb26/nemo_curator/download/doc_builder.py\">document builders</a> that can download and iterate through the dataset.&nbsp;</p>\n\n\n\n<p>To use Dask\u2019s parallelism, plug the document builder implementations into the <a href=\"https://github.com/NVIDIA/NeMo-Curator/blob/f1e993bf84725b6a052c0f5f89291e851e383290/nemo_curator/download/doc_builder.py#L152\">download_and_extract</a> helper that NeMo Curator provides. This helper uses Dask workers to download and parse data in parallel, speeding up the process significantly when handling many data sources.</p>\n\n\n\n<h3 id=\"document_builder_implementation\"  class=\"wp-block-heading\">Document builder implementation<a href=\"#document_builder_implementation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>First, implement the <code>DocumentDownloader</code> class, which takes the dataset\u2019s URL and downloads it using the requests library. For now, focus on the task of downloading and parsing GitHub repositories. You can similarly obtain Wikipedia and arXiv data later.</p>\n\n\n\n<p>To efficiently obtain GitHub repositories, download them as .zip archives, rather than cloning them through git commands. This method is faster and conserves disk space as you can work directly with .zip files.&nbsp;</p>\n\n\n\n<p>To download the .zip version of the repository, determine the name of the main branch for that repository. In a production pipeline, it is better to query the GitHub API directly and figure out the main branch for each repository. Because APIs are often subject to rate limits and require authentication, we show how to try a few different common branch names to see which works:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nimport requests\nfrom nemo_curator.download.doc_builder import DocumentDownloader\n\nclass GitHubDownloader(DocumentDownloader):\n    &quot;&quot;&quot;\n    A class for downloading repositories from GitHub.\n    &quot;&quot;&quot;\n\n    def __init__(self, github_root_dir: str):\n        &quot;&quot;&quot;\n        Initializes the DocBuilder object.\n\n        Args:\n            github_root_dir: The root directory for GitHub repositories.\n        &quot;&quot;&quot;\n        super().__init__()\n        # The path under which the repositories will be cloned.\n        self.clone_root_dir = os.path.join(github_root_dir, &quot;repos&quot;)\n        os.makedirs(github_root_dir, exist_ok=True)\n        os.makedirs(self.clone_root_dir, exist_ok=True)\n\n    def download(self, url: str) -&gt; str:\n        &quot;&quot;&quot;\n        Download a repository as a zip file.\n\n        Args:\n            url (str): The URL of the repository.\n\n        Returns:\n            str: The path to the downloaded zip file, or None if the download failed.\n        &quot;&quot;&quot;\n        repo_name = os.path.basename(url)\n        zip_file = os.path.join(self.clone_root_dir, repo_name + &quot;.zip&quot;)\n\n        if os.path.exists(zip_file):\n            print(f&quot;Repository &#039;{repo_name}&#039; already exists, skipping download.&quot;)\n            return zip_file\n\n        # Try the common branch names first. A better way to do this would be to\n        # query the GitHub API to get the default branch, but that is subject to rate limits.\n        success = False\n\n        for branch in &#x5B;&quot;master&quot;, &quot;main&quot;]:\n            zip_url = f&quot;https://github.com/{url}/archive/refs/heads/{branch}.zip&quot;\n\n            # Send a GET request to the URL\n            response = requests.get(zip_url)\n\n            # Check if the request was successful\n            if response.status_code == 200:\n                # Write the content of the response to a file\n                with open(zip_file, &quot;wb&quot;) as file:\n                    file.write(response.content)\n\n                # No need to try other branches\n                success = True\n                break\n\n        if not success:\n            print(\n                f&quot;Failed to clone repository &#039;{repo_name}&#039; from &#039;{url}&#039; (error code {response.status_code}).&quot;\n            )\n            return None\n\n        return zip_file\n</pre></div>\n\n\n<h3 id=\"parsing_and_iterating_the_dataset\"  class=\"wp-block-heading\">Parsing and iterating the dataset<a href=\"#parsing_and_iterating_the_dataset\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Implement the <code>DocumentIterator</code> and <code>DocumentExtractor</code> classes to walk through the data sources and parse all relevant source files. In the iterator implementation, you can add any other relevant metadata or restrict the files that are parsed.</p>\n\n\n\n<p>The following implementation opens each repository\u2019s .zip file and walks through all the files, skipping over all hidden files and directories. It determines the relevant files by their extension and determines each file\u2019s encoding using the <code>cchardet</code> library. In addition to the content of each file, this implementation stores some useful metadata and returns it to the caller.</p>\n\n\n\n<p>The extractor implementation returns the parsed contents of the file.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nimport os\nfrom zipfile import ZipFile, ZipInfo\nimport cchardet as chardet\nfrom nemo_curator.download.doc_builder import DocumentIterator\n\nclass GitHubIterator(DocumentIterator):\n    &quot;&quot;&quot;\n    GitHub document iterator. Will go through the files and parse the supported ones.\n    &quot;&quot;&quot;\n\n    # Mapping from file extensions to categories.\n    # Will also be used to to ignore irrelevant files.\n    SUPPORTED_EXTENSIONS_TO_CATEGORY = {\n        &quot;.v&quot;: &quot;VerilogVHDL&quot;,\n        &quot;.vh&quot;: &quot;VerilogVHDL&quot;,\n        &quot;.vhdl&quot;: &quot;VerilogVHDL&quot;,\n        &quot;.va&quot;: &quot;VerilogAnalog&quot;,\n        &quot;.c&quot;: &quot;CPP&quot;,\n        &quot;.cpp&quot;: &quot;CPP&quot;,\n        &quot;.h&quot;: &quot;CPP&quot;,\n        &quot;.hpp&quot;: &quot;CPP&quot;,\n        &quot;.py&quot;: &quot;Python&quot;,\n        &quot;.config&quot;: &quot;Config&quot;,\n        &quot;.mk&quot;: &quot;Makefile&quot;,\n        &quot;makefile&quot;: &quot;Makefile&quot;,\n        &quot;makeppfile&quot;: &quot;Makefile&quot;,\n        &quot;.pm&quot;: &quot;Perl&quot;,\n        &quot;.pl&quot;: &quot;Perl&quot;,\n        &quot;.tcl&quot;: &quot;Tcl&quot;,\n        &quot;.spec&quot;: &quot;Spec&quot;,\n        &quot;.yaml&quot;: &quot;Yaml&quot;,\n        &quot;.yml&quot;: &quot;Yaml&quot;,\n        &quot;.sp&quot;: &quot;Spice&quot;,\n        &quot;.cir&quot;: &quot;Spice&quot;,\n        &quot;.cmd&quot;: &quot;Spice&quot;,\n        &quot;.spf&quot;: &quot;Spice&quot;,\n        &quot;.spice&quot;: &quot;Spice&quot;,\n        &quot;.txt&quot;: &quot;text&quot;,\n        &quot;.json&quot;: &quot;text&quot;,\n        &quot;.xml&quot;: &quot;text&quot;,\n        &quot;.html&quot;: &quot;text&quot;,\n        &quot;.pdf&quot;: &quot;text&quot;,\n        &quot;.md&quot;: &quot;text&quot;,\n        &quot;&quot;: &quot;text&quot;,  # No extension\n    }\n\n    def parse_file(self, zip_ref: ZipFile, file_info: ZipInfo):\n        &quot;&quot;&quot;\n        Parses a file from a zip archive and extracts its metadata and content.\n\n        Args:\n            zip_ref: The zip archive object.\n            file_info: Information about the file in the zip archive.\n\n        Returns:\n            A tuple containing the metadata and the content of the file. The metadata is a dictionary.\n            If the file extension or filename is not supported, or if the file cannot be decoded,\n            None is returned.\n        &quot;&quot;&quot;\n        zip_path = zip_ref.filename\n        input_fp = file_info.filename\n        full_path = os.path.join(zip_path, input_fp)\n        # Extract the file name and extension in lower case.\n        filename = os.path.basename(input_fp)\n        filename_no_ext, ext = os.path.splitext(filename)\n        filename_no_ext = filename_no_ext.lower()\n        ext = ext.lower()\n\n        # If neither the file extension nor the filename is supported, return None\n        if ext not in GitHubIterator.SUPPORTED_EXTENSIONS_TO_CATEGORY:\n            if filename_no_ext not in GitHubIterator.SUPPORTED_EXTENSIONS_TO_CATEGORY:\n                return None\n\n            # The filename is there, but the extension is not. The category is determined by the filename.\n            category = GitHubIterator.SUPPORTED_EXTENSIONS_TO_CATEGORY&#x5B;filename_no_ext]\n        else:\n            category = GitHubIterator.SUPPORTED_EXTENSIONS_TO_CATEGORY&#x5B;ext]\n\n        # Open the file and read its content. Determine the encoding using cchardet. Skip over binary files.\n        with zip_ref.open(file_info, &quot;r&quot;) as file:\n            content = file.read()\n            # Determine the encoding of the file\n            encoding = chardet.detect(content)&#x5B;&quot;encoding&quot;]\n\n            if not encoding:\n                return None\n\n            try:\n                content = content.decode(encoding)\n            except UnicodeDecodeError:\n                # If the file cannot be decoded, return None\n                return None\n\n        # Extract the metadata\n        line_count = content.count(&quot;\\n&quot;) + 1\n        size_in_bytes = file_info.file_size\n\n        if category == &quot;text&quot;:\n            file_type = &quot;text&quot;\n        else:\n            file_type = &quot;code&quot;\n\n        metadata = {\n            # Use the file path as the unique ID\n            &quot;id&quot;: full_path,\n            &quot;file_extension&quot;: ext,\n            &quot;file_type&quot;: file_type,\n            &quot;category&quot;: category,\n            &quot;line_count&quot;: line_count,\n            &quot;size_in_bytes&quot;: size_in_bytes,\n            &quot;path&quot;: full_path,\n        }\n        return metadata, content\n\n    def iterate(self, file_path: str):\n        &quot;&quot;&quot;\n        Iterates over the files in a zip archive and yields the parsed content of each file.\n\n        Args:\n            file_path: The path to the zip archive.\n\n        Yields:\n            Parsed content of each file in the zip archive.\n        &quot;&quot;&quot;\n\n        if not file_path:\n            return\n\n        with ZipFile(file_path, &quot;r&quot;) as zip_ref:\n            for file_info in zip_ref.infolist():\n                filename = file_info.filename\n\n                # Skip directories and hidden files\n                if file_info.is_dir() or any(\n                    part.startswith(&quot;.&quot;) for part in filename.split(os.sep)\n                ):\n                    continue\n\n                parsed = self.parse_file(zip_ref, file_info)\n                if parsed:\n                    yield parsed\n\n\nclass GitHubExtractor(DocumentExtractor):\n    def extract(self, content: str):\n        # Just return the content.\n        return {}, content\n</pre></div>\n\n\n<h3 id=\"downloading_the_dataset\"  class=\"wp-block-heading\">Downloading the dataset<a href=\"#downloading_the_dataset\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Plug the implemented components above into NeMo Curator helpers to obtain the data from all available sources.</p>\n\n\n\n<p>The following code example demonstrates this process for the GitHub repositories. The <code>download_and_extract</code> function takes a list of dataset sources and forwards them to the downloader. It then runs the iterator and extractor implementations on every downloaded source to obtain the parsed data.&nbsp;</p>\n\n\n\n<p>The <code>output_format</code> dictionary serves to inform the underlying Dask modules about the type of each extracted field, which avoids the runtime penalty of type inference.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom nemo_curator.download.doc_builder import download_and_extract\n\ndownloader = GitHubDownloader(output_dir)\niterator = GitHubIterator()\nextractor = GitHubExtractor()\n\noutput_format = {\n    &quot;text&quot;: str,\n    &quot;id&quot;: str,\n    &quot;file_extension&quot;: str,\n    &quot;category&quot;: str,\n    &quot;line_count&quot;: int,\n    &quot;size_in_bytes&quot;: int,\n    &quot;path&quot;: str,\n}\n\ndataset = download_and_extract(\n    urls=urls,\n    output_paths=&#x5B;\n        os.path.join(output_jsonl_dir, os.path.basename(url)) for url in urls\n    ],\n    downloader=downloader,\n    iterator=iterator,\n    extractor=extractor,\n    output_format=output_format,\n    keep_raw_download=True,\n)\n</pre></div>\n\n\n<p>The <code>download_and_extract</code> function expects an output path for every dataset source. This path is used to store the parsed dataset in the JSONL format, which obviates the need for downloading and extracting sources multiple times.&nbsp;</p>\n\n\n\n<p>Upon completion, this function returns a <code>DocumentDataset</code> instance.</p>\n\n\n\n<h3 id=\"loading_the_dataset_using_the_document_builders\"  class=\"wp-block-heading\">Loading the dataset using the document builders<a href=\"#loading_the_dataset_using_the_document_builders\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In NeMo Curator, datasets are represented as objects of type <code>DocumentDataset</code>. This class provides helpers to load the datasets from disk in various formats. Having created the dataset in the JSONL format, you can use the following code to load it and start working with it:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom nemo_curator.datasets import DocumentDataset\n\n# define `code_files` to be the path to the JSONL file created above.\ndataset_code = DocumentDataset.read_json(code_files, add_filename=True)\n\n# define `text_files` to be the path to the JSONL file created from text sources.\ndataset_text = DocumentDataset.read_json(text_files, add_filename=True)\n</pre></div>\n\n\n<p>Considering that this data comes from different sources, it might be easier to store two separate dataset instances, one for data from text sources (for example, Wikipedia or arXiv papers), and another for data from code sources (such as GitHub repositories). This enables you to define source-specific processing pipelines, such as applying PII redaction on text sources, and license text removal for code sources.</p>\n\n\n\n<p>You now have everything needed to define a custom dataset curation pipeline and prepare your data.</p>\n\n\n\n<h2 id=\"unicode_formatting_and_text_unification\"  class=\"wp-block-heading\">Unicode formatting and text unification<a href=\"#unicode_formatting_and_text_unification\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>It is often a good practice to fix all Unicode issues in your datasets as text scraped from online sources may contain inconsistencies or Unicode issues.&nbsp;</p>\n\n\n\n<p>To modify documents, NeMo Curator provides a <code>DocumentModifier</code> interface along with the <code>Modify</code> helper, which defines how the given text from each document should be modified. For more information about implementing your own custom document modifiers, see the <a href=\"https://developer.nvidia.com/blog/curating-custom-datasets-for-llm-training-with-nvidia-nemo-curator#text_cleaning_and_unification\">Text cleaning and unification</a> section in the <em>Curating Custom Datasets for LLM Parameter-Efficient Fine-Tuning with NVIDIA NeMo Curator</em> tutorial.</p>\n\n\n\n<p>Here, it is sufficient to apply the NeMo Curator <code>UnicodeReformatter</code> modifier to your dataset.</p>\n\n\n\n<p>Also, modify all the quotation marks in your dataset and ensure that there are no angled quotation variants. You can do this by implementing the <code>DocumentModifier</code> interface with the required logic.</p>\n\n\n\n<p>Considering that each record has multiple fields, apply the operation only to the relevant field in the dataset (in this case, <code>\u201ctext\u201d</code> ). Chain these operations together using the&nbsp;<code>Sequential</code> class:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nSequential(&#x5B;\n    Modify(QuotationUnifier(), text_field=&quot;text&quot;),\n    Modify(UnicodeReformatter(), text_field=&quot;text&quot;),\n])\n</pre></div>\n\n\n<h2 id=\"dataset_filtering\"  class=\"wp-block-heading\">Dataset filtering<a href=\"#dataset_filtering\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>When all text in the dataset is unified, apply some filters to your dataset to ensure the documents meet certain criteria. For instance, they should all have reasonable lengths and be free of too many URLs or other repeated text.&nbsp;</p>\n\n\n\n<p>NeMo Curator provides many such filters. You can also create your own custom filters by implementing the DocumentFilter interface. For more information, see the <a href=\"https://developer.nvidia.com/blog/curating-custom-datasets-for-llm-parameter-efficient-fine-tuning-with-nvidia-nemo-curator/#designing_custom_dataset_filters\">Designing custom dataset filters</a> section in the <em>Curating Custom Datasets for LLM Parameter-Efficient Fine-Tuning with NVIDIA NeMo Curator</em> tutorial.</p>\n\n\n\n<p>The following code example shows the chaining of various filters suitable for textual data.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ndef filter_text(dataset: DocumentDataset) -&gt; DocumentDataset:\n    &quot;&quot;&quot;\n    Filters the given dataset based on various criteria.\n    Refer to the full list of all filters here:\n    https://github.com/NVIDIA/NeMo-Curator/blob/main/config/heuristic_filter_en.yaml\n    https://github.com/NVIDIA/NeMo-Curator/blob/main/tutorials/peft-curation/main.py\n\n    Args:\n        dataset (DocumentDataset): The dataset to be filtered.\n\n    Returns:\n        DocumentDataset: The filtered dataset.\n    &quot;&quot;&quot;\n    filters = Sequential(\n        &#x5B;\n            # If a document contains a number of words not\n            # within a specified range then discard\n            ScoreFilter(\n                WordCountFilter(min_words=50, max_words=100000),\n                text_field=&quot;text&quot;,\n                score_field=&quot;word_count&quot;,\n                score_type=int,\n            ),\n            # If the document shrinks by &gt; x% in terms of number of characters after\n            # removing the top n-grams then discard. Source: Gopher (Rae et al., 2021)\n            ScoreFilter(\n                RepeatingTopNGramsFilter(n=2, max_repeating_ngram_ratio=0.2),\n                text_field=&quot;text&quot;,\n                score_type=float,\n            ),\n            ScoreFilter(\n                RepeatingTopNGramsFilter(n=3, max_repeating_ngram_ratio=0.18),\n                text_field=&quot;text&quot;,\n                score_type=float,\n            ),\n            ScoreFilter(\n                RepeatingTopNGramsFilter(n=4, max_repeating_ngram_ratio=0.16),\n                text_field=&quot;text&quot;,\n                score_type=float,\n            ),\n            ScoreFilter(\n                RepeatedParagraphsFilter(max_repeated_paragraphs_ratio=0.7),\n                text_field=&quot;text&quot;,\n                score_type=float,\n            ),\n            # If more than 20% of the document is comprised of URLs then discard\n            ScoreFilter(\n                UrlsFilter(max_url_to_text_ratio=0.2),\n                text_field=&quot;text&quot;,\n                score_type=float,\n            ),\n        ]\n    )\n    filtered_dataset = filters(dataset)\n    return filtered_dataset\n</pre></div>\n\n\n<h2 id=\"pii_redaction\"  class=\"wp-block-heading\">PII redaction<a href=\"#pii_redaction\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Next, define a processing step to redact all PII from the records. Depending on the source of the data (text or code), ensure that the operation is applied to the appropriate dataset and the data field. Also, define the action to take when PII is detected. </p>\n\n\n\n<p>The following code example defines two functions for PII redaction, for text sources and code sources, respectively.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n   def redact_pii(dataset: DocumentDataset) -&gt; DocumentDataset:\n    redactor = Modify(\n        PiiModifier(\n            supported_entities=&#x5B;\n                &quot;PERSON&quot;,\n                &quot;EMAIL_ADDRESS&quot;,\n            ],\n            anonymize_action=&quot;replace&quot;,\n            device=&quot;gpu&quot;,\n        ),\n        text_field=&quot;extracted_comment&quot;,\n    )\n    return redactor(dataset)\n\ndef redact_code(dataset: DocumentDataset) -&gt; DocumentDataset:\n    # functions to extract comment lines from each row in a dataframe\n    def func(row):\n        return row&#x5B;&quot;text&quot;]&#x5B;row&#x5B;&quot;text&quot;].find(&quot;/*&quot;) : row&#x5B;&quot;text&quot;].find(&quot;*/&quot;) + 2]\n\n    def func2(row):\n        comment = row&#x5B;&quot;text&quot;]&#x5B;row&#x5B;&quot;text&quot;].find(&quot;/*&quot;) : row&#x5B;&quot;text&quot;].find(&quot;*/&quot;) + 2]\n        return row&#x5B;&quot;text&quot;].replace(comment, str(row&#x5B;&quot;extracted_comment&quot;]))\n\n    dataset.df&#x5B;&quot;extracted_comment&quot;] = dataset.df.apply(func, axis=1, meta=(None, str))\n    redacted_dataset = redact_pii(dataset)\n    redacted_dataset.df&#x5B;&quot;text&quot;] = redacted_dataset.df.apply(\n        func2, axis=1, meta=(None, str)\n    )\n    redacted_dataset.df = redacted_dataset.df.drop(&#x5B;&quot;extracted_comment&quot;], axis=1)\n\n    return redacted_dataset\n</pre></div>\n\n\n<h2 id=\"deduplication\"  class=\"wp-block-heading\">Deduplication<a href=\"#deduplication\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The obtained data might contain a lot of duplicate records. This is especially true for code files scraped from GitHub.&nbsp;</p>\n\n\n\n<p>Define a processing step where documents that contain identical information are detected and removed. This is often referred to as <em>exact deduplication</em> and is appropriate for many data curation pipelines.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ndef dedupe(dataset: DocumentDataset) -&gt; DocumentDataset:\n    &quot;&quot;&quot;\n    Remove exact duplicates from the given DocumentDataset.\n\n    Args:\n        dataset (DocumentDataset): The dataset containing documents.\n\n    Returns:\n        DocumentDataset: The deduplicated dataset.\n    &quot;&quot;&quot;\n    deduplicator = ExactDuplicates(id_field=&quot;id&quot;, text_field=&quot;text&quot;, hash_method=&quot;md5&quot;)\n    # Find the duplicates\n    duplicates = deduplicator(dataset)\n    docs_to_remove = duplicates.df.map_partitions(\n        lambda x: x&#x5B;x._hashes.duplicated(keep=&quot;first&quot;)]\n    )\n    # Remove the duplicates using their IDs.\n    duplicate_ids = list(docs_to_remove.compute().id)\n    dataset_df = dataset.df\n    deduped = dataset_df&#x5B;~dataset_df.id.isin(duplicate_ids)]\n    return DocumentDataset(deduped)\n</pre></div>\n\n\n<p>This function calculates a hash signature for every document in the dataset and marks the ones that share the same signature for removal.</p>\n\n\n\n<h2 id=\"putting_the_curation_pipeline_together\"  class=\"wp-block-heading\">Putting the curation pipeline together<a href=\"#putting_the_curation_pipeline_together\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Now that each step of the curation pipeline has been implemented, it\u2019s time to integrate everything and sequentially apply each operation to the dataset.&nbsp;</p>\n\n\n\n<p>Use the <code>Sequential</code> class to chain curation operations together.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Define data curation steps for text and pdf files\n    curation_steps_text = Sequential(\n        &#x5B;\n            clean_and_unify,\n            ScoreFilter(\n                TextLineCountFilter(), text_field=&quot;file_type_count&quot;, score_type=bool\n            ),\n            filter_text,\n            dedupe,\n        ]\n    )\n\n    # Define data curation steps for code files\n    curation_steps_code = Sequential(\n        &#x5B;\n            clean_and_unify,\n            ScoreFilter(\n                CodeLineCountFilter(), text_field=&quot;file_type_count&quot;, score_type=bool\n            ),\n            filter_code,\n            dedupe,\n            redact_code,\n        ]\n    )\n\n\ndataset_text = curation_steps_text(dataset_text).persist()\ndataset_code = curation_steps_text(dataset_code).persist()\n\ndataset_text.to_json(out_path, write_to_filename=True)\ndataset_code.to_json(out_path, write_to_filename=True)\n\n# Split the dataset by file category and save curated files (optional - to create blended datasets)\nseparated_data_text = separate_by_metadata(\n    dataset_text.df, out_path, &quot;category&quot;\n).compute()\nseparated_data_code = separate_by_metadata(\n    dataset_code.df, out_path, &quot;category&quot;\n).compute()\n</pre></div>\n\n\n<p>On the backend, NeMo Curator uses Dask to work with the dataset in a distributed manner. As Dask operations are lazy-evaluated, computations only begin when a function (like <code>.persist</code> in this case) is called to trigger them.\u00a0</p>\n\n\n\n<p>Save the datasets to disk and instruct the framework to write each record with an appropriate filename by providing <code>write_to_filename=True</code>.&nbsp;</p>\n\n\n\n<p>Lastly, if you plan to perform optional dataset shuffling and blending, split the dataset by categories.&nbsp;</p>\n\n\n\n<h2 id=\"dataset_blending_and_shuffling_optional\"  class=\"wp-block-heading\">Dataset blending and shuffling (optional)<a href=\"#dataset_blending_and_shuffling_optional\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The last step of the pipeline is to blend the datasets from different sources together and shuffle them. Offline blending and shuffling enhance a base LLM&#8217;s generalization by integrating diverse data and preventing overfitting through randomized data exposure.&nbsp;</p>\n\n\n\n<p>For this, define the blending function as shown in the following code example and provide each data source, the blending ratios, and the target size, which defines the final size of your dataset:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ndef blend_and_shuffle(\n    args: Any, dataset_paths: list, dataset_weights: list, target_size: int\n) -&gt; None:\n    &quot;&quot;&quot;\n    Blend and shuffle curated data based on file paths for continued pre-training\n\n    Args:\n        args (Any): Command-line arguments.\n        dataset_paths (list): List containing directory paths where the different JSONL files are stored.\n        dataset_weights (list): List setting weights for each directory path\n        target_size (int): Target number of data samples after blending\n    &quot;&quot;&quot;\n    root_path = os.path.join(DATA_DIR, &quot;curated&quot;)\n    output_path = root_path + &quot;/data_blended&quot;\n    if os.path.isdir(output_path):\n        shutil.rmtree(output_path)\n    os.makedirs(output_path)\n\n    # Blend the datasets\n    datasets = &#x5B;DocumentDataset.read_json(path) for path in dataset_paths]\n    blended_dataset = nc.blend_datasets(target_size, datasets, dataset_weights)\n\n    shuffle = nc.Shuffle(seed=42)\n    blended_dataset = shuffle(blended_dataset)\n\n    # Save the blend\n    blended_dataset.to_json(output_path)\n\n# Function call\nroot_path = os.path.join(DATA_DIR, &quot;curated&quot;)\ndataset_paths = &#x5B;\n    root_path + &quot;/CPP&quot;,\n    root_path + &quot;/VerilogVHDL&quot;,\n    root_path + &quot;/text&quot;,\n    root_path + &quot;/Python&quot;,\n]\ndataset_weights = &#x5B;1.0, 4.0, 4.0, 1.0]\nblend_and_shuffle(dataset_paths, dataset_weights, target_size=20)\n</pre></div>\n\n\n<p>After the call, the curated dataset is saved under <code>output_path</code>.</p>\n\n\n\n<h2 id=\"next_steps\"  class=\"wp-block-heading\">Next steps<a href=\"#next_steps\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Now that you&#8217;ve learned how to use NeMo Curator for processing data for DAPT, it&#8217;s time to experiment. Obtain the complete <a href=\"https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/dapt-curation\">source code for this tutorial</a>, adapt the code to curate data tailored to your domain, and develop powerful domain-specific LLMs.</p>\n\n\n\n<p>You can also request early access to the <a href=\"https://developer.nvidia.com/blog/simplify-custom-generative-ai-development-with-nvidia-nemo-microservices/\">NVIDIA NeMo Curator</a> microservice, which provides the easiest path for enterprises to get started with data curation from anywhere and offers streamlined performance and scalability to shorten the time to market.&nbsp;</p>\n\n\n\n<p>To apply, visit <a href=\"https://developer.nvidia.com/nemo-microservices-early-access\">NeMo Curator Microservice Early Access</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Domain-adaptive pretraining (DAPT) of large language models (LLMs) is an important step towards building domain-specific models. These models demonstrate greater capabilities in domain-specific tasks compared to their off-the-shelf open or commercial counterparts.&nbsp; Recently, NVIDIA published a paper about ChipNeMo, a family of foundation models that are geared toward industrial chip design applications. ChipNeMo models are &hellip; <a href=\"https://developer.nvidia.com/blog/streamlining-data-processing-for-domain-adaptive-pretraining-with-nvidia-nemo-curator/\">Continued</a></p>\n", "protected": false}, "author": 2039, "featured_media": 88387, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1483992", "discourse_permalink": "https://forums.developer.nvidia.com/t/streamlining-data-processing-for-domain-adaptive-pretraining-with-nvidia-nemo-curator/306358", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 3110], "tags": [453, 3737, 1066], "coauthors": [3748, 4013, 4014, 2946], "class_list": ["post-87876", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-generative-ai", "tag-featured", "tag-microservices", "tag-pre-trained-foundation-models"], "acf": {"post_industry": ["General"], "post_products": ["NeMo", "NeMo Curator"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/llm-nemo-curator-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mRm", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87876"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2039"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87876"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87876/revisions"}], "predecessor-version": [{"id": 90611, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87876/revisions/90611"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88387"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87876"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87876"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87876"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87876"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88489, "date": "2024-09-10T09:00:00", "date_gmt": "2024-09-10T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88489"}, "modified": "2024-09-19T12:33:05", "modified_gmt": "2024-09-19T19:33:05", "slug": "post-training-quantization-of-llms-with-nvidia-nemo-and-nvidia-tensorrt-model-optimizer", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/post-training-quantization-of-llms-with-nvidia-nemo-and-nvidia-tensorrt-model-optimizer/", "title": {"rendered": "Post-Training Quantization of LLMs with NVIDIA NeMo and NVIDIA TensorRT Model Optimizer"}, "content": {"rendered": "\n<p>As <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models</a> (LLMs) are becoming even bigger, it is increasingly important to provide easy-to-use and efficient deployment paths because the cost of serving such LLMs is becoming higher. One way to reduce this cost is to apply post-training quantization (PTQ), which consists of techniques to reduce computational and memory requirements for serving trained models.&nbsp;</p>\n\n\n\n<p>In this post, we provide an overview of how PTQ is implemented in NVIDIA NeMo. This is made available by using <a href=\"https://nvidia.github.io/TensorRT-Model-Optimizer/index.html\">NVIDIA TensorRT Model Optimizer</a>, which is a <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer\">library</a> that quantizes and compresses deep learning models for optimized inference on GPUs. It also uses <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT-LLM</a>, which is an <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">open-source library</a> for optimizing LLM inference. We present both accuracy and performance results for quantized models. Throughout the example, we use the <a href=\"https://ai.meta.com/blog/meta-llama-3/\">Llama 3</a> models.</p>\n\n\n\n<p>PTQ is a natural extension of the NeMo <a href=\"https://blogs.nvidia.com/blog/nemo-amazon-titan/\">LLM building and customizing</a> capabilities for seamless and efficient deployment paths using NVIDIA TensorRT Model Optimizer and NVIDIA TensorRT-LLM. As an example, <a href=\"https://www.nvidia.com/en-us/ai/.\">NVIDIA NIM</a> benefits from the PTQ workflow in NeMo.</p>\n\n\n\n<p>From a technical perspective, quantization has several benefits:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>It reduces model size, which makes it suitable for deploying using fewer GPUs with lower total device memory available.</li>\n\n\n\n<li>It reduces memory bandwidth pressure by using fewer-bit data types.</li>\n\n\n\n<li>It significantly speeds up <a href=\"https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html\">matrix multiplication (GEMM)</a> operations on the NVIDIA architecture, for example, up to <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s52166/\">2x for FP8 compared to FP16/BF16 data type</a> in microbenchmarks.</li>\n</ul>\n\n\n\n<h2 id=\"overview_of_nemo_features\"  class=\"wp-block-heading\">Overview of NeMo features<a href=\"#overview_of_nemo_features\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo</a> is an end-to-end platform for <a href=\"https://developer.nvidia.com/blog/develop-custom-enterprise-generative-ai-with-nvidia-nemo/\">developing custom generative AI</a>, anywhere. It includes tools for training, finetuning, retrieval-augmented generation, guardrailing, and toolkits, data curation tools, and pretrained models, offering enterprises an easy, cost-effective, and fast way to adopt <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI</a>.&nbsp;</p>\n\n\n\n<p>After you build a model in NeMo using a wide array of options offered by the toolkit, NeMo export and deployment tools can be used to apply PTQ methods and serve the optimized model.</p>\n\n\n\n<p>The recent NeMo container release is a self-contained toolkit coming with all the required dependencies for applying PTQ and deploying quantized LLMs.</p>\n\n\n\n<p>NeMo and TensorRT Model Optimizer offer a broad range of models suitable for quantization, including the following families:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/\">GPT</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/boosting-llama-3-1-405b-performance-by-up-to-44-with-nvidia-tensorrt-model-optimizer-on-nvidia-h200-gpus/\">Llama</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-revs-up-inference-for-google-gemma/\">Gemma</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/unlock-your-llm-coding-potential-with-starcoder2/\">StarCoder</a></li>\n\n\n\n<li><a href=\"https://huggingface.co/nvidia/nemotron-3-8b-base-4k\">Nemotron</a> (including the recently announced <a href=\"https://research.nvidia.com/publication/2024-06_nemotron-4-340b\">Nemotron4-340b</a>)</li>\n</ul>\n\n\n\n<p>PTQ support also comes with multi-node support for calibrating the largest size LLMs using appropriate <a href=\"https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/\">tensor and pipeline parallelism</a> settings.</p>\n\n\n\n<h2 id=\"quantizing_and_deploying_nemo_models\"  class=\"wp-block-heading\">Quantizing and deploying NeMo models<a href=\"#quantizing_and_deploying_nemo_models\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>At a high level, the PTQ workflow consists of the following steps:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Loading a model.</li>\n\n\n\n<li>Calibrating the model to obtain scaling factors for lower-precision GEMMs and exporting the quantized model to the <a href=\"https://nvidia.github.io/TensorRT-LLM/architecture/checkpoint.html\">TensorRT-LLM checkpoint</a>.</li>\n\n\n\n<li>Building the TensorRT-LLM engine.</li>\n\n\n\n<li>Deploying the model (for example, using <a href=\"https://triton-inference-server.github.io/pytriton/latest/\">PyTriton</a>).</li>\n</ol>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"912\" height=\"300\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ptq-workflow-nemo.png\" alt=\"Diagram shows the loading, calibrating, exporting, building, and deploying steps.\" class=\"wp-image-88497\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ptq-workflow-nemo.png 912w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ptq-workflow-nemo-300x99.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ptq-workflow-nemo-625x206.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ptq-workflow-nemo-179x59.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ptq-workflow-nemo-768x253.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ptq-workflow-nemo-645x212.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ptq-workflow-nemo-500x164.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ptq-workflow-nemo-160x53.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ptq-workflow-nemo-362x119.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ptq-workflow-nemo-334x110.png 334w\" sizes=\"(max-width: 912px) 100vw, 912px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. PTQ workflow in NeMo</em></figcaption></figure></div>\n\n\n<h3 id=\"loading_the_model\"  class=\"wp-block-heading\">Loading the model<a href=\"#loading_the_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>A&nbsp;typical PTQ use case starts with a model trained in a high-precision format, for example, FP16 or BF16, that should be served in a lower-precision data type, for example, FP8. The input model can be a foundation or instruction-tuned LLM obtained from previous pipeline steps. </p>\n\n\n\n<p>NeMo also offers <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/checkpoints/user_guide.html\">community model converters</a> for a wide array of models that can be used to produce corresponding NeMo checkpoints.</p>\n\n\n\n<h3 id=\"calibrating_and_exporting_the_quantized_model\"  class=\"wp-block-heading\">Calibrating and exporting the quantized model<a href=\"#calibrating_and_exporting_the_quantized_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In PTQ, calibrating is a process of getting scaling factors for matrix multiplication operations performed in model layers so they can be computed using lower precision formats than those used for training.&nbsp;</p>\n\n\n\n<p>This step can be conveniently launched directly from the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo\">NeMo container</a> (using <a href=\"https://pytorch.org/docs/stable/elastic/run.html\">torchrun</a>, for example) or using&nbsp;<a href=\"https://github.com/NVIDIA/NeMo-Framework-Launcher\">NeMo framework Launcher</a> on Slurm clusters for multi-node use cases.&nbsp;</p>\n\n\n\n<p>In short, quantization code boils down to the following code example:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\nfrom nemo.export.quantize import Quantizer\n\n# Set quantization and export configs appropriately, see https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/conf/megatron_gpt_ptq.yaml\n\nquantizer = Quantizer(quantization_config, export_config)\n\nmodel = MegatronGPTModel.restore_from(...)\n\ndataloader = ...  # A dataloader that yields lists of strings\n\ndef forward_loop(model):\n    # Model forward pass for collecting activation statistics for calibration\n    ...\n\nmodel = quantizer.quantize(model, forward_loop)\n\nquantizer.export(model)\n</pre></div>\n\n\n<p>The full script <a href=\"https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/megatron_gpt_ptq.py\">megatron_gpt_ptq.py</a> is the entry point for the calibration workflow. Important quantization parameters are specified in the <a href=\"https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/conf/megatron_gpt_ptq.yaml\">megatron_gpt_ptq.yaml</a> config with default settings recommended. Most importantly, the low-precision formats and quantization algorithms available are <a href=\"https://arxiv.org/abs/2209.05433\">FP8</a>, <a href=\"https://arxiv.org/abs/2306.00978\">INT4 AWQ</a>, and <a href=\"https://arxiv.org/abs/2211.10438\">INT8 SQ</a>.</p>\n\n\n\n<p>Typically, the choice of dataset does not significantly impact accuracy. However, for highly domain-specific applications, such as code completion models like <a href=\"https://developer.nvidia.com/blog/unlock-your-llm-coding-potential-with-starcoder2/\">StarCoder2</a>, using a&nbsp;code dataset is recommended to estimate calibration statistics accurately.</p>\n\n\n\n<p>The final step of the calibration step is to save the model in the <a href=\"https://nvidia.github.io/TensorRT-LLM/architecture/checkpoint.html\">TensorRT-LLM checkpoint</a> format that is suitable for building a TensorRT-LLM engine in the next step.</p>\n\n\n\n<p>Overall, the calibration process is a matter of minutes using an NVIDIA DGX H100 GPU node for a model of moderate size with 70B parameters using tensor parallelism.</p>\n\n\n\n<h3 id=\"building_the_tensorrt-llm_engine\"  class=\"wp-block-heading\">Building the TensorRT-LLM engine<a href=\"#building_the_tensorrt-llm_engine\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Before running TensorRT-LLM, you build the inference engine by compiling a set of binaries that take into account optimizations for the specific GPU hardware, model architecture, and inference settings.&nbsp;</p>\n\n\n\n<p>Use the same API as for regular NeMo models to build engines for the quantized checkpoint obtained in the calibrating step. For building FP8 engines, this step must be run using compute resources with the necessary FP8 support, for example, the NVIDIA H100 Hopper or the NVIDIA L40 Ada Lovelace GPUs.</p>\n\n\n\n<p>The following Python commands show how to build a TensorRT-LLM engine and pass an example prompt through the model.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom nemo.export.tensorrt_llm import TensorRTLLM\n\ntrt_llm_exporter = TensorRTLLM(model_dir=\u201dpath/to/trt_llm_engine\u201d)\ntrt_llm_exporter.export(\n    nemo_checkpoint_path=\u201dpath/to/model_qnemo\u201d,\n    max_batch_size=8,\n    max_input_len=2048,\n    max_output_len=512,\n)\ntrt_llm_exporter.forward(&#x5B;&quot;How does PTQ work?&quot;])\n</pre></div>\n\n\n<p>The export command takes typically several minutes to complete building or exporting a TensorRT-LLM engine, saving it into the <code>model_dir</code> parameter.</p>\n\n\n\n<h3 id=\"deploying_the_model\"  class=\"wp-block-heading\">Deploying the model<a href=\"#deploying_the_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>A given TensorRT-LLM engine can be conveniently deployed using <a href=\"https://triton-inference-server.github.io/pytriton/latest/\">PyTriton</a>.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom nemo.deploy import DeployPyTriton\nfrom nemo.export.tensorrt_llm import TensorRTLLM\n\n\ntrt_llm_exporter = TensorRTLLM(model_dir=&quot;path/to/trt_llm_engine&quot;)\n\nnm = DeployPyTriton(\n    model=trt_llm_exporter,\n    triton_model_name=&quot;llama3_70b_fp8&quot;,\n    port=8000,\n)\nnm.deploy()\nnm.serve()\n</pre></div>\n\n\n<p>Finally, on the client, NeMo Framework provides a dedicated class to send a query to the server. The following code example shows how to use it.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom nemo.deploy.nlp import NemoQueryLLM\n\n\nnq = NemoQueryLLM(\n    url=&quot;localhost:8000&quot;,\n    model_name=&quot;llama3_70b_fp8&quot;,\n)\n\nnq.query_llm(\n    prompts=&#x5B;&quot;How does PTQ work?&quot;],\n    top_k=1,\n)\n</pre></div>\n\n\n<h2 id=\"llama_3_ptq_example_and_results\"  class=\"wp-block-heading\">Llama 3 PTQ example and results<a href=\"#llama_3_ptq_example_and_results\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>For demonstration purposes, we present Llama 3 PTQ throughput and accuracy results for two pretrained Llama 3 model variants: 8B and 70B We evaluated TensorRT-LLM engine performance and accuracy using the <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer/blob/release/0.15.0/llm_ptq/benchmarks/benchmark.py\">benchmark.py</a> and <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer/blob/release/0.15.0/llm_eval/mmlu.py\">mmlu.py</a> scripts, respectively.</p>\n\n\n\n<p>The following results were obtained for NVIDIA H100 80GB GPUs with <a href=\"https://github.com/NVIDIA/TensorRT-LLM/releases/tag/v0.12.0\">TensorRT-LLM 0.12.0</a> and <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer\">TensorRT Model Optimizer 0.17.0</a>. The software stack comes with the latest <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo/tags\">NeMo framework container</a> to provide you with the complete environment.</p>\n\n\n\n<h3 id=\"accuracy_results\"  class=\"wp-block-heading\">Accuracy results<a href=\"#accuracy_results\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Figure 2 shows MMLU results for two Llama 3 model sizes across different quantization methods, along with the baseline FP16 result.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"634\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mmlu-accuracy-nemo-llama-3-8b-70b-1-1024x634.png\" alt=\"Graph shows that Llama 3 8B has the best accuracy for FP16 at 0.654 and Llama 3 70B has the best accuracy for FP16 at 0.79.\" class=\"wp-image-88803\" style=\"width:840px;height:auto\" title=\"Chart\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mmlu-accuracy-nemo-llama-3-8b-70b-1-1024x634.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mmlu-accuracy-nemo-llama-3-8b-70b-1-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mmlu-accuracy-nemo-llama-3-8b-70b-1-625x387.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mmlu-accuracy-nemo-llama-3-8b-70b-1-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mmlu-accuracy-nemo-llama-3-8b-70b-1-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mmlu-accuracy-nemo-llama-3-8b-70b-1-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mmlu-accuracy-nemo-llama-3-8b-70b-1-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mmlu-accuracy-nemo-llama-3-8b-70b-1-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mmlu-accuracy-nemo-llama-3-8b-70b-1-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mmlu-accuracy-nemo-llama-3-8b-70b-1-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mmlu-accuracy-nemo-llama-3-8b-70b-1.png 1370w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. MMLU accuracy results for Llama 3 models 8B and 70B</em></figcaption></figure></div>\n\n\n<p>Notably, FP8 quantization preserves the accuracy to the highest extent. In the case of the INT8 SQ and both Llama 3 model sizes, we found that the SmoothQuant <code>alpha</code> parameter can improve accuracy. This parameter governs quantization focus from weight-only to activation-only. It can be conveniently set in the quantization config. In the case of both Llama 3 model sizes an intermediate value of <code>alpha=0.8</code> yields the best MMLU results.</p>\n\n\n\n<p>In Table 1, the percentage number in brackets is a fraction of the baseline FP16 score and measures the extent to which a given scenario preserves accuracy.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td></td><td><strong>FP16</strong></td><td><strong>FP8</strong></td><td><strong>INT8</strong> <strong>SQ</strong></td><td><strong>INT4 AWQ</strong></td></tr><tr><td><strong>LLAMA 3 8B</strong></td><td>0.654</td><td>0.649&nbsp; (99.2%)</td><td>0.629 (96.2%)</td><td>0.629 (96.2%)</td></tr><tr><td><strong>LLAMA 3 70B</strong></td><td>0.790</td><td>0.787 (99.6%)</td><td>0.772 (97.7%)</td><td>0.777 (98.4%)</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. MMLU accuracy results for Llama 3 models</em></figcaption></figure>\n\n\n\n<h3 id=\"performance_results\"  class=\"wp-block-heading\">Performance results<a href=\"#performance_results\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Figure 3 shows inference speedups defined as the throughput ratio of a quantized model over the FP16 baseline for different quantization methods and two Llama 3 model sizes. The exact throughput results achieved are detailed later in this post.&nbsp;</p>\n\n\n\n<p>In all the experiments, we used input and output lengths of 2048 and 512, respectively, to build TensorRT-LLM engines and collect performance data. These values can be considered representative parameters for text summarization scenarios.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"538\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1-1024x538.png\" alt=\"Bar chart shows that inference speedup over FP16 baseline for batch size 32 is the highest at 1.81 for FP8 and INT8 SQ and for batch size 1 it is the highest at 2.66 for INT4 AWQ for Llama 3 70B model in both cases.\" class=\"wp-image-88804\" title=\"Chart\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1-1024x538.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1-300x158.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1-625x329.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1-179x94.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1-768x404.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1-1536x808.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1-645x339.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1-500x263.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1-160x84.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1-362x190.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1-209x110.png 209w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/inference-speedup-ptq-nemo-llama-3-8b-70b-1.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Performance benchmark on FP8, INT8 SQ, and INT4 AWQ</em></figcaption></figure></div>\n\n\n<p>The following tables show the number of GPUs used to build the engine for a given quantization format as well as the FP16 baseline results for two batch sizes, 32 and 1. The throughput is normalized with respect to the number of GPUs used.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>MODEL</strong></td><td><strong>FORMAT</strong></td><td><strong>GPUs</strong></td><td><strong>THROUGHPUT [TOKENS/SEC]</strong></td><td><strong>SPEEDUP</strong></td></tr><tr><td rowspan=\"4\"><strong>LLAMA 3 8B</strong></td><td>FP16</td><td>1</td><td>2293.08</td><td>\u2013</td></tr><tr><td>FP8</td><td>1</td><td>3330.85</td><td>1.45</td></tr><tr><td>INT8 SQ</td><td>1</td><td>3203.50</td><td>1.40</td></tr><tr><td>INT4 AWQ</td><td>1</td><td>2475.98</td><td>1.08</td></tr><tr><td rowspan=\"4\"><strong>LLAMA 3 70B</strong></td><td>FP16</td><td>4</td><td>256.10</td><td>\u2013</td></tr><tr><td>FP8</td><td>2</td><td>464.22</td><td>1.81</td></tr><tr><td>INT8 SQ</td><td>2</td><td>463.25</td><td>1.81</td></tr><tr><td>INT4 AWQ</td><td>2</td><td>360.57</td><td>1.41</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. TensorRT-LLM engine throughput results for batch size = 32 for the baseline and quantized Llama 3 models</em></figcaption></figure>\n\n\n\n<p>We observed 1.45, 1.40, and 1.08x speedups for FP8, <code>INT8 SQ</code>, and <code>INT4 AWQ</code>, respectively, for the smaller Llama 3 variant. In the case of the larger model, the speedup is up to 1.81x for both FP8 and <code>INT8 SQ</code>.\u00a0</p>\n\n\n\n<p>INT4 AWQ is a weight-only quantization method that is recommended for use with small batch sizes. It mostly improves memory bandwidth but becomes compute-bound for larger batches.\u00a0</p>\n\n\n\n<p>We present results for batch size = 1 for comparison. In this case, we obtained up to 1.56x and 2.66x performance benefits over the FP16 baseline for the Llama 3 8B and Llama 3 70B models, respectively. All the quantized variants of the Llama 3 70B model can be served using only one NVIDIA H100 GPU while the baseline FP16 precision requires at least two GPUs.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>MODEL</strong></td><td><strong>QFORMAT</strong></td><td><strong>GPUs</strong></td><td><strong>THROUGHPUT [TOKENS/SEC]</strong></td><td><strong>SPEEDUP</strong></td></tr><tr><td rowspan=\"4\"><strong>LLAMA 3 8B</strong></td><td>FP16</td><td>1</td><td>135.79</td><td>\u2013</td></tr><tr><td>FP8</td><td>1</td><td>170.75</td><td>1.26</td></tr><tr><td>INT8 SQ</td><td>1</td><td>158.90</td><td>1.17</td></tr><tr><td>INT4 AWQ</td><td>1</td><td>211.50</td><td>1.56</td></tr><tr><td rowspan=\"4\"><strong>LLAMA 3 70B</strong></td><td>FP16</td><td>2</td><td>17.75</td><td>\u2013</td></tr><tr><td>FP8</td><td>1</td><td>32.64</td><td>1.84</td></tr><tr><td>INT8 SQ</td><td>1</td><td>32.18</td><td>1.81</td></tr><tr><td>INT4 AWQ</td><td>1</td><td>47.13</td><td>2.66</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 3. TensorRT-LLM engine throughput results for batch size = 1 for the baseline and quantized Llama 3 models</em></figcaption></figure>\n\n\n\n<p>The throughput numbers reported should not be considered peak performance, as they could be further improved using other features of TensorRT-LLM such as <a href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/\">in-flight batching</a>, for example.</p>\n\n\n\n<p>We also examined performance statistics using the TensorRT-LLM <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main/benchmarks/cpp\">gptManagerBenchmark</a> tool, focusing on the FP16 baseline and FP8 quantized engines for <code>batch size = 32</code>.&nbsp;</p>\n\n\n\n<p>In the case of the Llama 3 8B model, time to first token (TTFT) improves and inter-token latency (ITL) speedups are roughly equivalent to the throughput-based speedups reported earlier in this post.&nbsp;</p>\n\n\n\n<p>For the larger Llama 3 70B model, both the TTFT and ITL results achieved by quantized engines running on 2x fewer GPUs are similar to the baseline FP16 results. This directly translates into 2x savings for resources used. With PTQ, models can be served more efficiently using fewer GPUs.</p>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\">Summary<a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This post showed you how to use PTQ in NeMo to build efficient TensorRT-LLM engines for LLM deployment. For future iterations, the number of bits used for models will decrease substantially, as FP4 support comes with the next-generation NVIDIA B100 Blackwell architecture.&nbsp;</p>\n\n\n\n<p>It is also worth mentioning that for some applications, PTQ may be sufficient while other applications might require quantization-aware Training (QAT) techniques to fine-tune quantized weights to maintain model accuracy. QAT is also available in NeMo to meet these needs.</p>\n\n\n\n<p>For more information, see <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/quantization.html\">Post-Training Quantization (PTQ)</a>. The entry point for PTQ is the <a href=\"https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/megatron_gpt_ptq.py\">megatron_gpt_ptq.py</a> script. You may also find the <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/ptq.html\">NeMo Framework Post-Training Quantization (PTQ)</a> playbook useful. It guides you through the whole deployment process using two example models: Llama 3 and Nemotron-340b.&nbsp;</p>\n\n\n\n<p>As for QAT, the entry point is the <a href=\"https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_qat.py\">megatron_gpt_qat.py</a> script and the corresponding playbook is <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/qat.html\">NeMo Framework Quantization Aware Training (QAT) for Llama2 SFT Model</a>. For more information, see <a href=\"https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-best-practices.md\">Best Practices for Tuning the Performance of TensorRT-LLM</a>.</p>\n\n\n\n<h3 id=\"acknowledgments\"  class=\"wp-block-heading\">Acknowledgments<a href=\"#acknowledgments\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><em>The help of many dedicated engineers across various teams at NVIDIA is greatly appreciated for their contributions to successful NeMo and TensorRT Model Optimizer integration, including Asma Kuriparambil Thekkumpate, Keval Morabia, Wei-Ming Chen, Huizi Mao, Ao Tang, Dong Hyuk Chang, Alexandros Koumparoulis, Enwei Zhu, and Simon Layton.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>As large language models (LLMs) are becoming even bigger, it is increasingly important to provide easy-to-use and efficient deployment paths because the cost of serving such LLMs is becoming higher. One way to reduce this cost is to apply post-training quantization (PTQ), which consists of techniques to reduce computational and memory requirements for serving trained &hellip; <a href=\"https://developer.nvidia.com/blog/post-training-quantization-of-llms-with-nvidia-nemo-and-nvidia-tensorrt-model-optimizer/\">Continued</a></p>\n", "protected": false}, "author": 2273, "featured_media": 85622, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1483970", "discourse_permalink": "https://forums.developer.nvidia.com/t/post-training-quantization-of-llms-with-nvidia-nemo-and-nvidia-tensorrt-model-optimizer/306347", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [296, 453, 2932], "coauthors": [4005, 3066, 3815, 4006], "class_list": ["post-88489", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "tag-ai-inference-microservices", "tag-featured", "tag-large-language-models"], "acf": {"post_industry": ["General"], "post_products": ["NeMo", "TensorRT", "TensorRT-LLM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/llm-megatron-core-blog-2967200-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-n1f", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88489"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2273"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88489"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88489/revisions"}], "predecessor-version": [{"id": 88805, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88489/revisions/88805"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/85622"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88489"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88489"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88489"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88489"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88813, "date": "2024-09-09T15:07:50", "date_gmt": "2024-09-09T22:07:50", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88813"}, "modified": "2024-10-09T13:02:11", "modified_gmt": "2024-10-09T20:02:11", "slug": "spotlight-shell-accelerates-co2-storage-modeling-100000x-using-nvidia-modulus", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/spotlight-shell-accelerates-co2-storage-modeling-100000x-using-nvidia-modulus/", "title": {"rendered": "Spotlight: Shell Accelerates CO2 Storage Modeling 100,000x Using NVIDIA Modulus"}, "content": {"rendered": "\n<p>As the world faces the urgent need to combat climate change, carbon capture and storage (CCS) has emerged as a crucial technology for achieving net-zero emissions. The CCS technology\u2014which involves capturing carbon dioxide (CO<sub>2</sub>), either from industrial emissions or through direct air capture (DAC), and securely storing it in the subsurface\u2014can drive much-needed decarbonization strategies and help achieve global climate targets. </p>\n\n\n\n<p>The success of CCS technology depends on the careful selection of storage sites and injection schemes. Accurate predictions of CO<sub>2</sub> plume migration and pressure buildup over extended periods, often spanning hundreds of years, are essential for ensuring the safety and efficacy of storage sites. Finding the optimal setup requires assessing tens of thousands of configurations under varying subsurface conditions, well locations, and injection rates. However, conventional flow simulators, which are typically used for these predictions, are computationally intensive and thus limit the high-throughput screening of potential sites and injection schemes.&nbsp;</p>\n\n\n\n<p>To address these challenges, Shell, in collaboration with NVIDIA, is leveraging cutting-edge technology through <a href=\"https://developer.nvidia.com/modulus\">NVIDIA Modulus</a> to enhance the efficiency and accuracy of CCS site screening processes. </p>\n\n\n\n<h2 id=\"strategy_to_build_a_rapid_screening_tool_for_carbon_capture_and_storage\"  class=\"wp-block-heading\">Strategy to build a rapid screening tool for carbon capture and storage<a href=\"#strategy_to_build_a_rapid_screening_tool_for_carbon_capture_and_storage\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The project employs machine learning (ML) models to enable rapid high-resolution modeling of subsurface CO<sub>2</sub> storage. This innovative approach not only accelerates the deployment of CCS technology but also enables more informed decisions about where and how to best store CO<sub>2</sub>, ultimately contributing to the global effort to mitigate climate change. The expertise of Shell in the energy industry is combined with the leadership on NVIDIA in AI and the computational modeling space to build the technological advancements and breakthroughs to rapidly screen potential sites for CO<sub>2</sub> storage in a cost-effective and timely manner.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"2605\" height=\"1629\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart.png\" alt=\"A flow chart showing various functionality centered around the CCS screening tool, including AI-based surrogate models, spatiotemporal prediction, improvised uncertainty assessment, and prior geology model operating conditions.\n\" class=\"wp-image-88819\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart.png 2605w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart-625x391.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart-768x480.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart-1536x961.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart-2048x1281.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart-645x403.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart-480x300.png 480w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart-144x90.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart-362x226.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart-176x110.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/css-screening-tool-flowchart-1024x640.png 1024w\" sizes=\"(max-width: 2605px) 100vw, 2605px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. A CCS screening tool enables spatiotemporal modeling of CO<sub>2</sub> in subsurface reservoirs reflecting realistic geologies. AI-based surrogates provide unprecedented speedup enabling improved uncertainty assessments</em></figcaption></figure>\n\n\n\n<p>A major challenge in identifying the most suitable sites for storage is assessing post-injection containment of CO<sub>2</sub> within a reservoir. This is because CO<sub>2</sub> migrates over time and potentially escapes from the reservoir, posing a risk to the environment. In addition, the reservoir pressure buildup caused by CO<sub>2</sub> injection needs to be carefully managed to avoid cracks in the geological layers, which are sealing the top of the reservoir and to prevent seismic hazards on the surface.&nbsp;</p>\n\n\n\n<p>Researchers from Stanford University, California Institute of Technology, and Purdue University have shown that advanced AI-based surrogate models of subsurface CO<sub>2</sub> behavior substantially reduce computational costs relative to traditional numerical models, while preserving high levels of accuracy. To learn more, see <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0309170822000562\">U-FNO\u2014An Enhanced Fourier Neural Operator-Based Deep-Learning Model for Multiphase Flow</a> and <a href=\"https://pubs.rsc.org/en/content/articlelanding/2007/6w/d2ee04204e/unauth\">Real-Time High-Resolution CO2 Geological Storage Prediction Using Nested Fourier Neural Operators</a>.&nbsp;</p>\n\n\n\n<p>Consequently, these models enable the examination of tens of thousands of injection configurations, facilitating more comprehensive and rapid screening of potential storage sites. To scale this research to industrial settings, Shell and NVIDIA jointly developed an AI framework based on Fourier neural operators (FNOs) to emulate the behavior of CO<sub>2</sub> in the reservoir. The software tool heavily leverages <a href=\"https://developer.nvidia.com/modulus\">NVIDIA Modulus</a>, an open-source framework for building, training, and fine-tuning Physics-ML models with a simple Python interface.&nbsp;</p>\n\n\n\n<p>NVIDIA Modulus provides an extensive collection of neural network and neural operator architectures alongside convenience functions for setting up and scaling out training and inference pipelines. With the heavy lifting on the ML side being done, Modulus enables domain scientists and engineers to apply state-of-the-art ML techniques to their problems and scale them to massively parallel settings and deployment in production.</p>\n\n\n\n<h2 id=\"results_and_applications\"  class=\"wp-block-heading\">Results and applications<a href=\"#results_and_applications\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The model is trained on a comprehensive dataset generated from realistic subsurface parameters simulating 1,000 years post-injection. The fully trained model offers O(10<sup>5</sup>[AC1] ) computational speedup with minimal sacrifice in prediction accuracy. It was shown that just one site assessment justifies training such a model. The fully trained model can be applied to numerous screening tasks, amplifying the benefits of this approach. </p>\n\n\n\n<p>To analyze the accuracy of the model, we focused on spatiotemporal distributions of mass of CO<sub>2</sub> (<img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=m_%7BCO_2%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"m_{CO_2}\" class=\"latex\" />), gas saturation (<img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=S_g&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"S_g\" class=\"latex\" />), and pressure buildup (<img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Cdelta+p&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;delta p\" class=\"latex\" />). As illustrated in Figure 2, the model\u2019s prediction shows strong qualitative agreement with the ground truth for all three variables.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"2160\" height=\"1278\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation.png\" alt=\"Figures depicting the simulation domain overlayed with pseudocolor plots of gas saturation, pressure buildup, and mass accumulation.\" class=\"wp-image-88839\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation.png 2160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation-300x178.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation-625x370.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation-179x106.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation-768x454.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation-1536x909.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation-2048x1212.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation-645x382.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation-500x296.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation-152x90.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation-362x214.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation-186x110.png 186w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/simulation-domain-plots-gas-saturation-pressure-buildup-mass-accumulation-1024x606.png 1024w\" sizes=\"(max-width: 2160px) 100vw, 2160px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Visual comparison of the fields for <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=S_g&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"S_g\" class=\"latex\" />, <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=m_%7BCO_2%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"m_{CO_2}\" class=\"latex\" />, and <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Cdelta+p&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;delta p\" class=\"latex\" /> [AC2] as predicted by the model with respective ground truth</em></figcaption></figure></div>\n\n\n<p>Figure 3a shows the mean absolute error (MAE) for the two solution variables of interest; MAE is a good metric for assessing the accuracy of the variables over the entire domain. In addition, <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=p_%7B90%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"p_{90}\" class=\"latex\" /> [<img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=m_%7BCO_2%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"m_{CO_2}\" class=\"latex\" />], an indicator for the migration distance of the CO<sub>2</sub> plume from the injection location is monitored. Figure 3b provides the <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=R%5E2&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"R^2\" class=\"latex\" /> correlation plots of <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=p_%7B90%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"p_{90}\" class=\"latex\" /> over time. For pressure buildup, global metrics are not informative. Instead, local, pointwise metrics for assessing the accuracy of the pressure predictions are explored. Histograms of maximum point-wise error in (<img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Cdelta+p&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;delta p\" class=\"latex\" />) across samples are shown in Figure 3c.</p>\n\n\n\n<p>While monitoring CCS sites, the location where the maximum pressure occurs is of specific interest\u2014typically near the injection well. Thus, the prediction from the model at the location where the true pressure is maximum (red circles in Figure 3d) is evaluated. To avoid location bias, we randomly select additional locations from the test set and assess predictions from the model at the same location (blue circles in Figure 3d). In both metrics, a <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=R%5E2&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"R^2\" class=\"latex\" /> score greater than 0.97 is observed, suggesting that the model would provide reliable predictions in most scenarios.</p>\n\n\n\n<p>For more detailed information, see <a href=\"https://openreview.net/pdf?id=DBHG4c9F6h\">Fourier Neural Operator Based Surrogates for Storage in Realistic Geologies</a>. The paper also presents super-resolution experiments and strategies for further improving the reliability of the predictions from the model, which is crucial while assessing actual geological sites.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"2159\" height=\"1501\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration.png\" alt=\"Error metrics a) MAE for mCO2 and Sg, with MAE representing average in time b) R2 correlation plots of 90% plume mass migration distance p90. c) Maximum pointwise error in \u03b4p d) R2 correlation plots of \u03b4p at maximum pressure location corresponding to the test sample shown with red circles. Correlation plots \u03b4p for randomly selected locations within the active domain of the test sample shown with blue circles.\n\" class=\"wp-image-88850\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration.png 2159w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration-300x209.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration-625x435.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration-165x115.png 165w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration-768x534.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration-1536x1068.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration-2048x1424.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration-645x448.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration-432x300.png 432w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration-129x90.png 129w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration-362x252.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration-158x110.png 158w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/quantitative-error-metrics-gas-saturation-pressure-buildup-plume-gas-migration-1024x712.png 1024w\" sizes=\"(max-width: 2159px) 100vw, 2159px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Quantitative error metrics for gas saturation, pressure buildup, and plume mass migration</em></figcaption></figure>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Shell, in collaboration with NVIDIA, has developed a machine learning model based on the Fourier neural operators for real-time, high-resolution simulation of CO<sub>2</sub> plume migration. This model is trained on an extensive dataset derived from realistic subsurface parameters. During inference, we observe a speedup of O(10<sup>5</sup>[AC3] ) over traditional numerical simulators of CO<sub>2</sub> flow fields with minimal reduction in accuracy. Along with fast surrogate models, we present and assess several physics-based accuracy metrics that are relevant for assessing and monitoring CCS sites.&nbsp;</p>\n\n\n\n<p>Additionally, we propose several strategies\u2014outlier detection, enforcing mass conservation\u2014to enhance the reliability of the model\u2019s predictions, which is vital when evaluating actual geological sites. Our work scales up scientific machine learning models to realistic systems that are more consistent with real-life subsurface reservoirs and builds the foundation for a first-of-its kind advanced screening tool for subsurface CCS applications.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>As the world faces the urgent need to combat climate change, carbon capture and storage (CCS) has emerged as a crucial technology for achieving net-zero emissions. The CCS technology\u2014which involves capturing carbon dioxide (CO2), either from industrial emissions or through direct air capture (DAC), and securely storing it in the subsurface\u2014can drive much-needed decarbonization strategies &hellip; <a href=\"https://developer.nvidia.com/blog/spotlight-shell-accelerates-co2-storage-modeling-100000x-using-nvidia-modulus/\">Continued</a></p>\n", "protected": false}, "author": 2288, "featured_media": 88817, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1483319", "discourse_permalink": "https://forums.developer.nvidia.com/t/spotlight-shell-accelerates-co2-storage-modeling-100-000x-using-nvidia-modulus/306236", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 503], "tags": [3268, 1916, 1913, 453, 2375, 3281], "coauthors": [4020, 3316, 4021, 2091], "class_list": ["post-88813", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-simulation-modeling-design", "tag-generative-ai-3d", "tag-computational-fluid-dynamics", "tag-climate-weather-ocean-modeling", "tag-featured", "tag-industrial-digitalization-digital-twin", "tag-physics-ml"], "acf": {"post_industry": ["Academia / Education", "Energy"], "post_products": ["Modulus"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/co2-storage-facility-3d-rendering.png", "jetpack_shortlink": "https://wp.me/pcCQAL-n6t", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88813"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2288"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88813"}], "version-history": [{"count": 34, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88813/revisions"}], "predecessor-version": [{"id": 88877, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88813/revisions/88877"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88817"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88813"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88813"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88813"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88813"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88415, "date": "2024-09-09T09:00:00", "date_gmt": "2024-09-09T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88415"}, "modified": "2024-09-19T12:33:34", "modified_gmt": "2024-09-19T19:33:34", "slug": "transform-live-media-pipelines-with-nvidia-holoscan-for-media", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/transform-live-media-pipelines-with-nvidia-holoscan-for-media/", "title": {"rendered": "Transform Live Media Pipelines with NVIDIA Holoscan for Media"}, "content": {"rendered": "\n<p><a href=\"https://developer.nvidia.com/holoscan-for-media\">NVIDIA Holoscan for Media</a> is now ready to be used in live production, taking advantage of the best of both networking and GPU technologies.&nbsp;</p>\n\n\n\n<p>Holoscan for Media is a software-defined, AI-enabled platform that enables live video pipelines to run on the same infrastructure as AI. It delivers applications from established and emerging vendors across the ecosystem on repurposable, NVIDIA-accelerated commercially available off-the-shelf hardware.&nbsp;</p>\n\n\n\n<h2 id=\"production_robustness\"  class=\"wp-block-heading\">Production robustness<a href=\"#production_robustness\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The platform inherently accommodates all types of IP media protocols, including uncompressed ST 2110, to effectively supplant or integrate with traditional SDI setups, delivering necessary quality, sync, and timing across the network.</p>\n\n\n\n<p>NVIDIA performed rigorous density and compliance assessments to guarantee the necessary performance and dependability for high-pressure production environments. Incorporating ST 2022-7 redundancy on top of hardware failover, the platform now uses Red Hat OpenShift 4.14 for production deployments as well as NVIDIA Network Operator 24.7. This enhances stability during high-traffic conditions while maintaining ST 2110 compliance and optimal performance.</p>\n\n\n\n<p>Every networking interface can handle up to 35 uncompressed 1080p60 streams or eight 4k 60fps streams with 100 Gb/s networking infrastructure. It is also ready for 200 Gb/s networking.</p>\n\n\n\n<p>The platform also offers precise hardware monitoring and alert management, enabling you to give industry-standard references to monitoring applications.</p>\n\n\n\n<p>The updated deployment guide features enhanced automation through new scripts, enabling the automatic setup of red/blue networks on network adapters and the installation of open-source services for block storage and load balancing.&nbsp;</p>\n\n\n\n<p>Several partners are using the platform to offer products such as vision mixers, multi-viewers, encoders/decoders/transcoders, playout servers, camera, and media controllers, as well as live graphics and AI-driven captioning solutions.</p>\n\n\n\n<h2 id=\"seamless_developer_experience\"  class=\"wp-block-heading\">Seamless developer experience<a href=\"#seamless_developer_experience\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With an improved developer experience, anyone can now develop using a single workstation.&nbsp;</p>\n\n\n\n<p>Our local developer guide has been updated, enabling you to run a full Holoscan for Media environment to build and test applications. An extended set of reference applications is now available:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Helm dashboard container:</strong> Deploy applications with a simple graphical interface.</li>\n\n\n\n<li><strong>NMOS Controller and Registry containers:</strong> Test automatic discovery and dynamic media connection while developing, with a new open-source companion library to integrate an NMOS Node seamlessly in any application.</li>\n\n\n\n<li><strong>Media Gateway container:</strong> Provides a multi-capable application for ingress/egress of ST 2110 with NMOS support without having to learn each of these protocols and also GPU-accelerated compressed media streaming capabilities through GStreamer. It is built on top of NVIDIA DeepStream SDK and full source code is also provided for the Media Gateway plugins.</li>\n\n\n\n<li><strong>New code sample:</strong> Demonstrate live GPU inference using any AI model within Media Gateway. Provided on request.</li>\n\n\n\n<li><strong>Integration code samples:</strong> Demonstrate direct integration of the foundational Rivermax SDK, CUDA, Vulkan, and Video Codec SDKs, while highlighting the flexibility of the platform and supported SDK. Also provided on request.</li>\n</ul>\n\n\n\n<h2 id=\"holoscan_for_media_in_action\"  class=\"wp-block-heading\">Holoscan for Media in action<a href=\"#holoscan_for_media_in_action\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA is showcasing live demonstrations of Holoscan for Media on the Sony, Monks, and Dell booths at <a href=\"https://www.nvidia.com/en-us/events/ibc/\">IBC 2024</a>, including the latest improvements and partner applications.&nbsp;</p>\n\n\n\n<p>For more information about the core technologies of the platform as well as how applications are deployed and connected, see the <a href=\"https://developer.nvidia.com/holoscan-for-media\">NVIDIA Holoscan for Media</a> developer page.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA Holoscan for Media is now ready to be used in live production, taking advantage of the best of both networking and GPU technologies.&nbsp; Holoscan for Media is a software-defined, AI-enabled platform that enables live video pipelines to run on the same infrastructure as AI. It delivers applications from established and emerging vendors across the &hellip; <a href=\"https://developer.nvidia.com/blog/transform-live-media-pipelines-with-nvidia-holoscan-for-media/\">Continued</a></p>\n", "protected": false}, "author": 1874, "featured_media": 88441, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1483145", "discourse_permalink": "https://forums.developer.nvidia.com/t/transform-live-media-pipelines-with-nvidia-holoscan-for-media/306210", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 852, 503], "tags": [453, 3592], "coauthors": [3502, 3186, 4011], "class_list": ["post-88415", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-graphics", "category-data-center-cloud", "category-simulation-modeling-design", "tag-featured", "tag-media-streaming"], "acf": {"post_industry": ["Media & Entertainment"], "post_products": ["Holoscan"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/holoscan-for-media-featured.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-n03", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88415"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1874"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88415"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88415/revisions"}], "predecessor-version": [{"id": 88577, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88415/revisions/88577"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88441"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88415"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88415"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88415"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88415"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88586, "date": "2024-09-09T08:08:15", "date_gmt": "2024-09-09T15:08:15", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88586"}, "modified": "2024-10-21T09:26:32", "modified_gmt": "2024-10-21T16:26:32", "slug": "high-tech-ai-framework-transforms-global-marine-pollution-tracking", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/high-tech-ai-framework-transforms-global-marine-pollution-tracking/", "title": {"rendered": "High-Tech AI Framework Transforms Global Marine Pollution Tracking"}, "content": {"rendered": "\n<p>An AI-powered remote sensing study offers a dynamic new tool for global ocean cleanup efforts. Detailed in the <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, the <a href=\"https://www.sciencedirect.com/science/article/pii/S0924271624000625#s0085\">breakthrough</a> unveils MariNeXt, a deep-learning framework that detects and identifies marine pollution using high-resolution Sentinel-2 imagery. MariNeXt could revolutionize how resource managers and agencies globally monitor and mitigate marine pollution by accurately detecting marine debris and oil spills on the sea surface.</p>\n\n\n\n<p>\u201cMarine debris is currently considered one of the most pressing issues in marine pollution. The ability to automatically and accurately identify debris is important for responding to and reducing the significant threats to ecosystem health and the blue economy,\u201d said Katerina Kikaki, study lead author and postdoctoral researcher at the National Technical University of Athens.\u00a0</p>\n\n\n\n<p>Sources of pollution such as oil spills, marine litter, and algal blooms are an ongoing threat to human health, aquatic life, and the economy. In the past, detecting marine pollutants using manual methods was labor-intensive and time-consuming, resulting in only a fraction being identified.&nbsp;</p>\n\n\n\n<p>\u201cAI is an increasingly powerful tool for ocean monitoring. Combined with remote sensing, it offers automated data collection and analysis across large spatial and temporal scales, enabling more comprehensive and cost-efficient monitoring,\u201d Kikaki said.&nbsp;&nbsp;</p>\n\n\n\n<p>Effective marine pollution monitoring systems are crucial for achieving UN Sustainable Development Goals, as they play a key role in ensuring the long-term health of marine environments. However, current AI algorithms fail to identify pollutants accurately.&nbsp;</p>\n\n\n\n<p>Most proposed methods have been designed to detect a single marine pollutant or a small number of sea surface features. Plus, they tend to operate locally, without the ability for large-scale monitoring. Another challenge is that marine pollutants have complex optical properties, and current satellite sensors aren&#8217;t always equipped to handle them.</p>\n\n\n\n<p>Looking \u200cto overcome these limitations, researchers from the National Technical University of Athens and King Abdullah University of Science and Technology developed MariNeXt. The deep learning framework integrates advanced data augmentation techniques and a multi-scale convolutional attention network, enabling it to learn and generalize from wide-ranging conditions and sea surface features.</p>\n\n\n\n<p>The researchers trained MariNeXt on the Marine Debris and Oil Spill (MADOS) dataset, which they also created using about 1.5M annotated pixels from 174 satellite scenes collected worldwide between 2015 and 2022. The comprehensive dataset includes 15 classes, including floating marine debris, oil spills, <em>Sargassum</em> macroalgae, natural organic material, ships, sea snot, and water-related conditions such as waves and turbid or shallow water.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"644\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-1024x644.png\" alt=\"A grid showing satellite image patches of the 15 classes.\" class=\"wp-image-88587\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-1024x644.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-300x189.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-625x393.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-768x483.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-1536x965.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-2048x1287.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-645x405.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-477x300.png 477w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-143x90.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-362x228.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/mados_overview-175x110.png 175w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. An overview of the MADOS patches showing marine pollutants and sea surface features annotated under various weather and sea state conditions</em></figcaption></figure></div>\n\n\n<p>The researchers developed and tested the model using the cuDNN-accelerated PyTorch framework on two <a href=\"https://www.nvidia.com/en-us/design-visualization/rtx-a5000/\">NVIDIA RTX A5000 GPUs</a>, each with 24 GB of memory. The researchers were awarded the two RTX A5000 as recipients of the <a href=\"https://www.nvidia.com/en-us/industries/higher-education-research/academic-grant-program/\">NVIDIA Academic Hardware Grant Program</a>.</p>\n\n\n\n<p>According to study coauthor Ioannis Kakogeorgiou, &#8220;The significant GPU capacity enabled the team to develop advanced deep-learning solutions beyond traditional machine-learning methods like random forests. This high-performance hardware allowed extensive experimentation with larger models, higher input resolutions, and increased batch sizes.&#8221;</p>\n\n\n\n<p>The MariNeXt model reached an overall accuracy of 89.1% in identifying marine pollutants and sea surface features across different ocean conditions. The AI framework also produced promising predictive maps and outperformed other machine learning baseline models, highlighting its potential for understanding and monitoring oceanic environments.</p>\n\n\n\n<p>While MariNeXt is a useful tool for ocean monitoring, it has limitations. The dataset is by nature unbalanced and some classes like marine water and oil spills, are abundant, while others, like foam and natural organic material, are less represented.&nbsp;</p>\n\n\n\n<p>This could limit the model\u2019s ability to detect \u200cless-represented classes in regions beyond the dataset\u2019s coverage. The researchers are currently working on improving MariNeXt&#8217;s predictive capabilities.</p>\n\n\n\n<p>\u201cPutting the limitations aside, MADOS is a valuable dataset that benchmarks machine learning algorithms for marine pollution detection from open Sentinel-2 data, supporting the development of future operational marine monitoring solutions,\u201d Kikaki said.&nbsp;</p>\n\n\n\n<p>Learn more and access the open-source code on <a href=\"https://github.com/gkakogeorgiou/mados?tab=readme-ov-file\">GitHub</a>.</p>\n\n\n\n<p>Read the <a href=\"https://www.sciencedirect.com/science/article/pii/S0924271624000625\">research</a><em> Detecting Marine Pollutants and Sea Surface Features with Deep Learning in Sentinel-2 imagery.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>An AI-powered remote sensing study offers a dynamic new tool for global ocean cleanup efforts. Detailed in the ISPRS Journal of Photogrammetry and Remote Sensing, the breakthrough unveils MariNeXt, a deep-learning framework that detects and identifies marine pollution using high-resolution Sentinel-2 imagery. MariNeXt could revolutionize how resource managers and agencies globally monitor and mitigate marine &hellip; <a href=\"https://developer.nvidia.com/blog/high-tech-ai-framework-transforms-global-marine-pollution-tracking/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 88590, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1481547", "discourse_permalink": "https://forums.developer.nvidia.com/t/high-tech-ai-framework-transforms-global-marine-pollution-tracking/305854", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 1903], "tags": [1949, 3941, 453, 4125, 1877], "coauthors": [2315], "class_list": ["post-88586", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-features", "tag-higher-education-and-academia", "tag-ai-impact", "tag-featured", "tag-nvidia-academic-grant-program", "tag-research"], "acf": {"post_industry": ["General", "Academia / Education"], "post_products": ["General"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Marine-pollutants-e1725469589110.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-n2O", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88586"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88586"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88586/revisions"}], "predecessor-version": [{"id": 88797, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88586/revisions/88797"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88590"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88586"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88586"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88586"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88586"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]