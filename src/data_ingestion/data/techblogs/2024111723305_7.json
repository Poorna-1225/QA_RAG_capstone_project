[{"id": 88550, "date": "2024-09-06T13:30:09", "date_gmt": "2024-09-06T20:30:09", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88550"}, "modified": "2024-09-19T12:34:01", "modified_gmt": "2024-09-19T19:34:01", "slug": "enhancing-application-portability-and-compatibility-across-new-platforms-using-nvidia-magnum-io-nvshmem-3-0", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/enhancing-application-portability-and-compatibility-across-new-platforms-using-nvidia-magnum-io-nvshmem-3-0/", "title": {"rendered": "Enhancing Application Portability and Compatibility across New Platforms Using NVIDIA Magnum IO NVSHMEM 3.0"}, "content": {"rendered": "\n<p><a href=\"https://developer.nvidia.com/nvshmem\">NVSHMEM</a> is a parallel programming interface that provides efficient and scalable communication for NVIDIA GPU clusters. Part of <a href=\"https://www.nvidia.com/en-us/data-center/magnum-io/\">NVIDIA Magnum IO</a> and based on <a href=\"http://openshmem.org/site/specification\">OpenSHMEM</a>, NVSHMEM creates a global address space for data that spans the memory of multiple GPUs and can be accessed with fine-grained GPU-initiated operations, CPU-initiated operations, and operations on CUDA streams.</p>\n\n\n\n<p>Existing communication models, such as the Message Passing Interface (MPI), orchestrate data transfers using the CPU. In contrast, NVSHMEM uses asynchronous, GPU-initiated data transfers, eliminating synchronization overheads between the CPU and the GPU.</p>\n\n\n\n<p>This post dives into the details of the NVSHMEM 3.0 release, including <a href=\"https://developer.nvidia.com/nvshmem-downloads\">new features and support</a> that we are enabling across platforms and systems.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"434\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nvshmem-mpi-comparison-625x434.png\" alt=\"Two workflow diagrams depict the difference between an MPI workflow and a NVSHMEM workflow. The MPI workflow depicts single in and out data streams from the GPU to the network, while the NVSHMEM workflow shows multiple (parallel) streams directly from the GPU to the network. \" class=\"wp-image-88557\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nvshmem-mpi-comparison-625x434.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nvshmem-mpi-comparison-300x209.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nvshmem-mpi-comparison-165x115.png 165w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nvshmem-mpi-comparison-768x534.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nvshmem-mpi-comparison-645x448.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nvshmem-mpi-comparison-432x300.png 432w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nvshmem-mpi-comparison-129x90.png 129w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nvshmem-mpi-comparison-362x252.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nvshmem-mpi-comparison-158x110.png 158w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/nvidia-nvshmem-mpi-comparison.png 1023w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVSHMEM and MPI comparison</em></figcaption></figure></div>\n\n\n<h2 id=\"new_features_and_interface_support_in_nvshmem_30\"  class=\"wp-block-heading\">New features and interface support in NVSHMEM 3.0<a href=\"#new_features_and_interface_support_in_nvshmem_30\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVSHMEM 3.0 introduces multi-node, multi-interconnect support, host-device ABI backward compatibility, and CPU-assisted InfiniBand GPU Direct Async (IBGDA).</p>\n\n\n\n<h3 id=\"multi-node_multi-interconnect_support&nbsp;\"  class=\"wp-block-heading\">Multi-node, multi-interconnect support&nbsp;<a href=\"#multi-node_multi-interconnect_support&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Previously, NVSHMEM has supported connectivity between multiple GPUs within a node over P2P interconnect (NVIDIA NVLink/PCIe) and multiple GPUs across a node over RDMA interconnects, such as InfiniBand, RDMA over Converged Ethernet (RoCE), Slingshot, and so on (Figure 2).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1545\" height=\"555\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks.png\" alt=\"This is a topological diagram with four compute nodes containing dual-GPUs connected by NVSwitch and each with a NIC, on the bottom layer, directly interconnected via the NICs in a criss-cross fashion to two Infiniband switch boxes above.\" class=\"wp-image-88615\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks.png 1545w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks-300x108.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks-625x225.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks-179x64.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks-768x276.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks-1536x552.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks-645x232.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks-500x180.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks-160x57.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks-362x130.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks-306x110.png 306w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nodes-rdma-networks-1024x368.png 1024w\" sizes=\"(max-width: 1545px) 100vw, 1545px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. A topology view of multiple nodes connected over RDMA networks</em></figcaption></figure></div>\n\n\n<p>NVSHMEM 2.11 added support for Multi-Node NVLink (MNNVL or <a href=\"https://www.nvidia.com/en-us/data-center/gb200-nvl72/\">NVIDIA GB200 NVL72</a>) systems (Figure 3). However, this support was limited to when NVLink was the only inter-node interconnect.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1159\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack.png\" alt=\"This is an architectural diagram with 18 compute nodes arranged in a matrix format, each node containing 4 GPUs and 2 CPUs, interconnected by 9 NVLink Switch trays, each with 2 NVLink Switch chips, in the middle, to show how all 72 nodes are connected within a single NVIDIA GB200 NVL72 rack cabinet.\" class=\"wp-image-88616\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack-300x174.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack-625x362.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack-179x104.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack-768x445.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack-1536x891.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack-645x374.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack-500x290.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack-155x90.png 155w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack-362x210.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack-190x110.png 190w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-nvidia-nvl72-rack-1024x594.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. A topology view of a single NVIDIA GB200 NVL72 rack</em></figcaption></figure></div>\n\n\n<p>To address this limitation, new platform support was added to enable multiple racks of NVIDIA GB200 NVL72 systems connected to each other through RDMA networks (Figure 4).<strong> </strong>NVSHMEM 3.0 adds this platform support such that when two GPUs are part of the same NVLink fabric (for example, within the same NVIDIA GB200 NVL72 rack), NVLink will be used for communication.&nbsp;</p>\n\n\n\n<p>In addition, when the GPUs are spread across NVLink fabrics (for example, across NVIDIA GB200 NVL72 racks), the remote network will be used for communication between those GPUs. This release also enhances the <code>NVSHMEM_TEAM_SHARED</code> capabilities to contain all GPUs that are part of the same NVLink clique that spans one or more nodes.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"889\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected.png\" alt=\"An architectural diagram showing many NVIDIA GB200 NVL72 racks (as shown in the Figure 3 diagram), all interconnected by multiple InfiniBand box switches above them.\" class=\"wp-image-88617\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected-300x133.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected-625x278.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected-179x80.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected-768x342.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected-1536x683.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected-645x287.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected-500x222.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected-160x71.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected-362x161.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected-247x110.png 247w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/topology-multiple-nvidia-nvl72-racks-rdma-network-connected-1024x455.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. A topology view of multiple NVIDIA GB200 NVL72 racks connected over RDMA networks</em></figcaption></figure></div>\n\n\n<h3 id=\"host-device_abi_backward_compatibility&nbsp;\"  class=\"wp-block-heading\"><strong>Host-device ABI backward compatibility</strong>&nbsp;<a href=\"#host-device_abi_backward_compatibility&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Historically, NVSHMEM has not supported backwards compatibility for applications or independently compiled bootstrap plug-ins. NVSHMEM 3.0 introduces backwards compatibility across NVSHMEM minor versions. \u00a0An ABI breakage will be denoted by a change in the major version of NVSHMEM.\u00a0</p>\n\n\n\n<p>This feature enables the following use cases for libraries or applications consuming ABI compatible versions of NVSHMEM:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Libraries linked to a minor version 3.X of NVSHMEM can be installed on a system with a newer installed version 3.Y of NVSHMEM (Y &gt; X).</li>\n\n\n\n<li>Multiple libraries shipped together in an SDK, which link to different minor versions of NVSHMEM, will be supported by a single newer version of NVSHMEM.</li>\n\n\n\n<li>A <a href=\"https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html\">CUDA binary</a> (also referred to as cubin) statically linked to an older version of the NVSHMEM device library can be loaded by a library using a newer version of NVSHMEM.</li>\n</ul>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td></td><td class=\"has-text-align-center\" data-align=\"center\">NVSHMEM Host Library 2.12</td><td class=\"has-text-align-center\" data-align=\"center\">NVSHMEM Host Library 3.0\u200b</td><td class=\"has-text-align-center\" data-align=\"center\">NVSHMEM Host Library 3.1</td><td class=\"has-text-align-center\" data-align=\"center\">NVSHMEM Host Library 3.2</td><td class=\"has-text-align-center\" data-align=\"center\">NVSHMEM Host Library 4.0</td></tr><tr><td>Application linked to NVSHMEM 3.1</td><td class=\"has-text-align-center\" data-align=\"center\">No</td><td class=\"has-text-align-center\" data-align=\"center\">No</td><td class=\"has-text-align-center\" data-align=\"center\">Yes</td><td class=\"has-text-align-center\" data-align=\"center\">Yes</td><td class=\"has-text-align-center\" data-align=\"center\">No</td></tr><tr><td>Cubin linked to NVSHMEM 3.0</td><td class=\"has-text-align-center\" data-align=\"center\">No</td><td class=\"has-text-align-center\" data-align=\"center\">Yes</td><td class=\"has-text-align-center\" data-align=\"center\">Yes</td><td class=\"has-text-align-center\" data-align=\"center\">Yes</td><td class=\"has-text-align-center\" data-align=\"center\">No</td></tr><tr><td>Multiple Libraries<br>Lib 1 NVSHMEM 3.1\u200b<br>Lib 2 NVSHMEM 3.2</td><td class=\"has-text-align-center\" data-align=\"center\">No</td><td class=\"has-text-align-center\" data-align=\"center\">No</td><td class=\"has-text-align-center\" data-align=\"center\">No</td><td class=\"has-text-align-center\" data-align=\"center\">Yes</td><td class=\"has-text-align-center\" data-align=\"center\">No</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Compatibility for NVSHMEM 3.0 and future versions</em></figcaption></figure>\n\n\n\n<h3 id=\"cpu-assisted_infiniband_gpu_direct_async&nbsp;\"  class=\"wp-block-heading\"><strong>CPU-assisted InfiniBand GPU Direct Async</strong>&nbsp;<a href=\"#cpu-assisted_infiniband_gpu_direct_async&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In previous releases, NVSHMEM supported traditional InfiniBand GPU Direct Async (IBGDA), where the GPU directly drives the IB NIC, enabling massively parallel control plane operations. It is responsible for managing the Network Interface Card (NIC) control plane, such as ringing the doorbell when a new work request is published to the NIC.&nbsp;</p>\n\n\n\n<p>NVSHMEM 3.0 adds support for a new modality in IBGDA called CPU-assisted IBGDA, which acts as an intermediate mode between proxy-based networking and traditional IBGDA. It splits responsibilities of the control plane between the GPU and CPU. The GPU generates work requests (control plane operations) and the CPU manages NIC doorbell-ringing requests for submitted work requests. It also enables dynamic selections of the NIC assistant to be CPU or GPU at runtime.&nbsp;</p>\n\n\n\n<p>CPU-assisted IBGDA relaxes existing administrative-level configuration constraints in IBGDA peer mapping, thereby helping to improve IBGDA adoption on non-coherent platforms where administrative level configuration constraints are harder to enforce in large-scale cluster deployments.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"434\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/comparison-traditional-cpu-assisted-ibgda-1-625x434.png\" alt=\"There are two block-based diagrams showing a comparison between traditional Infiniband GPU Direct Async (IBGDA) communications versus CPU-assisted Infiniband GPU Direct Async communications. The traditional IBGDA diagram shows a green block on top and a red block below it connected together by a line. While the CPU-assisted IBGDA diagram shows the same two blocks, but this time with a blue CPU block drawn in the middle.\" class=\"wp-image-88746\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/comparison-traditional-cpu-assisted-ibgda-1-625x434.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/comparison-traditional-cpu-assisted-ibgda-1-300x208.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/comparison-traditional-cpu-assisted-ibgda-1-166x115.png 166w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/comparison-traditional-cpu-assisted-ibgda-1-768x533.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/comparison-traditional-cpu-assisted-ibgda-1-645x448.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/comparison-traditional-cpu-assisted-ibgda-1-432x300.png 432w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/comparison-traditional-cpu-assisted-ibgda-1-130x90.png 130w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/comparison-traditional-cpu-assisted-ibgda-1-362x251.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/comparison-traditional-cpu-assisted-ibgda-1-158x110.png 158w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/comparison-traditional-cpu-assisted-ibgda-1.png 929w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Comparison between traditional and CPU-assisted IBGDA</em></figcaption></figure></div>\n\n\n<h2 id=\"non-interface_support_and_minor_enhancements\"  class=\"wp-block-heading\">Non-interface support and minor enhancements<a href=\"#non-interface_support_and_minor_enhancements\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVSHMEM 3.0 also introduces minor enhancements and non-interface support, as detailed in this section.&nbsp;</p>\n\n\n\n<h3 id=\"object-oriented_programming_framework_for_symmetric_heap\u00a0\"  class=\"wp-block-heading\"><strong>Object-oriented programming framework for symmetric heap</strong>\u00a0<a href=\"#object-oriented_programming_framework_for_symmetric_heap\u00a0\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Historically, NVSHMEM has supported multiple symmetric heap kinds using a procedural programming model. This has limitations, such as a lack of namespace-based data encapsulation and code duplication and data redundancy.&nbsp;</p>\n\n\n\n<p>NVSHMEM 3.0 introduces support for an object-oriented programming (OOP) framework that can manage different kinds of symmetric heaps such as static device memory and dynamic device memory using multi-level inheritance. This will enable easier extension to advanced features like on-demand registration of application buffers to symmetric heap in future releases.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"377\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/object-oriented-hierarchy-nvidia-nvshmem-3-0-625x377.png\" alt=\"Diagram showing five boxes: two green stacked on the left, and two red stacked on the right, with a single black box at the top middle. All five boxes have bidirectional lines between them and reach the top black box. The green and red boxes are not directly connected, but instead connect through the black box in a clockwise direction.\" class=\"wp-image-88748\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/object-oriented-hierarchy-nvidia-nvshmem-3-0-625x377.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/object-oriented-hierarchy-nvidia-nvshmem-3-0-300x181.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/object-oriented-hierarchy-nvidia-nvshmem-3-0-179x108.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/object-oriented-hierarchy-nvidia-nvshmem-3-0-768x463.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/object-oriented-hierarchy-nvidia-nvshmem-3-0-645x389.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/object-oriented-hierarchy-nvidia-nvshmem-3-0-498x300.png 498w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/object-oriented-hierarchy-nvidia-nvshmem-3-0-149x90.png 149w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/object-oriented-hierarchy-nvidia-nvshmem-3-0-362x218.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/object-oriented-hierarchy-nvidia-nvshmem-3-0-183x110.png 183w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/object-oriented-hierarchy-nvidia-nvshmem-3-0.png 976w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. NVSHMEM 3.0 object-oriented hierarchy</em></figcaption></figure></div>\n\n\n<h3 id=\"performance_improvements_and_bug_fixes\"  class=\"wp-block-heading\">Performance improvements and bug fixes<a href=\"#performance_improvements_and_bug_fixes\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>NVSHMEM 3.0 introduces various performance improvements and bug fixes for different components and scenarios. These include IBGDA setup, block scoped on-device reductions, system scoped atomic memory operation (AMO), IB Queue Pair (QP) mapping, memory registration, team management, and PTX build testing.</p>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\">Summary<a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The 3.0 release of the NVIDIA NVSHMEM parallel programming interface introduces several new features, including multi-node multi-interconnect support, host-device ABI backward compatibility, CPU-assisted InfiniBand GPU Direct Async (IBGDA), and object-oriented programming framework for symmetric heap.</p>\n\n\n\n<p>With host-device ABI backward compatibility, administrators can update to a new version of NVSHMEM without breaking already compiled applications, eliminating the need to modify the application source with each update.</p>\n\n\n\n<p>CPU-assisted InfiniBand GPU Direct Async (IBGDA) enables users to benefit from the high message rate of the IBGDA transport on clusters where enforcing administrative level driver settings is not possible.</p>\n\n\n\n<p>To learn more and get started, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://developer.nvidia.com/blog/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async/\">IBGDA: Improving Network Performance of HPC Systems Using NVIDIA Magnum IO NVSHMEM and GPUDirect Async</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/scaling-scientific-computing-with-nvshmem/\">Scaling Scientific Computing with NVSHMEM</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/accelerating-nvshmem-2-0-team-based-collectives-using-nccl/\">Accelerating NVSHMEM 2.0 Team-Based Collectives Using NCCL</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/hpc-sdk/nvshmem/index.html\">NVSHMEM Documentation</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/nvshmem/release-notes-install-guide/best-practice-guide/index.html\">NVSHMEM Best Practices Guide</a></li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/hpc-sdk/nvshmem/api/docs/index.html\">NVSHMEM API Documentation</a></li>\n\n\n\n<li><a href=\"http://openshmem.org/site/\">OpenSHMEM</a></li>\n\n\n\n<li><a href=\"https://forums.developer.nvidia.com/tag/nvshmem\">NVSHMEM Developer Forum</a></li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>NVSHMEM is a parallel programming interface that provides efficient and scalable communication for NVIDIA GPU clusters. Part of NVIDIA Magnum IO and based on OpenSHMEM, NVSHMEM creates a global address space for data that spans the memory of multiple GPUs and can be accessed with fine-grained GPU-initiated operations, CPU-initiated operations, and operations on CUDA streams. &hellip; <a href=\"https://developer.nvidia.com/blog/enhancing-application-portability-and-compatibility-across-new-platforms-using-nvidia-magnum-io-nvshmem-3-0/\">Continued</a></p>\n", "protected": false}, "author": 788, "featured_media": 88553, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1482350", "discourse_permalink": "https://forums.developer.nvidia.com/t/enhancing-application-portability-and-compatibility-across-new-platforms-using-nvidia-magnum-io-nvshmem-3-0/306039", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 1205, 503], "tags": [453, 1298], "coauthors": [1315, 3113, 4008, 3112, 2613, 2319], "class_list": ["post-88550", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-networking-communications", "category-simulation-modeling-design", "tag-featured", "tag-nvshmem"], "acf": {"post_industry": ["Cloud Services", "HPC / Scientific Computing"], "post_products": ["CUDA", "InfiniBand", "Magnum IO", "NCCL", "NVLink", "NVSHMEM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cube-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-n2e", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Networking / Communications", "link": "https://developer.nvidia.com/blog/category/networking-communications/", "id": 1205}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88550"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/788"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88550"}], "version-history": [{"count": 16, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88550/revisions"}], "predecessor-version": [{"id": 88751, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88550/revisions/88751"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88553"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88550"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88550"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88550"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88550"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88462, "date": "2024-09-06T09:30:00", "date_gmt": "2024-09-06T16:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88462"}, "modified": "2024-09-19T12:34:20", "modified_gmt": "2024-09-19T19:34:20", "slug": "using-generative-ai-models-in-circuit-design", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/using-generative-ai-models-in-circuit-design/", "title": {"rendered": "Using Generative AI Models in Circuit Design"}, "content": {"rendered": "\n<p>Generative models have been making big waves in the past few years, from intelligent text-generating <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models (LLMs)</a> to creative image and video-generation models. At NVIDIA, we are exploring using generative AI models to speed up the circuit design process and deliver better designs to meet the ever-increasing demands for computational power.</p>\n\n\n\n<p>Circuit design is a challenging optimization problem. Designers often need to balance several conflicting objectives, such as power and area, and satisfy constraints, such as hitting a specific timing. The design space is usually combinatorial, making it difficult to find optimal designs. Previous research into prefix circuit design used hand-crafted heuristics and <a href=\"https://developer.nvidia.com/blog/designing-arithmetic-circuits-with-deep-reinforcement-learning/\">reinforcement learning</a> to explore the vast design space. For more details, see <a href=\"https://dl.acm.org/doi/abs/10.1145/2463209.2488793\">Towards Optimal Performance-Area Trade-Off in Adders by Synthesis of Parallel Prefix Structures</a> and <a href=\"https://ieeexplore.ieee.org/abstract/document/8509188\">Cross-Layer Optimization for High Speed Adders: A Pareto Driven Machine Learning Approach</a>.</p>\n\n\n\n<p>While these methods help to overcome the vastness of the search space, they are associated with high computational costs to train and often have poor generalizability.\u00a0</p>\n\n\n\n<p>Our paper <a href=\"https://arxiv.org/abs/2406.09535\">CircuitVAE: Efficient and Scalable Latent Circuit Optimization</a>, recently published at the Design Automation Conference, provides a glimpse into the potential of generative models in circuit designs. We demonstrate that variational autoencoders (VAEs), a class of generative models, can produce better prefix adder designs at a fraction of the computational cost required by previous approaches.</p>\n\n\n\n<h2 id=\"the_complexity_of_circuit_design\"  class=\"wp-block-heading\">The complexity of circuit design<a href=\"#the_complexity_of_circuit_design\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The design space of prefix adders is vast: there are around <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=2%5E%7Bn%5E2%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"2^{n^2}\" class=\"latex\" /> possible designs for adding two n-bit numbers, so it is truly a needle-in-the-haystack challenge. A circuit consists of logic gates, such as AND, OR, and XOR. There are many ways of connecting them to implement the same functionality, though different connections yield changes in the observed power, performance, and area of the circuit.&nbsp;</p>\n\n\n\n<p>In our paper, we focus on optimizing prefix adders, a class of circuits prevalent in modern GPUs. We represent a prefix adder with a tree, as shown in Figure 1. We minimize two metrics, area and delay, which we combine using a weighted sum into a single objective.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"841\" height=\"265\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/32-bit-prefix-trees-costs.png\" alt=\"Three examples of prefix trees with their associated costs.\n\" class=\"wp-image-88474\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/32-bit-prefix-trees-costs.png 841w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/32-bit-prefix-trees-costs-300x95.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/32-bit-prefix-trees-costs-625x197.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/32-bit-prefix-trees-costs-179x56.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/32-bit-prefix-trees-costs-768x242.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/32-bit-prefix-trees-costs-645x203.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/32-bit-prefix-trees-costs-500x158.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/32-bit-prefix-trees-costs-160x50.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/32-bit-prefix-trees-costs-362x114.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/32-bit-prefix-trees-costs-349x110.png 349w\" sizes=\"(max-width: 841px) 100vw, 841px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Example 32-bit prefix trees and their costs</em></em></figcaption></figure>\n\n\n\n<h2 id=\"what_are_variational_autoencoders\"  class=\"wp-block-heading\">What are variational autoencoders?<a href=\"#what_are_variational_autoencoders\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>VAEs are generative models that estimate some data distribution. We can sample from the estimated distribution after training a VAE model. VAEs are versatile in modeling data of different modalities, from images to graphs. A VAE model consists of an encoder and a decoder.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1122\" height=\"484\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/variational-autoencoder-model.png\" alt=\"A variational autoencoder model consists of an encoder, a latent space, and a decoder in a sequence.\n\" class=\"wp-image-88476\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/variational-autoencoder-model.png 1122w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/variational-autoencoder-model-300x129.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/variational-autoencoder-model-625x270.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/variational-autoencoder-model-179x77.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/variational-autoencoder-model-768x331.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/variational-autoencoder-model-645x278.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/variational-autoencoder-model-500x216.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/variational-autoencoder-model-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/variational-autoencoder-model-362x156.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/variational-autoencoder-model-255x110.png 255w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/variational-autoencoder-model-1024x442.png 1024w\" sizes=\"(max-width: 1122px) 100vw, 1122px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. A variational autoencoder model. Credit: Wikipedia</em></em></figcaption></figure>\n\n\n\n<p>In the case of image generation, an encoder maps an input image to a distribution of vectors called a latent space. A decoder converts the vector of an encoded image back to an image. The VAE is trained by minimizing the reconstruction loss between inputs and outputs, along with a regularization loss on the latent space. VAEs are generative models because they can generate new outputs by sampling vectors from the latent space and decoding them with the learned decoder.</p>\n\n\n\n<h2 id=\"circuitvae_vae_for_circuit_design\"  class=\"wp-block-heading\">CircuitVAE: VAE for circuit design<a href=\"#circuitvae_vae_for_circuit_design\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>CircuitVAE is a search algorithm that embeds computation graphs in a continuous space and optimizes a learned surrogate of physical simulation by gradient descent. It learns to embed circuits into a continuous latent space and predict quality metrics, such as area and delay, from latent representations. The cost predictor is fully differentiable when it is instantiated with a neural network. Thus, it\u2019s possible to apply gradient descent in the latent space to optimize circuit metrics, circumventing the challenge of searching in a combinatorial design space.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1097\" height=\"554\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/circuitvae-algorithm-flowchart.png\" alt=\"A flowchart of the CircuitVAE algorithm. It consists of training a VAE with a cost predictor and optimizing the cost predictor to find new designs.\n\" class=\"wp-image-88477\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/circuitvae-algorithm-flowchart.png 1097w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/circuitvae-algorithm-flowchart-300x152.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/circuitvae-algorithm-flowchart-625x316.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/circuitvae-algorithm-flowchart-179x90.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/circuitvae-algorithm-flowchart-768x388.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/circuitvae-algorithm-flowchart-645x326.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/circuitvae-algorithm-flowchart-500x253.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/circuitvae-algorithm-flowchart-160x81.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/circuitvae-algorithm-flowchart-362x183.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/circuitvae-algorithm-flowchart-218x110.png 218w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/circuitvae-algorithm-flowchart-1024x517.png 1024w\" sizes=\"(max-width: 1097px) 100vw, 1097px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. The CircuitVAE algorithm flowchart</em></em></figcaption></figure>\n\n\n\n<h3 id=\"circuitvae_training\"  class=\"wp-block-heading\">CircuitVAE training<a href=\"#circuitvae_training\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The CircuitVAE training loss has two parts:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>The standard VAE reconstruction and regularization losses.</li>\n\n\n\n<li>The mean squared error between the true and the predicted area and delay produced by the cost predictor model using the encoded circuit latent vectors.&nbsp;</li>\n</ul>\n\n\n\n<p>While fitting the cost predictor, the latent space is organized according to costs, which is amenable to gradient-based optimization. A set of adders is generated through the genetic algorithm to bootstrap the training. One could also use a random sample of adders to start.</p>\n\n\n\n<h3 id=\"gradient-based_optimization\"  class=\"wp-block-heading\">Gradient-based optimization<a href=\"#gradient-based_optimization\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>After training a CircuitVAE model, it\u2019s used to find prefix tree structures that minimize costs. First choose a latent vector using cost-weighted sampling, a technique that ensures starting from a good design. This vector is then modified with gradient descent by minimizing the cost estimated by the cost predictor model. The final vector is decoded into a prefix tree and synthesized to obtain its actual cost.</p>\n\n\n\n<h3 id=\"iteration\"  class=\"wp-block-heading\">Iteration<a href=\"#iteration\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The full CircuitVAE algorithm interleaves the training and optimization stage. After each round of model training, more data is collected with gradient-based optimization and physical synthesis. Model fitting resumes with a growing dataset of circuits and associated metrics, resulting in a virtuous cycle where the cost predictor model increases in accuracy, leading to a more targeted optimization.</p>\n\n\n\n<h2 id=\"results\"  class=\"wp-block-heading\">Results<a href=\"#results\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>We tested our approach on the design of circuits that have 32 inputs and 64 inputs (the width of the prefix tree circuit, corresponding to 32 bits and 64 bits). To supply our physical synthesis with components needed for simulation, we used an open-source cell library called Nangate45.</p>\n\n\n\n<p>Figure 4 shows the cost progression while each method evaluates more designs through physical simulations. CircuitVAE consistently achieves the lowest costs compared to baseline methods. Both RL and GA optimize in the discrete domain and are slow to explore, while CircuitVAE is 2-3x faster, thanks to gradient-based optimization in the latent space.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"940\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders.png\" alt=\"Six plots with results on designing 32-bit and 64-bit prefix adders with three delay weights.\n\" class=\"wp-image-88479\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders-300x141.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders-625x294.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders-768x361.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders-1536x722.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders-645x303.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders-500x235.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders-160x75.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders-362x170.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders-234x110.png 234w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/six-plots-results-designing-32-bit-64-bit-prefix-adders-1024x482.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. Experiment results on designing 32-bit and 64-bit prefix adders</em></em></figcaption></figure>\n\n\n\n<p>We evaluated CircuitVAE in a real-world prefix adder task with a proprietary cell library with input-output timings captured from a complete datapath. Figure 5 shows that CircuitVAE generates designs that form a better Pareto frontier of area and delay than a commercial tool.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1212\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison.png\" alt=\"A Pareto plot of area and delay comparing CircuitVAE with a commercial tool and classical designs.\n\" class=\"wp-image-88481\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison-300x182.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison-625x379.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison-179x109.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison-768x466.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison-1536x931.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison-645x391.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison-495x300.png 495w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison-148x90.png 148w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison-362x219.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison-181x110.png 181w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pareto-plot-circuitvae-commercial-tool-classical-designs-comparison-1024x621.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 5. CircuitVAE Pareto dominates a commercial tool in designing a real-world datapath design</em></em></figcaption></figure>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>CircuitVAE showcases the power of generative models in circuit design tasks. Operating in a latent space, rather than the combinatorially large discrete space of circuit designs, reaps the benefits of continuous optimization in the form of reduced computational costs. We believe this transformation from discrete to continuous holds promise in other areas of hardware design, such as place-and-route. We anticipate that generative models will play an increasingly central role in hardware design.</p>\n\n\n\n<p>For more information about CircuitVAE, see <a href=\"https://arxiv.org/abs/2406.09535\">CircuitVAE: Efficient and Scalable Latent Circuit Optimization</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Generative models have been making big waves in the past few years, from intelligent text-generating large language models (LLMs) to creative image and video-generation models. At NVIDIA, we are exploring using generative AI models to speed up the circuit design process and deliver better designs to meet the ever-increasing demands for computational power. Circuit design &hellip; <a href=\"https://developer.nvidia.com/blog/using-generative-ai-models-in-circuit-design/\">Continued</a></p>\n", "protected": false}, "author": 2272, "featured_media": 88737, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1482267", "discourse_permalink": "https://forums.developer.nvidia.com/t/using-generative-ai-models-in-circuit-design/306022", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110, 503], "tags": [453, 2932, 1962], "coauthors": [4004, 2924], "class_list": ["post-88462", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "category-simulation-modeling-design", "tag-featured", "tag-large-language-models", "tag-nvidia-research"], "acf": {"post_industry": ["Hardware / Semiconductor"], "post_products": ["General"], "post_learning_levels": ["Intermediate Technical", "Advanced Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/scatter-plot-circuit-design.png", "jetpack_shortlink": "https://wp.me/pcCQAL-n0O", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88462"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2272"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88462"}], "version-history": [{"count": 14, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88462/revisions"}], "predecessor-version": [{"id": 88743, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88462/revisions/88743"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88737"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88462"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88462"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88462"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88462"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88329, "date": "2024-09-05T13:30:00", "date_gmt": "2024-09-05T20:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88329"}, "modified": "2024-09-19T12:34:33", "modified_gmt": "2024-09-19T19:34:33", "slug": "achieving-state-of-the-art-zero-shot-waveform-audio-generation-across-audio-types", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/achieving-state-of-the-art-zero-shot-waveform-audio-generation-across-audio-types/", "title": {"rendered": "Achieving State-of-the-Art Zero-Shot Waveform Audio Generation across Audio Types"}, "content": {"rendered": "\n<p>Stunning audio content is an essential component of virtual worlds. Audio generative AI plays a key role in creating this content, and NVIDIA is continuously pushing the limits in this field of research. <a href=\"https://research.nvidia.com/labs/adlr/projects/bigvgan/\">BigVGAN</a>, developed in collaboration with the <a href=\"https://research.nvidia.com/labs/adlr/\">NVIDIA Applied Deep Learning Research </a>\u00a0and <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo</a> teams, is a generative AI model specialized in audio waveform synthesis that achieves state-of-the-art results. BigVGAN generates waveforms orders of magnitude faster than real time and shows strong robustness with various audio types, including speech, environmental sounds, and music.\u00a0</p>\n\n\n\n<p>This post discusses BigVGAN v2, which delivers significant improvements in speed and quality, empowering a future where generated audio is indiscernible from real audio. BigVGAN v2 highlights include:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>State-of-the-art audio quality</strong> measured by diverse metrics across many audio types.&nbsp;</li>\n\n\n\n<li><strong>Up to 3x faster synthesis speed</strong> by leveraging optimized CUDA kernels.&nbsp;</li>\n\n\n\n<li><strong>Ready-to-use pretrained checkpoints</strong> supporting diverse audio configurations.\u00a0</li>\n\n\n\n<li><strong>Support for a sampling rate of up to 44 kHz</strong>, which covers the highest sound frequency humans can hear.\u00a0</li>\n</ul>\n\n\n\n<h2 id=\"bigvgan_a_universal_neural_vocoder&nbsp;\"  class=\"wp-block-heading\">BigVGAN: A universal neural vocoder&nbsp;<a href=\"#bigvgan_a_universal_neural_vocoder&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://research.nvidia.com/labs/adlr/projects/bigvgan/\">BigVGAN</a> is a universal neural vocoder specialized in synthesizing audio waveforms using Mel spectrograms as inputs. Neural vocoders are a cornerstone method in audio generative AI that generate sound waves from compact acoustic features, such as Mel spectrogram. BigVGAN is available as open source through <a href=\"https://github.com/NVIDIA/BigVGAN\">NVIDIA/BigVGAN</a> on GitHub.&nbsp;</p>\n\n\n\n<p>BigVGAN is a fully convolutional architecture (Figure 1) with several upsampling blocks using transposed convolution followed by multiple residual dilated convolution layers. It features a novel module, called anti-aliased multiperiodicity composition (AMP), which is specifically designed for generating waveforms.&nbsp;</p>\n\n\n\n<p>AMP is specialized in synthesizing high-frequency and periodic sound waves, drawing inspiration from audio signal processing principles. It applies a periodic activation function, called <a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/file/1160453108d3e537255e9f7b931f4e90-Paper.pdf\">Snake</a>, which provides an inductive bias to the architecture in generating periodic sound waves. It also applies anti-aliasing filters to reduce undesired artifacts in the generated waveforms. To learn more, see <a href=\"https://research.nvidia.com/labs/adlr/projects/bigvgan/\">BigVGAN: A Universal Neural Vocoder with Large-Scale Training</a>.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"827\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis.png\" alt=\"The BigVGAN diagram details the flow and interaction between different layers and modules within the BigVGAN framework, highlighting the innovative use of periodic activation functions and filtering methods to improve synthesis fidelity. \n\" class=\"wp-image-88339\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis-300x124.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis-625x259.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis-179x74.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis-768x318.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis-1536x635.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis-645x267.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis-500x207.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis-160x66.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis-362x150.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis-266x110.png 266w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-architecture-universal-waveform-synthesis-1024x424.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. BigVGAN architecture and anti-aliased multi-periodicity composition modules (AMP) using periodic activation function (Snake1d) and low-pass filters for universal waveform synthesis</em></figcaption></figure>\n\n\n\n<h2 id=\"generating_every_sound_in_the_world&nbsp;\"  class=\"wp-block-heading\">Generating every sound in the world&nbsp;<a href=\"#generating_every_sound_in_the_world&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Waveform audio generation, a crucial component in building virtual worlds, has long been an active research area. Despite its importance, current vocoding methods often produce audio lacking fine details in high-frequency sound waves. BigVGAN v2 effectively addresses this issue, providing high-quality audio with enhanced fine details.</p>\n\n\n\n<p>BigVGAN v2 is trained using <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100 Tensor Core GPUs</a> and up to more than 100x larger audio data than its predecessor. Aimed at encapsulating every sound in the world, the dataset includes speech in multiple languages, environmental sounds from everyday objects, and diverse instruments. As a result, BigVGAN v2 can generate high-quality sound waves from numerous domains with a single model.&nbsp;</p>\n\n\n\n<p>Below, listen to audio comparisons of real recordings and generated samples from BigVGAN and BigVGAN v2 at the 24 kHz sampling rate. BigVGAN v2 generates high-quality sound waves.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-1 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h5 class=\"wp-block-heading has-text-align-center\">Recordings (24 kHz)</h5>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/recordings_1-3.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/recordings_2.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/recordings_3.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h5 class=\"wp-block-heading has-text-align-center\">BigVGAN</h5>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan_1.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan_2.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan_3.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h5 class=\"wp-block-heading has-text-align-center\">BigVGAN v2</h5>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan_v2_1.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan_v2_2.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan_v2_3.wav\"></audio></figure>\n</div>\n</div>\n\n\n\n<h2 id=\"reaching_the_highest_frequency_sound_the_human_ear_can_detect&nbsp;\"  class=\"wp-block-heading\">Reaching the highest frequency sound the human ear can detect&nbsp;<a href=\"#reaching_the_highest_frequency_sound_the_human_ear_can_detect&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Previous waveform synthesizers were limited to sampling rates between 22 kHz and 24 kHz. BigVGAN v2, however, expands this range to a 44 kHz sampling rate, encapsulating the entire human auditory spectrum. This matches the <a href=\"https://en.wikipedia.org/wiki/Hearing_range\">highest frequencies the human ear can detect</a>, which do not exceed a sampling rate of 40 kHz. As a result, BigVGAN v2 can reproduce comprehensive soundscapes, capturing everything from the robust reverberations of drums to the crisp shimmer of crash cymbals in music, for example.</p>\n\n\n\n<p>Below, listen to audio comparisons of real recordings and generated samples from two BigVGAN v2 models, one for the 24 kHz sampling rate and another for the 44 kHz sampling rate. </p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-2 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h5 class=\"wp-block-heading has-text-align-center\">Recordings (44 kHz)&nbsp;</h5>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/recordings_44khz_1.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/recordings_44khz_2.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/recordings_44khz_3.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h5 class=\"wp-block-heading has-text-align-center\">BigVGAN v2 (24 kHz)&nbsp;</h5>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan_v2_24khz_1.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan_v2_24khz_2.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan_v2_24khz_3.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h5 class=\"wp-block-heading has-text-align-center\">BigVGAN v2 (44 kHz)&nbsp;</h5>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan_v2_44khz_1.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan_v2_44khz_2.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan_v2_44khz_3.wav\"></audio></figure>\n</div>\n</div>\n\n\n\n<h2 id=\"faster_synthesis_with_custom_cuda_kernels&nbsp;\"  class=\"wp-block-heading\">Faster synthesis with custom CUDA kernels&nbsp;<a href=\"#faster_synthesis_with_custom_cuda_kernels&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Compared to its predecessor, BigVGAN v2 also features accelerated synthesis speed by using custom CUDA kernels, with up to 3x faster inference speed than the original BigVGAN. The optimized inference CUDA kernels written for BigVGAN v2 can generate audio waveforms up to 240x faster than real time on a single NVIDIA A100 GPU.&nbsp;</p>\n\n\n\n<h2 id=\"bigvgan_v2_audio_quality_results&nbsp;\"  class=\"wp-block-heading\">BigVGAN v2 audio quality results&nbsp;<a href=\"#bigvgan_v2_audio_quality_results&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>BigVGAN v2 24 kHz shows better audio quality for speech and general audio compared to its open-sourced predecessor\u2014and by a significant margin (Figures 2 and 3).&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"503\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data-625x503.png\" alt=\"The quality of generated waveforms metrics obtained for BigVGAN v2 24 kHz and BigVGAN models tested on LibriTTS-dev speech data. \n\" class=\"wp-image-88347\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data-625x503.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data-300x241.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data-143x115.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data-768x618.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data-1536x1236.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data-645x519.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data-373x300.png 373w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data-112x90.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data-362x291.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data-137x110.png 137w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data-1024x824.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-libritts-dev-speech-data.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. BigVGAN v2 24 kHz versus BigVGAN results on LibriTTS-dev speech data&nbsp;</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"254\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data-625x254.png\" alt=\"The quality of generated waveforms metrics obtained for BigVGAN v2 24 kHz and BigVGAN models tested on MUSDB18-HQ music data. \n\" class=\"wp-image-88351\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data-625x254.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data-300x122.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data-179x73.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data-768x312.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data-1536x623.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data-645x262.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data-500x203.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data-160x65.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data-362x147.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data-271x110.png 271w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data-1024x415.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-comparison-musdb18-hq-music-data.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>&nbsp;Figure 3. BigVGAN v2 24 kHz versus BigVGAN model results on MUSDB18-HQ music data&nbsp;</em></figcaption></figure>\n\n\n\n<p>In addition, the new BigVGAN v2 44 kHz model shows comparable audio quality to <a href=\"https://github.com/descriptinc/descript-audio-codec\">Descript Audio Codec (.dac)</a>, an open-source high-quality neural audio codec (Figures 4 and 5).&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"501\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data-625x501.png\" alt=\"The quality of generated waveforms metrics obtained for BigVGAN v2 44 kHz and Descript Audio Codec models tested on HiFi-TTS-dev speech data.\" class=\"wp-image-88353\" style=\"width:625px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data-625x501.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data-300x240.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data-143x115.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data-768x615.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data-1536x1231.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data-645x517.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data-374x300.png 374w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data-112x90.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data-362x290.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data-137x110.png 137w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data-1024x821.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-comparison-hifi-tts-dev-speech-data.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. BigVGAN v2 44 kHz versus Descript Audio Codec results using HiFi-TTS-dev speech data</em></figcaption></figure></div>\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"252\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1-625x252.png\" alt=\"The quality of generated waveforms metrics obtained for BigVGAN v2 44 kHz and Descript Audio Codec (DAC) models tested on MUSDB18-HQ music data. \n\" class=\"wp-image-88355\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1-625x252.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1-300x121.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1-179x72.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1-768x309.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1-1536x619.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1-645x260.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1-500x201.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1-160x64.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1-362x146.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1-273x110.png 273w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1-1024x412.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bigvgan-descript-audio-codec-musdb18-hq-music-data-1.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. BigVGAN v2 44 kHz versus Descript Audio Codec results using MUSDB18-HQ music data</em></figcaption></figure>\n\n\n\n<p>All results show the quality of generated waveforms with respect to the following metrics:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://en.wikipedia.org/wiki/Perceptual_Evaluation_of_Speech_Quality\">Perceptual Evaluation of Speech Quality</a> (PESQ)&nbsp;</li>\n\n\n\n<li><a href=\"https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-015-0054-9\">Virtual Speech Quality Objective Listener</a> (ViSQOL)&nbsp;</li>\n\n\n\n<li><a href=\"https://arxiv.org/abs/1910.11480\">Multi-Resolution Short-Time Fourier Transform</a> (M-STFT)&nbsp;</li>\n\n\n\n<li><a href=\"https://arxiv.org/abs/2110.10139\">Periodicity Root Mean Square Error</a> (Periodicity)&nbsp;</li>\n\n\n\n<li><a href=\"https://arxiv.org/abs/2110.10139\">Voice/Unvoiced F1 Score</a> (V/UV F1)&nbsp;</li>\n</ul>\n\n\n\n<h2 id=\"conclusion\u00a0\"  class=\"wp-block-heading\">Conclusion\u00a0<a href=\"#conclusion\u00a0\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA is committed to delivering the best audio generative AI accessible to all. The release of BigVGAN v2 pushes neural vocoder technology and audio quality to new heights, even reaching the limits of human auditory perception.&nbsp;</p>\n\n\n\n<p>BigVGAN v2 sets a new standard in audio synthesis, delivering state-of-the-art quality across all audio types, covering the full range of human hearing. Its synthesis is now up to 3x faster than the original BigVGAN, ensuring efficient processing for diverse audio configurations.&nbsp;</p>\n\n\n\n<p>Before diving into <a href=\"https://github.com/NVIDIA/BigVGAN\">BigVGAN v2</a>, we encourage users to review the <a href=\"https://github.com/NVIDIA/BigVGAN/blob/main/nv-modelcard%2B%2B/overview.md\">model card</a> for a seamless experience.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Stunning audio content is an essential component of virtual worlds. Audio generative AI plays a key role in creating this content, and NVIDIA is continuously pushing the limits in this field of research. BigVGAN, developed in collaboration with the NVIDIA Applied Deep Learning Research \u00a0and NVIDIA NeMo teams, is a generative AI model specialized in &hellip; <a href=\"https://developer.nvidia.com/blog/achieving-state-of-the-art-zero-shot-waveform-audio-generation-across-audio-types/\">Continued</a></p>\n", "protected": false}, "author": 2260, "featured_media": 88335, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1481591", "discourse_permalink": "https://forums.developer.nvidia.com/t/achieving-state-of-the-art-zero-shot-waveform-audio-generation-across-audio-types/305863", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [1078, 453, 1962, 3166], "coauthors": [4002, 1398], "class_list": ["post-88329", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "tag-speech-synthesis", "tag-featured", "tag-nvidia-research", "tag-speech-ai"], "acf": {"post_industry": ["General"], "post_products": ["A100", "NeMo"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ai-voice-graphic-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mYF", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Conversational AI", "link": "https://developer.nvidia.com/blog/category/conversational-ai/", "id": 1050}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88329"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2260"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88329"}], "version-history": [{"count": 39, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88329/revisions"}], "predecessor-version": [{"id": 88653, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88329/revisions/88653"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88335"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88329"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88329"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88329"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88329"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88127, "date": "2024-09-05T11:30:00", "date_gmt": "2024-09-05T18:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88127"}, "modified": "2024-11-05T18:27:13", "modified_gmt": "2024-11-06T02:27:13", "slug": "low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch/", "title": {"rendered": "Low Latency Inference Chapter 1: Up to 1.9x Higher Llama 3.1 Performance with Medusa on NVIDIA HGX H200 with NVLink Switch"}, "content": {"rendered": "\n<p>As large language models (LLMs) continue to grow in size and complexity, multi-GPU compute is a must-have to deliver the low latency and high throughput that real-time generative AI applications demand.&nbsp;</p>\n\n\n\n<p>Performance depends both on the ability for the combined GPUs to process requests as \u201cone mighty GPU\u201d with ultra-fast GPU-to-GPU communication and advanced software able to take full advantage of the multiple GPUs. By splitting the calculations of each model layer across the available GPUs using a technique called tensor parallelism in tandem with advanced algorithms like speculative decoding, token generation latency can be reduced, delivering an interactive user experience.&nbsp;</p>\n\n\n\n<p>For very low latency Llama 3.1 serving, cloud services can use a full NVIDIA HGX H200 server, each incorporating eight H200 Tensor Core GPUs and four all-to-all NVLink Switch chips. Each GPU within the server can communicate at the full 900 GB/s bandwidth to any other GPU via NVLink Switch. High GPU-to-GPU fabric bandwidth is required to keep multi-GPU communication from becoming the bottleneck in interactive use cases.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"2400\" height=\"1800\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy.jpg\" alt=\"A photograph of an HGX H200 baseboard with the four NVSwitch \" class=\"wp-image-88130\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy.jpg 2400w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy-300x225.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy-625x469.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy-153x115.jpg 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy-768x576.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy-1536x1152.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy-2048x1536.jpg 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy-645x484.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy-400x300.jpg 400w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy-120x90.jpg 120w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy-362x272.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy-147x110.jpg 147w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-product-photo-close-up-copy-1024x768.jpg 1024w\" sizes=\"(max-width: 2400px) 100vw, 2400px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. An NVIDIA HGX H200 with four NVLink Switch chips.&nbsp;</em></figcaption></figure></div>\n\n\n<p>To efficiently implement optimization algorithms on NVIDIA H200 HGX systems, NVIDIA TensorRT-LLM is used. TensorRT-LLM is an open-source TensorRT library that delivers state-of-the-art inference performance on the latest LLMs using a variety of techniques, including tensor parallelism and speculative decoding.</p>\n\n\n\n<p>Upcoming TensorRT-LLM optimizations, including the improvement of a speculative decoding algorithm called Medusa, provide outstanding low latency performance on Llama 3.1 70B and Llama 3.1 405B of <strong>268</strong> tokens/second/user and <strong>108</strong> tokens/second/user, respectively on HGX H200.</p>\n\n\n\n<h2 id=\"medusa_boosts_token_generation_by_up_to_19x_on_nvidia_hgx_h200\"  class=\"wp-block-heading\">Medusa boosts token generation by up to 1.9x on NVIDIA HGX H200<a href=\"#medusa_boosts_token_generation_by_up_to_19x_on_nvidia_hgx_h200\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Transformer-based LLMs are auto-regressive, meaning that tokens need to be generated sequentially, limiting throughput per generation step to just one token. Typically, during LLM inference, the rate at which a single token is generated depends on how quickly model weights are loaded into memory. This means that the workload can leave the substantial Tensor Core capabilities of H200 GPUs underutilized.&nbsp;</p>\n\n\n\n<p>Speculative decoding is a technique that increases token generation throughput per token generation step by using a &#8220;draft model&#8221; to try to predict multiple subsequent tokens beyond the next token. The target LLM then &#8220;batches\u201d the prediction candidates and validates them in parallel with the next token, making more effective use of available parallel GPU compute resources. If any candidate sequence is accepted by the original LLM, multiple tokens are generated in the generation step and therefore accelerate token generation.&nbsp;</p>\n\n\n\n<p>Medusa, described in this <a href=\"https://arxiv.org/pdf/2401.10774\" target=\"_blank\" rel=\"noreferrer noopener\">paper</a>, is a speculative decoding algorithm that uses the original model as the draft model, avoiding the system complexity and distribution discrepancy of using a separate draft model. This technique employs additional decoding \u201cheads\u201d, called Medusa heads, to predict candidate tokens beyond the next token. Each Medusa head generates a distribution of tokens beyond the previous. Then a tree-based attention mechanism samples some candidate sequences for the original model to validate. The number of parallel candidate sequences is called the draft length and the average number of tokens accepted per generation step is the acceptance rate. A greater acceptance rate increases overall token generation throughput.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1131\" height=\"713\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Figure-2.-NVIDIA-HGX-H200-with-NVLink-Switch-performance-on-Llama-3.1-models-with-and-without-Medusa.png\" alt=\"A bar chart showing HGX H200 Llama 3.1 70B performance on the left, 184 tokens/second/user without Medusa and 268 tokens/second/user with Medusa. On the right is Llama 3.1 405B performance showing 56 tokens/second/user without Medusa and 108 tokens/second/user with Medusa.\" class=\"wp-image-88139\" style=\"width:840px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Figure-2.-NVIDIA-HGX-H200-with-NVLink-Switch-performance-on-Llama-3.1-models-with-and-without-Medusa.png 1131w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Figure-2.-NVIDIA-HGX-H200-with-NVLink-Switch-performance-on-Llama-3.1-models-with-and-without-Medusa-300x189.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Figure-2.-NVIDIA-HGX-H200-with-NVLink-Switch-performance-on-Llama-3.1-models-with-and-without-Medusa-625x394.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Figure-2.-NVIDIA-HGX-H200-with-NVLink-Switch-performance-on-Llama-3.1-models-with-and-without-Medusa-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Figure-2.-NVIDIA-HGX-H200-with-NVLink-Switch-performance-on-Llama-3.1-models-with-and-without-Medusa-768x484.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Figure-2.-NVIDIA-HGX-H200-with-NVLink-Switch-performance-on-Llama-3.1-models-with-and-without-Medusa-645x407.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Figure-2.-NVIDIA-HGX-H200-with-NVLink-Switch-performance-on-Llama-3.1-models-with-and-without-Medusa-476x300.png 476w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Figure-2.-NVIDIA-HGX-H200-with-NVLink-Switch-performance-on-Llama-3.1-models-with-and-without-Medusa-143x90.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Figure-2.-NVIDIA-HGX-H200-with-NVLink-Switch-performance-on-Llama-3.1-models-with-and-without-Medusa-362x228.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Figure-2.-NVIDIA-HGX-H200-with-NVLink-Switch-performance-on-Llama-3.1-models-with-and-without-Medusa-174x110.png 174w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Figure-2.-NVIDIA-HGX-H200-with-NVLink-Switch-performance-on-Llama-3.1-models-with-and-without-Medusa-1024x646.png 1024w\" sizes=\"(max-width: 1131px) 100vw, 1131px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. NVIDIA HGX H200 with NVLink Switch performance on Llama 3.1 models with and without Medusa.</em><br><br><em>Additional Footnote: MTBench Coding dataset, single turn, accuracy bitwise identical to base model, internal build of TensorRT-LLM, HGX H200 TP8, FP8, BS=1</em></figcaption></figure></div>\n\n\n<p>With Medusa, an HGX H200 is able to produce 268 tokens per second per user for Llama 3.1 70B and 108 for Llama 3.1 405B. This is over 1.5x<strong> </strong>faster on Llama 3.1 70B and over 1.9x faster on Llama 3.1 405B than without Medusa. Although there is variability in the Medusa acceptance rate between tasks depending on how the heads are fine-tuned, its overall performance is generalized across a wide range of tasks.</p>\n\n\n\n<p>Medusa heads for both Llama 3.1 70B and Llama 3.1 405B were trained using the NVIDIA TensorRT Model Optimizer integration with NVIDIA NeMo framework. The Medusa head training used a frozen backbone, ensuring that use of Medusa yields identical accuracy to the base model.</p>\n\n\n\n<h2 id=\"nvidia_full-stack_innovation_never_stops\"  class=\"wp-block-heading\">NVIDIA full-stack innovation never stops<a href=\"#nvidia_full-stack_innovation_never_stops\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA HGX H200 with NVLink Switch and TensorRT-LLM already delivers excellent real-time inference performance on popular and most demanding community models. To continue improving user experiences and reduce inference cost, we relentlessly innovate across every layer of the technology stack \u2013 chips, systems, software libraries, algorithms, and more.&nbsp;</p>\n\n\n\n<p>We look forward to sharing future updates on our low latency inference performance as both our platform and the LLM ecosystem advances.&nbsp;</p>\n\n\n\n<p><em>This blog is part of a series &#8211; view <a href=\"https://developer.nvidia.com/blog/low-latency-inference-chapter-2-blackwell-is-coming-nvidia-gh200-nvl32-with-nvlink-switch-gives-signs-of-big-leap-in-time-to-first-token-performance/\">Low Latency Inference Chapter 2: Blackwell is Coming. NVIDIA GH200 NVL32 with NVLink Switch Gives Signs of Big Leap in Time to First Token Performance</a>.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>As large language models (LLMs) continue to grow in size and complexity, multi-GPU compute is a must-have to deliver the low latency and high throughput that real-time generative AI applications demand.&nbsp; Performance depends both on the ability for the combined GPUs to process requests as \u201cone mighty GPU\u201d with ultra-fast GPU-to-GPU communication and advanced software &hellip; <a href=\"https://developer.nvidia.com/blog/low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch/\">Continued</a></p>\n", "protected": false}, "author": 1355, "featured_media": 88129, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1476590", "discourse_permalink": "https://forums.developer.nvidia.com/t/low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch/304917", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1903], "tags": [296, 453, 4159, 3933, 2932], "coauthors": [2732, 3849, 506, 2940], "class_list": ["post-88127", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "category-features", "tag-ai-inference-microservices", "tag-featured", "tag-inference-performance", "tag-llama", "tag-large-language-models"], "acf": {"post_industry": ["General"], "post_products": ["AI Foundation Models", "HGX", "NeMo", "NVLink", "TensorRT", "TensorRT-LLM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/HGX-H200-tech-blog-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mVp", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88127"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1355"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88127"}], "version-history": [{"count": 10, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88127/revisions"}], "predecessor-version": [{"id": 89504, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88127/revisions/89504"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88129"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88127"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88127"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88127"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88127"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88574, "date": "2024-09-05T10:27:27", "date_gmt": "2024-09-05T17:27:27", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88574"}, "modified": "2024-10-21T09:26:32", "modified_gmt": "2024-10-21T16:26:32", "slug": "ai-powered-platform-advances-personalized-cancer-diagnostics-and-treatments", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/ai-powered-platform-advances-personalized-cancer-diagnostics-and-treatments/", "title": {"rendered": "AI-Powered Platform Advances Personalized Cancer Diagnostics and Treatments"}, "content": {"rendered": "\n<p>A recent <a href=\"https://www.cell.com/cell-reports-medicine/fulltext/S2666-3791(24)00418-X\">study</a> introduced a cutting-edge AI-powered pathology platform that can help doctors diagnose and evaluate lung cancer in patients quickly and accurately. Developed by a team of researchers at the University of Cologne\u2019s Faculty of Medicine and University Hospital Cologne, the tool provides fully automated and in-depth analysis of benign and cancerous tissues, for faster and more personalized treatment.&nbsp;</p>\n\n\n\n<p>Lung cancer is known for high mortality rates, but precise diagnostics and personalized treatments lead to better outcomes for patients. Traditionally, oncologists manually examine tissue samples under a microscope to identify cellular and structural characteristics that reveal cancer. However, even with expert analysis, the process is time-consuming, subjective, and prone to variability, which can lead to misdiagnosis.&nbsp;</p>\n\n\n\n<p>The researchers developed a deep-learning-based multi-class tissue segmentation platform that automatically analyzes digitized lung tissue samples. It screens for cancer and provides cellular details of the region.&nbsp;</p>\n\n\n\n<p>The AI model was trained and validated on a large dataset from six institutions, totaling&nbsp;4,097 annotated slides from 1,527 patients.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"2880\" height=\"1456\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training.jpg\" alt=\"An illustration showing the digitized slides samples of the different classes the AI tool can identify. \" class=\"wp-image-88575\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training.jpg 2880w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training-300x152.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training-625x316.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training-179x90.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training-768x388.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training-1536x777.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training-2048x1035.jpg 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training-645x326.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training-500x253.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training-160x81.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training-362x183.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training-218x110.jpg 218w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/cancer-AI-training-1024x518.jpg 1024w\" sizes=\"(max-width: 2880px) 100vw, 2880px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The 11 types of tissue classes the AI algorithm can identify</em></figcaption></figure></div>\n\n\n<p>According to study senior author Yuri Tolkach, \u201cThe algorithm can differentiate between 11 tissue types, ranging from tumor tissue, tumor-associated classes (e.g., tumor stroma, necrotic debris, mucin) to cartilage and lymphatic tissue. It showed very high pixel-wise accuracy for segmentation of different classes with an average Dice Score 0.893.\u201d</p>\n\n\n\n<p>The researchers used the University of Cologne\u2019s high-performance computing cluster equipped with 12 <a href=\"https://www.nvidia.com/en-us/data-center/v100/\">NVIDIA V100 GPUs</a>, four <a href=\"https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf\">NVIDIA A100 GPUs</a> on the pathology institute\u2019s AI server, and PC stations equipped with <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3090-3090ti/\">NVIDIA GeForce RTX 3090</a> and <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4090/\">NVIDIA RTX 4090</a> GPUs.&nbsp;</p>\n\n\n\n<p>The setup enables quick analysis of entire slide images. It takes about 1 to 5 minutes to analyze each whole-slide image ranging from 200 to 2000 Mb.&nbsp;</p>\n\n\n\n<p>\u201cThe formation of our <a href=\"https://tolklab.de/\">research group</a> and our first large <a href=\"https://www.nature.com/articles/s42256-020-0200-7\">cancer study</a> published in <em>Nature Machine Intelligence</em> was made possible through an NVIDIA Quadro P6000 GPU grant from the <a href=\"https://www.nvidia.com/en-us/industries/higher-education-research/academic-grant-program/\">NVIDIA Academic Grant Program</a>,\u201d Tolkach said.</p>\n\n\n\n<p>The AI tool can also reveal detailed characteristics of tumor and immune cells in the cellular environment. This unveils how the cancer is interacting within the body.</p>\n\n\n\n<p>Identifying subtle patterns and relationships within the tissue sample not visible to the naked eye could help inform more precise and effective treatments, and offer insight into a patient\u2019s response to a specific cancer therapy.</p>\n\n\n\n<p>The code used in this study is available on <a href=\"https://github.com/cpath-ukk/lung_cancer\">GitHub</a>.<br>Read the study <em><a href=\"https://www.cell.com/cell-reports-medicine/fulltext/S2666-3791(24)00418-X?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS266637912400418X%3Fshowall%3Dtrue\">Next-generation lung cancer pathology: Development and validation of diagnostic and prognostic algorithms</a>.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>A recent study introduced a cutting-edge AI-powered pathology platform that can help doctors diagnose and evaluate lung cancer in patients quickly and accurately. Developed by a team of researchers at the University of Cologne\u2019s Faculty of Medicine and University Hospital Cologne, the tool provides fully automated and in-depth analysis of benign and cancerous tissues, for &hellip; <a href=\"https://developer.nvidia.com/blog/ai-powered-platform-advances-personalized-cancer-diagnostics-and-treatments/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 88579, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1481514", "discourse_permalink": "https://forums.developer.nvidia.com/t/ai-powered-platform-advances-personalized-cancer-diagnostics-and-treatments/305848", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 1903], "tags": [1949, 3941, 453, 90, 4125, 1877], "coauthors": [2315], "class_list": ["post-88574", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-features", "tag-higher-education-and-academia", "tag-ai-impact", "tag-featured", "tag-medical-imaging", "tag-nvidia-academic-grant-program", "tag-research"], "acf": {"post_industry": ["Healthcare & Life Sciences", "HPC / Scientific Computing"], "post_products": ["RTX GPU", "V100"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/AI-cancer-platform.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-n2C", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88574"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88574"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88574/revisions"}], "predecessor-version": [{"id": 88654, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88574/revisions/88654"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88579"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88574"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88574"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88574"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88574"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88512, "date": "2024-09-04T12:40:27", "date_gmt": "2024-09-04T19:40:27", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88512"}, "modified": "2024-09-09T14:06:55", "modified_gmt": "2024-09-09T21:06:55", "slug": "accelerated-production-ready-graph-analytics-for-networkx-users", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerated-production-ready-graph-analytics-for-networkx-users/", "title": {"rendered": "Accelerated, Production-Ready Graph Analytics for NetworkX Users"}, "content": {"rendered": "\n<p>NetworkX is a popular, easy-to-use Python library for graph analytics. However, its performance and scalability may be unsatisfactory for medium-to-large-sized networks, which can significantly hinder user productivity.&nbsp;</p>\n\n\n\n<p>NVIDIA and <a href=\"https://arangodb.com/\">ArangoDB</a> have collectively addressed these performance and scaling issues with a solution that requires <a href=\"https://medium.com/rapids-ai/rapids-24-02-release-43ad88db0627\">zero code changes to NetworkX</a>. This solution integrates three main components:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>The NetworkX API</li>\n\n\n\n<li>Graph acceleration using <a href=\"https://github.com/rapidsai/cugraph\">RAPIDS cuGraph</a></li>\n\n\n\n<li>Production-ready analytics at scale in ArangoDB</li>\n</ul>\n\n\n\n<p>In this post, I discuss how this makes life easier for NetworkX users, show you an example implementation, and explain how to get started with early access.&nbsp;</p>\n\n\n\n<h2 id=\"easy_graph_analytics_with_networkx\"  class=\"wp-block-heading\">Easy graph analytics with NetworkX<a href=\"#easy_graph_analytics_with_networkx\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NetworkX is widely used by data scientists, students, and many others for graph analytics. It is open-source, well-documented, and supports plenty of algorithms with a simple API.&nbsp;</p>\n\n\n\n<p>That said, one known limitation is its performance for medium-to-large graphs, which significantly hampers its usefulness for production applications.&nbsp;</p>\n\n\n\n<h2 id=\"accelerating_graph_analytics_with_cugraph\"  class=\"wp-block-heading\">Accelerating graph analytics with cuGraph<a href=\"#accelerating_graph_analytics_with_cugraph\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The <a href=\"https://github.com/rapidsai/cugraph\">RAPIDS cuGraph graph analytics acceleration library</a> bridges the gap between NetworkX and GPU-based graph analytics:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Graph creation and manipulation: </strong>Create and manipulate graphs using NetworkX, with data seamlessly passed to cuGraph for accelerated processing on large graphs.</li>\n\n\n\n<li><strong>Fast graph algorithms: </strong>Real-time analytics using the power of NVIDIA GPUs.</li>\n\n\n\n<li><strong>Data interoperability: </strong>Support for data in NetworkX graph objects and other formats, enabling simple data exchange between machine learning, ETL tasks, and graph analytics.</li>\n</ul>\n\n\n\n<p>The best part? You get the benefits of GPU acceleration without changing your code. Just install the <code>nx-cugraph</code> library and specify the cuGraph backend. For more information about installation and performance benchmarks, see <a href=\"https://developer.nvidia.com/blog/accelerating-networkx-on-nvidia-gpus-for-high-performance-graph-analytics/\">Accelerating NetworkX on NVIDIA GPUs for High Performance Graph Analytics</a>.&nbsp;</p>\n\n\n\n<p>In short, for varying sizes of k from 10\u20131000, GPUs speed up a single run of betweenness centrality by 11\u2013600x.&nbsp;&nbsp;</p>\n\n\n\n<h2 id=\"production-ready_graph_analytics_with_arangodb\"  class=\"wp-block-heading\">Production-ready graph analytics with ArangoDB<a href=\"#production-ready_graph_analytics_with_arangodb\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NetworkX users have typically had to undertake a complex set of methods for persisting graph data:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Manual data exports to flat files</li>\n\n\n\n<li>Relational databases</li>\n\n\n\n<li>Ad-hoc solutions, such as using in-memory storage</li>\n</ul>\n\n\n\n<p>Each of these methods has a unique set of challenges and forces you to spend time and effort managing and manipulating graph data rather than focusing on analysis and data science tasks.</p>\n\n\n\n<p>ArangoDB\u2019s data persistence layer makes it easier for one or more users to perform graph operations on any network too large to fit in memory. By integrating ArangoDB as the persistent data layer, you will see several potential benefits:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Scalability:</strong> Graph data can scale horizontally, not just vertically, across multiple nodes, handling large datasets.</li>\n\n\n\n<li><strong>Performance: </strong>Fast read and write operations for real-time analysis and manipulation of graph data.</li>\n\n\n\n<li><strong>Flexibility: </strong>Support for all popular data models: graph, document, full-text search, key/value, and geospatial, all in a single, fully integrated platform. Multi-tenancy is also supported.</li>\n</ul>\n\n\n\n<p>Figure 1 shows how integrating ArangoDB into the workflow of NetworkX users transforms the way graph data is stored and accessed. By providing this new persistence layer, ArangoDB enables data scientists to focus on what they do best, not data manipulation and other minutia.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"670\" height=\"607\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-db-persistence-layer.png\" alt=\"Workflow diagram shows starting with a query into NetworkX that has been loaded with data using Python DataFrames and persisting data in ArangoDB.\" class=\"wp-image-88519\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-db-persistence-layer.png 670w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-db-persistence-layer-300x272.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-db-persistence-layer-625x566.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-db-persistence-layer-127x115.png 127w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-db-persistence-layer-645x584.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-db-persistence-layer-331x300.png 331w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-db-persistence-layer-99x90.png 99w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-db-persistence-layer-362x328.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-db-persistence-layer-121x110.png 121w\" sizes=\"(max-width: 670px) 100vw, 670px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. ArangoDB as the NetworkX persistence layer</em></figcaption></figure></div>\n\n\n<p>Data persistence enables users to take advantage of work done by other team members. Data does not have to be loaded from the source and compiled into a graph for each user. Instead, they can load the graph from the database.&nbsp;</p>\n\n\n\n<p>The results of graph algorithms can also be stored and retrieved rather than run again by every single user. Ultimately, this saves users time and money.&nbsp;</p>\n\n\n\n<h2 id=\"gpu-accelerated_analytics_with_cugraph_and_arangodb\"  class=\"wp-block-heading\">GPU-accelerated analytics with cuGraph and ArangoDB<a href=\"#gpu-accelerated_analytics_with_cugraph_and_arangodb\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Large datasets take a long time to analyze in NetworkX. That\u2019s why ArangoDB uses RAPIDS cuGraph to analyze graph data, especially when data grows large enough that performance slows down.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"707\" height=\"399\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-networkx-cugraph-analytics.png\" alt=\"Workflow diagram shows starting with a query into NetworkX that has been loaded with data using Python DataFrames; using cuGraph in memory on a GPU for algorithms and processing; and persisting data in ArangoDB.\" class=\"wp-image-88518\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-networkx-cugraph-analytics.png 707w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-networkx-cugraph-analytics-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-networkx-cugraph-analytics-625x353.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-networkx-cugraph-analytics-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-networkx-cugraph-analytics-645x364.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-networkx-cugraph-analytics-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-networkx-cugraph-analytics-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-networkx-cugraph-analytics-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/arangox-networkx-cugraph-analytics-195x110.png 195w\" sizes=\"(max-width: 707px) 100vw, 707px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Using ArangoDB, NetworkX, and cuGraph to analyze large-scale graphs</em></figcaption></figure></div>\n\n\n<p>There are several benefits to scaling ArangoDB with GPUs through a NetworkX interface. First, data extraction from ArangoDB is much faster with a GPU compared to a CPU. That is because ArangoDB optimizes its data extraction tools to uniquely cater to cuGraph data structures, namely the coordinate list (COO) graph format.&nbsp;</p>\n\n\n\n<p>Second, you can analyze large graph data through your laptop or another client. NetworkX acts as a client API library for graph algorithms that require more memory than the client could provide.&nbsp;</p>\n\n\n\n<p>Finally, no code changes are necessary. cuGraph supports zero code changes for NetworkX users so you can use tools that are already familiar to you.</p>\n\n\n\n<h2 id=\"example_implementation\"  class=\"wp-block-heading\">Example implementation<a href=\"#example_implementation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Thanks to the capabilities of the NetworkX backend-to-backend interface, <code>nx-arangodb</code> graphs can use the GPU capabilities of <a href=\"https://developer.nvidia.com/blog/accelerating-networkx-on-nvidia-gpus-for-high-performance-graph-analytics/\">nx-cugraph</a>, as long as an NVIDIA GPU is available on the machine. In other words, the choice to run CPU or GPU algorithms through NetworkX remains when using <code>nx-arangodb</code>.</p>\n\n\n\n<p>The following sections show how to create and persist a graph in ArangoDB using NetworkX and the <code>nx-arangodb</code> library:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Downloading the data</li>\n\n\n\n<li>Creating the NetworkX graph</li>\n\n\n\n<li>Running a cuGraph algorithm without ArangoDB</li>\n\n\n\n<li>Persisting the NetworkX graph to ArangoDB</li>\n\n\n\n<li>Instantiating the NetworkX-ArangoDB graph</li>\n\n\n\n<li>Running a cuGraph algorithm with ArangoDB</li>\n</ul>\n\n\n\n<h3 id=\"test_environment\"  class=\"wp-block-heading\">Test environment<a href=\"#test_environment\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>For this post, I used an Intel Xeon CPU with 13 GB of system RAM and compared it against an NVIDIA A100 GPU with 84 GB of system RAM and 40 GB of GPU RAM. I worked with CUDA 12.2.</p>\n\n\n\n<p>The Stanford Network Analysis Platform (SNAP) <a href=\"https://snap.stanford.edu/data/cit-Patents.html\">Citation Patents dataset</a> is a citation graph of patents granted between 1975 and 1999, totaling 3.7M nodes and 16.5M edges. The code examples rely on the betweenness centrality graph algorithm to help you find which patents are more central than others and get an idea of their relative importance.</p>\n\n\n\n<p>For this post, I used an ArangoDB instance provisioned through the <a href=\"https://arangodb.com/arangograph-managedgraphdb/\">ArangoGraph Managed Service</a>, which enabled me to persist any created graphs for future sessions. It is running as Enterprise Edition 3.11.8 as a sharded database with six nodes, each with 32 GB of memory.</p>\n\n\n\n<h3 id=\"step_0_downloading_the_data\"  class=\"wp-block-heading\">Step 0: Downloading the data<a href=\"#step_0_downloading_the_data\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>First, download the <a href=\"https://snap.stanford.edu/data/cit-Patents.html\">Citation Patents dataset</a> and write it to a text file.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Median Time: 10 seconds  \n\nimport gzip\nimport shutil\nimport requests\n\nurl = &#039;https://snap.stanford.edu/data/cit-Patents.txt.gz&#039;\nname = &#039;cit-Patents.txt&#039;\n\n# Download gz\nresponse = requests.get(url, stream=True)\nresponse.raise_for_status()\n\n# Stream gz data &amp; write to text file\nwith response.raw as r, gzip.open(r, &#039;rb&#039;) as f_in, open(name, &#039;wb&#039;) as f_out:\n    shutil.copyfileobj(f_in, f_out)\n</pre></div>\n\n\n<h3 id=\"step_1_creating_the_networkx_graph\"  class=\"wp-block-heading\">Step 1: Creating the NetworkX graph<a href=\"#step_1_creating_the_networkx_graph\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Next, instantiate the NetworkX graph using a pandas edge list.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Median Time: 90 seconds \n\nimport pandas as pd\nimport networkx as nx\n\n# Read into Pandas \npandas_edgelist = pd.read_csv(\n    &quot;cit-Patents.txt&quot;,\n    skiprows=4,\n    delimiter=&quot;\\t&quot;,\n    names=&#x5B;&quot;src&quot;, &quot;dst&quot;],\n    dtype={&quot;src&quot;: &quot;int32&quot;, &quot;dst&quot;: &quot;int32&quot;},\n)\n\n# Create NetworkX Graph from Edgelist\nG_nx = nx.from_pandas_edgelist(\n    pandas_edgelist, source=&quot;src&quot;, target=&quot;dst&quot;, create_using=nx.DiGraph\n)\n</pre></div>\n\n\n<h3 id=\"step_2_running_a_cugraph_algorithm_without_arangodb\"  class=\"wp-block-heading\">Step 2: Running a cuGraph algorithm without ArangoDB<a href=\"#step_2_running_a_cugraph_algorithm_without_arangodb\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>A NetworkX algorithm can be invoked with <code>backend</code> set to <code>cugraph</code>. This uses the GPU-accelerated algorithm implementation of <a href=\"https://developer.nvidia.com/blog/accelerating-networkx-on-nvidia-gpus-for-high-performance-graph-analytics/\">nx-cugraph</a> with zero code changes.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Median Time: 5 seconds\n\nresult = nx.betweenness_centrality(G_nx, k=10, backend=&quot;cugraph&quot;)\n</pre></div>\n\n\n<p>Alternately, set the <code>NETWORKX_AUTOMATIC_BACKENDS</code> environment variable to specify <code>cugraph</code> as the selected NetworkX backend instead of specifying the <code>backend</code> parameter.</p>\n\n\n\n<h3 id=\"step_3_persisting_the_networkx_graph_to_arangodb\"  class=\"wp-block-heading\">Step 3: Persisting the NetworkX graph to ArangoDB<a href=\"#step_3_persisting_the_networkx_graph_to_arangodb\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>At this point, you can choose to persist the local NetworkX graph into ArangoDB. Assuming that you <a href=\"https://drive.google.com/file/d/1PiDnZDh4NsW3Hyz2OMX21JvvxMnAgQMy/view?usp=sharing\">have an ArangoDB instance running</a> at the <code>DATABASE_HOST</code> provided, you can load the graph by instantiating a <code>nxadb.DiGraph</code> object, and using the <code>incoming_graph_data</code> parameter along with a specific name.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Median Time: 3 Minutes \n\nimport os\nimport nx_arangodb as nxadb\n\nos.environ&#x5B;&quot;DATABASE_HOST&quot;] = &quot;https://123.arangodb.cloud:8529&quot;\nos.environ&#x5B;&quot;DATABASE_USERNAME&quot;] = &quot;root&quot;\nos.environ&#x5B;&quot;DATABASE_PASSWORD&quot;] = &quot;password&quot;\nos.environ&#x5B;&quot;DATABASE_NAME&quot;] = &quot;myDB&quot; \n\n# Load the DiGraph into ArangoDB \nG_nxadb = nxadb.DiGraph(\n    name=&quot;cit_patents&quot;,\n    incoming_graph_data=G_nx,\n    write_batch_size=50000\n)\n</pre></div>\n\n\n<p>Now, assume that a new Python session has been created. It is up to you whether to create the new session on the same machine or a different machine. This can be useful when you are working with a teammate for collaborative development.</p>\n\n\n\n<h3 id=\"step_4_instantiating_the_networkx-arangodb_graph\"  class=\"wp-block-heading\">Step 4: Instantiating the NetworkX-ArangoDB Graph<a href=\"#step_4_instantiating_the_networkx-arangodb_graph\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Re-connecting to the persisted graph can be done by specifying the connection credentials using environment variables and re-instantiating <code>nxadb.DiGraph</code>. Optional <code>read_batch_size</code> and <code>read_parallelism</code> parameters are provided for optimizing data read.&nbsp;</p>\n\n\n\n<p>Graph instantiation does not pull the graph into memory but establishes the remote connection to the persisted graph.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Median Time: 0 seconds \n\nimport nx_arangodb as nxadb\n\nos.environ&#x5B;&quot;DATABASE_HOST&quot;] = &quot;https://123.arangodb.cloud:8529&quot;\nos.environ&#x5B;&quot;DATABASE_USERNAME&quot;] = &quot;root&quot;\nos.environ&#x5B;&quot;DATABASE_PASSWORD&quot;] = &quot;password&quot;\nos.environ&#x5B;&quot;DATABASE_NAME&quot;] = &quot;myDB&quot; \n\n# Connect to the persisted Graph in ArangoDB\n# This doesn&#039;t pull the graph; You&#039;re just establishing a remote connection.\nG_nxadb = nxadb.DiGraph(\n    name=&quot;cit_patents&quot;,\n    read_parallelism=15,\n    read_batch_size=3000000\n)\n</pre></div>\n\n\n<h3 id=\"step_5_running_a_cugraph_algorithm_with_arangodb\"  class=\"wp-block-heading\">Step 5: Running a cuGraph algorithm with ArangoDB<a href=\"#step_5_running_a_cugraph_algorithm_with_arangodb\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>With the use of a GPU, you can rely on the same algorithm to fetch the GPU representation of the ArangoDB graph, which has a significantly smaller memory footprint than that of the CPU representation. After the ArangoDB graph has been pulled, it is cached as a NetworkX-cuGraph graph, which enables you to run more algorithms without needing to pull it again unless the user specifically requests to do so.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Option 1: Explicit Graph Creation\n\nfrom nx_arangodb.convert import nxadb_to_nxcg\n\n# Pull the graph from ArangoDB and cache it\n# Median Time: 30 seconds\nG_nxcg = nxadb_to_nxcg(G_nxadb)\n\n# Median Time: 5 seconds\nresult = nx.betweenness_centrality(G_nxcg, k=10)\n</pre></div>\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# Option 2 (recommended): On-demand Graph Creation\n# This pulls the graph from ArangoDB on the first algorithm call &amp; caches it  \n\n# Median Time: 35 seconds \nresult = nx.betweenness_centrality(G_nxadb, k=10)\n</pre></div>\n\n\n<h3 id=\"verdict_data_persisted_in_arangodb\"  class=\"wp-block-heading\"><br>Verdict: Data persisted in ArangoDB<a href=\"#verdict_data_persisted_in_arangodb\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Given the new ability to persist NetworkX graphs in ArangoDB, you can load new sessions 3x faster than without having a database involved.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>Description</strong></td><td><strong>Steps</strong></td><td><strong>Time (sec)</strong></td></tr><tr><td>Without data persisted in ArangoDB</td><td>0-2</td><td>105</td></tr><tr><td>Data persisted in ArangoDB</td><td>5</td><td>35</td></tr><tr><td><strong>Speedup</strong></td><td></td><td><strong>3X</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Workflow comparison with and without ArangoDB</em></figcaption></figure>\n\n\n\n<p>Running multiple sessions on the data or requiring multiple people to analyze the same data without ArangoDB would require the inconvenience of starting from scratch. Having a persistence layer facilitates this workflow. It makes the combination of cuGraph and ArangoDB a key strategy for working with large graphs in NetworkX.</p>\n\n\n\n<h3 id=\"step_6_using_crud_functionality_with_networkx-arangodb\"  class=\"wp-block-heading\">Step 6: Using CRUD functionality with NetworkX-ArangoDB<a href=\"#step_6_using_crud_functionality_with_networkx-arangodb\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>More functionality is available with NetworkX-ArangoDB should you choose to use it for CRUD functionality. NetworkX-ArangoDB puts a strong emphasis on zero-code change, implying that the CRUD interface for NetworkX-ArangoDB Graphs is identical to that of NetworkX graphs.&nbsp;</p>\n\n\n\n<p>Persisting to ArangoDB also enables you to take advantage of ArangoDB\u2019s multi-model query language; the Arango Query Language (AQL). This is a unified query language to perform graph traversals, full-text search, document retrieval, and key-value lookups on one platform.&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nimport nx_arangodb as nxadb\n\nG_nxadb = nxadb.DiGraph(name=&quot;cit_patents&quot;) # Connect to ArangoDB \n\nassert G_nxadb.number_of_nodes() == G_nx.number_of_nodes() \nassert G_nxadb.number_of_edges() == G_nx.number_of_edges() \nassert len(G_nxadb&#x5B;5526234]) == len(G_nx&#x5B;5526234])\n\nG_nxadb.nodes&#x5B;1]&#x5B;&quot;foo&quot;] = &quot;bar&quot;\ndel G_nxadb.nodes&#x5B;1]&#x5B;&quot;foo&quot;]\n\nG_nxadb&#x5B;5526234]&#x5B;4872081]&#x5B;&quot;object&quot;] = {&quot;foo&quot;: &quot;bar&quot;}\nG_nxadb&#x5B;5526234]&#x5B;4872081]&#x5B;&quot;object&quot;]&#x5B;&quot;foo&quot;] = &quot;bar!&quot;\ndel G_nxadb&#x5B;5526234]&#x5B;4872081]&#x5B;&quot;object&quot;]\n\nG_nxadb.add_edge(&quot;A&quot;, &quot;B&quot;, bar=&quot;foo&quot;)\nG_nxadb&#x5B;&quot;A&quot;]&#x5B;&quot;B&quot;]&#x5B;&quot;bar&quot;] = &quot;foo!&quot;\ndel G_nxadb.nodes&#x5B;&quot;A&quot;]\ndel G_nxadb.nodes&#x5B;&quot;B&quot;]\n</pre></div>\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Combining the NetworkX Graph API with persistence in ArangoDB and fast processing with cuGraph gives you a production-quality workbench for building models and processes. This technical integration between ArangoDB and NVIDIA represents a major evolution in graph database analytics.&nbsp;</p>\n\n\n\n<p>By persisting graph data in ArangoDB, you will find that you can avoid the complexities and inefficiencies typical of manual data exports or using in-memory storage. To be precise, in-memory storage, while fast in some cases, is not ideal for large graphs because of memory constraints and the high risk of data loss during system crashes and other unplanned downtime.</p>\n\n\n\n<p>For NetworkX users, ArangoDB offers an ideal and easy-to-implement transparent persistence layer, transforming how graph data is stored and accessed. You can now run large-scale graph analytics without leaving the familiarity of NetworkX. Existing ArangoDB customers will also see the benefits of advanced graph analytics and accelerated performance of NetworkX backed by cuGraph.</p>\n\n\n\n<p>For more information about the full potential of this powerful integration and to get early access, see <a href=\"https://arangodb.com/introducing-the-arangodb-networkx-persistence-layer/\">Introducing The ArangoDB NetworkX Persistence Layer</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NetworkX is a popular, easy-to-use Python library for graph analytics. However, its performance and scalability may be unsatisfactory for medium-to-large-sized networks, which can significantly hinder user productivity.&nbsp; NVIDIA and ArangoDB have collectively addressed these performance and scaling issues with a solution that requires zero code changes to NetworkX. This solution integrates three main components:&nbsp; In &hellip; <a href=\"https://developer.nvidia.com/blog/accelerated-production-ready-graph-analytics-for-networkx-users/\">Continued</a></p>\n", "protected": false}, "author": 2280, "featured_media": 88513, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1480721", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerated-production-ready-graph-analytics-for-networkx-users/305706", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [453, 276], "coauthors": [4012], "class_list": ["post-88512", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "tag-featured", "tag-graph-analytics"], "acf": {"post_industry": ["Financial Services"], "post_products": ["cuGraph", "RAPIDS"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/networkx-graph-analytics-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-n1C", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88512"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2280"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88512"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88512/revisions"}], "predecessor-version": [{"id": 88736, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88512/revisions/88736"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88513"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88512"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88512"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88512"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88512"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88598, "date": "2024-09-04T10:47:42", "date_gmt": "2024-09-04T17:47:42", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88598"}, "modified": "2024-09-05T10:57:08", "modified_gmt": "2024-09-05T17:57:08", "slug": "hands-on-training-at-nvidia-ai-summit-in-washington-dc", "status": "publish", "type": "post", "link": "https://nvda.ws/3ATa9d6", "title": {"rendered": "Hands-On Training at NVIDIA AI Summit in Washington, DC"}, "content": {"rendered": "\n<p>Immerse yourself in NVIDIA technology with our full-day, hands-on technical workshops at our AI Summit in Washington D.C. on October 7, 2024.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Immerse yourself in NVIDIA technology with our full-day, hands-on technical workshops at our AI Summit in Washington D.C. on October 7, 2024.</p>\n", "protected": false}, "author": 1289, "featured_media": 88600, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1480690", "discourse_permalink": "https://forums.developer.nvidia.com/t/hands-on-training-at-nvidia-ai-summit-in-washington-dc/305697", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3ATa9d6", "_links_to_target": "_blank"}, "categories": [696, 3110], "tags": [3965, 2964, 453, 2932, 1958, 3613], "coauthors": [2631], "class_list": ["post-88598", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-generative-ai", "tag-ai-agent", "tag-dli", "tag-featured", "tag-large-language-models", "tag-news", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["Public Sector"], "post_products": ["cuDF", "NIM", "RAPIDS"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/Hands-On-Training-and-Certification-at-AI-Summit-D.C.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-n30", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88598"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1289"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88598"}], "version-history": [{"count": 1, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88598/revisions"}], "predecessor-version": [{"id": 88602, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88598/revisions/88602"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88600"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88598"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88598"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88598"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88598"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88388, "date": "2024-09-04T08:00:00", "date_gmt": "2024-09-04T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88388"}, "modified": "2024-09-05T10:57:08", "modified_gmt": "2024-09-05T17:57:08", "slug": "nvidia-deep-learning-institute-releases-new-generative-ai-teaching-kit", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-deep-learning-institute-releases-new-generative-ai-teaching-kit/", "title": {"rendered": "NVIDIA Deep Learning Institute Releases New Generative AI Teaching Kit"}, "content": {"rendered": "\n<p>Generative AI, powered by advanced machine learning models and deep neural networks, is revolutionizing industries by generating novel content and driving innovation in fields like healthcare, finance, and entertainment.&nbsp;</p>\n\n\n\n<p>NVIDIA is leading this transformation with its cutting-edge GPU architectures and software ecosystems, such as the <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">H100 Tensor Core</a> GPU and CUDA platform, which optimize the development and deployment of generative models. <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a> enhances the efficiency and scalability of AI inference tasks, enabling rapid deployment and iteration across various computing environments and accelerating advancements in generative AI applications.</p>\n\n\n\n<h2 id=\"the_importance_of_generative_ai_education\"  class=\"wp-block-heading\">The importance of generative AI education<a href=\"#the_importance_of_generative_ai_education\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>As generative AI models, such as GANs and transformers, become increasingly sophisticated, there is a growing demand for skilled professionals who can develop, refine, and ethically deploy these technologies. A strong educational foundation in generative AI equips students with the practical skills and theoretical knowledge needed to innovate in areas like content creation, drug discovery, and autonomous systems.&nbsp;</p>\n\n\n\n<p>College and university education in generative AI is crucial due to the rapidly expanding role of AI in almost every industry. By integrating generative AI into their curriculum, universities prepare the next generation of AI researchers, engineers, and thought leaders to advance the field and address the complex challenges associated with AI-driven innovation.&nbsp;</p>\n\n\n\n<p>The new Generative AI Teaching Kit, a collaboration between the NVIDIA Deep Learning Institute (DLI) and Dartmouth College, is set to empower the next generation of professionals with the skills and knowledge needed in this rapidly evolving field.&nbsp;</p>\n\n\n\n<p>This comprehensive teaching resource enables educators to provide students access to cutting-edge tools, frameworks, and practical exercises that are crucial for understanding the complexities of Generative AI and large language model development and deployment. By equipping students with a deep understanding of generative AI techniques, the Teaching Kit enables educators to foster future innovation and creativity in AI-driven industries.&nbsp;</p>\n\n\n\n<p>As students transition into the workforce, they will be better prepared to tackle global challenges, from improving healthcare and science to advancing sustainable technologies.</p>\n\n\n\n<p>Sam Raymond, adjunct assistant professor of engineering at Dartmouth College, was instrumental in developing the content. \u201cEmpowering students with skills to understand and potentially develop their own GPU-accelerated Generative AI applications is the primary objective,\u201d said Raymond. \u201cI believe students who go through this course will be at a significant advantage in the job market and help bridge the knowledge gap in industries today.\u201d</p>\n\n\n\n<h2 id=\"overview_of_the_generative_ai_teaching_kit&nbsp;\"  class=\"wp-block-heading\">Overview of the Generative AI Teaching Kit&nbsp;<a href=\"#overview_of_the_generative_ai_teaching_kit&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>All Teaching Kits include lecture slides, hands-on labs, Jupyter notebooks, knowledge checks, and free online self-paced courses that provide certificates of competency for students, all comprehensively packaged up and ready for classroom and curriculum integration.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1200\" height=\"628\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/genai-social-rag-ai-nims-screen-1200x628-1.jpg\" alt=\"Photo of a student looking at a laptop, which has the PDF Retrieval NIM Agent Blueprint on the screen, with the message, &quot;Loaded Successfully.&quot;. \" class=\"wp-image-88562\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/genai-social-rag-ai-nims-screen-1200x628-1.jpg 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/genai-social-rag-ai-nims-screen-1200x628-1-300x157.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/genai-social-rag-ai-nims-screen-1200x628-1-625x327.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/genai-social-rag-ai-nims-screen-1200x628-1-179x94.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/genai-social-rag-ai-nims-screen-1200x628-1-768x402.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/genai-social-rag-ai-nims-screen-1200x628-1-645x338.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/genai-social-rag-ai-nims-screen-1200x628-1-500x262.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/genai-social-rag-ai-nims-screen-1200x628-1-160x84.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/genai-social-rag-ai-nims-screen-1200x628-1-362x189.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/genai-social-rag-ai-nims-screen-1200x628-1-210x110.jpg 210w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/genai-social-rag-ai-nims-screen-1200x628-1-1024x536.jpg 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The Generative AI Teaching Kit includes free access to all DLI online self-paced courses, offering certificates for students</em>.</figcaption></figure></div>\n\n\n<p>The aim of the Generative AI Teaching Kit is to introduce the foundational concepts of natural language processing (NLP) that are essential for understanding LLMs and generative AI more broadly. Key concepts of LLMs are then examined using NVIDIA GPUs, tools, and services, as well as open-source libraries and frameworks. A simple pretraining exercise of a GPT model shows basic training processes in the cloud.&nbsp;</p>\n\n\n\n<p>The kit also covers diffusion models to explore the application of generative AI in image and video generation. Multi-modal LLM architectures are then introduced, with a focus on optimizing various LLM architectures during fine-tuning using the NVIDIA NeMo framework. Advancements in inference and the refinement of tools like chatbots are also discussed, using NVIDIA NIM, NeMo Guardrails, TensorRT, and TensorRT-LLM to enhance efficiency and scalability in production environments.</p>\n\n\n\n<p>The Generative AI Teaching Kit contains focused modules that combine theory, algorithms, programming, and examples. This first release includes the following modules:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Introduction to Generative AI</li>\n\n\n\n<li>Diffusion Models in Generative AI</li>\n\n\n\n<li>LLM Orchestration</li>\n</ul>\n\n\n\n<p>More modules will be available in future releases of the kit.</p>\n\n\n\n<p>This content is valuable for educators across various fields, especially in computer science and engineering. Its modular design enables instructors to tailor the course to meet the specific needs of their students and create a customized learning experience. Select professors from around the world have already been given early access to first-release modules.<br><br>\u201cI\u2019m eager to integrate the Generative AI Teaching Kit in my AI in Materials Engineering class,\u201d said Mohadeseh Taheri-Mousavi, assistant professor in the Materials Science and Engineering department at Carnegie Mellon University. \u201cThe comprehensive lecture notes with well-structured coding labs with examples from various fields, and associated online courses with certificates, will provide my students with the cutting-edge resources to deeply understand the broad applications of generative AI techniques in various fields.\u201d</p>\n\n\n\n<p>Professor Payam Barnaghi from the Department of Brain Sciences at Imperial College London uses LLMs and generative AI in his research using electronic health records and healthcare data. \u201cNVIDIA Generative AI Teaching Kit content is a wonderful resource for students learning the latest developments in AI and machine learning,\u201d said Barnaghi. \u201cAs a result of having early access to the first modules, I plan to use this content as the basis for teaching advanced topics in my machine learning for neuroscience courses.\u201d</p>\n\n\n\n<p>Given the fast-paced advancements in generative AI, educators can expect the teaching materials to be updated over time. NVIDIA is dedicated to offering high-end educational resources and welcomes feedback to continuously improve the content.</p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Educators can access the first release of the Generative AI Teaching Kit and other kits for free by joining the<a href=\"https://developer.nvidia.com/teaching-kits\"> NVIDIA DLI Teaching Kit Program</a>.</p>\n\n\n\n<h3 id=\"about_the_nvidia_deep_learning_institute\"  class=\"wp-block-heading\">About the NVIDIA Deep Learning Institute<a href=\"#about_the_nvidia_deep_learning_institute\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The <a href=\"https://learn.nvidia.com/\">NVIDIA Deep Learning Institute</a> (DLI) offers resources for diverse learning needs, from learning materials to self-paced and live training to educator programs. Individuals, teams, organizations, educators, and students can now find everything they need to advance their knowledge in AI, accelerated computing, accelerated data science, graphics, simulation, and more.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Generative AI, powered by advanced machine learning models and deep neural networks, is revolutionizing industries by generating novel content and driving innovation in fields like healthcare, finance, and entertainment.&nbsp; NVIDIA is leading this transformation with its cutting-edge GPU architectures and software ecosystems, such as the H100 Tensor Core GPU and CUDA platform, which optimize the &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-deep-learning-institute-releases-new-generative-ai-teaching-kit/\">Continued</a></p>\n", "protected": false}, "author": 1121, "featured_media": 88511, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1480588", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-deep-learning-institute-releases-new-generative-ai-teaching-kit/305676", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [2964, 453], "coauthors": [2345], "class_list": ["post-88388", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-dli", "tag-featured"], "acf": {"post_industry": ["Academia / Education"], "post_products": ["General"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/her-genai-teaching-kit-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mZC", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88388"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1121"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88388"}], "version-history": [{"count": 7, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88388/revisions"}], "predecessor-version": [{"id": 88573, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88388/revisions/88573"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88511"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88388"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88388"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88388"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88388"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87932, "date": "2024-09-03T09:00:00", "date_gmt": "2024-09-03T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87932"}, "modified": "2024-09-05T10:57:09", "modified_gmt": "2024-09-05T17:57:09", "slug": "real-time-neural-receivers-drive-ai-ran-innovation", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/real-time-neural-receivers-drive-ai-ran-innovation/", "title": {"rendered": "Real-Time Neural Receivers Drive AI-RAN Innovation"}, "content": {"rendered": "\n<p>Today\u2019s 5G New Radio (5G NR) wireless communication systems rely on highly optimized signal processing algorithms to reconstruct transmitted messages from noisy channel observations in mere microseconds. This remarkable achievement is the result of decades of relentless effort by telecommunications engineers and researchers, who have continuously improved signal processing algorithms to meet the demanding real-time constraints of wireless communications.</p>\n\n\n\n<p>Initially, some algorithms were largely forgotten due to their prohibitive complexity at the time of discovery. The low-density parity-check (LDPC) codes discovered by Gallager in the 1960s are a notable example. Once rediscovered by David MacKay in the 1990s, they have now become the backbone of 5G NR. This case illustrates that even the best algorithms are impracticable unless they meet the stringent computational and latency requirements of telecommunications.</p>\n\n\n\n<p>AI for wireless communications has received a lot of attention from researchers in academia and industry, as discussed in <a href=\"https://arxiv.org/pdf/1702.00832\">An Introduction to Deep Learning for the Physical Layer</a> and <a href=\"https://arxiv.org/pdf/2308.05315\">An Overview of the 3GPP Study on Artificial Intelligence for 5G New Radio</a>. It is increasingly acknowledged that it has the potential to offer superior reliability and accuracy when compared to many of the traditional physical layer algorithms. This inspires the concept of an <a href=\"https://developer.nvidia.com/blog/boosting-ai-driven-innovation-in-6g-with-the-ai-ran-alliance-3gpp-and-o-ran/\">AI radio access network (AI-RAN)</a>. So far, most studies are simulation-based and only little is known about the implications of real-time inference latency to the proposed solutions.&nbsp;</p>\n\n\n\n<p>The latency and throughput requirements of wireless communication systems impose strict constraints on the neural network (NN) design, effectively limiting their size and depth. It is thus an open and interesting challenge to deploy and validate AI components in the physical layer of an actual cellular system under realistic latency restrictions.</p>\n\n\n\n<p>This post discusses the opportunities and challenges associated with deploying NN-based receiver components in the physical layer of the future AI-RAN. We present an optimized neural network architecture and the necessary toolchain to enable real-time inference. Additionally, we discuss the potential for site-specific training and the concept of pilotless communications through end-to-end learning, offering insights into possible research directions for 6G.</p>\n\n\n\n<h2 id=\"nvidia_opens_its_research_lab&nbsp;\"  class=\"wp-block-heading\">NVIDIA opens its research lab&nbsp;<a href=\"#nvidia_opens_its_research_lab&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA has developed a research prototype of a neural network-based wireless receiver that replaces parts of the physical layer signal processing by learned components. Special emphasis has been placed on the ability of the neural network architecture to perform real-time inference. For details, see <a href=\"https://arxiv.org/pdf/2312.02601\">A Neural Receiver for 5G NR Multi-user MIMO</a>.</p>\n\n\n\n<p>To empower AI-RAN researchers and engineers, NVIDIA has released the <a href=\"https://github.com/NVlabs/neural_rx\">research code</a>, which provides the entire toolchain required to design, train, and evaluate <a href=\"https://developer.nvidia.com/blog/towards-environment-specific-base-stations-ai-ml-driven-neural-5g-nr-multi-user-mimo-receiver/\">NN-based receivers</a>. Real-time inference is enabled through <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT</a> on GPU-accelerated hardware platforms. As such, NVIDIA offers the unique software and hardware stack for a seamless transition from conceptual prototyping in <a href=\"https://developer.nvidia.com/sionna\">NVIDIA Sionna</a> through early field evaluations using TensorRT up to commercial-grade deployment in <a href=\"https://developer.nvidia.com/aerial\">NVIDIA Aerial.</a></p>\n\n\n\n<p>Parts of the project have already been showcased, including <a href=\"https://www.youtube.com/watch?v=BQyxBYzdg5k\">hardware-in-the-loop verification of neural receivers</a>, <a href=\"https://www.keysight.com/us/en/assets/3124-1306/demos/6G-AI-Neural-Receiver-Design.mp4?utm_source=linkedin&amp;utm_medium=organic_social&amp;utm_campaign=bb680c94-070b-4cb0-9827-592690c5d42a\">site-specific training</a>, and <a href=\"https://www.rohde-schwarz.com/us/knowledge-center/videos/towards-6g-ai-native-interface-neural-receiver-video-detailpage_251220-1476827.html\">end-to-end learning</a>.</p>\n\n\n\n<h2 id=\"from_handcrafted_signal_processing_blocks_to_neural_receivers\"  class=\"wp-block-heading\">From handcrafted signal processing blocks to neural receivers<a href=\"#from_handcrafted_signal_processing_blocks_to_neural_receivers\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Neural receivers (NRX) are based on the idea of training a single NN to jointly perform channel estimation, equalization, and demapping (Figure 1). The NN is trained to estimate the transmitted bits from the channel observations and can be used as a drop-in replacement<em> </em>for existing signal processing algorithms. For more details and performance evaluations of the NRX concept, see <a href=\"https://developer.nvidia.com/blog/towards-environment-specific-base-stations-ai-ml-driven-neural-5g-nr-multi-user-mimo-receiver/\">Towards Environment-Specific Base Stations: AI/ML-driven Neural 5G NR Multi-user MIMO Receiver</a>.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1069\" height=\"547\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-classical-receiver-diagram.png\" alt=\"A neural receiver replaces channel estimation, equalization and demapping by a single neural network. \n\" class=\"wp-image-87933\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-classical-receiver-diagram.png 1069w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-classical-receiver-diagram-300x154.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-classical-receiver-diagram-625x320.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-classical-receiver-diagram-179x92.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-classical-receiver-diagram-768x393.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-classical-receiver-diagram-645x330.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-classical-receiver-diagram-500x256.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-classical-receiver-diagram-160x82.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-classical-receiver-diagram-362x185.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-classical-receiver-diagram-215x110.png 215w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-classical-receiver-diagram-1024x524.png 1024w\" sizes=\"(max-width: 1069px) 100vw, 1069px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Sending and receiving bits of information: classical receiver and neural receiver</em></figcaption></figure>\n\n\n\n<p>From an algorithmic point of view, the NRX is primarily defined by tensor operations, including matrix multiplications and convolutions. As with many AI applications, these operations can be significantly accelerated using NVIDIA hardware. Further, the extensive NVIDIA ecosystem of profiling and optimization tools enables refining the NRX architecture, effectively eliminating performance bottlenecks. The resulting NRX architecture achieves an inference latency of less than 1 ms on an NVIDIA A100 GPU using the NVIDIA TensorRT inference library.</p>\n\n\n\n<h2 id=\"5g_nr_standard_compliance_and_reconfiguration\"  class=\"wp-block-heading\"><strong>5G NR standard compliance and reconfiguration</strong><a href=\"#5g_nr_standard_compliance_and_reconfiguration\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Although the NRX concept is rather simple, its integration in the 5G NR standard comes with several engineering challenges that need to be addressed (Figure 2). As the network configuration in a practical setup may change dynamically within milliseconds, the proposed NRX architecture is adaptive and capable of supporting different modulation and coding schemes (MCS) without the need for any re-training and without introducing any additional inference complexity.&nbsp;</p>\n\n\n\n<p>Furthermore, arbitrary numbers of sub-carriers are supported and multi-user MIMO with a <a href=\"https://arxiv.org/pdf/2312.02601\">varying number of active users is possible</a>. Another important aspect for practical deployment is the capability to deal with 5G NR compliant reference signals.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1197\" height=\"593\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-architecture-key-capabilities.png\" alt=\"The neural receiver architecture is 5G NR compliant and supports dynamic MCS re-configuration, adaptive multi-user MIMO and flexible PRB allocations without any retraining.  Site-specific finetuning allows one to further optimize the performance after deployment.\n\" class=\"wp-image-87935\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-architecture-key-capabilities.png 1197w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-architecture-key-capabilities-300x149.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-architecture-key-capabilities-625x310.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-architecture-key-capabilities-179x89.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-architecture-key-capabilities-768x380.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-architecture-key-capabilities-645x320.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-architecture-key-capabilities-500x248.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-architecture-key-capabilities-160x79.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-architecture-key-capabilities-362x179.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-architecture-key-capabilities-222x110.png 222w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/neural-receiver-architecture-key-capabilities-1024x507.png 1024w\" sizes=\"(max-width: 1197px) 100vw, 1197px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Key capabilities of the neural receiver architecture</em></figcaption></figure>\n\n\n\n<p>To maintain the resilience of the NRX under unseen channel conditions, the training is conducted in the <a href=\"https://nvlabs.github.io/sionna/api/channel.wireless.html#urban-microcell-umi\">urban microcell (UMi)</a> scenario from 3GPP 38.901 using randomized macro-parameters such as the signal-to-noise ratio (SNR), Doppler spreads, and the number of active users. This allows for pre-training a robust and universal NRX that generalizes to a huge variety of radio environments.</p>\n\n\n\n<p>As the NRX is software-defined, <a href=\"https://developer.nvidia.com/blog/towards-environment-specific-base-stations-ai-ml-driven-neural-5g-nr-multi-user-mimo-receiver/\">site-specific fine-tuning</a> unlocks continuous improvements of the receiver even after deployment. A subsequent section of this post provides a detailed fine-tuning example using simulation outcomes based on ray tracing the radio environment, known as a digital twin. For more technical details, see the <a href=\"https://github.com/NVlabs/neural_rx/blob/main/notebooks/jumpstart_tutorial.ipynb\">jumpstart tutorial</a> and the <a href=\"https://github.com/NVlabs/neural_rx/blob/main/notebooks/nrx_architecture.ipynb\">neural receiver architecture overview notebook</a>.</p>\n\n\n\n<h2 id=\"performance_evaluation_under_real-time_constraints\"  class=\"wp-block-heading\">Performance evaluation under real-time constraints<a href=\"#performance_evaluation_under_real-time_constraints\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>As discussed previously, deploying AI algorithms comes with strict real-time constraints, and even robust NRX architectures may become impractical unless they operate within the required latency. In other words, the optimal network for deployment is not necessarily the one with the best error-rate performance, but rather the one that delivers the best accuracy within a defined computing latency budget.</p>\n\n\n\n<p>Estimating the inference latency of a given neural network architecture is a complex task, as the results depend heavily on the targeted hardware platform, the specific software stack, and the extent of code optimization. Therefore, metrics like the number of floating-point operations (FLOPs), weights, or layers are often used as proxies for a model\u2019s computational complexity. However, these metrics may be misleading due to the high degree of parallelism and potential memory bottlenecks during inference. Hence, we deploy the NRX using the TensorRT inference library on the targeted NVIDIA A100 GPU. This ensures realistic latency measurements, and the profiler helps eliminate bottlenecks on the critical path.&nbsp;</p>\n\n\n\n<p>After training in <a href=\"https://www.tensorflow.org/\">TensorFlow</a>, we exported the trained model as an <a href=\"https://onnx.ai/\">ONNX</a> file and built a <a href=\"https://developer.nvidia.com/tensorrt\">TensorRT inference engine</a>. TensorRT automatically optimizes the inference of the neural network for the target platform. If required, detailed profiling outputs are provided. An example is provided in the <a href=\"https://github.com/NVlabs/neural_rx/blob/main/notebooks/real_time_nrx.ipynb\">real-time tutorial notebook</a>.&nbsp;</p>\n\n\n\n<p>As expected, the computational complexity is heavily influenced by the 5G system configuration, including parameters like the number of allocated subcarriers and active users. The NRX architecture is designed and trained with a configurable network depth, enabling control of the computational latency after training. With this flexibility, the NRX can be easily reconfigured once the targeted hardware platform or system parameters change.&nbsp;</p>\n\n\n\n<p>Figure 3 shows the performance evaluation of the NRX executed on an NVIDIA A100 GPU using TensorRT. The performance under real-time constraints differs from the computationally unrestricted version of the network. However, we\u2019d like to emphasize that even under real-time constraints, the performance of the NRX is competitive or even outperforms many classical receiver algorithms.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1584\" height=\"1068\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph.png\" alt=\"The inference latency of the neural receiver can be controlled by the depth of the neural network. To achieve the real-time requirement of less than 1ms latency, a performance degradation of approximately 0.7dB is observed as compared to the receiver that has no such constraints.\" class=\"wp-image-88522\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph.png 1584w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph-300x202.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph-625x421.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph-171x115.png 171w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph-768x518.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph-1536x1036.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph-645x435.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph-445x300.png 445w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph-133x90.png 133w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph-362x244.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph-163x110.png 163w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/09/neural-receiver-graph-1024x690.png 1024w\" sizes=\"(max-width: 1584px) 100vw, 1584px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Performance evaluation of the NRX, varying its network depth and consequently the inference latency of the neural network</em></figcaption></figure></div>\n\n\n<h2 id=\"beyond_classical_algorithms_site-specific_fine-tuning\"  class=\"wp-block-heading\">Beyond classical algorithms: site-specific fine-tuning<a href=\"#beyond_classical_algorithms_site-specific_fine-tuning\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>An intriguing feature of AI-RAN components is their ability to undergo site-specific fine-tuning, which enables the refinement of neural network weights even after deployment. This fine-tuning relies on two key enablers:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>AI-based algorithms such as the NRX</li>\n\n\n\n<li>Software-defined RANs that facilitate the extraction of training data while the system is actively in use&nbsp;</li>\n</ul>\n\n\n\n<p>Once the data is collected, the training can be conducted either locally or offline in the cloud.&nbsp;</p>\n\n\n\n<p>To demonstrate site-specific finetuning of the neural receiver, we sampled a training dataset of 1,000 random user positions and velocities across the entire scene using the <a href=\"https://nvlabs.github.io/sionna/examples/Sionna_Ray_Tracing_Introduction.html\">Sionna ray tracer</a>. Figure 4 shows user positions for performance evaluation of the fine-tuned receiver. The red dot indicates the position of the base station, the gray line represents the user trajectories used for evaluation. New scenes can be <a href=\"https://youtu.be/7xHLDxUaQ7c\">directly loaded from OpenStreetMap</a>.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"637\" height=\"363\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/map-overlay-site-specific-channel-simulations-nrx.png\" alt=\"Munich scene used to generate site-specific channel simulations. An overlay of a coverage map shows the received signal strength for each user position.\" class=\"wp-image-87938\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/map-overlay-site-specific-channel-simulations-nrx.png 637w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/map-overlay-site-specific-channel-simulations-nrx-300x171.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/map-overlay-site-specific-channel-simulations-nrx-625x356.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/map-overlay-site-specific-channel-simulations-nrx-179x102.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/map-overlay-site-specific-channel-simulations-nrx-500x285.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/map-overlay-site-specific-channel-simulations-nrx-158x90.png 158w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/map-overlay-site-specific-channel-simulations-nrx-362x206.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/map-overlay-site-specific-channel-simulations-nrx-193x110.png 193w\" sizes=\"(max-width: 637px) 100vw, 637px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Environment used for site-specific fine-tuning and evaluation of the NRX</em></figcaption></figure></div>\n\n\n<p>As the fine-tuning starts with the pre-trained receiver network weights, it only takes a small number of training steps and moderate computing resources. Note that the NRX architecture itself remains unchanged. Figure 5 shows that already a minute of fine-tuning on a single GPU substantially improves the error-rate performance in the specific radio environment. Site-specific training enables adapting a smaller NRX to a specific radio environment, enabling it to perform at the level of a 4x larger, universally pretrained NRX. This saves a significant amount of compute during inference while maintaining superior error-rate performance.</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"670\" height=\"536\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/snr-performance-improvement.png\" alt=\"When fine-tuned on the specific environment, the NRX performance improves by up to 2.2 dB as compared to the pre-trained receiver. \" class=\"wp-image-87939\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/snr-performance-improvement.png 670w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/snr-performance-improvement-300x240.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/snr-performance-improvement-625x500.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/snr-performance-improvement-144x115.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/snr-performance-improvement-645x516.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/snr-performance-improvement-375x300.png 375w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/snr-performance-improvement-113x90.png 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/snr-performance-improvement-362x290.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/snr-performance-improvement-138x110.png 138w\" sizes=\"(max-width: 670px) 100vw, 670px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. SNR performance improvement through site-specific receiver fine-tuning using a fixed number of only 1,000 data samples\u00a0</em></figcaption></figure></div>\n\n\n\n<p>It is a unique capability of the AI-enabled RAN to continuously adapt to the actual RF environment. As such, we envision fully software-defined and AI-driven next generation base stations that improve even after deployment.&nbsp;</p>\n\n\n\n<h2 id=\"moving_from_5g_compliance_to_6g_research\"  class=\"wp-block-heading\">Moving from 5G compliance to 6G research<a href=\"#moving_from_5g_compliance_to_6g_research\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Finally, we\u2019d like to emphasize that neural receivers are not only a powerful replacement for existing receiver algorithms. They are a key enabler for a host of novel features such as <a href=\"https://arxiv.org/pdf/2009.05261\">pilotless communications using end-to-end learning</a> and site-specific retraining after deployment.&nbsp;</p>\n\n\n\n<p>Figure 6 illustrates the end-to-end learning approach where the NRX is extended by a trainable custom constellation that can be used instead of the traditional quadrature amplitude modulation (QAM).&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"749\" height=\"337\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pilotless-communication-scheme-nrx-trainable-custom-constellation.png\" alt=\"The NRX concept can be extended by a trainable custom constellation. If no classical reference signal is used for piloting, the NRX will learn to communicate without the need for any classical piloting scheme. \n\" class=\"wp-image-87940\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pilotless-communication-scheme-nrx-trainable-custom-constellation.png 749w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pilotless-communication-scheme-nrx-trainable-custom-constellation-300x135.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pilotless-communication-scheme-nrx-trainable-custom-constellation-625x281.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pilotless-communication-scheme-nrx-trainable-custom-constellation-179x81.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pilotless-communication-scheme-nrx-trainable-custom-constellation-645x290.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pilotless-communication-scheme-nrx-trainable-custom-constellation-500x225.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pilotless-communication-scheme-nrx-trainable-custom-constellation-160x72.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pilotless-communication-scheme-nrx-trainable-custom-constellation-362x163.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pilotless-communication-scheme-nrx-trainable-custom-constellation-244x110.png 244w\" sizes=\"(max-width: 749px) 100vw, 749px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. End-to-end learning of a pilotless communication scheme by extending the NRX with a trainable custom constellation&nbsp;</em></figcaption></figure>\n\n\n\n<p>The combination of a trainable custom constellation with a pilot-free slot structure forces the NRX to learn the signal reconstruction without relying on any reference signals. Intuitively, the NRX learns new constellations that implicitly include some type of superimposed piloting scheme which can be exploited for joint channel estimation and equalization. After training, the resulting scheme shows a similar error-rate performance when compared to the classical 5G system, but benefits from a higher data rate as the pilot overhead is completely removed. Further details can be found in the <a href=\"https://github.com/NVlabs/neural_rx/blob/main/notebooks/e2e_pilotless_communications.ipynb\">end-to-end learning notebook</a>.</p>\n\n\n\n<p>Although the resulting constellations are not compliant with the 5G NR standard, they are indicators of how AI may enable novel 6G features for higher reliability and increased throughput. To learn more, visit <a href=\"https://github.com/NVlabs/neural_rx\">NVlabs/neural_rx</a> on GitHub.</p>\n\n\n\n<h3 id=\"acknowledgments\"  class=\"wp-block-heading\"><strong>Acknowledgments</strong><a href=\"#acknowledgments\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><em>This work has received \ufb01nancial support from the European Union under Grant Agreement 101096379 (CENTRIC). Views and opinions expressed are however those of the author(s) only and do not necessarily re\ufb02ect those of the European Union or the European Commission (granting authority). Neither the European Union nor the granting authority can be held responsible for them.</em></p>\n\n\n\n<figure class=\"wp-block-image size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"178\" height=\"47\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/eu-logo.png\" alt=\"EU logo\" class=\"wp-image-87941\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/eu-logo.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/eu-logo-160x42.png 160w\" sizes=\"(max-width: 178px) 100vw, 178px\" /></figure>\n", "protected": false}, "excerpt": {"rendered": "<p>Today\u2019s 5G New Radio (5G NR) wireless communication systems rely on highly optimized signal processing algorithms to reconstruct transmitted messages from noisy channel observations in mere microseconds. This remarkable achievement is the result of decades of relentless effort by telecommunications engineers and researchers, who have continuously improved signal processing algorithms to meet the demanding real-time &hellip; <a href=\"https://developer.nvidia.com/blog/real-time-neural-receivers-drive-ai-ran-innovation/\">Continued</a></p>\n", "protected": false}, "author": 1733, "featured_media": 87945, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1479866", "discourse_permalink": "https://forums.developer.nvidia.com/t/real-time-neural-receivers-drive-ai-ran-innovation/305554", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 1205, 503], "tags": [817, 453, 1962], "coauthors": [3337, 3926, 3925, 3928], "class_list": ["post-87932", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-networking-communications", "category-simulation-modeling-design", "tag-5g", "tag-featured", "tag-nvidia-research"], "acf": {"post_industry": ["Telecommunications"], "post_products": ["Aerial", "Sionna"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/server-blade.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mSg", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87932"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1733"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87932"}], "version-history": [{"count": 14, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87932/revisions"}], "predecessor-version": [{"id": 88549, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87932/revisions/88549"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87945"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87932"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87932"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87932"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87932"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 85619, "date": "2024-08-30T09:00:04", "date_gmt": "2024-08-30T16:00:04", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=85619"}, "modified": "2024-09-05T10:57:10", "modified_gmt": "2024-09-05T17:57:10", "slug": "fast-inversion-for-real-time-image-editing-with-text", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/fast-inversion-for-real-time-image-editing-with-text/", "title": {"rendered": "Fast Inversion for Real-Time Image Editing with Text"}, "content": {"rendered": "\n<p>Text-to-image diffusion models can generate diverse, high-fidelity images based on user-provided text prompts. They operate by mapping a random sample from a high-dimensional space, <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_%7BT%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"z_{T}\" class=\"latex\" /> conditioned on a user-provided text prompt, through a series of denoising steps. This results in a representation of the corresponding image, <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_%7B0%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"z_{0}\" class=\"latex\" />.</p>\n\n\n\n<p>These models can also be used for more complex tasks such as image editing, learning to depict a <a href=\"https://textual-inversion.github.io/\">personalized concept</a>, or <a href=\"https://github.com/dvirsamuel/SeedSelect\">semantic data augmentation</a>. In this context, image editing refers to the task of making local changes to a given image based on a text prompt, while the other parts of the image remain unchanged.</p>\n\n\n\n<p>All these additional tasks involve a process called <em>inversion</em>: Given an image representation <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_%7B0%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"z_{0}\" class=\"latex\" /> and its corresponding text prompt <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=p&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"p\" class=\"latex\" />, you seek a noise seed <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_%7BT%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"z_{T}\" class=\"latex\" /> that, when fed into the denoising process, yields the reconstructed image <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_%7B0%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"z_{0}\" class=\"latex\" />.</p>\n\n\n\n<p>Regularized Newton-Raphson Inversion (RNRI), a novel inversion technique, was recently proposed. RNRI outperforms existing inversion approaches by balancing rapid convergence with superior accuracy, execution time, and memory efficiency, enabling real-time image editing for the first time.</p>\n\n\n\n<figure class=\"wp-block-gallery aligncenter has-nested-images columns-default is-cropped wp-block-gallery-2 is-layout-flex wp-block-gallery-is-layout-flex\">\n<figure class=\"wp-block-image size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"600\" height=\"338\" data-id=\"88407\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-editing-fast-inversion-lion-1.gif\" alt=\"GIF shows real-time editing of several images. Given a photo of a lion sitting in the grass, a text prompt is used to transform the lion into a raccoon while preserving the background. All edits involve two processes, inversion and generation, both being fast to make the full process interactive.\" class=\"wp-image-88407\"/><figcaption class=\"wp-element-caption\"><em>a) a lion is sitting in the green grass at sunset</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"600\" height=\"338\" data-id=\"88408\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-editing-fast-inversion-cat.gif\" alt=\"GIF shows real-time editing of several images. Given a photo of a cat sitting next to a glass vase with flowers, a text prompt is used to transform the cat into a fish while preserving the background. All edits involve two processes, inversion and generation, both being fast to make the full process interactive.\" class=\"wp-image-88408\"/><figcaption class=\"wp-element-caption\"><em>b) a cat laying next to a glass vase with flowers</em></figcaption></figure>\n<figcaption class=\"blocks-gallery-caption wp-element-caption\"><em>Figure 1. Real-time image editing with fast inversion</em></figcaption></figure>\n\n\n\n<h2 id=\"inversion_as_solving_an_implicit_equation\"  class=\"wp-block-heading\">Inversion as solving an implicit equation<a href=\"#inversion_as_solving_an_implicit_equation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Inverting a diffusion model requires searching in the space of possible seeds for one that would reconstruct a given image. This search may be computationally demanding.</p>\n\n\n\n<p>To understand how it can be achieved efficiently, consider first the forward (noising) process.</p>\n\n\n\n<p>Sampling from diffusion models can be viewed as solving an ordinary differential equation. The popular DDIM deterministic scheduler presented in <a href=\"https://arxiv.org/abs/2010.02502\">Denoising Diffusion Implicit Models</a> denoises a latent noise vector in the following way:</p>\n\n\n\n<p class=\"has-text-align-right has-small-font-size\">Equation 1</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_%7Bt-1%7D%3D%5Csqrt%7B%5Cfrac%7B%5Calpha_%7Bt-1%7D%7D%7B%5Calpha_%7Bt%7D%7D%7Dz_%7Bt%7D+-+%5Csqrt%7B%5Calpha_%7Bt-1%7D%7D+%5Ccdot+%5CDelta+%5Cpsi%28%5Calpha_%7Bt%7D%29%5Ccdot+%5Cepsilon_%7B%5Ctheta%7D%28z_%7Bt%7D%2Ct%2Cp%29+%5Cquad%5Cquad&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"z_{t-1}=&#92;sqrt{&#92;frac{&#92;alpha_{t-1}}{&#92;alpha_{t}}}z_{t} - &#92;sqrt{&#92;alpha_{t-1}} &#92;cdot &#92;Delta &#92;psi(&#92;alpha_{t})&#92;cdot &#92;epsilon_{&#92;theta}(z_{t},t,p) &#92;quad&#92;quad\" class=\"latex\" /></p>\n\n\n\n<p>In this equation (equation 1), <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Calpha_t+%3D+1-%5Cbeta_t&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;alpha_t = 1-&#92;beta_t\" class=\"latex\" />, <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Cpsi%28%5Calpha%29+%3D+%5Csqrt%7B%5Cfrac%7B1%7D%7B%5Calpha%7D-1%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;psi(&#92;alpha) = &#92;sqrt{&#92;frac{1}{&#92;alpha}-1}\" class=\"latex\" /> and <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5CDelta+%5Cpsi%28%5Calpha_t%29+%3D+%5Cpsi%28%5Calpha_t%29+-+%5Cpsi%28%5Calpha_%7Bt-1%7D%29&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;Delta &#92;psi(&#92;alpha_t) = &#92;psi(&#92;alpha_t) - &#92;psi(&#92;alpha_{t-1})\" class=\"latex\" />.</p>\n\n\n\n<h2 id=\"ddim_inversion\"  class=\"wp-block-heading\">DDIM inversion<a href=\"#ddim_inversion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To derive inversion, the first equation is first rewritten as follows:</p>\n\n\n\n<p class=\"has-text-align-right has-small-font-size\">Equation 2</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_t+%3D+%5Csqrt%7B%5Cfrac%7B%5Calpha_t%7D%7B%5Calpha_%7Bt-1%7D%7D%7Dz_%7Bt-1%7D+%2B+%5Csqrt%7B%5Calpha_%7Bt%7D%7D+%5Ccdot+%5CDelta+%5Cpsi%28%5Calpha_t%29+%5Ccdot+%5Cepsilon_%7B%5Ctheta%7D%28z_%7Bt%7D%2Ct%2Cp%29&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"z_t = &#92;sqrt{&#92;frac{&#92;alpha_t}{&#92;alpha_{t-1}}}z_{t-1} + &#92;sqrt{&#92;alpha_{t}} &#92;cdot &#92;Delta &#92;psi(&#92;alpha_t) &#92;cdot &#92;epsilon_{&#92;theta}(z_{t},t,p)\" class=\"latex\" /></p>\n\n\n\n<p>This gives an implicit equation in <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_%7Bt%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"z_{t}\" class=\"latex\" />, that cannot be solved in closed form. DDIM inversion approximates it by replacing <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_%7Bt%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"z_{t}\" class=\"latex\" /> with <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_%7Bt-1%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"z_{t-1}\" class=\"latex\" />:</p>\n\n\n\n<p class=\"has-text-align-right has-small-font-size\">Equation 3</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Capprox+%5Csqrt%7B%5Cfrac%7B%5Calpha_t%7D%7B%5Calpha_%7Bt-1%7D%7D%7Dz_%7Bt-1%7D+%2B+%5Csqrt%7B%5Calpha_%7Bt%7D%7D+%5Ccdot+%5CDelta+%5Cpsi%28%5Calpha_t%29+%5Ccdot%5Cepsilon_%7B%5Ctheta%7D%28%5Cboxed%7Bz_%7Bt-1%7D%7D%2Ct%2Cp%29&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"&#92;approx &#92;sqrt{&#92;frac{&#92;alpha_t}{&#92;alpha_{t-1}}}z_{t-1} + &#92;sqrt{&#92;alpha_{t}} &#92;cdot &#92;Delta &#92;psi(&#92;alpha_t) &#92;cdot&#92;epsilon_{&#92;theta}(&#92;boxed{z_{t-1}},t,p)\" class=\"latex\" /></p>\n\n\n\n<p>DDIM inversion is a fast method but often an inaccurate inversion.</p>\n\n\n\n<h2 id=\"fixed-point_and_gradient_descent_inversion_methods\"  class=\"wp-block-heading\">Fixed-point and gradient descent inversion methods<a href=\"#fixed-point_and_gradient_descent_inversion_methods\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Several papers improve the previous approximation using iterative methods to approximately solve the second equation. For example, directly solving the equation using <a href=\"https://en.wikipedia.org/wiki/Fixed-point_iteration\">fixed-point iterations</a> is a method widely used in numerical analysis for solving implicit functions. For more information, see <a href=\"https://arxiv.org/abs/2309.04907\">Effective Real Image Editing with Accelerated Iterative Diffusion Inversion</a>.</p>\n\n\n\n<p>In a related way, a more precise inversion equation can be solved, obtained by employing higher-order terms using gradient descent. For more information, see <a href=\"https://arxiv.org/abs/2311.18387\">On Exact Inversion of DPM-Solvers.</a></p>\n\n\n\n<p>Fixed-point iterations and gradient descent methods provide better accuracy than DDIM, but have a linear convergence rate and may take many seconds to compute.</p>\n\n\n\n<h2 id=\"regularized_newton-raphson_inversion_method\"  class=\"wp-block-heading\">Regularized Newton-Raphson Inversion method<a href=\"#regularized_newton-raphson_inversion_method\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>A faster and more accurate alternative is based on the well-known <a href=\"https://en.wikipedia.org/wiki/Newton%27s_method\">Newton-Raphson iterative method</a> (NR).</p>\n\n\n\n<p>NR is a method for iteratively finding the roots of a system of equations. A naive application of NR to the full latent space would require solving <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_t+%3D+f%28z_t%29&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"z_t = f(z_t)\" class=\"latex\" />. This formulation is impractical because it requires inverting a high-dimensional Jacobian matrix.</p>\n\n\n\n<p>Instead, define a multivariable scalar function <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Chat%7Br%7D%3A+R%5Ed+%5Crightarrow+R&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;hat{r}: R^d &#92;rightarrow R\" class=\"latex\" />:</p>\n\n\n\n<p class=\"has-text-align-right has-small-font-size\">Equation 4</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Chat%7Br%7D%28z_t%29+%3A%3D+%7C%7Cz_t+-+f%28z_t%29%7C%7C&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"&#92;hat{r}(z_t) := ||z_t - f(z_t)||\" class=\"latex\" /></p>\n\n\n\n<p>Seek its roots <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Chat%7Br%28z_t%29%7D%3D0&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;hat{r(z_t)}=0\" class=\"latex\" />. Because <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Chat%7Br%28z_t%29%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;hat{r(z_t)}\" class=\"latex\" /> is a scalar function, the Jacobian matrix is a vector and can be computed quickly.</p>\n\n\n\n<p>Solving equation 4 can be done quickly, but its solutions are not guaranteed to reconstruct the image well because the equation is underdetermined. Also, some roots of <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Chat%7Br%28z_t%29%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;hat{r(z_t)}\" class=\"latex\" /> may be out of distribution for the diffusion model.</p>\n\n\n\n<p>To address this issue, add a regularization term to the NR objective:</p>\n\n\n\n<p class=\"has-text-align-right has-small-font-size\">Equation 5</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=q%28z_%7Bt%7D%7Cz_%7Bt-1%7D%29+%3A%3D+%5Cmathcal%7BN%7D%28z_%7Bt%7D%3B%5Cmu_t%3D%5Csqrt%7B1-%5Cbeta_%7Bt%7D%7Dz_%7Bt-1%7D%2C%5CSigma_t%3D%5Cbeta_%7Bt%7DI%29&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"q(z_{t}|z_{t-1}) := &#92;mathcal{N}(z_{t};&#92;mu_t=&#92;sqrt{1-&#92;beta_{t}}z_{t-1},&#92;Sigma_t=&#92;beta_{t}I)\" class=\"latex\" /></p>\n\n\n\n<p>As each noising step in the diffusion process follows a Gaussian distribution, it is incorporated as a prior over the values of <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_t&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"z_t\" class=\"latex\" />. The negative log-likelihood is added as a regularizing penalty term, forming the objective:</p>\n\n\n\n<p class=\"has-text-align-right has-small-font-size\">Equation 6</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5CL%28z_t%29+%3A%3D+%7C%7Cz_t+-+f%28z_t%29%7C%7C+-+%5Clambda+%5Clog+q%28z_t+%7C+z_%7Bt-1%7D%29&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"&#92;L(z_t) := ||z_t - f(z_t)|| - &#92;lambda &#92;log q(z_t | z_{t-1})\" class=\"latex\" /></p>\n\n\n\n<p>The Newton-Raphson iteration for this function can be computed efficiently using automatic differentiation engines, initializing the process with <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=z_%7Bt-1%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"z_{t-1}\" class=\"latex\" /> from the previous diffusion timestep. Regularized Newton Raphson Inversion (RNRI), converges in 1\u20132 iterations (~0.5 sec for latent consistency models).</p>\n\n\n\n<p>Figure 2 compares the quality of reconstructed images (measured using PSNR) of the <a href=\"https://cocodataset.org/#home\">COCO validation set</a>, against the time it takes to compute the inversion. It shows that RNRI improves in terms of PSNR or run time over recent methods. For a fair time comparison, run time is measured on a single NVIDIA A100 GPU for all methods. The dashed black line denotes the upper bound that is due to the inherent distortion caused by the Stable Diffusion VAE.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1188\" height=\"485\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/image-inversion-method-results.png\" alt=\"Two graphs comparing the performance of different image inversion methods in terms of reconstruction quality (PSNR) and runtime. The left graph shows results for a latent diffusion model, where RNRI achieves high PSNR with significantly faster inversion-reconstruction time compared to other methods. The right graph shows results for a latent consistency model, where RNRI achieves the highest PSNR in less than 0.5 seconds, much faster than the other methods.\" class=\"wp-image-85626\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/image-inversion-method-results.png 1188w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/image-inversion-method-results-300x122.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/image-inversion-method-results-625x255.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/image-inversion-method-results-179x73.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/image-inversion-method-results-768x314.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/image-inversion-method-results-645x263.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/image-inversion-method-results-500x204.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/image-inversion-method-results-160x65.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/image-inversion-method-results-362x148.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/image-inversion-method-results-269x110.png 269w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/image-inversion-method-results-1024x418.png 1024w\" sizes=\"(max-width: 1188px) 100vw, 1188px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Inversion results for PSNR compared to runtime</em></figcaption></figure></div>\n\n\n<p>Figure 3 provides a qualitative comparison between RNRI and previous state-of-the-art inversion approaches. It shows cases where RNRI accurately edits images that have high fidelity with the input image and also adhere well to the target prompt. Alternative approaches may struggle with editing these images and prompts. Baselines were run until they converged, whereas RNRI was run for only two iterations per diffusion step.</p>\n\n\n\n<p>For example, in the first row, RNRI succeeds in converting the pizza into slices of bread. Other methods either fail to achieve this or incorrectly modify other elements. In the third row, all methods struggle to accurately substitute bananas with oranges or alter the background. In contrast, RNRI accurately edits the object while maintaining the original background.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1229\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-edits.png\" alt=\"Three comparisons of various text generation models applied to a sequence of images. Each row demonstrates a transformation of an initial image with different inversion approaches.\" class=\"wp-image-85629\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-edits.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-edits-300x184.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-edits-625x384.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-edits-179x110.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-edits-768x472.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-edits-1536x944.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-edits-645x397.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-edits-488x300.png 488w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-edits-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-edits-362x223.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-edits-1024x630.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. RNRI edits images more naturally while preserving the structure of the original image</em></figcaption></figure></div>\n\n\n<h3 id=\"evaluation_of_rnri_results\"  class=\"wp-block-heading\">Evaluation of RNRI results<a href=\"#evaluation_of_rnri_results\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Following the previous work, editing performance is measured using the following metrics:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>An LPIPS score quantifies the extent to which structure is preserved (lower is better).</li>\n\n\n\n<li>A CLIP-based score quantifies how well the generated images match the text prompt (higher is better).</li>\n</ul>\n\n\n\n<p>Values are averages across 100 MS-COCO images. Figure 4 shows that editing with RNRI yields a superior CLIP and LPIPS score, achieving state-of-the-art editing of real images.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1209\" height=\"519\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-graph-results.png\" alt=\"Two graphs evaluating different models on text prompt compliance and image structure preservation. The left graph shows that RNRI outperforms other baselines on the Latent Diffusion Model, in terms of CLIP and LPIPS scores. The right graph indicates that RNRI achieves better performance also using the Latent Consistency Model.\" class=\"wp-image-85630\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-graph-results.png 1209w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-graph-results-300x129.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-graph-results-625x268.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-graph-results-179x77.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-graph-results-768x330.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-graph-results-645x277.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-graph-results-500x215.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-graph-results-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-graph-results-362x155.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-graph-results-256x110.png 256w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-graph-results-1024x440.png 1024w\" sizes=\"(max-width: 1209px) 100vw, 1209px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. RNRI achieves superior CLIP and LPIPS scores, indicating better compliance with text prompts and higher structure preservation</em></figcaption></figure></div>\n\n\n<p>Finally, Figure 5 shows additional real-time editing results.</p>\n\n\n\n<figure class=\"wp-block-gallery aligncenter has-nested-images columns-default is-cropped wp-block-gallery-3 is-layout-flex wp-block-gallery-is-layout-flex\">\n<figure class=\"wp-block-image size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"600\" height=\"338\" data-id=\"85635\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-editing-laptop.gif\" alt=\"GIF shows real-time editing of various images. One part features a photo of a desk with a laptop, a cup of coffee near a black pen, and a smartphone. The coffee is first transformed into a glass of milk, then the pen changes to yellow. Next, the phone becomes a metal box. The photo's lighting is then altered to show a night scene, followed by a daytime scene.\" class=\"wp-image-85635\"/><figcaption class=\"wp-element-caption\"><em>a) A laptop, a cup of milk, a black pen, and a smartphone</em></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"600\" height=\"337\" data-id=\"85636\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rnri-image-editing-wooden-board.gif\" alt=\"GIF shows real-time editing of various images. One part features a photo of a cartoon of a wooden board with tomatoes and grapes and carrots on the tablecloth. The tomatoes are transformed into grapes, near a knife. Next, the carrots are transformed into oranges on a green tablecloth.\" class=\"wp-image-85636\"/><figcaption class=\"wp-element-caption\"><em>b) A cartoon of a wooden board with tomatoes, grapes, and orange on the green tablecloth</em></figcaption></figure>\n<figcaption class=\"blocks-gallery-caption wp-element-caption\"><em>Figure 5. Examples of real-time image editing using RNRI</em></figcaption></figure>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Image inversion in diffusion models is key for applications like image editing, semantic augmentation, and generating rare-concept images. Current methods often sacrifice inversion quality for computational efficiency, requiring significantly more resources for high-quality results.</p>\n\n\n\n<p>Regularized Newton-Raphson Inversion (RNRI) balances rapid convergence with superior accuracy, execution time, and memory efficiency. The RNRI method outperforms existing approaches in both latent diffusion and latent consistency models, enabling real-time image editing.</p>\n\n\n\n<p>For more information, see the full paper, <a href=\"https://arxiv.org/abs/2312.12540\">Regularized Newton Raphson Inversion for Text-to-Image Diffusion Models</a>. You can also <a href=\"https://barakmam.github.io/rnri.github.io/\">try RNRI</a> yourself.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Text-to-image diffusion models can generate diverse, high-fidelity images based on user-provided text prompts. They operate by mapping a random sample from a high-dimensional space, conditioned on a user-provided text prompt, through a series of denoising steps. This results in a representation of the corresponding image, . These models can also be used for more complex &hellip; <a href=\"https://developer.nvidia.com/blog/fast-inversion-for-real-time-image-editing-with-text/\">Continued</a></p>\n", "protected": false}, "author": 2206, "featured_media": 88410, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1478146", "discourse_permalink": "https://forums.developer.nvidia.com/t/fast-inversion-for-real-time-image-editing-with-text/305216", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 1235, 3110], "tags": [453, 3576, 3257], "coauthors": [3931], "class_list": ["post-85619", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-graphics", "category-generative-ai", "tag-featured", "tag-image-photo-editing", "tag-generative-ai"], "acf": {"post_industry": ["General"], "post_products": ["General"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fast-inversion-real-time-editing-featured.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-mgX", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/85619"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2206"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=85619"}], "version-history": [{"count": 39, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/85619/revisions"}], "predecessor-version": [{"id": 88409, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/85619/revisions/88409"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88410"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=85619"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=85619"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=85619"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=85619"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87334, "date": "2024-08-30T08:58:23", "date_gmt": "2024-08-30T15:58:23", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87334"}, "modified": "2024-09-05T10:57:10", "modified_gmt": "2024-09-05T17:57:10", "slug": "accelerating-predictive-maintenance-in-manufacturing-with-rapids-ai", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/accelerating-predictive-maintenance-in-manufacturing-with-rapids-ai/", "title": {"rendered": "Accelerating Predictive Maintenance in Manufacturing with RAPIDS AI"}, "content": {"rendered": "\n<p>The International Society of Automation (ISA) reports that <a href=\"https://blog.isa.org/predictive-maintenance-embraces-analytics\">5% of plant production is lost annually</a> due to downtime. Putting that into a different context, roughly $647B is surrendered on a global basis by manufacturers across all industry segments, the corresponding portion of nearly $13T in production. The challenge at hand is predicting the maintenance needs of these machines to minimize downtime, reduce operational costs, and optimize maintenance schedules.&nbsp;</p>\n\n\n\n<p>This problem is especially prevalent in companies that offer Desktop as a Service (DaaS) services, which lease computing devices for commercial use and have tight SLAs to meet. The industry of DaaS is valued at US$3B and is expected to grow 12%.</p>\n\n\n\n<p>In this post, we discuss a case where we built a predictive model to estimate the remaining useful life (RUL) of computing assets based on various operational parameters, sensor data, and historical maintenance records.</p>\n\n\n\n<h2 id=\"latentview_analytics\"  class=\"wp-block-heading\">LatentView Analytics<a href=\"#latentview_analytics\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>LatentView Analytics supports multiple DaaS clients and offers operationally complex services with advanced data analytics consulting in areas like business intelligence, data analytics and science, data engineering, machine learning, and AI.</p>\n\n\n\n<p>We\u2019ve found that organizations can save valuable downtime by detecting equipment failure before it happens with predictive maintenance algorithms. Advances in data science have made prediction and forecasting widespread within enterprises. Compared to standard operating procedures like routine\u2013 or time-based preventative maintenance, predictive maintenance gets ahead of the problem.&nbsp;</p>\n\n\n\n<p>LatentView built a solution called PULSE, which is an advanced predictive maintenance solution designed to redefine manufacturing efficiency. By connecting IoT-enabled assets, PULSE uses cutting-edge analytics to provide real-time insights, enabling your team to take proactive measures.&nbsp;</p>\n\n\n\n<p>PULSE helps to reduce and remove unplanned downtime, excessive maintenance costs, and operational inefficiencies. You can predict machine failure with precision, eliminate downtime hassles, and enhance manufacturing efficiency.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"442\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-1024x442.png\" alt=\"Diagram shows the PULSE model workflow and data stack, starting from data sources and moving through data ingestion frameworks, messaging, processing, security, governance, monitoring, profiling, access, and then data consumption in the form of reports, dashboards, files, applications, and relational databases.\" class=\"wp-image-87459\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-1024x442.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-300x129.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-625x270.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-179x77.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-768x331.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-1536x662.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-645x278.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-500x216.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-362x156.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-255x110.png 255w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. LatentView ML workflow</em></figcaption></figure></div>\n\n\n<h2 id=\"remaining_useful_life_use_case\"  class=\"wp-block-heading\">Remaining useful life use case<a href=\"#remaining_useful_life_use_case\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>A leading computing device manufacturing client wanted to implement effective preventive maintenance. Part failure on millions of leased computing devices resulted in customer churn and dissatisfaction. The ability to identify failure earlier and make recommendations on repair and replacement would reduce that churn, increasing customer loyalty as well as profitability.&nbsp;</p>\n\n\n\n<p>To address the customer\u2019s pain point, we decided to use a predictive maintenance model to forecast the RUL of each machine. The model would help identify how long each machine would run before repair or replacement was required, eliminating part failure when machines went to customers.&nbsp;</p>\n\n\n\n<p>To build this predictive maintenance model for computing devices, we first needed to aggregate data from the key thermal, battery, fan, disk, and CPU sensors that measured the temperature, cycles, and multiple aspects of the machine. That data was aggregated and applied to a forecasting model.</p>\n\n\n\n<p>The following sections describe our initial attempt, learning, and how GPU-accelerated data science helped us speed up implementation to deliver a successful project to our client.</p>\n\n\n\n<h2 id=\"challenges_faced\"  class=\"wp-block-heading\">Challenges faced<a href=\"#challenges_faced\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In our first attempt to build the proof-of-concept for the client, we faced many challenges that centered on computational bottlenecks and extended cycle processing times with our predictive maintenance platform offering, PULSE. This was primarily due to the high volume and constant stream of data required to make effective predictions, which in turn attracted more and larger nodes and images to meet the computational requirements.&nbsp;</p>\n\n\n\n<p>While these challenges are holistic to the problem, we primarily wanted a tool and library integration with a solution that can scale well to dynamic operational conditions. The solution should minimize the time it takes to see the results and optimize the TCO, including infrastructure cost.</p>\n\n\n\n<p>Some of the problems we faced included the following:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Large, real-time datasets</li>\n\n\n\n<li>Sparse and noisy sensor data</li>\n\n\n\n<li>Multivariate relationships</li>\n\n\n\n<li>Protracted timeline</li>\n\n\n\n<li>Cost aspect</li>\n\n\n\n<li>Inferencing challenges</li>\n</ul>\n\n\n\n<h3 id=\"large_real-time_datasets\"  class=\"wp-block-heading\">Large, real-time datasets<a href=\"#large_real-time_datasets\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Over 1 TB of data is collected daily due to the millions of deployed machines across multiple locations, with multiple sensors in each machine and data collection intervals every 5 minutes. This made data processing and cleansing the most time-consuming and tedious task, as we spent almost 60% of our time preparing the data.&nbsp;</p>\n\n\n\n<p>Continuous iteration of training the model with the latest training data, data cleaning, adding new features, and experimenting with multiple models to finalize the production model, also increase the total effort, time, and compute power being used.</p>\n\n\n\n<h3 id=\"sparse_and_noisy_sensor_data\"  class=\"wp-block-heading\">Sparse and noisy sensor data<a href=\"#sparse_and_noisy_sensor_data\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In a manufacturing or DaaS environment, sensor data from each machine was often sparse (most of the values being zero or empty), collected at irregular intervals, and prone to noise.</p>\n\n\n\n<h3 id=\"multivariate_relationships\"  class=\"wp-block-heading\">Multivariate relationships<a href=\"#multivariate_relationships\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The number of sensor types that had to be considered in a single model for the use case created a complex multivariate situation that increased computational needs.&nbsp;</p>\n\n\n\n<h3 id=\"protracted_timeline\"  class=\"wp-block-heading\">Protracted timeline<a href=\"#protracted_timeline\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Creating accurate predictions requires a large dataset with a lot of examples to train the model.&nbsp;</p>\n\n\n\n<p>As big data use cases continue to grow, CPU performance becomes a major bottleneck. These limitations increase cycle time and costs and become visible in our PoC outcome.</p>\n\n\n\n<h3 id=\"cost_aspect\"  class=\"wp-block-heading\">Cost aspect<a href=\"#cost_aspect\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Infrastructure must be scaled to reduce cycle time. Large-scale CPU infrastructure incurs significant costs, reducing the return on investment for data-driven enterprises.</p>\n\n\n\n<h3 id=\"inferencing_challenges&nbsp;\"  class=\"wp-block-heading\">Inferencing challenges&nbsp;<a href=\"#inferencing_challenges&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Deploying a large-scale prediction process is arduous. It usually requires significant software refactoring or sometimes rewriting the codes that are optimized to the use case and hand-offs between teams. In that case, insight generation can be delayed substantially.</p>\n\n\n\n<h2 id=\"an_accelerated_predictive_maintenance_solution_with_rapids&nbsp;\"  class=\"wp-block-heading\">An accelerated predictive maintenance solution with RAPIDS&nbsp;<a href=\"#an_accelerated_predictive_maintenance_solution_with_rapids&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>PULSE was built to run on CPU infrastructure using the PyData ecosystem. With the introduction to RAPIDS, we wanted to offer an accelerated PULSE platform to our customers with <a href=\"https://developer.nvidia.com/rapids\">RAPIDS</a> as a drop-in replacement to all of the PyData libraries.</p>\n\n\n\n<p>We saw the following high-level benefits of using NVIDIA RAPIDS for the PULSE system:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Creating faster data pipelines</li>\n\n\n\n<li>Working in a known platform</li>\n\n\n\n<li>Navigating dynamic operational conditions</li>\n\n\n\n<li>Addressing sparse and noisy sensor data</li>\n\n\n\n<li>Benefiting from faster data loading and preprocessing, model training</li>\n</ul>\n\n\n\n<h3 id=\"creating_faster_data_pipelines\"  class=\"wp-block-heading\">Creating faster data pipelines<a href=\"#creating_faster_data_pipelines\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>With the power of the GPU, workloads are parallelized, making the processing of large volumes of near-real-time data less of a burden on CPU infrastructure. With improved performance, we anticipated cost savings as the infrastructure performs more efficiently with fewer resources.</p>\n\n\n\n<h3 id=\"working_in_a_known_platform\"  class=\"wp-block-heading\">Working in a known platform<a href=\"#working_in_a_known_platform\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Data scientists in our team use Python packages like <code>pandas</code> and <code>scikit-learn</code>. RAPIDS offers syntactically similar or identical packages with the RAPIDS cuDF and cuML libraries that run workloads on the GPU, helping to speed up development time without requiring new skill development.&nbsp;</p>\n\n\n\n<h3 id=\"navigating_dynamic_operational_conditions&nbsp;\"  class=\"wp-block-heading\">Navigating dynamic operational conditions&nbsp;<a href=\"#navigating_dynamic_operational_conditions&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>With GPU acceleration at our disposal, the model seamlessly adjusted to dynamic conditions with additional training data, ensuring that it remained robust and responsive to the evolving patterns and recent trends.</p>\n\n\n\n<h3 id=\"addressing_sparse_and_noisy_sensor_data\"  class=\"wp-block-heading\">Addressing sparse and noisy sensor data<a href=\"#addressing_sparse_and_noisy_sensor_data\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>RAPIDS proved instrumental in tackling the intricacies of sparse and noisy sensor data. Using the GPU-accelerated cuDF library, we experienced a significant boost in data preprocessing speed. Missing values were imputed with unprecedented efficiency, noise was filtered out, and irregularities in data collection were addressed with precision.&nbsp;</p>\n\n\n\n<p>RAPIDS laid the foundation for a clean and reliable dataset, setting the stage for more accurate predictive models.</p>\n\n\n\n<h3 id=\"faster_data_loading_and_preprocessing_model_training\"  class=\"wp-block-heading\">Faster data loading and preprocessing, model training<a href=\"#faster_data_loading_and_preprocessing_model_training\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>RAPIDS\u2019s data loading, preprocessing, and ETL features are built on Apache Arrow for loading, joining, aggregating, filtering, and otherwise manipulating data, all in pandas-like API leading to a &gt;10x speedup. This reduced the model iteration time and enabled us to try multiple models in a short time.&nbsp;</p>\n\n\n\n<p>Figure 2. shows a high-level representative description of where NVIDIA GPU-accelerated libraries have been consumed. We have integrated RAPIDS and RAPIDS Accelerator for SPARK in the existing PULSE solution to accelerate the AI/ML layer, as well as data processing and advanced analytics. The integration was seamless and with the feasibility of no-code or low-code changes.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"441\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids-1024x441.png\" alt=\"Diagram shows the aspects of the PULSE workflow accelerated by RAPIDS, including the advanced AI / ML layer, processed layer, and reports and dashboards.\u00a0\" class=\"wp-image-87458\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids-1024x441.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids-300x129.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids-625x269.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids-179x77.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids-768x331.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids-1536x662.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids-645x278.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids-500x215.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids-160x69.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids-362x156.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids-255x110.png 255w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latentview-pulse-ml-workflow-with-rapids.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. LatentView ML workflow with RAPIDS</em></figcaption></figure></div>\n\n\n<h2 id=\"cpu_and_rapids_performance_comparison\"  class=\"wp-block-heading\"><strong>CPU and RAPIDS performance comparison</strong><a href=\"#cpu_and_rapids_performance_comparison\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To evaluate GPU acceleration for the project, we planned a proof-of-concept to benchmark an accelerated performance by comparing our CPU-only model to RAPIDS on GPUs. These were the solutions we compared:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>CPU-only test run of the solution with Pandas</li>\n\n\n\n<li>Test run with RAPIDS cuDF to accelerate Pandas</li>\n</ul>\n\n\n\n<p>This pilot was an on-premises setup with the following GPU infrastructure:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>CPU Sockets: </strong>&nbsp;2</li>\n\n\n\n<li><strong>CPU Model:</strong>&nbsp; AMD EPYC 7742</li>\n\n\n\n<li><strong>CPU Cores:</strong> 128 Cores (256 hyper-threading)</li>\n\n\n\n<li><strong>System Memory:</strong> 512GiB</li>\n\n\n\n<li><strong>GPU: </strong>&nbsp;NVIDIA A100 PCIe</li>\n\n\n\n<li><strong>GPU VRAM:</strong>&nbsp; 80GiB</li>\n</ul>\n\n\n\n<h3 id=\"dataset\"  class=\"wp-block-heading\">Dataset<a href=\"#dataset\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>For this pilot project, we could not use the production data due to security and data compliance issues. These benchmarks were done outside of the customer\u2019s secure infrastructure.&nbsp;</p>\n\n\n\n<p>We had to fall back to synthetic data with stand-in columns. The current production deployment uses Databricks with Microsoft Azure. For our test, we wanted to keep the synthetic data as close to the actual data as possible. We used the Spark-based <a href=\"https://databrickslabs.github.io/dbldatagen/public_docs/index.html\">dbldatagen</a> utility from Databricks labs for generating data where the size was 12 GB.&nbsp;</p>\n\n\n\n<p>The following example code shows the dataset details:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfrom pyspark.sql.functions import * \nfrom pyspark.sql.types import * \nimport pandas \nimport numpy \nimport os \npandas.set_option(&#039;display.max_columns&#039;, None)\nimport dbldatagen as dg \n\n# Column definitions are stubs only - modify to generate correct data\n\ngeneration_spec = ( dg.DataGenerator(sparkSession=spark, name=&#039;synthetic_data&#039;, rows=350000000, random=True, partitions=8 ) .withColumn(&#039;Unique_Key&#039;, &#039;string&#039;, template=r&#039;\\\\w&#039;) .withColumn(&#039;Month&#039;, &#039;string&#039;, values=&#x5B;201908, 202106, 201910, 202102, 202010, 202009, 201907, 202012, 202006, 202207, 201909, 202004, 201906, 202206, 202008, 202104, 201912, 202204, 202001, 202110, 202011, 202108, 202007, 201911, 202208, 202002, 202003, 202203, 202101, 202109, 202103, 202005, 202112, 202210, 202105, 202205, 202202, 202209, 202301, 202201, 202107, 202302, 202111, 202211, 202212]) \n.withColumn(&#039;Part_No&#039;, &#039;bigint&#039;, minValue=1000000000, maxValue=10000000000) \n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026..\n)\n\ndf1 = generation_spec.build()\n\ndf1.coalesce(1).write.format(&quot;csv&quot;).mode(&quot;overwrite&quot;).save(&quot;/rapids/notebooks/host/hostdata1/lv_data/final_data/final_2&quot;, header = True)\n\ndata = spark.read.csv(&quot;/rapids/notebooks/host/hostdata1/lv_data/final_data/final/header.csv&quot;, header=True,inferSchema=True)\n\ndata.head(1)\n\n&#x5B;Row(Unique_Key=&#039;laboris&#039;, Month=201908, Part_No=4821032423, Region=&#039;LA&#039;, classification=&#039;Erratic&#039;, order_type=&#039;SVC&amp;RPB&#039;, WIB=12226712, CPIB=1723240, COIB=122034, Order_Qty=1111, Business_Segment=&#039;other&#039;, Market_Segment=&#039;EMEA_Consumer&#039;, Exception_Flag=&#039;N&#039;, Load_date=202202, Product_Line=&#039;5X&#039;, Part_Commodity=&#039;Service/Support Kit&#039;, commoditygroup=&#039;nan&#039;, Active_flag=&#039;Y&#039;, Part_Life=&#039;Sustaining&#039;, FCS_date=41.62, RV=&#039;A&#039;, ABC=&#039;X&#039;, XYZ=&#039;N&#039;, Roll_Flag=52705, Order_Qty_agg=None)]\n</pre></div>\n\n\n<p>The code example generates synthetic data of the required size defined by the parameter <code>rows</code>. The data generation spec defined earlier is in conjunction with the customer production data. </p>\n\n\n\n<p>The generated data has various data types that must pass through preprocessing and feature engineering steps. This helps with extracting&nbsp;and transforming variables from the generated data that can be used with further advanced data analytics and supervised or unsupervised learning. </p>\n\n\n\n<h3 id=\"data_processing_transformation_and_feature_engineering\"  class=\"wp-block-heading\">Data processing, transformation, and feature engineering<a href=\"#data_processing_transformation_and_feature_engineering\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In the current deployment for the RUL use case, we looked at various parameters from the machines by collecting data from the thermal, battery, fan, disk, and CPU sensors. Our test included the following tasks, as these are the prerequisites to prepare a dataset to train a machine-learning model:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Reading data</li>\n\n\n\n<li>Feature engineering\n<ul class=\"wp-block-list\">\n<li>Drop duplicates</li>\n\n\n\n<li>Drop NA</li>\n\n\n\n<li>One-shot encoding.</li>\n\n\n\n<li>Data normalization</li>\n</ul>\n</li>\n\n\n\n<li>Correlation analysis</li>\n\n\n\n<li>Group-by operations (calculating mean, std, and max for a group)</li>\n</ul>\n\n\n\n<p>We ran these tasks with <a href=\"https://developer.nvidia.com/blog/rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes/\">RAPIDS cuDF using the pandas accelerator mode</a>. This new mode brings acceleration with zero code changes to the pandas code by adding the command <code>%load_ext cudf.pandas</code> in a Jupyter notebook.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; highlight: [2]; title: ; notranslate\" title=\"\">\n# Magic command for using original pandas code with no-code change with RAPIDS\n%load_ext cudf.pandas\n\n%%time\n#Read/Load data \ndf = pd.read_csv(&quot;/rapids/notebooks/hostdata1/lv_data/q0/t3/part1.csv&quot;)\n\n# Feature engineering\n# Drop duplicates\ndf = df.drop_duplicates() \n\n# Drop rows with null values \ndf = df.dropna() \n\n# Select categorical columns for one-hot encoding categorical_columns = &#x5B;&#039;cat_0&#039;, &#039;cat_1&#039;, &#039;cat_2&#039;, &#039;cat_3&#039;, &#039;cat_4&#039;] \n\n# Perform one-hot encoding df_encoded = pd.get_dummies(df, columns=categorical_columns)\n\n# Normalizing values between 0 and 1 \nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ncolumns_to_normalize = &#x5B;&#039;cont_0&#039;, &#039;cont_1&#039;, &#039;cont_2&#039;, &#039;cont_3&#039;, &#039;cont_4&#039;, &#039;cont_5&#039;, &#039;cont_6&#039;, &#039;cont_7&#039;, &#039;cont_8&#039;, &#039;cont_9&#039;, &#039;cont_10&#039;, &#039;cont_11&#039;, &#039;cont_12&#039;, &#039;cont_13&#039;, &#039;target&#039;,&#039;new_feature_2&#039;, &#039;new_feature_3&#039;, &#039;new_feature_1&#039;] df&#x5B;columns_to_normalize] = scaler.fit_transform(df&#x5B;columns_to_normalize])\n\n# Group-by operations\ngrouped = df.groupby(&#039;cat_0&#039;).agg({&#039;target&#039;: &#x5B;&#039;mean&#039;, &#039;std&#039;], &#039;new_feature_1&#039;: &#039;max&#039;})\n\n# Compute and plot pairwise correlation of columns\n\n# Select continuous features for correlation analysis continuous_features = &#x5B;&#039;cont_0&#039;, &#039;cont_1&#039;, &#039;cont_2&#039;, &#039;cont_3&#039;, &#039;cont_4&#039;, &#039;cont_5&#039;, &#039;cont_6&#039;, &#039;cont_7&#039;, &#039;cont_8&#039;, &#039;cont_9&#039;, &#039;cont_10&#039;, &#039;cont_11&#039;, &#039;cont_12&#039;, &#039;cont_13&#039;, &#039;target&#039;, &#039;new_feature_1&#039;, &#039;new_feature_2&#039;, &#039;new_feature_3&#039;] \n\n# Create a cuxfilter DataFrame \ncux_df = cuxfilter.DataFrame.from_dataframe(df) \n\n# Standardize the data using cuml StandardScaler \n\nscaler = StandardScaler() \ndf_std = scaler.fit_transform(cux_df.data&#x5B;continuous_features]) \n\n# Add the standardized data to cux_df \nfor i, col in enumerate(continuous_features):\n    cux_df.data&#x5B;col] = df_std&#x5B;i] \n\n# Calculate the correlation matrix \ncorrelation_matrix = cux_df.data&#x5B;continuous_features].corr() \n\n# Convert correlation matrix to cuDF DataFrame correlation_matrix_cudf = cudf.DataFrame(correlation_matrix) \n# Use seaborn for plotting \nplt.figure(figsize=(12, 8)) sns.heatmap(correlation_matrix_cudf.to_pandas(), annot=True, cmap=&#039;coolwarm&#039;, fmt=&quot;.2f&quot;) \nplt.title(&#039;Correlation Matrix Heatmap&#039;) \nplt.show()\n</pre></div>\n\n\n<p>This workflow saved the code migration effort with the absolutely no-code change feature. It also provided a speedup of ~171x in performance improvement by calculating the average of the four tasks improvement ratio over CPU for end-to-end workflow, including data preparation, visualization, model training, and tuning (Figure 3).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"596\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1-1024x596.jpg\" alt=\"Diagram shows a 637x speedup for RAPIDS in feature engineering; 39x for group-by operations; 5x for reading data; and 3x for correlation analysis.\" class=\"wp-image-88378\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1-1024x596.jpg 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1-300x175.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1-625x364.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1-179x104.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1-768x447.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1-1536x894.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1-645x376.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1-500x291.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1-155x90.jpg 155w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1-362x211.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1-189x110.jpg 189w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/pulse-speedup-with-rapids-1.jpg 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. PyData compared to RAPIDS speedup&nbsp;</em></figcaption></figure></div>\n\n\n<p>Table 1 provides more details on the exact runtime for each step from Figure 3.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td>&nbsp;</td><td><strong>Steps</strong></td><td><strong>CPU</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>GPU</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Improvement Ratio</strong></td></tr><tr><td rowspan=\"3\"><strong>Data Prep</strong></td><td>Reading Data</td><td>87</td><td class=\"has-text-align-center\" data-align=\"center\">14.9</td><td class=\"has-text-align-center\" data-align=\"center\">5.8</td></tr><tr><td>Feature Engineering</td><td>382</td><td>0.6</td><td class=\"has-text-align-center\" data-align=\"center\">637.7</td></tr><tr><td>Group-by operation</td><td>703</td><td>17.7</td><td class=\"has-text-align-center\" data-align=\"center\">39.8</td></tr><tr><td><strong>Correlation</strong></td><td>Correlation Analysis</td><td>27.1</td><td class=\"has-text-align-center\" data-align=\"center\">8.3</td><td class=\"has-text-align-center\" data-align=\"center\">3.3</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. GPU-accelerated ML workflow&nbsp;measured in time taken (seconds)</em></figcaption></figure>\n\n\n\n<p>Table 1 shows that the compute-intensive workload of feature engineering and group-by operation achieved a tremendous 639x and 39.8x speedups over Pandas. The workflow covered 10 GB of data: rows 43780407 and columns 23.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Our client found these simulated results compelling, and it is now deployed as a proof-of-concept in the customer environment. We anticipate it being taken to production to improve the RUL prediction model by Q4 2024. After this success, LatentView is excited to continue to apply RAPIDS acceleration in modeling projects across manufacturing portfolios.</p>\n\n\n\n<p>To try RAPIDS cuDF yourself, <a href=\"https://developer.nvidia.com/blog/rapids-cudf-instantly-accelerates-pandas-up-to-50x-on-google-colab/\">get started with Google Colab</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The International Society of Automation (ISA) reports that 5% of plant production is lost annually due to downtime. Putting that into a different context, roughly $647B is surrendered on a global basis by manufacturers across all industry segments, the corresponding portion of nearly $13T in production. The challenge at hand is predicting the maintenance needs &hellip; <a href=\"https://developer.nvidia.com/blog/accelerating-predictive-maintenance-in-manufacturing-with-rapids-ai/\">Continued</a></p>\n", "protected": false}, "author": 2264, "featured_media": 88372, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1478144", "discourse_permalink": "https://forums.developer.nvidia.com/t/accelerating-predictive-maintenance-in-manufacturing-with-rapids-ai/305215", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [3273, 453], "coauthors": [3991, 3990, 3989, 3988], "class_list": ["post-87334", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "tag-accelerated-data-analytics", "tag-featured"], "acf": {"post_industry": ["Manufacturing"], "post_products": ["cuDF", "RAPIDS"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVIDIA-RAPIDS-logo.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mIC", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87334"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2264"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87334"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87334/revisions"}], "predecessor-version": [{"id": 88517, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87334/revisions/88517"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88372"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87334"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87334"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87334"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87334"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88363, "date": "2024-08-29T15:18:14", "date_gmt": "2024-08-29T22:18:14", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88363"}, "modified": "2024-09-05T10:57:11", "modified_gmt": "2024-09-05T17:57:11", "slug": "spotlight-clicoh-accelerates-last-mile-delivery-20x-with-nvidia-cuopt", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/spotlight-clicoh-accelerates-last-mile-delivery-20x-with-nvidia-cuopt/", "title": {"rendered": "Spotlight: clicOH Accelerates Last-Mile Delivery 20x with NVIDIA cuOpt"}, "content": {"rendered": "\n<p>Driven by shifts in consumer behavior and the pandemic, e-commerce continues its explosive growth and transformation. As a result, logistics and transportation firms find themselves at the forefront of a parcel delivery revolution. This new reality is especially evident in last-mile delivery, which is now the most expensive element of supply chain logistics. It represents more than <a href=\"https://www.capgemini.com/insights/research-library/the-last-mile-delivery-challenge/\">41% of total supply chain costs</a> across industries, from retail to manufacturing. Understandably, the surging costs of last-mile delivery are prompting efforts to identify and mitigate the underlying causes.</p>\n\n\n\n<p>The last-mile delivery challenge is further complicated by the vehicle routing problem (VRP). A generalization of the traveling salesman problem, VRP asks, &#8220;What is the optimal set of routes that a fleet of vehicles should undertake to make deliveries to a specific set of customers?&#8221; With just 10 delivery destinations, over 3 million permutations and combinations of trips are possible. With 15 destinations, the number of possible routes can exceed 1 trillion. As the number of destinations increases, the corresponding number of possible trips surpasses even the capabilities of the fastest supercomputers. And this doesn\u2019t account for common operational constraints, like fleet availability, navigation capabilities, and access limitations.</p>\n\n\n\n<h2 id=\"transforming_routing_services\"  class=\"wp-block-heading\">Transforming routing services<a href=\"#transforming_routing_services\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>These constraints, along with possible adaptations and the constant evolution within the transportation and logistics field, make it increasingly challenging for businesses to establish, or even outsource, effective route optimization services.&nbsp;</p>\n\n\n\n<p><a href=\"https://clicoh.com/\">clicOH</a>, a member of the <a href=\"https://www.nvidia.com/en-us/startups/\">NVIDIA Inception</a> program for startups, has developed a proprietary routing model to address these challenges. Its solution leverages the latest technologies from NVIDIA, from heuristic and metaheuristic optimization algorithms to machine learning and AI. And, by relying on the efficiency of NVIDIA libraries, clicOH\u2019s application quickly adapts to the different requirements in package distribution density, cost efficiency, and delivery time optimization for last-mile delivery.</p>\n\n\n\n<h2 id=\"optimizing_last-mile_delivery_costs&nbsp;\"  class=\"wp-block-heading\">Optimizing last-mile delivery costs&nbsp;<a href=\"#optimizing_last-mile_delivery_costs&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>clicOH aims to address a range of routing challenges using NVIDIA libraries. For example, the company adopted <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/cuopt/\">NVIDIA cuOpt</a> to support its work related to the traveling salesman problem and to determine optimal delivery routes. The cuOpt library works with GPUs and libraries like <a href=\"https://developer.nvidia.com/rapids\">RAPIDS</a> and <a href=\"https://developer.nvidia.com/cuda-toolkit\">CUDA</a> to generate faster and more accurate delivery routes.&nbsp;</p>\n\n\n\n<p>Additionally, RAPIDS enables clicOH to implement unsupervised machine learning algorithms without the need to modify code, resulting in more efficient data analyses. These unsupervised algorithms enable the clustering of high-demand zip codes for more efficient delivery, as well as the identification of hard-to-reach areas. When combined with NVIDIA cuOpt, these algorithms can process thousands of routings in minutes or even seconds, optimizing delivery times while accounting for local routing constraints. This ultimately reduces delivery costs.</p>\n\n\n\n<p>Using NVIDIA GPUs on AWS development environments, clicOH analyzed thousands of pre-existing routes across multiple cities to map routing inefficiencies. This analysis enabled clicOH to streamline the development of its logistics solution and enhanced the application\u2019s adaptive capabilities.&nbsp;</p>\n\n\n\n<p>clicOH has also developed a deep learning model to optimize delivery times, maximize fleet utilization, and identify zip codes that experience delivery challenges due to scheduling constraints. By optimizing its AI models with NVIDIA accelerated computing, clicOH has achieved a 20x speedup in cluster route planning and a 15% reduction in overall operating costs.</p>\n\n\n\n<p><a href=\"https://clicoh.com/\">Learn more about clicOH accelerated logistics solutions</a>. To further explore how <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/cuopt/\">NVIDIA cuOpt</a> can enhance your fleet routing workflows, visit the <a href=\"https://forums.developer.nvidia.com/c/ai-data-science/nvidia-cuopt/514\">NVIDIA Developer forums</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Driven by shifts in consumer behavior and the pandemic, e-commerce continues its explosive growth and transformation. As a result, logistics and transportation firms find themselves at the forefront of a parcel delivery revolution. This new reality is especially evident in last-mile delivery, which is now the most expensive element of supply chain logistics. It represents &hellip; <a href=\"https://developer.nvidia.com/blog/spotlight-clicoh-accelerates-last-mile-delivery-20x-with-nvidia-cuopt/\">Continued</a></p>\n", "protected": false}, "author": 2270, "featured_media": 88368, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1477562", "discourse_permalink": "https://forums.developer.nvidia.com/t/spotlight-clicoh-accelerates-last-mile-delivery-20x-with-nvidia-cuopt/305075", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 503], "tags": [501, 3157, 453, 3301, 1961], "coauthors": [4003], "class_list": ["post-88363", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-simulation-modeling-design", "tag-automotive", "tag-cuopt", "tag-featured", "tag-route-optimization", "tag-nvidia-inception"], "acf": {"post_industry": ["Retail / Consumer Packaged Goods"], "post_products": ["CUDA", "cuOpt", "RAPIDS"], "post_learning_levels": ["General Interest"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/clicoh-package-delivery.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mZd", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88363"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2270"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88363"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88363/revisions"}], "predecessor-version": [{"id": 88461, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88363/revisions/88461"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88368"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88363"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88363"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88363"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88363"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87823, "date": "2024-08-29T10:00:00", "date_gmt": "2024-08-29T17:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87823"}, "modified": "2024-09-05T10:57:12", "modified_gmt": "2024-09-05T17:57:12", "slug": "boosting-cuda-efficiency-with-essential-techniques-for-new-developers", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/boosting-cuda-efficiency-with-essential-techniques-for-new-developers/", "title": {"rendered": "Boosting CUDA Efficiency with Essential Techniques for New Developers"}, "content": {"rendered": "\n<p>To fully harness the capabilities of NVIDIA GPUs, optimizing <a href=\"https://developer.nvidia.com/cuda-toolkit\">NVIDIA CUDA</a> performance is essential, particularly for developers new to GPU programming. This talk is specifically designed for those stepping into the world of CUDA, providing a solid foundation in GPU architecture principles and optimization techniques.</p>\n\n\n\n<p>Athena Elafrou, a developer technology engineer at NVIDIA, leads a foundational session that dives into the basics of writing high-performance CUDA kernels tailored for NVIDIA GPUs. You&#8217;ll gain insights into critical aspects of GPU architecture, focusing on the <a href=\"https://www.nvidia.com/en-us/data-center/h200/\">NVIDIA H200 Tensor Core GPU</a>, and learn how to use its features to enhance performance.</p>\n\n\n\n<script src=\"https://api-prod.nvidia.com/search/nvidia-search-library.js\"></script>\n \n\n<div id=\"nvidia-event-details-widget\"></div>\n<style>\n.nvidia-search-widget .cleanslate , .nvidia-search-widget .player-overlay {\ndisplay:none;\n}\n</style>\n \n\n<script>\n \n NvidiaSearchLibrary.EventSessionDetailsWidget.mount({\n          site: 'https://www.nvidia.com',\n          language: 'en-us',\n          sessionId: 'gtc24-s62191',\n          jwtToken: '',\n \u2002\u2002\u2002\u2002voltronApiUrl:  'https://api-prod.nvidia.com/services/nod/api/v1/',\n          apiUrl: 'https://api-prod.nvidia.com/search/graphql',\n           onLogin: () => { },\n          onLogout: () => { },\n       \n          onSeeAllSessions: (speakerName) => {\n            window.location.href =  'https://www.nvidia.com/en-us/on-demand/search/?q=\"' + speakerName+'\"';\n          },\n          searchApiUrl: 'https://api-prod.nvidia.com/search/graphql',\n          searchToken: '',\n          uiConfId: '50468382',\n          showSessionRating: false,\n          anonToken: '',\n        });\n \n</script>\n\n\n\n<p>Follow along with a <a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/CUDA-Programming-and-Optimization.pdf\">PDF of the session</a>, which emphasizes fundamental memory access optimization techniques, where you&#8217;ll discover how to boost memory throughput by aligning and coalescing memory accesses. It also explores strategies to increase parallelism in your applications by improving instruction-level parallelism (ILP) and thread-level parallelism (TLP), key techniques for hiding latencies, and maximizing the overall throughput of your CUDA programs.\u00a0</p>\n\n\n\n<p>Additionally, you&#8217;ll learn how to manage atomic operations efficiently through practical examples and tested optimization techniques.</p>\n\n\n\n<p>You\u2019ll walk through real-world examples and performance analyses to provide you with actionable knowledge that you can directly apply to your CUDA development work. Whether you&#8217;re just starting with CUDA or looking to refine your skills, this session will equip you with the tools needed to unlock the power of NVIDIA GPUs.</p>\n\n\n\n<p>Watch the talk <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s62191/\">Introduction to CUDA Programming and Performance Optimization</a>, explore more videos on NVIDIA On-Demand, and gain valuable skills and insights from industry experts by joining the <a href=\"https://developer.nvidia.com/developer-program\">NVIDIA Developer Program</a>.</p>\n\n\n\n<p><em>This content was partially crafted with the assistance of generative AI and LLMs. It underwent careful review and was edited by the NVIDIA Technical Blog team to ensure precision, accuracy, and quality.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>To fully harness the capabilities of NVIDIA GPUs, optimizing NVIDIA CUDA performance is essential, particularly for developers new to GPU programming. This talk is specifically designed for those stepping into the world of CUDA, providing a solid foundation in GPU architecture principles and optimization techniques. Athena Elafrou, a developer technology engineer at NVIDIA, leads a &hellip; <a href=\"https://developer.nvidia.com/blog/boosting-cuda-efficiency-with-essential-techniques-for-new-developers/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 87828, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1475404", "discourse_permalink": "https://forums.developer.nvidia.com/t/boosting-cuda-efficiency-with-essential-techniques-for-new-developers/304792", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696, 1903], "tags": [1935, 453, 3986], "coauthors": [2315], "class_list": ["post-87823", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "category-features", "tag-education-and-training", "tag-featured", "tag-nvidia-on-demand"], "acf": {"post_industry": ["General", "HPC / Scientific Computing"], "post_products": ["CUDA"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Video"], "post_collections": ["GTC March 2024"]}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/CUDA-efficiency.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mQv", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87823"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87823"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87823/revisions"}], "predecessor-version": [{"id": 88014, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87823/revisions/88014"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87828"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87823"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87823"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87823"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87823"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88370, "date": "2024-08-29T09:00:58", "date_gmt": "2024-08-29T16:00:58", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88370"}, "modified": "2024-09-05T10:57:13", "modified_gmt": "2024-09-05T17:57:13", "slug": "just-released-rapids-24-08", "status": "publish", "type": "post", "link": "https://nvda.ws/476hhi6", "title": {"rendered": "Just Released: RAPIDS 24.08"}, "content": {"rendered": "\n<p>RAPIDS 24.08 is now available with significant updates geared towards processing larger workloads and seamless CPU/GPU interoperability.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>RAPIDS 24.08 is now available with significant updates geared towards processing larger workloads and seamless CPU/GPU interoperability.</p>\n", "protected": false}, "author": 1425, "featured_media": 88372, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1477424", "discourse_permalink": "https://forums.developer.nvidia.com/t/just-released-rapids-24-08/305053", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/476hhi6", "_links_to_target": "_blank"}, "categories": [696], "tags": [3273, 453, 1958, 1731], "coauthors": [2878], "class_list": ["post-88370", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "tag-accelerated-data-analytics", "tag-featured", "tag-news", "tag-pandas"], "acf": {"post_industry": ["General"], "post_products": ["cuDF", "cuML", "RAPIDS"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Announcement", "News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVIDIA-RAPIDS-logo.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mZk", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88370"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1425"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88370"}], "version-history": [{"count": 1, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88370/revisions"}], "predecessor-version": [{"id": 88373, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88370/revisions/88373"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88372"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88370"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88370"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88370"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88370"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88017, "date": "2024-08-28T12:30:00", "date_gmt": "2024-08-28T19:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88017"}, "modified": "2024-11-14T07:58:41", "modified_gmt": "2024-11-14T15:58:41", "slug": "boosting-llama-3-1-405b-performance-by-up-to-44-with-nvidia-tensorrt-model-optimizer-on-nvidia-h200-gpus", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/boosting-llama-3-1-405b-performance-by-up-to-44-with-nvidia-tensorrt-model-optimizer-on-nvidia-h200-gpus/", "title": {"rendered": "Boosting Llama 3.1 405B Performance up to 1.44x with NVIDIA TensorRT Model Optimizer on NVIDIA H200 GPUs"}, "content": {"rendered": "\n<p>The Llama 3.1 405B <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language model (LLM)</a>, developed by Meta, is an open-source community model that delivers state-of-the-art performance and supports a variety of use cases.&nbsp;</p>\n\n\n\n<p>With 405 billion parameters and support for context lengths of up to 128K tokens, Llama 3.1 405B is also one of the most demanding LLMs to run. To deliver both low latency to optimize the user experience and high throughput to optimize cost, a high-performance, full-stack platform is required.</p>\n\n\n\n<p>This post shows how the FP8 quantization recipe of <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer\">NVIDIA TensorRT Model Optimizer</a> with <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">NVIDIA TensorRT-LLM</a> delivers up to 1.44x more throughput compared to the <a href=\"https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/\">performance at model launch</a> using Meta\u2019s official Llama 3.1 FP8 quantization recipe. In addition, TensorRT Model Optimizer can fit Llama 3.1 405B on only two GPUs using INT4 AWQ quantization. These TensorRT Model Optimizer improvements will be available in the next release, v0.17, in early September. Learn more about advancements coming to <a href=\"https://developer.nvidia.com/blog/low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch/\">low latency Llama 3.1 on NVIDIA GPUs</a>.</p>\n\n\n\n<h2 id=\"outstanding_llama_31_405b_inference_throughput_with_tensorrt-llm&nbsp;\"  class=\"wp-block-heading\">Outstanding Llama 3.1 405B inference throughput with TensorRT-LLM&nbsp;<a href=\"#outstanding_llama_31_405b_inference_throughput_with_tensorrt-llm&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main\">TensorRT-LLM</a> already delivered outstanding Llama 3.1 405B inference throughput on the day the model was released. This was the result of many <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\">optimizations</a>, such as in-flight batching, KV caching, and optimized attention kernels to accelerate inference performance with lower precision compute.</p>\n\n\n\n<p>TensorRT-LLM added support for the official Llama FP8 quantization recipe at a <a href=\"https://github.com/pytorch/glow/blob/master/docs/Quantization.md#row-wise-quantization\">row-wise</a> granularity level. This involves calculating a static scaling factor for each output weight channel (before execution) and a dynamic scaling factor for each token (during execution) to preserve maximum accuracy.&nbsp;</p>\n\n\n\n<p>TensorRT-LLM also optimizes user-defined kernels, such as the matrix multiplications from <a href=\"https://github.com/pytorch/FBGEMM\">FBGEMM</a> for the Llama 3.1 models using <a href=\"https://nvidia.github.io/TensorRT-LLM/architecture/core-concepts.html#plugins\">plug-ins</a> that are explicitly inserted into the network graph definition at compile time to accelerate execution.</p>\n\n\n\n<h2 id=\"boosting_performance_up_to_144x_with_tensorrt_model_optimizer&nbsp;\"  class=\"wp-block-heading\">Boosting performance up to 1.44x with TensorRT Model Optimizer&nbsp;<a href=\"#boosting_performance_up_to_144x_with_tensorrt_model_optimizer&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To boost performance further, NVIDIA developed a custom FP8 <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/llm_ptq\">post-training quantization (PTQ)</a> recipe. This recipe, available through the <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer\">TensorRT Model Optimizer library</a>, enables higher Llama 3.1 405B throughput and lower latency while delivering the same accuracy. This means that developers can now run the model more cost effectively.&nbsp;</p>\n\n\n\n<p>This custom quantization recipe incorporates FP8 KV cache quantization, as well as self-attention static quantization. The latter technique pre-computes scaling factors at compile time, rather than at run time, reducing inference compute overhead. This scaling is applied at per-tensor granularity.</p>\n\n\n\n<p>Table 1 shows maximum throughput performance, representing \u201coffline\u201d use cases, across a range of input and output sequence lengths, running on a system based on an 8-GPU HGX H200. This system features eight <a href=\"https://www.nvidia.com/en-us/data-center/h200/\">NVIDIA H200 Tensor Core GPUs</a>, each with 141 GB of fast HBM3e memory, and four <a href=\"https://www.nvidia.com/en-us/data-center/nvlink/\">NVLink Switches</a>, providing 900 GB/s of GPU-to-GPU bandwidth between the GPUs.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"4\"><strong>Maximum Throughput Performance \u2013 Output Tokens/Second</strong><br><strong>8 NVIDIA H200 Tensor Core GPUs</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Input | Output Sequence Lengths</td><td class=\"has-text-align-center\" data-align=\"center\">2,048 | 128</td><td class=\"has-text-align-center\" data-align=\"center\">32,768 | 2,048</td><td class=\"has-text-align-center\" data-align=\"center\">120,000 | 2,048</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><br>TensorRT Model Optimizer FP8</td><td class=\"has-text-align-center\" data-align=\"center\">463.1</td><td class=\"has-text-align-center\" data-align=\"center\">320.1</td><td class=\"has-text-align-center\" data-align=\"center\">71.5</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Official Llama FP8 Recipe</td><td class=\"has-text-align-center\" data-align=\"center\">399.9</td><td class=\"has-text-align-center\" data-align=\"center\">230.8</td><td class=\"has-text-align-center\" data-align=\"center\">49.6</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Speedup</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.16x</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.39x</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.44x</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 1. Maximum throughput performance of Llama 3.1 405B with NVIDIA internal measurements&nbsp;</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">TensorRT Model Optimizer recipe data measured on 8/24/2024. Output tokens/second is inclusive of time to generate the first token (tokens/s = total generated tokens&nbsp;/ total latency). DGX H200, TP8, FP8 batch size tuned for maximum node throughput, TensorRT-LLM version 0.13.0.dev2024082000, TensorRT Model Optimizer v0.17.0a (pre-release)\u200b.</p>\n\n\n\n<p>Table 2 shows minimum latency performance using the same input and output sequence lengths.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"4\"><strong>Batch Size = 1 Performance \u2013 Output Tokens/Second</strong><br><strong>8 NVIDIA H200 Tensor Core GPUs</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Input | Output Sequence Lengths</td><td class=\"has-text-align-center\" data-align=\"center\">2,048 | 128</td><td class=\"has-text-align-center\" data-align=\"center\">32,768 | 2,048</td><td class=\"has-text-align-center\" data-align=\"center\">120,000 | 2,048</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">TensorRT Model Optimizer FP8</td><td class=\"has-text-align-center\" data-align=\"center\">49.6</td><td class=\"has-text-align-center\" data-align=\"center\">44.2</td><td class=\"has-text-align-center\" data-align=\"center\">27.2</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Official Llama&nbsp; FP8 Recipe</td><td class=\"has-text-align-center\" data-align=\"center\">37.4</td><td class=\"has-text-align-center\" data-align=\"center\">33.1</td><td class=\"has-text-align-center\" data-align=\"center\">22.8</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Speedup</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.33x</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.33x</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.19x</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. Minimum latency performance of Llama 3.1 405B with NVIDIA internal measurements&nbsp;</em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">TensorRT Model Optimizer recipe data measured on 8/24/2024.&nbsp;Output tokens/second is inclusive of time to generate the first token (tokens/s = total generated tokens&nbsp;/ total latency). DGX H200, TP8, FP8 batch size = 1, TensorRT-LLM version 0.13.0.dev2024082000, TensorRT Model Optimizer v0.17.0a (pre-release)\u200b.&nbsp;</p>\n\n\n\n<p>As these results show, H200 GPUs with TensorRT-LLM and TensorRT Model Optimizer software are delivering notably better performance on Llama 3.1 405B, in both latency-optimized and throughput-optimized scenarios.&nbsp;</p>\n\n\n\n<p>Additionally, the TensorRT Model Optimizer FP8 recipe achieved accuracy on par with the official Llama 3.1 FP8 recipe for both the Massively Multitask Language Understanding (MMLU) and MT-Bench benchmarks. </p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"3\"><strong>Accuracy Benchmarks</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"></td><td class=\"has-text-align-center\" data-align=\"center\">MT-Bench</td><td class=\"has-text-align-center\" data-align=\"center\">MMLU</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Official Llama FP8 Recipe</td><td class=\"has-text-align-center\" data-align=\"center\">9.14</td><td class=\"has-text-align-center\" data-align=\"center\">0.86</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">TensorRT Model Optimizer FP8 Recipe</td><td class=\"has-text-align-center\" data-align=\"center\">9.18</td><td class=\"has-text-align-center\" data-align=\"center\">0.86</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">TensorRT Model Optimizer INT4 AWQ</td><td class=\"has-text-align-center\" data-align=\"center\">9.12</td><td class=\"has-text-align-center\" data-align=\"center\">0.86</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 3. Inference accuracy results of Llama 3.1 405B using MMLU and MT-Bench</em></figcaption></figure>\n\n\n\n<p>The MT-Bench accuracy score with the new PTQ technique and measured with TensorRT-LLM is 9.18 and MMLU benchmark accuracy score is 0.86, compared to 9.14 and 0.86, respectively, using the Meta official FP8 recipe.&nbsp;</p>\n\n\n\n<h2 id=\"fitting_llama_31_405b_on_just_two_h200_gpus_with_int4_awq\"  class=\"wp-block-heading\">Fitting Llama 3.1 405B on just two H200 GPUs with INT4 AWQ<a href=\"#fitting_llama_31_405b_on_just_two_h200_gpus_with_int4_awq\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In addition to the improved FP8 recipe, developers with hardware resource constraints can use <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer/blob/main/llm_ptq/README.md#model-quantization-and-trt-llm-conversion\">INT4 AWQ in TensorRT Model Optimizer</a> to further compress the model. The INT4 AWQ technique reduces the required memory footprint significantly, enabling a very large LLM like Llama 3.1 405B to fit on just two H200 GPUs.&nbsp;</p>\n\n\n\n<p>This works by compressing the weights of the model down to 4-bit integers in the linear layers (Matmuls). The activations are encoded using FP16. The <a href=\"https://arxiv.org/pdf/2306.00978\">activation-aware weight quantization (AWQ)</a> method reduces quantization errors for low-bit weight-only quantization by preserving the salient weights through scaling of the salient weight channels.&nbsp;</p>\n\n\n\n<p>To help achieve this change in precision at high performance and reduce memory usage, TensorRT-LLM features custom kernels. Tables 4 and 5 show the maximum throughput and minimum latency performance measurements. The MT-Bench accuracy score with INT4 AWQ and measured with TensorRT-LLM is 9.12 and MMLU benchmark accuracy score is 0.86, providing comparable accuracy scores to the 9.14 and 0.86, respectively, of the Llama 3.1 official FP8 recipe from Meta.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"4\"><strong>Maximum Throughput Performance \u2013 Output Tokens/Second</strong><br><strong>2 NVIDIA H200 Tensor Core GPUs</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Input | Output Sequence Lengths</td><td class=\"has-text-align-center\" data-align=\"center\">2,048 | 128</td><td class=\"has-text-align-center\" data-align=\"center\">32,768 | 2,048</td><td class=\"has-text-align-center\" data-align=\"center\">60,000 | 2,048</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">TensorRT Model Optimizer INT4 AWQ</td><td class=\"has-text-align-center\" data-align=\"center\">75.6</td><td class=\"has-text-align-center\" data-align=\"center\">28.7</td><td class=\"has-text-align-center\" data-align=\"center\">16.2</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 4. Maximum throughput performance <em>of Llama 3.1 405B</em></em> <em>with NVIDIA internal measurements</em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">TensorRT Model Optimizer recipe data measured on 8/24/2024. Output tokens/second is inclusive of time to generate the first token (tokens/s = total generated tokens&nbsp;/ total latency). DGX H200, TP2, INT4 AWQ batch size tuned for maximum node throughput, TensorRT-LLM version 0.13.0.dev2024082000, TensorRT Model Optimizer v0.17.0a (pre-release)\u200b.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"4\"><strong>Batch Size = 1 Performance \u2013 Output Tokens/Second</strong><br><strong>2 NVIDIA H200 Tensor Core GPUs</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Input | Output Sequence Lengths</td><td class=\"has-text-align-center\" data-align=\"center\">2,048 | 128</td><td class=\"has-text-align-center\" data-align=\"center\">32,768 | 2,048</td><td class=\"has-text-align-center\" data-align=\"center\">60,000 | 2,048</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">TensorRT Model Optimizer INT4 AWQ</td><td class=\"has-text-align-center\" data-align=\"center\">21.6</td><td class=\"has-text-align-center\" data-align=\"center\">18.7</td><td class=\"has-text-align-center\" data-align=\"center\">12.8</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 5. Minimum latency performance <em>of Llama 3.1 405B</em></em> <em>with NVIDIA internal measurements&nbsp;</em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">TensorRT Model Optimizer recipe data measured on 8/24/2024.&nbsp; Output tokens/second is inclusive of time to generate the first token (tokens/s = total generated tokens&nbsp;/ total latency). DGX H200, TP2, INT4 AWQ batch size = 1, TensorRT-LLM version 0.13.0.dev2024082000, TensorRT Model Optimizer v0.17.0a (pre-release).</p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With the NVIDIA accelerated computing platform, you can build models and supercharge your applications with most performant Llama 3.1 models on any platform\u2014from the data center and cloud to local workstations. Enterprises seeking the fastest time to value can leverage <a href=\"https://www.nvidia.com/en-us/ai/#referrer=ai-subdomain\">NVIDIA NIM</a>, part of the <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> software platform, which offers optimized inference on Llama 3.1 models from NVIDIA and its partner ecosystem.</p>\n\n\n\n<p>NVIDIA is committed to advancing, optimizing, and contributing to open-source software and models. Learn more about <a href=\"https://nvidia.github.io/TensorRT-LLM/\">NVIDIA TensorRT-LLM</a> and <a href=\"https://nvidia.github.io/TensorRT-Model-Optimizer/\">NVIDIA TensorRT Model Optimizer</a>. The quantized FP8 and INT4 AWQ checkpoints from Model Optimizer will soon be available for <a href=\"https://huggingface.co/nvidia\">download on Hugging Face</a>.&nbsp;</p>\n\n\n\n<h3 id=\"acknowledgments\"  class=\"wp-block-heading\">Acknowledgments<a href=\"#acknowledgments\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><em>We would like to thank Chenjie Luo, Lalit Vaidya, and Jie-Fang Zhang for their efforts in supporting this post.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>The Llama 3.1 405B large language model (LLM), developed by Meta, is an open-source community model that delivers state-of-the-art performance and supports a variety of use cases.&nbsp; With 405 billion parameters and support for context lengths of up to 128K tokens, Llama 3.1 405B is also one of the most demanding LLMs to run. To &hellip; <a href=\"https://developer.nvidia.com/blog/boosting-llama-3-1-405b-performance-by-up-to-44-with-nvidia-tensorrt-model-optimizer-on-nvidia-h200-gpus/\">Continued</a></p>\n", "protected": false}, "author": 1837, "featured_media": 88025, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1476792", "discourse_permalink": "https://forums.developer.nvidia.com/t/boosting-llama-3-1-405b-performance-up-to-1-44x-with-nvidia-tensorrt-model-optimizer-on-nvidia-h200-gpus/304955", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110], "tags": [296, 453, 4159, 3933, 2932], "coauthors": [3461, 2732, 2940, 3727], "class_list": ["post-88017", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "tag-ai-inference-microservices", "tag-featured", "tag-inference-performance", "tag-llama", "tag-large-language-models"], "acf": {"post_industry": ["General"], "post_products": ["AI Enterprise", "NIM", "NVLink", "TensorRT", "TensorRT-LLM"], "post_learning_levels": ["General Interest", "Intermediate Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/llama-sunglasses-meadow.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mTD", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88017"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1837"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88017"}], "version-history": [{"count": 29, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88017/revisions"}], "predecessor-version": [{"id": 88502, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88017/revisions/88502"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88025"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88017"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88017"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88017"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88017"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87970, "date": "2024-08-28T09:00:00", "date_gmt": "2024-08-28T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87970"}, "modified": "2024-09-05T11:37:49", "modified_gmt": "2024-09-05T18:37:49", "slug": "nvidia-triton-inference-server-achieves-outstanding-performance-in-mlperf-inference-4-1-benchmarks", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-triton-inference-server-achieves-outstanding-performance-in-mlperf-inference-4-1-benchmarks/", "title": {"rendered": "NVIDIA Triton Inference Server Achieves Outstanding Performance in MLPerf Inference 4.1 Benchmarks"}, "content": {"rendered": "\n<p>Six years ago, we embarked on a journey to develop an AI inference serving solution specifically designed for high-throughput and time-sensitive production use cases from the ground up. At that time, ML developers were deploying bespoke, framework-specific AI solutions, which were driving up their operational costs and not meeting their latency and throughput service level agreements.&nbsp;</p>\n\n\n\n<p>We made the decision early on to build a versatile open-source server capable of serving any model, irrespective of its AI backend framework.&nbsp;</p>\n\n\n\n<p>Today, NVIDIA Triton Inference Server stands as one of NVIDIA\u2019s most widely downloaded open-source projects, used by some of the world&#8217;s leading organizations to deploy AI models in production, including <a href=\"https://aws.amazon.com/blogs/machine-learning/how-amazon-music-uses-sagemaker-with-nvidia-to-optimize-ml-training-and-inference-performance-and-cost/\">Amazon</a>, <a href=\"https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/leveraging-nvidia-triton-inference-server-and-azure-ai-for/ba-p/4084776\">Microsoft</a>, <a href=\"https://blogs.nvidia.com/blog/ai-inference-oci-triton/\">Oracle Cloud</a>, <a href=\"https://resources.nvidia.com/en-us-inference-customer-story/american-express-prevents-fraud?ncid=no-ncid\">American Express</a>, <a href=\"https://developer.nvidia.com/blog/enhancing-the-apparel-shopping-experience-with-ai-emoji-aware-ocr-and-snapchats-screenshop/\">Snap</a>, <a href=\"https://www.nvidia.com/en-us/case-studies/docusign-accelerates-agreement-management-with-inference-platform/\">Docusign</a>, and many others.&nbsp;</p>\n\n\n\n<p>We are excited to announce that NVIDIA Triton, running on a system with eight <a href=\"https://www.nvidia.com/en-us/data-center/h200/\">H200 GPUs</a>, has achieved a significant milestone, achieving virtually identical performance compared to the NVIDIA bare-metal submission on the Llama 2 70B benchmark in MLPerf Inference v4.1. This shows that enterprises no longer need to choose between a feature-rich, production-grade AI inference server and peak throughput performance\u2014they can achieve both simultaneously with NVIDIA Triton.</p>\n\n\n\n<p>This post explores the NVIDIA Triton key features that have driven its rapid adoption and provides detailed insights into our MLPerf Inference v4.1 results.</p>\n\n\n\n<h2 id=\"nvidia_triton_key_features\"  class=\"wp-block-heading\">NVIDIA Triton key features<a href=\"#nvidia_triton_key_features\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA Triton is an open-source AI model-serving platform that streamlines and accelerates the deployment of AI inference workloads in production. It helps ML developers and researchers reduce the complexity of model-serving infrastructure, shorten the time needed to deploy new AI models, and increase AI inferencing and prediction capacity. </p>\n\n\n\n<p>NVIDIA Triton key features include the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Universal AI framework support</li>\n\n\n\n<li>Seamless cloud integration</li>\n\n\n\n<li>Business logic scripting</li>\n\n\n\n<li>Model Ensembles</li>\n\n\n\n<li>Model Analyzer</li>\n</ul>\n\n\n\n<h3 id=\"universal_ai_framework_support\"  class=\"wp-block-heading\">Universal AI framework support<a href=\"#universal_ai_framework_support\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>When NVIDIA Triton launched in 2016, it initially supported the <a href=\"https://developer.nvidia.com/tensorrt\">NVIDIA TensorRT backend</a>, an open-source framework for running performant AI models on NVIDIA GPUs. Since then, it has expanded its support to include CPUs and encompass all major frameworks: </p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>TensorFlow</li>\n\n\n\n<li>PyTorch</li>\n\n\n\n<li>ONNX</li>\n\n\n\n<li>OpenVINO</li>\n\n\n\n<li>Python</li>\n\n\n\n<li>RAPIDS FIL</li>\n\n\n\n<li>TensorRT-LLM </li>\n\n\n\n<li>vLLM</li>\n</ul>\n\n\n\n<p>Today, developers using NVIDIA Triton in production use it to accelerate time to market for AI applications. Instead of deploying a new AI framework-specific server for each new use case that arises, you can seamlessly load a new model into an existing NVIDIA Triton production instance, regardless of its backend framework. This capability reduces the time to market for new use cases from <a href=\"https://www.nvidia.com/en-us/case-studies/machine-learning-models-and-inference/\">months to mere minutes</a>.</p>\n\n\n\n<p>NVIDIA Triton also empowers you to streamline operations by eliminating the need to patch, secure, and maintain multiple AI framework\u2013specific inference servers. This reduction in overhead enhances efficiency and enables you to focus more on AI innovation rather than maintenance tasks.</p>\n\n\n\n<h3 id=\"seamless_cloud_integration\"  class=\"wp-block-heading\">Seamless cloud integration<a href=\"#seamless_cloud_integration\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>We collaborated closely with every major cloud service provider to ensure that NVIDIA Triton can be seamlessly deployed in the cloud with minimal or no code required:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/triton.html\">AWS SageMaker</a></li>\n\n\n\n<li><a href=\"https://cloud.google.com/vertex-ai/docs/predictions/using-nvidia-triton\">Google Vertex AI</a></li>\n\n\n\n<li><a href=\"https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-with-triton?view=azureml-api-2&amp;tabs=azure-cli%2Cendpoint\">Azure ML</a></li>\n\n\n\n<li><a href=\"https://blogs.oracle.com/ai-and-datascience/post/oci-nvidia-triton-inference-server\">OCI\u2019s Data Science cloud tools</a></li>\n</ul>\n\n\n\n<p>Whatever you use, NVIDIA Triton integrates deeply with the AI tools that your IT teams are certified and trained on. This integration saves valuable setup time, reduces costs, and accelerates developer productivity.</p>\n\n\n\n<p>For instance, if you use the OCI Data Science platform, deploying NVIDIA Triton is as simple as passing <code>Triton</code> as an environment variable in your command line arguments during model deployment, instantly launching an NVIDIA Triton inference endpoint.&nbsp;</p>\n\n\n\n<p>Likewise, with the Azure ML CLI, you can deploy NVIDIA Triton by adding <code>triton_model</code> to your YAML deployment configuration file.&nbsp;</p>\n\n\n\n<p>GCP provides a one-click deployment option through their GKE-managed clusters, while AWS offers NVIDIA Triton on their AWS Deep Learning containers.</p>\n\n\n\n<p>NVIDIA Triton also uses popular serving protocols like KServe, ensuring that it scales automatically to meet the evolving needs of your users in a Kubernetes cluster.</p>\n\n\n\n<p>If your organization has standardized on any of the major CSP MLOps tools, you will find a&nbsp;no-code or low-code deployment approach to deploying NVIDIA Triton on your favorite cloud.&nbsp;</p>\n\n\n\n<h3 id=\"business_logic_scripting\"  class=\"wp-block-heading\">Business logic scripting<a href=\"#business_logic_scripting\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Recognizing the need for organizations to incorporate custom logic and scripts into their AI workloads to differentiate their use cases and tailor them to their end users, we introduced <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/python_backend/README.html#business-logic-scripting\">business logic scripting (BLS)</a>. This collection of utility functions enables you to seamlessly integrate custom Python or C++ code into production pipelines.&nbsp;</p>\n\n\n\n<p>Companies like <a href=\"https://developer.nvidia.com/blog/enhancing-the-apparel-shopping-experience-with-ai-emoji-aware-ocr-and-snapchats-screenshop/\">SNAP</a> have used scripting to seamlessly transition workloads from notebooks to production environments.</p>\n\n\n\n<h3 id=\"model_ensembles\"  class=\"wp-block-heading\">Model Ensembles<a href=\"#model_ensembles\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Responding to feedback from users who operate integrated AI pipelines rather than standalone models in production, we developed <a href=\"https://developer.nvidia.com/blog/serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models/\">Model Ensembles</a>. This no-code development tool enables enterprises to effortlessly connect pre\u2013 and post-processing workflows into cohesive pipelines without the need for programming.</p>\n\n\n\n<p>You can choose to run the pre\u2013 and post-processing steps on CPUs and the AI model on GPUs to optimize infrastructure costs or run the entire pipeline on GPUs for ultra-low latency applications.</p>\n\n\n\n<h3 id=\"model_analyzer\"  class=\"wp-block-heading\">Model Analyzer<a href=\"#model_analyzer\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>A standout feature of NVIDIA Triton, <a href=\"https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_3-optimizing_triton_configuration\">Model Analyzer</a> enables you to experiment with various deployment configurations by adjusting the number of concurrent models loaded on the GPU and the number of requests that are batched together during inference run time. It then visually maps out these configurations on an intuitive chart, facilitating the quick identification and deployment of the most efficient setup for production use.</p>\n\n\n\n<p>For organizations deploying LLMs, we introduced <a href=\"https://developer.nvidia.com/blog/measuring-generative-ai-model-performance-using-nvidia-genai-perf-and-an-openai-compatible-api/\">GenA-Perf</a>, a new generative AI performance benchmarking tool designed specifically to provide generative AI performance metrics, including first-token latency and token-to-token latency.&nbsp;</p>\n\n\n\n<h2 id=\"exceptional_throughput_results_at_mlperf_41&nbsp;\"  class=\"wp-block-heading\">Exceptional throughput results at MLPerf 4.1&nbsp;<a href=\"#exceptional_throughput_results_at_mlperf_41&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>At this year&#8217;s MLPerf Inf v4.1 benchmark, hosted by <a href=\"https://mlcommons.org/benchmarks/inference-datacenter/\">MLCommons</a>, we showcased the performance of NVIDIA Triton on a TensorRT-LLM optimized Llama-v2-70B model. We wanted to demonstrate that enterprises can use the advanced production-grade capabilities of NVIDIA Triton without incurring the high latency and throughput overhead typically associated with inference serving platforms.</p>\n\n\n\n<p>In our submissions, NVIDIA Triton achieved virtually identical performance to our submissions that did not use NVIDIA Triton (bare-metal). This demonstrates that enterprises no longer have to trade off and choose between a feature-rich, production-grade AI inference server and peak throughput performance. They can accomplish both at the same time with NVIDIA Triton, as demonstrated by the exceptional performance in MLPerf v4.1.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"477\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mlperf-llama-2-70b-performance-comparison-625x477.jpg\" alt=\"An image showing MLPerf results on Llama 2 70B with and without Triton Inference Server. \" class=\"wp-image-88080\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mlperf-llama-2-70b-performance-comparison-625x477.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mlperf-llama-2-70b-performance-comparison-300x229.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mlperf-llama-2-70b-performance-comparison-151x115.jpg 151w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mlperf-llama-2-70b-performance-comparison-768x586.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mlperf-llama-2-70b-performance-comparison-645x492.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mlperf-llama-2-70b-performance-comparison-393x300.jpg 393w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mlperf-llama-2-70b-performance-comparison-118x90.jpg 118w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mlperf-llama-2-70b-performance-comparison-362x276.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mlperf-llama-2-70b-performance-comparison-144x110.jpg 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mlperf-llama-2-70b-performance-comparison-1024x781.jpg 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mlperf-llama-2-70b-performance-comparison.jpg 1146w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. MLPerf Llama 2 70B performance</em></figcaption></figure></div>\n\n\n<p class=\"has-small-font-size\">MLPerf Inference v4.1 Closed, Data Center. Results retrieved from<a href=\"http://www.mlperf.org/\"> www.mlperf.org</a> on August 28, 2024. All results using eight GPUs and retrieved from the following entries :4.1-0048, 4.1-0050. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. For more information, see<a href=\"http://www.mlcommons.org/\"> www.mlcommons.org</a>.</p>\n\n\n\n<h3 id=\"mlperf_benchmark_submission_details\"  class=\"wp-block-heading\">MLPerf benchmark submission details<a href=\"#mlperf_benchmark_submission_details\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Our submission consisted of the following scenarios:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Offline:</strong> All the inputs of the workload are passed to the inference serving system at one time as a single batch.&nbsp;&nbsp;</li>\n\n\n\n<li><strong>Server:</strong> Input requests are sent discretely to the inference serving system to mimic real-world production deployments. This scenario is more challenging as it imposes strict constraints on first-token latency and inter-token latency, placing stringent expectations on responsiveness and the speed of the inference system.</li>\n</ul>\n\n\n\n<p>The NVIDIA Triton implementation of the benchmark included a client and server. For the client, we used the <a href=\"https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/grpc/_client.py#L119\">NVIDIA Triton gRPC client</a> to communicate to the NVIDIA Triton server instance and the benchmark\u2019s load generator. This enabled a simplistic and readable Pythonic interface into an LLM inference server.&nbsp;</p>\n\n\n\n<p>For the server, we used <a href=\"https://github.com/triton-inference-server/server/tree/mlperf-inf-v4.1\">NVIDIA Triton Inference Server</a> to provide a gRPC endpoint to interact with <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">TensorRT-LLM</a>, loosely coupled to it with the <a href=\"https://github.com/triton-inference-server/tensorrtllm_backend/tree/97feb8fbca593c6b89eb9d4ed7f016bf37ceb848\">TensorRT-LLM backend</a>. The server is agnostic to the version of TensorRT-LLM, as implementation details are abstracted into the backend to provide peak performance.</p>\n\n\n\n<p>For round v4,1, we submitted our NVIDIA Triton implementation of the benchmark under <a href=\"https://github.com/mlcommons/inference_policies/blob/master/inference_rules.adoc#411-constraints-for-the-closed-division\">closed division</a>, which meant that both client and server run on the same node, communicating through a loopback interface. This implementation can be easily extended to multi-node scenarios with the client running on a separate node from the server, communicating with each other using gRPC.&nbsp;</p>\n\n\n\n<p>NVIDIA Triton also supports an <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/protocol/extension_generate.html#generate-extension\">HTTP communication</a> option that could be used for generative AI workloads.</p>\n\n\n\n<h2 id=\"next_in-person_user_meetup&nbsp;\"  class=\"wp-block-heading\">Next in-person user meetup&nbsp;<a href=\"#next_in-person_user_meetup&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>While we are excited about our achievements so far, the NVIDIA Triton journey keeps going.&nbsp;</p>\n\n\n\n<p>To foster ongoing open-source innovation, we are thrilled to announce the next NVIDIA Triton user meetup on September 9, 2024, at the Fort Mason Center For Arts &amp; Culture in San Francisco, where we will share new LLM features and envision the future together. <a href=\"https://lu.ma/87q3nvnh\">Register for the NVIDIA Triton meetup now.</a>&nbsp;</p>\n\n\n\n<p>We look forward to seeing you at the event as we embark on the next phase of the NVIDIA Triton journey.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Six years ago, we embarked on a journey to develop an AI inference serving solution specifically designed for high-throughput and time-sensitive production use cases from the ground up. At that time, ML developers were deploying bespoke, framework-specific AI solutions, which were driving up their operational costs and not meeting their latency and throughput service level &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-triton-inference-server-achieves-outstanding-performance-in-mlperf-inference-4-1-benchmarks/\">Continued</a></p>\n", "protected": false}, "author": 2008, "featured_media": 87973, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1476709", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-triton-inference-server-achieves-outstanding-performance-in-mlperf-inference-4-1-benchmarks/304939", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 3110], "tags": [296, 453, 2932], "coauthors": [3708, 2732, 3998], "class_list": ["post-87970", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-data-science", "category-generative-ai", "tag-ai-inference-microservices", "tag-featured", "tag-large-language-models"], "acf": {"post_industry": ["General"], "post_products": ["Triton Inference Server"], "post_learning_levels": ["General Interest"], "post_content_types": ["Benchmark"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/triton-inference-service-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mSS", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87970"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2008"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87970"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87970/revisions"}], "predecessor-version": [{"id": 88082, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87970/revisions/88082"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87973"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87970"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87970"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87970"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87970"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87263, "date": "2024-08-28T09:00:00", "date_gmt": "2024-08-28T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87263"}, "modified": "2024-09-09T12:37:08", "modified_gmt": "2024-09-09T19:37:08", "slug": "new-foundational-models-and-training-capabilities-with-nvidia-tao-5-5", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/new-foundational-models-and-training-capabilities-with-nvidia-tao-5-5/", "title": {"rendered": "New Foundational Models and Training Capabilities with NVIDIA TAO 5.5"}, "content": {"rendered": "\n<p><a href=\"https://developer.nvidia.com/tao-toolkit\">NVIDIA TAO</a> is a framework designed to simplify and accelerate the development and deployment of AI models. It enables you to use pretrained models, fine-tune them with your own data, and optimize the models for specific use cases without needing deep AI expertise.</p>\n\n\n\n<p>TAO integrates seamlessly with the NVIDIA hardware and software ecosystem, providing tools for efficient AI model training, deployment, and inferencing and speeding up time-to-market for AI-driven applications.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram.png\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"552\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-1024x552.png\" alt=\"Flowchart shows the integration of models from the NGC Model Catalog and ONNX Model Zoo into TAO, which includes components like AutoML, Training &amp; Optimization, AI-assisted annotation, and Augmentation. They are connected through an API gateway, which interfaces with training data and REST APIs.\" class=\"wp-image-87653\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-1024x552.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-300x162.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-625x337.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-179x97.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-768x414.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-1536x828.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-2048x1104.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-645x348.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-500x270.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-160x86.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-362x195.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/metropolis-iva-tao-5.5-release-diagram-204x110.png 204w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA TAO architecture&nbsp;&nbsp;</em></figcaption></figure></div>\n\n\n<p>Figure 1 shows that TAO supports frameworks like PyTorch, TensorFlow, and ONNX. Training can be done on multiple platforms, and the resulting models can be deployed on various inference platforms, such as GPU, CPU, MCU, and DLA.</p>\n\n\n\n<p>NVIDIA just released TAO 5.5, bringing state-of-the-art foundation models and groundbreaking features to enhance any AI model development. The new features include the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Multi-modal sensor fusion models:</strong> Integrate data from multiple sensors into a unified bird&#8217;s-eye view (BEV) representation, preserving both geometric and semantic information.</li>\n\n\n\n<li><strong>Auto-labeling with text prompts:</strong> Automatically create label datasets for object detection and segmentation using text prompts.</li>\n\n\n\n<li><strong>Open-vocabulary detection:</strong> Identify objects from any category using natural language descriptions instead of predefined labels.</li>\n\n\n\n<li><strong>Knowledge distillation: </strong>Create smaller, more efficient, and accurate networks from the knowledge of larger networks.</li>\n</ul>\n\n\n\n<p>In this post, we discuss the new features of TAO 5.5 in more detail.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>Model</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>NVIDIA TensorRT Optimized</strong></td><td><strong>Training Dataset</strong></td><td><strong>Supported Backbone</strong></td></tr><tr><td><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/grounding_dino\">GroundingDINO</a></td><td class=\"has-text-align-center\" data-align=\"center\">\u2713</td><td>Real data: 1.8M images,<br>Labels: 14.5M instances of object detection and grounding annotations with Auto-labeler</td><td>swin_tiny_224_1k<br>swin_base_224_22k<br>swin_base_384_22k<br>swin_large_224_22k<br>swin_large_384_22k</td></tr><tr><td><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/mask_grounding_dino\">Mask-GroundingDINO</a></td><td class=\"has-text-align-center\" data-align=\"center\"><br>\u2713</td><td><a href=\"https://arxiv.org/abs/2304.07193\">DINOv2-L<br></a><a href=\"https://arxiv.org/abs/2103.00020\">CLIP-B</a> (pretrained with an NVIDIA proprietary dataset)</td><td>swin_tiny_224_1k<br>swin_base_224_22k<br>swin_base_384_22k<br>swin_large_224_22k<br>swin_large_384_22k</td></tr><tr><td><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/bevfusion\">BEVFusion</a></td><td class=\"has-text-align-center\" data-align=\"center\">&lt;coming up&gt;</td><td>Synthetic data: ~5M images</td><td>Image: Swin-Transformers and ResNet-50Lidar: Second</td></tr><tr><td><a href=\"https://build.nvidia.com/nvidia/nvclip\" target=\"_blank\" rel=\"noreferrer noopener\">NVCLIP</a></td><td class=\"has-text-align-center\" data-align=\"center\">\u2713</td><td>Real data: 700M images</td><td>ViT-H-336<br>ViT-L-336</td></tr><tr><td><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/segic\">SEGIC</a></td><td class=\"has-text-align-center\" data-align=\"center\">\u2713</td><td>Real data: 110K images<br>Synthetic data: 50K images</td><td><a href=\"https://arxiv.org/abs/2304.07193\">DINOv2-L<br></a><a href=\"https://arxiv.org/abs/2103.00020\">CLIP-B</a> (pretrained with an NVIDIA proprietary dataset)</td></tr><tr><td><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/foundationpose\">FoundationPose</a></td><td class=\"has-text-align-center\" data-align=\"center\">&nbsp; &nbsp; \u2713&nbsp;&nbsp;&nbsp;&nbsp;</td><td>Synthetic data: 1M images&nbsp;</td><td>&#8211;</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. New foundation and multi-modal models in TAO 5.5&nbsp;</em></figcaption></figure>\n\n\n\n<p>NVIDIA TAO integrates open-source, foundation, and proprietary models, all trained on extensive proprietary and commercially viable datasets, making them versatile for tasks such as object detection, pose detection, image classification, segmentation, and so on. TAO simplifies fine-tuning these models for specific use cases, making it easy to customize and deploy them commercially.&nbsp;</p>\n\n\n\n<p>Most models are accelerated by NVIDIA TensorRT and optimized for performance on NVIDIA hardware, ensuring powerful and efficient AI solutions.</p>\n\n\n\n<p>Swapping model backbones in TAO is straightforward and requires no coding, just a simple configuration change. This flexibility enables you to experiment with different architectures like ResNet, shifted window (Swin) transformers, and Fully Attentional Network (FAN), tailoring models to specific needs.</p>\n\n\n\n<p>This ease of customization supports diverse applications, such as product identification in retail, medical imaging classification in healthcare, robotic assembly monitoring in manufacturing, and traffic management in smart cities.</p>\n\n\n\n<h2 id=\"groundingdino_model\"  class=\"wp-block-heading\"><strong>GroundingDINO model</strong><a href=\"#groundingdino_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/8z7urVmLrPw?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Open vocabulary object detection with NVIDIA GroundingDINO</em></figcaption></figure>\n\n\n\n<p>Traditional object detection models are limited to recognizing objects from predefined categories (closed-set detection). This constraint makes them ineffective for applications requiring the identification of arbitrary objects based on human inputs, such as specific category names or detailed referring expressions.</p>\n\n\n\n<p><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/grounding_dino\">GroundingDINO</a> addresses this limitation by integrating a text encoder into the DINO model, transforming it into an open-set object detector. This enables the model to detect any object described by human inputs.&nbsp;</p>\n\n\n\n<p>GroundingDINO effectively fuses language and vision modalities by employing a feature enhancer, language-guided query selection, and a cross-modality decoder. This enables the model to generalize concepts beyond predefined categories, achieving superior performance.</p>\n\n\n\n<h3 id=\"groundingdino_with_tao\"  class=\"wp-block-heading\"><strong>GroundingDINO with TAO</strong><a href=\"#groundingdino_with_tao\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>This model was trained with a Swin-Tiny backbone using a supervised manner on the NVIDIA proprietary dataset for commercial usage. In addition, BERT-Base was used as the starting weight for the text tower. Finally, GroundingDINO was trained end-to-end on about 1.8M images that were collected from publicly available datasets.&nbsp;</p>\n\n\n\n<p>All the raw images used during training have commercial licenses to ensure safe commercial usage.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>Model</strong></td><td><strong>Precision</strong></td><td><strong>mAP</strong></td><td><strong>mAP50</strong></td><td><strong>mAP75</strong></td><td><strong>mAPs</strong></td><td><strong>mAPm</strong></td><td><strong>mAPI</strong></td></tr><tr><td>grounding_dino_swin_tiny</td><td>BF16</td><td>46.1</td><td>59.9</td><td>51.0</td><td>30.5</td><td>49.3</td><td>60.8</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. GroundingDINO KPIs on the COCO validation dataset</em></figcaption></figure>\n\n\n\n<h2 id=\"mask-groundingdino_model\"  class=\"wp-block-heading\"><strong>Mask-GroundingDINO model</strong><a href=\"#mask-groundingdino_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/mask_grounding_dino\">Mask-GroundingDINO</a> is a single-stage, open-vocabulary instance segmentation model that generates a segmentation mask around a specific instance of an object. This model is built on top of the GroundingDINO architecture and Conditional Convolutions for Instance Segmentation (CondInst) inspire the segmentation graph.<mark style=\"background-color:#fcb900\" class=\"has-inline-color\"></mark></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding.jpg\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"260\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-1024x260.jpg\" alt=\"Diagram shows input as an image and a prompt or list of objects. The image is processed through an image backbone such as Swin and text is processed through a text backbone such as BERT. A feature enhancer is added to align both image and text-based features. Image and text features then go to a query selection and a cross-modality decoder. The decoded features then go to Mask features and eventually go to Mask FCN where it gets combined with object query followed by convolutions and controller. Eventually, it outputs the mask and bbox for the object being requested in the input query.\" class=\"wp-image-87658\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-1024x260.jpg 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-300x76.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-625x159.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-179x45.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-768x195.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-1536x390.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-2048x520.jpg 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-645x164.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-500x127.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-160x41.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-362x92.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/MaskGrounding-433x110.jpg 433w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 2. Mask-GroundingDINO model architecture</em></figcaption></figure></div>\n\n\n<h3 id=\"mask-groundingdino_with_tao\"  class=\"wp-block-heading\"><strong>Mask-GroundingDINO with TAO</strong><a href=\"#mask-groundingdino_with_tao\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>CondInst proposes a conditional convolution head for instance segmentation, which updates its convolutional kernel weights based on the input or feature maps. We extended the mask branch and instance-aware mask heads originally designed only for CNNs in CondInst to Transformers or query-based models.&nbsp;</p>\n\n\n\n<p>In Mask-GroundingDINO, we chose to use the same encoder and decoder design as in DINO. Still, similar connections can be made to other Transformer encoders and decoders.</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Input:</strong> Text or image.\n<ul class=\"wp-block-list\">\n<li>Text can be either a list of categories (box, forklift) or a scene/object description in natural language.</li>\n</ul>\n</li>\n\n\n\n<li><strong>Backbone:</strong> Text or image.\n<ul class=\"wp-block-list\">\n<li>Text backbone can be BERT or other NLP models.</li>\n\n\n\n<li>Image backbone can be a Swin Transformer model or other Transformer or CNN-based backbones.</li>\n</ul>\n</li>\n\n\n\n<li><strong>Feature Enhancer:</strong> A stack of self-attention, text-to-image cross-attention, and image-to-text cross-attention layers that promote multimodal alignment.</li>\n\n\n\n<li><strong>Query Selection module:</strong> Selects top K queries based on the dot product between text features and image features.</li>\n\n\n\n<li><strong>Mask FCN head:</strong> A few convolutional layers with dynamic kernels.</li>\n\n\n\n<li><strong>Controller:</strong> A few convolutional layers that predict the weights of the dynamic kernels in the Mask FCN head.</li>\n</ul>\n\n\n\n<p>This model was fine-tuned with the commercial GroundingDINO pretrained model on the pseudo-labeled <a href=\"https://github.com/openimages/dataset\">Open Images dataset</a>, which allows commercial usage.<strong> </strong>The model uses a Swin-Tiny backbone and its segmentation head was fine-tuned end-to-end on about 830K images that have pseudo-labeled ground-truth masks.&nbsp;</p>\n\n\n\n<p>You can use this model as is or fine-tune it with TAO. For fine-tuning, we provide the <a href=\"https://github.com/NVIDIA/tao_tutorials/tree/main/notebooks/tao_launcher_starter_kit/mask_grounding_dino/mask_grounding_dino.ipynb\">Mask-GroundingDINO</a> Jupyter notebook for an easy and interactive environment where minimal coding is required.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>Model</strong></td><td><strong>Precision</strong></td><td><strong>mAP</strong></td><td><strong>mAP50</strong></td><td><strong>mask_mAP</strong></td><td><strong>mask_mAP50</strong></td></tr><tr><td>mask_grounding_dino_swin_tiny</td><td>BF16</td><td>47.5</td><td>61.7</td><td>32.9</td><td>55.7</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 3. Mask-GroundingDINO KPIs on the COCO validation dataset</em></figcaption></figure>\n\n\n\n<h2 id=\"bevfusion_model\"  class=\"wp-block-heading\"><strong>BEVFusion model</strong><a href=\"#bevfusion_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/I8RONnecD-s?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 2. <em>Create Custom Multi-Modal Fusion Models</em></em></em></figcaption></figure>\n\n\n\n<p>In many domains, from autonomous driving to robotics and smart cities, systems rely on various sensors to perceive and interact with their environment. Each sensor type, such as cameras, LiDAR, or radar, provides unique information but also has inherent limitations. For example, cameras capture rich visual details but struggle with depth perception, while LiDAR excels in measuring distances but lacks semantic context.&nbsp;</p>\n\n\n\n<p>The challenge is effectively combining these diverse data sources to improve system performance and reliability.</p>\n\n\n\n<p><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/bevfusion\">BEVFusion</a> offers a solution to this challenge by integrating data from multiple sensors into a unified bird&#8217;s-eye view (BEV) representation. It revolutionizes this process by unifying multi-modal features into a shared BEV representation, preserving both geometric (from LiDAR) and semantic (from camera) information.</p>\n\n\n\n<h3 id=\"bevfusion_with_tao\"  class=\"wp-block-heading\"><strong>BEVFusion with TAO</strong><a href=\"#bevfusion_with_tao\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In TAO, we began with the <a href=\"https://arxiv.org/pdf/2205.13542\">BEVFusion</a> codebase from mmdet3d to train our model. The original code was enhanced to handle three angles in 3D space (roll, pitch, yaw), whereas initially, it supported only yaw. The TAO-trained BEVFusion model detects people within a single camera view and a LiDAR and creates a 3-D bounding box around people.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>Content</strong></td><td><strong>AP11</strong></td><td><strong>AP40</strong></td></tr><tr><td>Evaluation Set</td><td>77.71%</td><td>79.35%</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 4. TAO BEVFusion KPIs on the COCO validation dataset&nbsp; for a single camera, single Lidar people detection</em></figcaption></figure>\n\n\n\n<h2 id=\"clip_model\"  class=\"wp-block-heading\"><strong>CLIP model</strong><a href=\"#clip_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Efficiently learning meaningful and contextually rich representations of both images and text is a significant challenge, especially for applications requiring multimodal understanding. Use cases such as zero-shot learning, image captioning, visual search, content moderation, and multimodal interaction demand robust models that can accurately interpret and integrate visual and textual data.</p>\n\n\n\n<p>The Contrastive Language-Image Pre-training (CLIP) model addresses this challenge by employing a dual encoder architecture to simultaneously process images and text. Using contrastive learning, CLIP maximizes the similarity between correct image-text pairs while minimizing it for incorrect pairs, enabling the model to learn general representations that capture a wide range of concepts.&nbsp;</p>\n\n\n\n<p>This approach enables CLIP to excel in various applications, such as generating descriptive image captions, performing visual searches based on textual queries, and enabling zero-shot learning, where the model can classify images into unseen categories by leveraging its learned representations.</p>\n\n\n\n<h3 id=\"nvclip_with_tao\"  class=\"wp-block-heading\"><strong>NVCLIP with TAO</strong><a href=\"#nvclip_with_tao\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>In TAO, we provide a pretrained TensorRT-accelerated CLIP model on the NVIDIA dataset. We used ~700M images to train this model. For evaluation, we used 50K validation images from the ImageNet dataset.</p>\n\n\n\n<p>You can deploy the model from NVIDIA Jetson to NVIDIA Ampere architecture GPUs.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>Model</strong></td><td><strong>Top-1 accuracy</strong></td></tr><tr><td>ViT-H-336</td><td>0.7786</td></tr><tr><td>ViT-L-336</td><td>0.7629</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 5. Zero-shot ImageNet validation accuracy of NVCLIP with ViT-H-336 and ViT-L-336 backbones</em></figcaption></figure>\n\n\n\n<h2 id=\"segic_model\"  class=\"wp-block-heading\">SEGIC model<a href=\"#segic_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/KvZpYO0hYdc?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 3. Use visual prompt for In-context segmentation with NVIDIA TAO</em></figcaption></figure>\n\n\n\n<p><a href=\"https://arxiv.org/abs/2311.14671\">SEGIC</a> is an innovative end-to-end segment-in-context framework that revolutionizes in-context segmentation with a single-vision foundation model (VFM).&nbsp;</p>\n\n\n\n<p>Unlike traditional methods, SEGIC captures dense relationships between target images and in-context examples, extracting geometric, visual, and meta instructions to guide the final mask prediction. This approach significantly reduces labeling and training costs while achieving state-of-the-art performance on one-shot segmentation benchmarks.&nbsp;</p>\n\n\n\n<p>SEGIC&#8217;s versatility extends to diverse tasks, including video object segmentation and open-vocabulary segmentation, making it a powerful tool in the field of image segmentation.</p>\n\n\n\n<h3 id=\"segic_with_tao\"  class=\"wp-block-heading\"><strong>SEGIC with TAO</strong><a href=\"#segic_with_tao\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>For the SEGIC model training in TAO, we provided a pretrained ONNX model and a deployment recipe with <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a>. For training, we used the following resources:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Image encoder:</strong> DINOv2-L, trained with proprietary images from NVIDIA.</li>\n\n\n\n<li><strong>Meta description encoder:</strong> CLIP-B, trained with proprietary images from NVIDIA.</li>\n</ul>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>Model</strong></td><td><strong>mIOU</strong></td></tr><tr><td>SEGIC: <a href=\"https://arxiv.org/abs/2304.07193\">DINOv2-L</a> , <a href=\"https://arxiv.org/abs/2103.00020\">&nbsp;CLIP-B</a>, mask_decoder</td><td>69.57</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 6. SEGIC model KPIs evaluated against the </em><a href=\"https://paperswithcode.com/sota/few-shot-semantic-segmentation-on-coco-20i-1\">COCO20i</a><em> dataset</em></figcaption></figure>\n\n\n\n<h2 id=\"foundationpose_model\"  class=\"wp-block-heading\"><strong>FoundationPose model</strong><a href=\"#foundationpose_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/z7qBoTIXA_M?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 4. 6D object pose estimation and tracking with the NVIDIA TAO FoundationPose model</em></figcaption></figure>\n\n\n\n<p>Accurate 6D object pose estimation and tracking are crucial for various applications, such as robotics, augmented reality, and autonomous driving. However, existing methods often require extensive fine-tuning and are limited by their dependency on either model-based or model-free setups. This creates a challenge in achieving robust performance across different scenarios and objects, particularly when CAD models are unavailable or when dealing with novel objects.</p>\n\n\n\n<p><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/foundationpose\">FoundationPose</a> addresses this problem by providing a unified foundation model for 6D object pose estimation and tracking that supports both model-based and model-free setups. It can be applied instantly to novel objects at test time, leveraging either CAD models or a few reference images.</p>\n\n\n\n<p>By using a neural implicit representation for novel view synthesis and employing large-scale synthetic training with advanced techniques like an LLM, a novel transformer-based architecture, and contrastive learning, FoundationPose achieves strong generalizability and outperforms specialized methods across various public datasets. </p>\n\n\n\n<p>For example, in augmented reality applications, FoundationPose enables seamless integration of virtual objects into real-world environments by accurately estimating and tracking the pose of objects without the need for extensive manual adjustments.</p>\n\n\n\n<h3 id=\"foundationpose_with_tao\"  class=\"wp-block-heading\">FoundationPose with TAO<a href=\"#foundationpose_with_tao\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>We provided a pretrained ONNX model and deployment recipe with <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a>. Our training data consisted of scenes rendered in high-quality photorealism, using 3D assets from Scanned Objects by Google Research and Objaverse. Each data point includes RGB images, depth information, object poses, camera poses, instance segmentation, and 2D bounding boxes, with extensive domain randomization. We then created ~1M synthetic images with <a href=\"https://developer.nvidia.com/isaac/sim\">NVIDIA Isaac Sim</a>.</p>\n\n\n\n<p>In the worldwide BOP leaderboard, model-based novel object pose estimation was at the #1 position as of March 2024 (Figure 3).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"486\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark-1024x486.png\" alt=\"Screenshot of the NVIDIA FoundationPose model securing the top position on the leaderboard that shows the overall ranking for Model-based 6D localization of unseen objects on the BOP-Classic-Core benchmark.\" class=\"wp-image-87280\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark-1024x486.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark-300x142.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark-625x296.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark-179x85.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark-768x364.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark-1536x728.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark-645x306.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark-500x237.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark-160x76.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark-362x172.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark-232x110.png 232w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bop-leaderboard-benchmark.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Benchmark for 6D object pose estimation</em></figcaption></figure></div>\n\n\n<h2 id=\"prompt-based_auto-labeling_for_object_detection_and_segmentation\"  class=\"wp-block-heading\">Prompt-based auto-labeling for object detection and segmentation<a href=\"#prompt-based_auto-labeling_for_object_detection_and_segmentation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Having a well-labeled dataset is essential for training and fine-tuning models, especially in tasks like object detection and segmentation. However, obtaining a labeled dataset for new categories and instances takes a lot of time and effort. This is particularly true for instance segmentation, where annotating each object with precise masks can take up to 10x longer than drawing simple bounding boxes.&nbsp;</p>\n\n\n\n<p>This is where a simple-to-use, prompt-based, auto-labeling method can help:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/grounding_dino\">GroundingDINO</a>: An open-vocabulary object detection model that uses prompts such as object categories to generate bounding boxes.&nbsp;</li>\n\n\n\n<li><a href=\"https://docs.nvidia.com/tao/tao-toolkit/text/instance_segmentation/mal.html#mask-auto-labeler\">Mask Auto-Labeler</a>: A Transformer-based framework that uses the bounding boxes to produce high-quality instance segmentation masks.&nbsp;</li>\n</ul>\n\n\n\n<p>This combination significantly reduces the effort required to create detailed labels, making it easier and faster to build robust datasets for training advanced models.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"210\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-1024x210.jpg\" alt=\"Diagram shows that the pipeline takes input as an image along with class labels such as person, and helmet. The input is processed with the GroundingDINO model, which outputs a list of bounding boxes for the defined class labels. These bounding boxes are then processed with a Mask Auto-labeler tool, which output ssegmentation masks for these objects.\" class=\"wp-image-87727\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-1024x210.jpg 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-300x62.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-625x128.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-179x37.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-768x158.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-1536x316.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-2048x421.jpg 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-645x133.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-500x103.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-160x33.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-362x74.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/AutoLabeling-535x110.jpg 535w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Auto-labeling with the GroundingDINO model and Mask Auto-labeler tool&nbsp;</em></figcaption></figure></div>\n\n\n<p>We provide a complete end-to-end Jupyter notebook (<a href=\"https://github.com/NVIDIA/tao_tutorials/tree/main/notebooks/tao_data_services/text2box/text2box.ipynb\">text2box.ipynb</a>) for <a href=\"https://docs.nvidia.com/tao/tao-toolkit/text/data_services/auto-label.html?highlight=mal#auto-label\">auto labeling</a> with no coding needed. We provide two spec files, one for <a href=\"https://docs.nvidia.com/tao/tao-toolkit/text/ata_services/auto-label.html?highlight=mal#grounding-dino-configuration\">bbox labels</a> and another for <a href=\"https://docs.nvidia.com/tao/tao-toolkit/text/ata_services/auto-label.html?highlight=mal#mal-configuration\">segmentation</a>, where you can define the objects to label, for example [person, helmet], the path of the unlabeled dataset, and the result directory in which to store the labels.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Z0JIpC2SGSM?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 5. Use text prompts for auto-labeling with NVIDIA TAO</em></figcaption></figure>\n\n\n\n<p>Generate the auto labels with the <code>generate</code> command:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ntao dataset auto_label generate &#x5B;-h] -e &lt;spec file&gt;\n                                &#x5B;results_dir=&lt;results_dir&gt;]\n                                &#x5B;num_gpus=&lt;num_gpus&gt;]\n</pre></div>\n\n\n<h2 id=\"efficient_ai_with_knowledge_distillation\"  class=\"wp-block-heading\">Efficient AI with knowledge distillation<a href=\"#efficient_ai_with_knowledge_distillation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><em>Knowledge distillation</em> is a technique in machine learning where a smaller, more efficient model (the student) learns to mimic the behavior of a larger, more complex model (the teacher). This process helps reduce training and fine-tuning time, as the student model uses the knowledge already captured by the teacher, including more nuanced information like soft labels, and doesn\u2019t need to be trained from scratch for any new task.&nbsp;</p>\n\n\n\n<p>By distilling this knowledge, the student model can achieve similar performance with significantly fewer computational resources, making it ideal for deployment in resource-constrained environments and speeding up the training process.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"374\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-1024x374.png\" alt=\"Diagram shows a teacher model, which is a larger, more complex neural network, transferring knowledge to a student model, which is a smaller, simpler neural network. The process involves distilling and transferring knowledge, with data feeding into both models.\" class=\"wp-image-87660\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-1024x374.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-300x109.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-625x228.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-179x65.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-768x280.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-1536x560.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-2048x747.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-645x235.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-500x182.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-160x58.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-362x132.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/knowledge-distillation-diagram-301x110.png 301w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Knowledge distillation process</em></figcaption></figure></div>\n\n\n<p>TAO supports knowledge distillation. Specify the binding between various layers to formulate compute the distillation loss. For more information, see <a href=\"https://docs.nvidia.com/tao/tao-toolkit/text/qat_and_amp_for_training.html#knowledge-distillation\">Optimizing the Training Pipeline</a>.</p>\n\n\n\n<p>We provide the <a href=\"https://github.com/NVIDIA/tao_tutorials/blob/main/notebooks/tao_launcher_starter_kit/dino/dino_distillation.ipynb\">dino_distillation.ipynb</a> reference notebook to showcase knowledge distillation and how to distill the intermediate feature maps from DINO and FAN small to DINO and ResNet-50 using an L2 loss.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ndistill.yaml:\nmodel:\n  backbone: resnet_50 # Student model\n  train_backbone: True\ndistill:\n  teacher:\n    backbone: fan_small\n    train_backbone: False\n    num_feature_levels: 4 # Number of feature-maps to map from teacher\n  pretrained_teacher_model_path: /workspace/tao-experiments/dino/pretrained_dino_coco_vdino_fan_small_trainable_v1.0/dino_fan_small_ep12.pth\n  bindings:\n  - teacher_module_name: &#039;model.backbone.0.body&#039;\n    student_module_name: &#039;model.backbone.0.body&#039;\n    criterion: L2\n</pre></div>\n\n\n<p>In the <code>distill.yaml</code> file, you can define the teacher model, student model backbone, and loss function. Train the model:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n!tao model dino distill \\ -e $SPECS_DIR/distill.yaml \\ results_dir=$RESULTS_DIR/\n</pre></div>\n\n\n<p>After the student model is trained, you can use the TAO <code>evaluate</code> tool to evaluate the distilled model, <code>inference</code> tool to visualize the inference, <code>export</code> tool to export the trained model to ONNX, and eventually generate the <code>gen_trt_engine</code> file to export the DINO and ResNet-50 model to TensorRT.</p>\n\n\n\n<h2 id=\"getting_started_with_tao_55\"  class=\"wp-block-heading\">Getting started with TAO 5.5<a href=\"#getting_started_with_tao_55\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Developers around the world are using NVIDIA TAO to accelerate AI training for their vision AI applications. Use the new capabilities of TAO 5.5 to enhance your AI application.</p>\n\n\n\n<p>For more information, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://github.com/NVIDIA/tao_tutorials\">Download NVIDIA TAO</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/tao-toolkit-get-started\">Get started with NVIDIA TAO</a></li>\n\n\n\n<li><a href=\"https://forums.developer.nvidia.com/c/accelerated-computing/intelligent-video-analytics/tao-toolkit/17\">Ask technical questions in the forum</a>&nbsp;</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA TAO is a framework designed to simplify and accelerate the development and deployment of AI models. It enables you to use pretrained models, fine-tune them with your own data, and optimize the models for specific use cases without needing deep AI expertise. TAO integrates seamlessly with the NVIDIA hardware and software ecosystem, providing tools &hellip; <a href=\"https://developer.nvidia.com/blog/new-foundational-models-and-training-capabilities-with-nvidia-tao-5-5/\">Continued</a></p>\n", "protected": false}, "author": 841, "featured_media": 87633, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1475378", "discourse_permalink": "https://forums.developer.nvidia.com/t/new-foundational-models-and-training-capabilities-with-nvidia-tao-5-5/304785", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 2758], "tags": [453, 1950, 354, 1066], "coauthors": [1420, 3984, 1332, 1038, 2299], "class_list": ["post-87263", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-edge-computing", "tag-featured", "tag-image-recognition", "tag-image-segmentation", "tag-pre-trained-foundation-models"], "acf": {"post_industry": ["Manufacturing", "Retail / Consumer Packaged Goods", "Smart Cities / Spaces"], "post_products": ["TAO Toolkit"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3386650_Metropolis_IVA_TAO_5-5_Auto-labeling_v02.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-mHt", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87263"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/841"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87263"}], "version-history": [{"count": 13, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87263/revisions"}], "predecessor-version": [{"id": 88862, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87263/revisions/88862"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87633"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87263"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87263"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87263"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87263"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87957, "date": "2024-08-28T08:00:00", "date_gmt": "2024-08-28T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87957"}, "modified": "2024-09-05T10:57:17", "modified_gmt": "2024-09-05T17:57:17", "slug": "nvidia-blackwell-platform-sets-new-llm-inference-records-in-mlperf-inference-v4-1", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-blackwell-platform-sets-new-llm-inference-records-in-mlperf-inference-v4-1/", "title": {"rendered": "NVIDIA Blackwell Platform Sets New LLM Inference Records in MLPerf Inference v4.1"}, "content": {"rendered": "\n<p><a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">Large language model (LLM)</a> inference is a full-stack challenge. Powerful GPUs, high-bandwidth GPU-to-GPU interconnects, efficient acceleration libraries, and a highly optimized inference engine are required for high-throughput, low-latency inference.&nbsp;</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/\">MLPerf</a> Inference v4.1 is the latest version of the popular and widely recognized MLPerf Inference benchmarks, developed by the MLCommons consortium. The benchmark includes many popular AI models covering diverse use cases, from LLMs and generative AI to recommenders and computer vision. The benchmarks are regularly updated to ensure market relevance.&nbsp;</p>\n\n\n\n<p>In this round, NVIDIA submitted many great results, enabled by innovation across the NVIDIA technology stack. Highlights include:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>First submission using the <a href=\"https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/\">NVIDIA Blackwell architecture</a>, delivering up to 4x more performance on Llama 2 70B compared to the <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100 Tensor Core GPU</a>.</li>\n\n\n\n<li><a href=\"https://www.nvidia.com/en-us/data-center/h200/\">NVIDIA H200 Tensor Core GPU</a> submissions on every data center workload, delivering up to 1.5x more performance compared to the H100 submissions.</li>\n\n\n\n<li>Up to 27% more performance on H200 due to software improvements compared to preview submissions on H200 made in the prior round.&nbsp;</li>\n\n\n\n<li>First Llama 2 70B submissions using <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a>, delivering similar performance to <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">NVIDIA TensorRT-LLM</a> submissions.&nbsp;</li>\n\n\n\n<li>Up to 6.2x higher performance on the GPT-J benchmark in the edge category compared to the prior round using the <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/\">NVIDIA Jetson</a> AGX Orin platform.</li>\n</ul>\n\n\n\n<p>This post provides a closer look at these results.&nbsp;</p>\n\n\n\n<h2 id=\"nvidia_blackwell_shines_in_mlperf_inference_debut\"  class=\"wp-block-heading\">NVIDIA Blackwell shines in MLPerf Inference debut<a href=\"#nvidia_blackwell_shines_in_mlperf_inference_debut\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Introduced at NVIDIA GTC 2024, the <a href=\"https://nvdam.widen.net/s/xqt56dflgh/nvidia-blackwell-architecture-technical-brief\">NVIDIA Blackwell architecture</a> is a new class of AI superchip. Crafted with 208 billion transistors, and using the TSMC 4NP process tailored for NVIDIA, it is the largest GPU ever built. The Blackwell architecture also features the new second-generation Transformer Engine, which uses new Blackwell Tensor Core technology combined with TensorRT-LLM innovations, to enable fast and accurate FP4 AI inference.&nbsp;&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1125\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node.jpg\" alt=\"A photo of a GB200 compute node, with chips, cooling, among other components. \" class=\"wp-image-87985\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node.jpg 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node-300x169.jpg 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node-625x352.jpg 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node-179x101.jpg 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node-768x432.jpg 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node-1536x864.jpg 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node-645x363.jpg 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node-960x540.jpg 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node-500x281.jpg 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node-160x90.jpg 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node-362x204.jpg 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node-195x110.jpg 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell-compute-node-1024x576.jpg 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. An NVIDIA Blackwell compute node&nbsp;</em></em></figcaption></figure></div>\n\n\n<p>In this round of MLPerf Inference, NVIDIA made its first submissions using Blackwell. On the Llama 2 70B LLM benchmark, Blackwell delivered up to 4x higher tokens per second per GPU compared to the H100 GPU.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>MLPerf Inference v4.1&nbsp;Llama 2 70B</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Server<br>tokens/s</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Offline<br>tokens/s</strong></td></tr><tr><td>1 NVIDIA B200 GPU</td><td class=\"has-text-align-center\" data-align=\"center\">10,756</td><td class=\"has-text-align-center\" data-align=\"center\">11,264</td></tr><tr><td>Per-GPU increase</td><td class=\"has-text-align-center\" data-align=\"center\">4x</td><td class=\"has-text-align-center\" data-align=\"center\">3.7x</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 1. Per-GPU performance increases compared to NVIDIA Hopper on the MLPerf Llama 2 70B benchmark. H100 per-GPU throughput obtained by dividing submitted eight-GPU results by eight</em></em></figcaption></figure>\n\n\n\n<p class=\"has-text-align-left has-small-font-size\">MLPerf Inference&nbsp; v4.1 Closed, Data Center. Results retrieved from <a href=\"http://www.mlperf.org/\">www.mlperf.org</a> on August 28, 2024. Blackwell results measured on single GPU and retrieved from entry 4.1-0074 in the Closed, Preview category. H100 results from entry 4.1-0043 in the Closed, Available category on eight&nbsp; H100 system and divided by GPU count for per GPU comparison. Per-GPU throughput is not a primary metric of MLPerf Inference. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is&nbsp; strictly prohibited. See <a href=\"http://www.mlcommons.org/\">www.mlcommons.org</a> for more information.</p>\n\n\n\n<p>This submission made extensive use of the Blackwell FP4 Transformer Engine. This submission was also in the Closed division, meaning that the inference results delivered this performance without modifications to the model while still meeting the high accuracy requirements of the benchmark. FP4 quantization was performed using the <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer\">NVIDIA TensorRT Model Optimizer</a> library, which incorporates state-of-the-art model optimization techniques, and did not require model re-training.\u00a0</p>\n\n\n\n<h2 id=\"nvidia_h200_tensor_core_gpu_delivers_outstanding_performance_on_every_benchmark\"  class=\"wp-block-heading\">NVIDIA H200 Tensor Core GPU delivers outstanding performance on every benchmark<a href=\"#nvidia_h200_tensor_core_gpu_delivers_outstanding_performance_on_every_benchmark\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The NVIDIA H200 GPU upgrades the <a href=\"https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/\">NVIDIA Hopper architecture</a> with HBM3e, the industry\u2019s fastest AI memory. Compared to the H100, this increases memory capacity by 1.8x and memory bandwidth by 1.4x, benefiting memory-sensitive use cases.</p>\n\n\n\n<p>This round, NVIDIA submitted results using eight H200 GPUs on every workload, and did so in the available category.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>Benchmark</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>GPU</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Server</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Offline</strong></td></tr><tr><td>Llama 2 70B&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">8 H200<br>(1,000 W)</td><td class=\"has-text-align-center\" data-align=\"center\">32,790&nbsp;token/s</td><td class=\"has-text-align-center\" data-align=\"center\">34,864&nbsp;token/s</td></tr><tr><td>Mixtral 8x7B</td><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"10\"><br><br><br><br><br><br><br><br><br><br>8 H200<br>(700 W)</td><td class=\"has-text-align-center\" data-align=\"center\">57,177&nbsp;token/s</td><td class=\"has-text-align-center\" data-align=\"center\">59,022&nbsp;token/s</td></tr><tr><td>GPT-J</td><td class=\"has-text-align-center\" data-align=\"center\">19,243&nbsp;token/s</td><td class=\"has-text-align-center\" data-align=\"center\">20,086&nbsp;token/s</td></tr><tr><td>Stable Diffusion XL</td><td class=\"has-text-align-center\" data-align=\"center\">16.78&nbsp;queries/s</td><td class=\"has-text-align-center\" data-align=\"center\">17.42&nbsp;samples/s</td></tr><tr><td>DLRM v2 99%</td><td class=\"has-text-align-center\" data-align=\"center\">585,208&nbsp;queries/s</td><td class=\"has-text-align-center\" data-align=\"center\">637,342<br>samples/s</td></tr><tr><td>DLRM v2 99.9%</td><td class=\"has-text-align-center\" data-align=\"center\">370,083&nbsp;queries/s</td><td class=\"has-text-align-center\" data-align=\"center\">390,953<br>samples/s</td></tr><tr><td>ResNet-50 v1.5</td><td class=\"has-text-align-center\" data-align=\"center\">632,229&nbsp;queries/s</td><td class=\"has-text-align-center\" data-align=\"center\">756,960<br>samples/s</td></tr><tr><td>BERT 99%</td><td class=\"has-text-align-center\" data-align=\"center\">57,609&nbsp;queries/s</td><td class=\"has-text-align-center\" data-align=\"center\">73,310&nbsp;samples/s</td></tr><tr><td>BERT 99.9%</td><td class=\"has-text-align-center\" data-align=\"center\">51,212&nbsp;queries/s</td><td class=\"has-text-align-center\" data-align=\"center\">63,950&nbsp;samples/s</td></tr><tr><td>RetinaNet</td><td class=\"has-text-align-center\" data-align=\"center\">13,604&nbsp;queries/s</td><td class=\"has-text-align-center\" data-align=\"center\">14,439&nbsp;samples/s</td></tr><tr><td>3D U-Net</td><td class=\"has-text-align-center\" data-align=\"center\">Not part of benchmark</td><td class=\"has-text-align-center\" data-align=\"center\">54.71&nbsp;samples/s</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 2.&nbsp;NVIDIA MLPerf Inference v4.1 data center results using H200 GPUs. Llama 2 70B results based on H200 configured at 1000W, all other results using H200 at 700W&nbsp;</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">MLPerf Inference v4.1 Closed, Data Center. Results retrieved from <a href=\"http://www.mlperf.org/\">www.mlperf.org</a> on August 28, 2024. All results using eight GPUs and retrieved from the following entries: 4.1-0046, 4.1-0048, 4.1-0050. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See <a href=\"http://www.mlcommons.org/\">www.mlcommons.org</a> for more information.</p>\n\n\n\n<p>The following subsections describe the improvements achieved across several benchmarks.</p>\n\n\n\n<h3 id=\"llama_2_70b\"  class=\"wp-block-heading\">Llama 2 70B<a href=\"#llama_2_70b\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The Llama 2 70B benchmark was first introduced in the prior round and continues to represent popular, 70B-class dense LLMs.</p>\n\n\n\n<p>NVIDIA also continues to enhance TensorRT-LLM software, providing users with more LLM inference performance from the GPUs they already have. Through software improvements alone, Llama 2 70B performance improved by up to 14% on H200 compared to the preview submission in the prior round.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>MLPerf Llama 2 70B improvements since v4.0</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Server</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Offline</strong></td></tr><tr><td>H200 (700 W)</td><td class=\"has-text-align-center\" data-align=\"center\">1.14x</td><td class=\"has-text-align-center\" data-align=\"center\">1.12x</td></tr><tr><td>H100 (700 W)</td><td class=\"has-text-align-center\" data-align=\"center\">1.05x</td><td class=\"has-text-align-center\" data-align=\"center\">1.12x</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 3.&nbsp;Hopper GPU improvements on Llama 2 70B benchmark compared to prior round&nbsp;&nbsp;&nbsp;</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">MLPerf Inference v4.0 and&nbsp; v4.1 Closed, Data Center. Results retrieved from <a href=\"http://www.mlperf.org/\">www.mlperf.org</a> on August 28, 2024. All results using eight GPUs and retrieved from the following entries:4.0-0062, 4.0-0070&nbsp; 4.1-0043, 4.1-0048, 4.1-0050. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See <a href=\"http://www.mlcommons.org/\">www.mlcommons.org</a> for more information.</p>\n\n\n\n<p>Key improvements this round included <a href=\"https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/XQA-kernel.md\">XQA kernel</a> optimizations as well as additional layer fusions.&nbsp;</p>\n\n\n\n<p>Additionally, NVIDIA submitted Llama 2 70B results using H200 GPUs using a custom thermal solution and with the thermal design power (TDP) increased to 1,000 watts. This enabled an additional performance increase of up to 12% on the Llama 2 70B benchmark, compared to H200 configured at a 700-watt TDP.&nbsp;</p>\n\n\n\n<p>This round, NVIDIA also submitted Llama 2 70B results using H200 GPUs running Triton Inference Server, delivering similar performance to the bare metal submission. In the server scenario, H200 with Triton Inference Server even outperformed H200 without Triton Inference Server.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>MLPerf Llama 2 70B benchmark</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Server</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Offline</strong></td></tr><tr><td>8 H200 with Triton Inference Server</td><td class=\"has-text-align-center\" data-align=\"center\">30,128</td><td class=\"has-text-align-center\" data-align=\"center\">31,059</td></tr><tr><td>8 H200 without Triton Inference Server</td><td class=\"has-text-align-center\" data-align=\"center\">29,228</td><td class=\"has-text-align-center\" data-align=\"center\">31,303</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 4.&nbsp;Performance of eight H200 GPUs with and without Triton Inference Server</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">MLPerf Inference v4.1 Closed, Data Center. Results retrieved from <a href=\"http://www.mlperf.org/\">www.mlperf.org</a> on August 28, 2024. All results using eight GPUs and retrieved from the following entries :4.1-0048, 4.1-0050. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See <a href=\"http://www.mlcommons.org/\">www.mlcommons.org</a> for more information.</p>\n\n\n\n<p>This shows that organizations looking to deploy popular models need not trade functionality for performance when using Triton Inference Server.</p>\n\n\n\n<p>And, finally, NVIDIA submitted Llama 2 70B results in the Open division using a single H200 GPU, showcasing the possible performance gains that can result from more extensive model-level optimizations.&nbsp;</p>\n\n\n\n<p>First, depth pruning and width pruning were applied to the model, greatly reducing the total number of parameters in the model by intelligently removing layers and MLP intermediate dimensions that are less important to the overall model output.&nbsp;</p>\n\n\n\n<p>Then, to recover accuracy, fine-tuning was performed on the model using the MLPerf OpenORCA development dataset. The final pruned model has 32 layers and 14,336 MLP intermediate dimensions\u2014a significant reduction compared to the original model\u2019s 80 layers and 28,672 intermediate dimensions.&nbsp;</p>\n\n\n\n<p>Although model accuracy is slightly below the 99% threshold, the model is significantly smaller, enabling much higher throughput (offline) of 11,189 token/s\u2014or almost 3x higher throughput compared to the throughput achieved in the Closed division.&nbsp;&nbsp;</p>\n\n\n\n<p class=\"has-small-font-size\">MLPerf Inference v4.1, Data Center, Open Division. Result from entry 4.1-0089. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See <a href=\"http://www.mlcommons.org/\">www.mlcommons.org</a> for more information.</p>\n\n\n\n<h3 id=\"mixtral_8x7b\"  class=\"wp-block-heading\">Mixtral 8x7B<a href=\"#mixtral_8x7b\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>A new LLM workload was added in this round, based on the Mixtral 8x7B model, developed by Mistral AI. Mixtral 8x7B employs a sparse mixture of experts (MoE) architecture with eight experts, 46.7B total parameters, with two experts and 12.9B parameters used per token.&nbsp;</p>\n\n\n\n<p>NVIDIA submitted Mixtral 8x7B results using both the H100 and H200 GPUs, running TensorRT-LLM software and extensively used FP8 precision.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>MLPerf Mixtral 8x7B benchmark</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Server</strong><br><strong>tokens/s</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Offline</strong><br><strong>tokens/s</strong></td></tr><tr><td>8 H200</td><td class=\"has-text-align-center\" data-align=\"center\">57,177</td><td class=\"has-text-align-center\" data-align=\"center\">59,022</td></tr><tr><td>8 H100</td><td class=\"has-text-align-center\" data-align=\"center\">50,796</td><td class=\"has-text-align-center\" data-align=\"center\">52,416</td></tr><tr><td>H200 advantage</td><td class=\"has-text-align-center\" data-align=\"center\">1.13x</td><td class=\"has-text-align-center\" data-align=\"center\">1.13x</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 5. H100 and H200 performance and uplift for the latter on MLPerf Mixtral 8x7B benchmark&nbsp;</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">MLPerf Inference v4.1 Closed, Data Center. Results retrieved from <a href=\"http://www.mlperf.org/\">www.mlperf.org</a> on August 28, 2024. All results using eight GPUs and retrieved from the following entries :4.1-0043, 4.1-0048. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See <a href=\"http://www.mlcommons.org/\">www.mlcommons.org</a> for more information.</p>\n\n\n\n<p>Only systems using NVIDIA GPUs submitted Mixtral 8x7B results. NVIDIA continues to submit great results on workloads as they are added to the MLPerf benchmark suite, showing that the NVIDIA platform delivers high performance and exceptional versatility for the large and expanding universe of AI models.&nbsp;</p>\n\n\n\n<h3 id=\"stable_diffusion_xl\"  class=\"wp-block-heading\">Stable Diffusion XL<a href=\"#stable_diffusion_xl\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>This round, H200 performance was improved to generate two images per second, which is 27% more performance on Stable Diffusion XL compared to the prior round. This represented a new record for the benchmark.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>MLPerf Stable Diffusion XL improvements since v4.0</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Server</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Offline</strong></td></tr><tr><td>8 H200 (700 W)</td><td class=\"has-text-align-center\" data-align=\"center\">1.22x</td><td class=\"has-text-align-center\" data-align=\"center\">1.27x</td></tr><tr><td>8 H100 (700 W)</td><td class=\"has-text-align-center\" data-align=\"center\">1.17x</td><td class=\"has-text-align-center\" data-align=\"center\">1.25x</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 6. Stable Diffusion XL performance increases in MLPerf Inference v4.1 compared to v4.0 on both H100 and H200 GPUs&nbsp;</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">MLPerf Inference v4.0 and v4.1 Closed, Data Center. Results retrieved from <a href=\"http://www.mlperf.org/\">www.mlperf.org</a> on August 28, 2024. All results using eight GPUs and retrieved from the following entries :4.0-0062, 4.0-0070, 4.1-0043, 4.1-0048. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See <a href=\"http://www.mlcommons.org/\">www.mlcommons.org</a> for more information.</p>\n\n\n\n<p>These gains were due primarily to several key optimizations to the NVIDIA software stack, including:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>UNet FP8 support:</strong> By using TensorRT Model Optimizer, the NVIDIA submission this round used FP8 precision while meeting accuracy requirements. This represented the largest portion of the round-to-round performance gain on Hopper GPUs.&nbsp;</li>\n\n\n\n<li><strong>VAE INT8 support:</strong> The NVIDIA submission this round was able to quantize certain layers to INT8 and others to FP16, compared to use of FP32 in the prior round. This improved VAE performance by 70%, translating into about a 4% end-to-end speedup.&nbsp;</li>\n</ul>\n\n\n\n<p><strong>Variational autoencoder (VAE) batch splitting:</strong> The VAE portion of the SDXL pipeline requires a very large memory footprint. By employing batch splitting, the NVIDIA submission this round was able to increase batch size from 8 to 64, improving performance.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1836\" height=\"613\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture.png\" alt=\"Diagram showing how a text prompt flows through the Stable Diffusion XL model to generate an output image.  \" class=\"wp-image-88002\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture.png 1836w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture-300x100.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture-625x209.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture-179x60.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture-768x256.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture-1536x513.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture-645x215.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture-500x167.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture-160x53.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture-362x121.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture-329x110.png 329w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/stable-diffusion-xl-model-architecture-1024x342.png 1024w\" sizes=\"(max-width: 1836px) 100vw, 1836px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. Stable Diffusion XL model architecture</em></em></figcaption></figure></div>\n\n\n<p>Additionally, NVIDIA submitted SDXL results in the Open division submission, which incorporates these optimizations with Latent Consistency Model (LCM), accelerating the Closed division offline throughput by almost 5x to 11 samples/s on H200. This showcased the further performance gains possible from more extensive model-level optimizations for diffusion models.</p>\n\n\n\n<h2 id=\"a_giant_generative_ai_leap_on_jetson_agx_orin\"  class=\"wp-block-heading\">A giant generative AI leap on Jetson AGX Orin<a href=\"#a_giant_generative_ai_leap_on_jetson_agx_orin\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Jetson AGX Orin offers high AI compute performance, large unified memory, and comprehensive software for generative AI at the edge.&nbsp;</p>\n\n\n\n<p>Through extensive software optimization, <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/\">NVIDIA Jetson AGX Orin</a> 64 GB delivers a large giant leap for generative AI models for the edge, delivering up to 6.2x more throughput and 2.4x better latency on the GPT-J 6B parameter LLM benchmark. Generative AI models at the edge can transform sensor data, such as images and videos, into real-time, actionable insights with strong contextual awareness.&nbsp;</p>\n\n\n\n<p>Backed by the NVIDIA software stack, Jetson AGX Orin is uniquely positioned as the leading platform for running transformer models like GPT-J, vision transformers, and Stable Diffusion at the Edge. Developers can take advantage of other platform services like <a href=\"https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/\">Jetson Generative AI Lab</a> and <a href=\"https://developer.nvidia.com/blog/power-cloud-native-microservices-at-the-edge-with-nvidia-jetpack-6-0-now-ga/\">Jetson Platform Services</a> to bring great solutions to life.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>GPT-J (Edge)</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Single stream latency (ms)</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Offline tokens/s</strong></td></tr><tr><td>Jetson AGX Orin 64 GB v4.1</td><td class=\"has-text-align-center\" data-align=\"center\">4,176</td><td class=\"has-text-align-center\" data-align=\"center\">64.47</td></tr><tr><td>Jetson AGX Orin 64 GB&nbsp; v4.0</td><td class=\"has-text-align-center\" data-align=\"center\">10,132</td><td class=\"has-text-align-center\" data-align=\"center\">10.35</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 7. GPT-J LLM performance in the MLPerf Inference; Edge (v4.0 and v4.1) on Jetson AGX Orin</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">MLPerf Inference v4.0 and v4.1 Closed, Edge. Results retrieved from <a href=\"http://www.mlperf.org/\">www.mlperf.org</a> on August 28, 2024. All results using eight GPUs and retrieved from the following entries :4.0-0072, 4.1-0051. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use is strictly prohibited. See <a href=\"http://www.mlcommons.org/\">www.mlcommons.org</a> for more information.</p>\n\n\n\n<p>This performance boost was made possible through numerous software optimizations to TensorRT-LLM, including the use of in-flight batching, as well as the application of INT4 Activation-aware Weight Quantization (AWQ). AWQ keeps the 1% \u201csalient weights\u201d in higher precision FP16 and quantizes the remaining weights to four-bit integer (INT4) precision. This technique significantly reduces memory footprints, enabling larger batches to be processed at once, dramatically increasing inference throughput.&nbsp;</p>\n\n\n\n<p>NVIDIA also submitted results of the demanding Llama 2 70B model running on Jetson AGX Orin in the Open division, demonstrating the possibilities of more extensive model optimization techniques. The submission model was the same 16B depth and width pruned model as in the H200 submission. INT4 AWQ\u2014used in the GPT-J submission on Jetson AGX Orin in the Closed division\u2014was also used in this submission. Model parameter pruning plus INT4 quantization together greatly shrink the memory footprint of the model weights to around only 8 GB for the Llama 2 70B model.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In its debut submission, <a href=\"https://nvdam.widen.net/s/xqt56dflgh/nvidia-blackwell-architecture-technical-brief\">NVIDIA Blackwell</a> delivered outstanding performance\u2014up to 4x compared to H100 on Llama 2 70B. And, among available solutions, Hopper GPUs delivered the highest multi-GPU generative AI performance and the highest performance per accelerator across all workloads, and continues to benefit from ongoing software optimization. <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a> also achieved great results this round, delivering similar performance to bare metal submissions. For edge and embedded AI, Jetson AGX Orin, and the rich NVIDIA software stack enable running capable models, like GPT-J 6B, with performance improving by up to 6.2x in just one round.&nbsp;</p>\n\n\n\n<p>NVIDIA continues to innovate rapidly across the full technology stack to deliver world-class inference performance on today\u2019s models as well as tomorrow\u2019s, from the largest AI factories to compact, low-power edge devices.</p>\n\n\n\n<h3 id=\"acknowledgments\"  class=\"wp-block-heading\">Acknowledgments<a href=\"#acknowledgments\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><em>The work of many NVIDIA employees made these outstanding results happen. We would like to acknowledge the tireless efforts of Chen-Han Yu, Kai Xu, Justin Xin, Asma Kuriparambil Thekkumpate, Linnan Wang, Wei-Ming Chen Kaiyu Xie, Shobhit Verma, Viraat Chandra, among many others.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language model (LLM) inference is a full-stack challenge. Powerful GPUs, high-bandwidth GPU-to-GPU interconnects, efficient acceleration libraries, and a highly optimized inference engine are required for high-throughput, low-latency inference.&nbsp; MLPerf Inference v4.1 is the latest version of the popular and widely recognized MLPerf Inference benchmarks, developed by the MLCommons consortium. The benchmark includes many popular &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-blackwell-platform-sets-new-llm-inference-records-in-mlperf-inference-v4-1/\">Continued</a></p>\n", "protected": false}, "author": 1355, "featured_media": 88083, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1476681", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-blackwell-platform-sets-new-llm-inference-records-in-mlperf-inference-v4-1/304934", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1903], "tags": [453, 2932, 973], "coauthors": [2732, 2788, 3034, 3313, 4001, 4000, 3751], "class_list": ["post-87957", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "category-features", "tag-featured", "tag-large-language-models", "tag-mlperf"], "acf": {"post_industry": ["Hardware / Semiconductor"], "post_products": ["Blackwell", "H100", "Hopper", "Jetson", "TensorRT", "TensorRT-LLM", "Triton Inference Server"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Benchmark"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-blackwell.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mSF", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87957"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1355"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87957"}], "version-history": [{"count": 34, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87957/revisions"}], "predecessor-version": [{"id": 88157, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87957/revisions/88157"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88083"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87957"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87957"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87957"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87957"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87948, "date": "2024-08-28T08:00:00", "date_gmt": "2024-08-28T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87948"}, "modified": "2024-11-13T20:04:51", "modified_gmt": "2024-11-14T04:04:51", "slug": "build-an-enterprise-scale-multimodal-document-retrieval-pipeline-with-nvidia-nim-agent-blueprint", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/build-an-enterprise-scale-multimodal-document-retrieval-pipeline-with-nvidia-nim-agent-blueprint/", "title": {"rendered": "Build an Enterprise-Scale Multimodal PDF Data Extraction Pipeline with an NVIDIA AI Blueprint"}, "content": {"rendered": "\n<p>Trillions of PDF files are generated every year, each file likely consisting of multiple pages filled with various content types, including text, images, charts, and tables. This goldmine of data can only be used as quickly as humans can read and understand it.&nbsp;</p>\n\n\n\n<p>But with <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI</a> and <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation</a> (RAG), this untapped data can be used to uncover business insights that can help employees work more efficiently and result in lower costs.&nbsp;</p>\n\n\n\n<p>Imagine being able to accurately extract the knowledge contained in massive volumes of enterprise data\u2014effectively talking to the data\u2014to quickly make your digital human an expert on any topic. In turn, this enables your employees to make smarter decisions faster.&nbsp;</p>\n\n\n\n<p>In this post, we show how the <a href=\"https://build.nvidia.com/nvidia/multimodal-pdf-data-extraction-for-enterprise-rag\">NVIDIA AI Blueprint for multimodal PDF data extraction </a> combines <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo<sup> </sup>\u00a0Retriever</a> and <a href=\"https://developer.nvidia.com/nim\">NVIDIA NIM microservices</a>, along with reference code and documentation to do this.</p>\n\n\n\n<h2 id=\"tackling_the_challenge_of_complex_data_extraction&nbsp;\"  class=\"wp-block-heading\">Tackling the challenge of complex data extraction&nbsp;<a href=\"#tackling_the_challenge_of_complex_data_extraction&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>PDFs are content-rich documents that store refined information expressed across modalities to make it more concise and digestible. For example, a PDF might include a mixture of text, tables, charts, plots, and diagrams used to convey complex information. From the lens of information retrieval, each of these modalities presents unique challenges.&nbsp;</p>\n\n\n\n<p>To build pipelines for solving these challenges, you can use the following <a href=\"https://developer.nvidia.com/nim\">NVIDIA NIM</a> microservices:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>PDF Ingestion NIM microservices\n<ul class=\"wp-block-list\">\n<li><strong>nv-yolox-structured-image:</strong> A fine-tuned object detection model to detect charts, plots, and tables in PDFs.</li>\n\n\n\n<li><strong>Deplot:</strong> A popular community pix2struct model for generating descriptions of charts.</li>\n\n\n\n<li><strong>CACHED:</strong> An object detection model used to identify various elements in graphs.</li>\n\n\n\n<li><strong>PaddleOCR:</strong> An optical character recognition (OCR) model to transcribe text from tables and charts.</li>\n</ul>\n</li>\n\n\n\n<li>NVIDIA NeMo Retriever NIM microservices\n<ul class=\"wp-block-list\">\n<li><a href=\"https://build.nvidia.com/nvidia/nv-embedqa-e5-v5\">nv-embedqa-e5-v5</a>: A popular community base-embedding model optimized for text question-answering retrieval.</li>\n\n\n\n<li><a href=\"https://build.nvidia.com/nvidia/nv-rerankqa-mistral-4b-v3\">nv-rerankqa-mistral4b-v3</a>: A popular community base model fine-tuned for text reranking for high-accuracy question answering.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p>For more information, see <a href=\"https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/\">An Easy Introduction to Multimodal Retrieval-Augmented Generation</a>.</p>\n\n\n\n<h2 id=\"multimodal_retrieval_blueprint_for_rag_on_pdfs\"  class=\"wp-block-heading\">Multimodal retrieval blueprint for RAG on PDFs<a href=\"#multimodal_retrieval_blueprint_for_rag_on_pdfs\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Building a multimodal PDF data extraction pipeline involves two key steps:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Ingest documents with multimodal data.</li>\n\n\n\n<li>Retrieve relevant context based on a user query.</li>\n</ol>\n\n\n\n<h3 id=\"ingest_documents_with_multimodal_data\"  class=\"wp-block-heading\">Ingest documents with multimodal data<a href=\"#ingest_documents_with_multimodal_data\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>This is the first half of the workflow, which effectively extracts information and makes it available for retrieval. This involves the following steps:</p>\n\n\n\n<p>First, parse the PDFs to separate out the modalities (text, images, charts, tables, plots, and other diagrams). Text is parsed as structured JSON while pages are parsed as images, where each page in the document is rendered as an image.</p>\n\n\n\n<p>Next extract textual metadata from charts and tables. Use NIM microservices to accurately extract information from images:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>nv-yolox-structured-image:</strong> Identify the charts and tables in the PDF&nbsp;</li>\n\n\n\n<li><strong>DePlot, CACHED, and PaddleOCR</strong>: Extract information from charts. DePlot transcribes the graphs and CACHED with PaddleOCR extracts important additional metadata about the graph.&nbsp;&nbsp;</li>\n\n\n\n<li><strong>PaddleOCR:</strong> Extract text information from tables, maintaining the reading order of the table.&nbsp;</li>\n</ul>\n\n\n\n<p>Finally, filter the extracted information, chunking and creating a VectorStore. The extracted information undergoes filtering to avoid duplicates and gets broken down into appropriate chunks. The NeMo Retriever embedding NIM microservice then converts the chunks into embeddings and stores them in a VectorStore.&nbsp;</p>\n\n\n\n<h3 id=\"retrieve_relevant_context_based_on_a_user_query\"  class=\"wp-block-heading\">Retrieve relevant context based on a user query<a href=\"#retrieve_relevant_context_based_on_a_user_query\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>When a user submits a query, the relevant information is retrieved from the vast repository of the ingested documents. This is achieved as follows:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>The NeMo Retriever embedding NIM microservice embeds the user query, which is used to retrieve the most relevant chunks using vector similarity search from the VectorStore.&nbsp;</li>\n\n\n\n<li>The NeMo Retriever reranking NIM microservice acts as a layer of refinement, carefully evaluating and re-ranking the results to ensure the most accurate and useful chunks are used for responding to the query.&nbsp;</li>\n\n\n\n<li>With the most pertinent information at hand, the LLM NIM microservice generates a response that is informed, accurate, and contextually relevant.&nbsp;</li>\n</ul>\n\n\n\n<p>By using the comprehensive knowledge base built from the ingested documents, this workflow enables users to access precise and relevant information, providing valuable insights and answers to their queries.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"430\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-pipeline-1024x430.png\" alt=\"Diagram shows that the PDF data extraction pipeline has two major steps, ingestion and and retrieval, addressed using NIM microservices built for embedding, reranking, object detection, extraction, and LLMs.\" class=\"wp-image-87961\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-pipeline-1024x430.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-pipeline-300x126.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-pipeline-625x262.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-pipeline-179x75.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-pipeline-768x322.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-pipeline-645x271.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-pipeline-500x210.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-pipeline-160x67.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-pipeline-362x152.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-pipeline-262x110.png 262w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-pipeline.png 1428w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Architecture of the NVIDIA multimodal PDF data extraction workflow connecting to the retrieval pipeline</em></figcaption></figure></div>\n\n\n<h2 id=\"building_a_cost-effective_enterprise-grade_rag_pipeline\"  class=\"wp-block-heading\">Building a cost-effective enterprise-grade RAG pipeline<a href=\"#building_a_cost-effective_enterprise-grade_rag_pipeline\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Here are the benefits of using NIM microservices to create a multimodal PDF data extraction pipeline: cost and stability.</p>\n\n\n\n<p>Cost has two considerations:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Time to market:</strong> NVIDIA NIM microservices are designed to be easy-to-use and scalable model inference solutions, enabling enterprise application developers to focus on working on their application logic rather than having to spend cycles on building and scaling out the infrastructure. NIM microservices are containerized solutions, which come with industry-standard APIs and Helm charts to scale.</li>\n\n\n\n<li><strong>Cost of deployment: </strong>NIM uses the full suite of <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> software to accelerate model inference, maximizing the value enterprises can derive from their models and in turn reducing the cost of deploying the pipelines at scale. Figure 2 demonstrates the improvements in accuracy and throughput achieved in testing this ingestion and extraction pipeline.&nbsp;</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"680\" height=\"399\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-retrieval-accuracy-ingestion-throughput.png\" alt=\"Two charts show the benefit of NIM Off versus NIM On showing 20% fewer incorrect answers and 3X improved ingestion throughput\" class=\"wp-image-87962\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-retrieval-accuracy-ingestion-throughput.png 680w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-retrieval-accuracy-ingestion-throughput-300x176.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-retrieval-accuracy-ingestion-throughput-625x367.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-retrieval-accuracy-ingestion-throughput-179x105.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-retrieval-accuracy-ingestion-throughput-645x378.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-retrieval-accuracy-ingestion-throughput-500x293.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-retrieval-accuracy-ingestion-throughput-153x90.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-retrieval-accuracy-ingestion-throughput-362x212.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multimodal-pdf-retrieval-accuracy-ingestion-throughput-187x110.png 187w\" sizes=\"(max-width: 680px) 100vw, 680px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Comparison of retrieval accuracy and ingestion throughput&nbsp; for multimodal PDFs</em></figcaption></figure></div>\n\n\n<p class=\"has-small-font-size\">Multimodal PDF retrieval accuracy evaluated on publicly available dataset of PDFs consisting of text, charts, and tables, with NIM-On: nv-yolox-structured-image-v1, DePlot, CACHED, PaddleOCR, nv-embedqa-e5-v5, nv-rerankqa-mistral-4b-v3 compared with NIM-Off: open-source alternatives, on 2xA100 GPUs.</p>\n\n\n\n<p class=\"has-small-font-size\">Multimodal PDF ingestion throughput pages per second, evaluated on publicly available dataset of PDFs consisting of text, charts, and tables, with NIM-On: nv-yolox-structured-image-v1, DePlot, CACHED, PaddleOCR, nv-embedqa-e5-v5, nv-rerankqa-mistral-4b-v3 compared with NIM-Off: open-source alternative running on multithreaded CPUs.</p>\n\n\n\n<p>NIM microservices are part of the NVIDIA AI Enterprise license, which offers API stability, security patches, quality assurance, and support for a smooth transition from prototype to production for enterprises that run their businesses on AI (Figure 3).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"401\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nim-microservice-components-625x401.png\" alt=\"Diagram includes industry-standard APIs, prebuilt containers and Helm charts, domain-specific code, and optimized inference engines.\u00a0\" class=\"wp-image-88040\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nim-microservice-components-625x401.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nim-microservice-components-300x193.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nim-microservice-components-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nim-microservice-components-768x493.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nim-microservice-components-645x414.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nim-microservice-components-467x300.png 467w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nim-microservice-components-140x90.png 140w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nim-microservice-components-362x232.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nim-microservice-components-171x110.png 171w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nim-microservice-components.png 843w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. What\u2019s included in a NIM microservice</em></figcaption></figure></div>\n\n\n<h2 id=\"uncovering_intelligence_in_enterprise_data&nbsp;\"  class=\"wp-block-heading\">Uncovering intelligence in enterprise data&nbsp;<a href=\"#uncovering_intelligence_in_enterprise_data&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To enable enterprises to make the most of their troves of data, NVIDIA is partnering with data and storage platform partners including Box, Cloudera, Cohesity, DataStax, Dropbox, and Nexla.</p>\n\n\n\n<h3 id=\"cloudera\"  class=\"wp-block-heading\">Cloudera<a href=\"#cloudera\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>\u201cWith the integration of NVIDIA NIM microservices in the Cloudera AI Inference service (available now as <a href=\"https://blog.cloudera.com/cloudera-introduces-ai-inference-service-with-nvidia-nim/\">Tech Preview</a>), companies can match the exabytes of private data managed in Cloudera with the high-performance models powering RAG use cases,\u201d said Priyank Patel, vice president of enterprise AI products at Cloudera.&nbsp;</p>\n\n\n\n<p>\u201cOur collaboration with NVIDIA enables best-in-class AI platform capabilities for enterprises wherever they choose to run their AI, on-premises and on cloud.\u201d</p>\n\n\n\n<h3 id=\"cohesity\"  class=\"wp-block-heading\">Cohesity<a href=\"#cohesity\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>\u201cTo unlock the full potential of their proprietary data for AI applications, enterprises must efficiently process and analyze vast amounts of information stored in their backups and archives,&#8221; said Greg Statton, CTO of Data &amp; AI at Cohesity.&nbsp;</p>\n\n\n\n<p>&#8220;The NeMo Retriever multimodal PDF workflow has the potential to add generative AI intelligence to our customers&#8217; data backups and archives, enabling them to extract valuable insights from millions of documents quickly and accurately. Bringing together this workflow with Cohesity Gaia can allow our customers to focus on innovation and strategic decision-making rather than grappling with complex data integration challenges.&#8221;</p>\n\n\n\n<h3 id=\"datastax\"  class=\"wp-block-heading\">Datastax<a href=\"#datastax\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>\u201cUnlocking value from proprietary enterprise data for AI applications requires ingesting and extracting knowledge from millions of structured and unstructured documents,\u201d said Ed Anuff, chief product officer at Datastax.&nbsp;</p>\n\n\n\n<p>\u201cWe\u2019re teaming with NVIDIA to leverage the speed and scale of accelerated computing and the NeMo Retriever data extraction workflow for PDFs, with DataStax AstraDB and DataStax Hyper-Converged Database to enable customers to focus on innovation rather than complex data integration challenges.\u201d</p>\n\n\n\n<h3 id=\"dropbox\"  class=\"wp-block-heading\">Dropbox<a href=\"#dropbox\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>\u201cExpanding beyond text retrieval to tables and images can enable customers to unlock insights across their cloud content,\u201d said Manik Singh, general manager at Dropbox.&nbsp;</p>\n\n\n\n<p>\u201cWe are evaluating NeMo Retriever multimodal PDF extraction workflow, as an option to explore bringing new generative AI capabilities to help our customers uncover these valuable insights.\u201d&nbsp;</p>\n\n\n\n<h3 id=\"nexla\"  class=\"wp-block-heading\">Nexla<a href=\"#nexla\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>&#8220;Scaling generative AI demos to production-grade solutions is a big challenge for enterprises. Our collaboration can address this with the integration of NVIDIA NIM in Nexla&#8217;s no-code/low-code platform for Document ETL, with the potential to scale multimodal ingestion across millions of documents in enterprise systems including Sharepoint, SFTP, S3, Network Drives, Dropbox, and more,\u201d said Saket Saurabh, CEO and co-founder of Nexla.&nbsp;</p>\n\n\n\n<p>\u201cNexla will support NIM in both cloud and private data center environments, across the full capability set including embedding generation, model execution, reasoning, and retrieval solutions to help customers accelerate their AI roadmap,&#8221; said Saurabh.</p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Experience the <a href=\"https://build.nvidia.com/nvidia/multimodal-pdf-data-extraction-for-enterprise-rag\">NVIDIA AI Blueprint for multimodal PDF data extraction </a> with our interactive demo in the <a href=\"http://build.nvidia.com\">NVIDIA API catalog</a>. Apply for <a href=\"https://developer.nvidia.com/nemo-microservices\">early access to preview this workflow blueprint</a> using open-source code, customization instructions, and a Helm chart for deployment.\u00a0\u00a0</p>\n\n\n\n<p>Join developers from around the globe in building a RAG application, elevating your skills, and competing for exciting prizes by registering for the <a href=\"https://developer.nvidia.com/llamaindex-developer-contest/\">NVIDIA and LlamaIndex developer contest</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Trillions of PDF files are generated every year, each file likely consisting of multiple pages filled with various content types, including text, images, charts, and tables. This goldmine of data can only be used as quickly as humans can read and understand it.&nbsp; But with generative AI and retrieval-augmented generation (RAG), this untapped data can &hellip; <a href=\"https://developer.nvidia.com/blog/build-an-enterprise-scale-multimodal-document-retrieval-pipeline-with-nvidia-nim-agent-blueprint/\">Continued</a></p>\n", "protected": false}, "author": 969, "featured_media": 87950, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1476680", "discourse_permalink": "https://forums.developer.nvidia.com/t/build-an-enterprise-scale-multimodal-document-retrieval-pipeline-with-nvidia-nim-agent-blueprint/304933", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [453, 4134, 3613], "coauthors": [1721, 3328, 2598, 3999], "class_list": ["post-87948", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-nim-agent-blueprint", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["General"], "post_products": ["NeMo", "NeMo Retriever", "NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-blueprints-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mSw", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87948"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/969"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87948"}], "version-history": [{"count": 20, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87948/revisions"}], "predecessor-version": [{"id": 91934, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87948/revisions/91934"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87950"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87948"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87948"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87948"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87948"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 88097, "date": "2024-08-28T06:00:00", "date_gmt": "2024-08-28T13:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=88097"}, "modified": "2024-11-14T08:09:00", "modified_gmt": "2024-11-14T16:09:00", "slug": "deploy-diverse-ai-apps-with-multi-lora-support-on-rtx-ai-pcs-and-workstations", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/deploy-diverse-ai-apps-with-multi-lora-support-on-rtx-ai-pcs-and-workstations/", "title": {"rendered": "Deploy Diverse AI Apps with Multi-LoRA Support on RTX AI PCs and Workstations"}, "content": {"rendered": "\n<p>Today\u2019s large language models (LLMs) achieve unprecedented results across many use cases. Yet, application developers often need to customize and tune these models to work specifically for their use cases, due to the general nature of foundation models.&nbsp;</p>\n\n\n\n<p>Full fine-tuning requires a large amount of data and compute infrastructure, resulting in model weights being updated. This method requires multiple instances of a model being hosted and run on GPU memory to serve many use cases on a single device.</p>\n\n\n\n<p>Example use cases include a multilingual translation assistant, where a user needs results simultaneously and in multiple languages. This can be a challenge for on-device AI, due to memory constraints.</p>\n\n\n\n<p>Hosting multiple LLMs on device memory simultaneously is nearly impossible, especially when considering running suitable latency and throughput requirements to engage with users. On the other hand, users often run multiple apps and tasks at any given time, sharing system resources across applications.&nbsp;</p>\n\n\n\n<p>Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) help developers attach custom adapters to a single LLM to serve multiple use cases. This requires minimal additional memory, while still providing capabilities for task-specific AI. The technique makes it easy for developers to scale the number of use cases and applications that can be served on-device.&nbsp;</p>\n\n\n\n<p>Multi-LoRA support is now available in NVIDIA TensorRT-LLM, part of the <a href=\"https://developer.nvidia.com/rtx/ai-toolkit\">NVIDIA RTX AI Toolkit</a>. This new feature enables RTX AI PCs and workstations to handle various use cases during inference time.</p>\n\n\n\n<h2 id=\"introduction_to_lora\"  class=\"wp-block-heading\">Introduction to LoRA<a href=\"#introduction_to_lora\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>LoRA emerged as a popular parameter-efficient fine-tuning technique that tunes a small amount of parameters. The additional parameters are called LoRA adapters, representing the low-rank decomposition of the changes in the dense layers of the network.&nbsp;</p>\n\n\n\n<p>Only these low-rank additional adapters are customized, while the remaining parameters of the model are held frozen during the process. Once trained, these adapters are deployed by merging into the foundation model during inference time, adding minimal to no overhead on inference latencies and throughput.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"314\" height=\"290\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/LoRA-technique.png\" alt=\"A diagram showing the LoRA fine-tuning technique. \" class=\"wp-image-88099\" style=\"width:464px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/LoRA-technique.png 314w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/LoRA-technique-300x277.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/LoRA-technique-125x115.png 125w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/LoRA-technique-97x90.png 97w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/LoRA-technique-119x110.png 119w\" sizes=\"(max-width: 314px) 100vw, 314px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Parameters in A and B represent trainable parameters to showcase the LoRA technique </em></em><br><em><em>(credit: </em><a href=\"https://arxiv.org/pdf/2106.09685\"><em>LoRA: Low-Rank Adaptation of Large Language Models</em></a><em>)</em></em></figcaption></figure></div>\n\n\n<p>Figure 1 showcases additional details on the LoRA technique:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>The weights (W) of the pretrained model are frozen during customization.</li>\n\n\n\n<li>Instead of updating W, two smaller trainable matrices, A and B are injected, which learn task-specific information. The matrix multiplication B*A forms a matrix with the same dimensions as W, thus it can be added to W (= W + BA).&nbsp;</li>\n</ul>\n\n\n\n<p>The ranks of A and B matrices are small values like 8, 16, and so on. This rank (r) parameter is customizable at training time. A larger rank value enables the model to capture more nuances relevant to the downstream task, approaching the capacity of fully supervised fine-tuning by updating all the parameters in the model.&nbsp;</p>\n\n\n\n<p>On the downside, larger ranks are also more expensive for training and inference, in terms of memory and compute requirements. In practice, LoRA fine-tuning with a rank value as small as eight is already very effective and is a good starting point for many downstream tasks.</p>\n\n\n\n<p>Today, within the RTX AI Toolkit there&#8217;s support for quantization and low-rank adaptation (QLoRA), a variation of the LoRA technique, to perform parameter-efficient fine-tuning on RTX systems. This technique is an adaptation that reduces memory usage. </p>\n\n\n\n<p>During backpropagation, gradients are passed through a frozen, 4-bit quantized pretrained model into low-rank adapters. Effectively, the QLoRA algorithm saves memory, without sacrificing model performance. Refer <a href=\"https://arxiv.org/abs/2305.14314\">to the following paper</a> for additional information on QLoRA.</p>\n\n\n\n<h2 id=\"multi-lora_support_in_tensorrt-llm&nbsp;\"  class=\"wp-block-heading\">Multi-LoRA support in TensorRT-LLM&nbsp;<a href=\"#multi-lora_support_in_tensorrt-llm&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With the latest updates in TensorRT-LLM, RTX AI Toolkit now natively supports the ability to serve multiple LoRA adapters with a single quantized base checkpoint at inference time. This new technique enables serving multiple FP16 LoRA adapters with INT4 quantized base-model checkpoints.&nbsp;</p>\n\n\n\n<p>Mixed-precision deployments can be useful in Windows PC environments as they have limited memory that must be shared across apps. Mixed-precision deployments can help by reducing the memory needed for model storage and inference, without sacrificing model quality or the ability to serve multiple clients with custom models.&nbsp;</p>\n\n\n\n<p>There are several ways in which a developer can deploy multiple LoRA adapters in their application and they include the following.</p>\n\n\n\n<p><strong>Single LoRA adapter deployment</strong></p>\n\n\n\n<p>In this setup, developers choose which LoRA adapter to activate for each request best for serving specialized content. For example, a language learning app can switch between adapters fine-tuned for different languages, offering focused practice based on the user&#8217;s current needs.</p>\n\n\n\n<p><strong>Concurrent LoRA adapters for a single request (batch mode)</strong></p>\n\n\n\n<p>In this method, a single input prompt generates multiple different responses, with each response produced by a different LoRA adapter in batch mode. This is useful for complex applications like a multilingual virtual assistant, where one query can simultaneously yield responses in English, Spanish, and Japanese, each tailored by a specific adapter.</p>\n\n\n\n<p><strong>Concurrent LoRA adapters for multiple requests (batch mode)</strong></p>\n\n\n\n<p>This approach processes several input prompts at the same time. Each prompt is paired with a different LoRA adapter and generates multiple output prompts. For example, multiple PC applications send inference requests to the same model, and depending on the request, a different adapter is selected, ensuring that each application receives a tailored response specific to its needs.</p>\n\n\n\n<p>Learn more about TensorRT-LLM support for <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-models-with-lora\">multiple LoRA adapter checkpoints</a>.&nbsp;</p>\n\n\n\n<h2 id=\"showcasing_the_power_of_multi-lora_on_rtx_pcs\"  class=\"wp-block-heading\">Showcasing the power of multi-LoRA on RTX PCs<a href=\"#showcasing_the_power_of_multi-lora_on_rtx_pcs\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To demonstrate the power of multi-LoRA support, we\u2019ve developed a sample application designed for story creation and illustration driven by a single prompt. The process unfolds in two key steps.&nbsp;</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>The user inputs a basic idea for a story, and the Llama 3 model fleshes out this concept, expanding on the initial idea to provide a detailed foundation.</li>\n\n\n\n<li>The application uses the same Llama 3 model, now enhanced with two distinct LoRA adapters, to further refine the story and generate corresponding imagery. One LoRA adapter generates a Stable Diffusion prompt, which is used to illustrate the story visually through a locally deployed Stable Diffusion XL model. The other is fine-tuned for story writing, and crafts a well-structured and engaging narrative.</li>\n</ol>\n\n\n\n<p>This approach ensures that the space required for the process doesn\u2019t increase significantly, as the same model is used for both passes. The second pass, which involves text and image generation, is performed using batched inference, making the process fast and efficient. Users can rapidly iterate through different versions of their stories, refining the narrative and illustrations easily.</p>\n\n\n\n<p>By using multi-LoRA support in this streamlined two-step process, we showcase how creative and computational efficiency can be maximized, all from a single prompt. The following is a sample set of input-output pairs.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>User Prompt:\n\nGenerated Story:\n*The Whispering Woods*\n\n\nIn the heart of the Whispering Woods, a legend whispered among the ancient trees spoke of a green giant, said to possess the secrets of the forest. The giant, named Eira, was rumored to be able to communicate with the trees themselves, and its presence was said to bring balance and harmony to the forest. One day, a young adventurer named Lyra stumbled upon the clearing where Eira was said to reside. As she approached the giant, she felt an eerie silence fall over the clearing, as if the trees themselves were holding their breath in anticipation. Eira's massive form loomed before her, its skin a deep, mossy green that seemed to blend seamlessly into the surrounding foliage. The giant's eyes, like ancient pools of wisdom, gazed down at Lyra with a gentle intensity.\n\nAs she drew closer, Lyra noticed that the trees around her were indeed communicating with Eira, their leaves rustling softly to convey ancient secrets and whispers of the forest. The air was thick with the scent of damp earth and decaying leaves, and Lyra felt the weight of the forest's history bearing down upon her.\n\nEira's voice, like a gentle breeze, whispered secrets to the trees, and they responded with soft murmurs and creaks. Lyra felt a sense of wonder wash over her as she realized that the giant was indeed the keeper of the forest's secrets. She reached out a trembling hand, and to her surprise, Eira's massive fingers wrapped around hers, like a gentle giant's grasp.\n\nIn that moment, Lyra felt the forest's balance and harmony flow through her, and she knew that she had stumbled upon something much greater than a mere legend. As she stood there, surrounded by the ancient trees and the whispers of the forest, Lyra felt a sense of belonging wash over her, as if she had finally found her true home.</code></pre>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"512\" height=\"512\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA.png\" alt=\"Stable Diffusion-generated image of a green giant in the forest based on LoRA output for the given prompt.\" class=\"wp-image-88100\" style=\"width:620px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA.png 512w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA-300x300.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA-115x115.png 115w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA-90x90.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA-150x150.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA-362x362.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/generated-image-LoRA-110x110.png 110w\" sizes=\"(max-width: 512px) 100vw, 512px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. A generated image of a character, Eira, from the prompt</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"883\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA.png\" alt=\"This diagram shows the flow from a user prompt to LoRA outputs using Llama3-8B as base model.\" class=\"wp-image-88102\" style=\"width:964px;height:auto\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA-300x133.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA-625x276.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA-179x79.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA-768x339.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA-1536x678.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA-645x285.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA-500x221.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA-160x71.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA-362x160.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA-249x110.png 249w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Stable-Diffusion-LoRA-1024x452.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. A diagram of Stable Diffusion using Llama3-8B and LoRA</em></figcaption></figure></div>\n\n\n<h2 id=\"accelerating_multi-lora_use_cases_on_windows_pcs_with_tensorrt-llm\"  class=\"wp-block-heading\">Accelerating multi-LoRA use cases on Windows PCs with TensorRT-LLM<a href=\"#accelerating_multi-lora_use_cases_on_windows_pcs_with_tensorrt-llm\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The following figure shows NVIDIA internal measurements showcasing throughput performance on the NVIDIA GeForce RTX 4090 using a Llama-3-8B model, with the foundation model and multiple LoRA adapters at varying batch sizes using TensorRT-LLM.&nbsp;</p>\n\n\n\n<p>The results show throughput at an input sequence length of 43 and an output sequence length of 100 tokens. At batch sizes larger than 1, each sample uses a unique LoRA adapter with a maximum engine rank of 64.</p>\n\n\n\n<p>Higher throughput is better and we see about a 3% performance degradation when running multiple LoRA adapters.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1350\" height=\"836\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/throughput-LoRA.png\" alt=\"Performance of Llama-3-8B when multiple LoRA adapters are used, varying by batch size. \" class=\"wp-image-88103\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/throughput-LoRA.png 1350w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/throughput-LoRA-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/throughput-LoRA-625x387.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/throughput-LoRA-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/throughput-LoRA-768x476.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/throughput-LoRA-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/throughput-LoRA-484x300.png 484w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/throughput-LoRA-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/throughput-LoRA-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/throughput-LoRA-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/throughput-LoRA-1024x634.png 1024w\" sizes=\"(max-width: 1350px) 100vw, 1350px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. NVIDIA internal throughput performance measurements on an RTX 4090 PC</em></figcaption></figure></div>\n\n\n<p>Figure 5 shows NVIDIA latency performance measurements on an RTX 4090 PC using a Llama-3-8B model with both the pre-trained foundation model and multiple LoRA adapters at varying batch sizes using TensorRT-LLM 0.11.</p>\n\n\n\n<p>The results showcase latency at an input sequence length of 43, and an output sequence length of 100 tokens. At batch sizes larger than 1, each sample uses a unique LoRA adapter with a maximum engine rank of 64.\u00a0 Lower latency is better and we see about a 3% performance degradation when running multiple LoRA adapters.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1350\" height=\"836\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latency-LoRA.png\" alt=\"Latency performance shown for Llama-3-8B model when multiple LoRA adapters are used with varying batch sizes. \" class=\"wp-image-88104\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latency-LoRA.png 1350w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latency-LoRA-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latency-LoRA-625x387.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latency-LoRA-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latency-LoRA-768x476.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latency-LoRA-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latency-LoRA-484x300.png 484w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latency-LoRA-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latency-LoRA-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latency-LoRA-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latency-LoRA-1024x634.png 1024w\" sizes=\"(max-width: 1350px) 100vw, 1350px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. NVIDIA latency performance measurements on RTX 4090 PC</em></figcaption></figure></div>\n\n\n<p>Figures 4 and 5 show that TensorRT-LLM 0.11 delivers great performance with minimal throughput and latency degradation across batch sizes when using multiple LoRA adapters at inference time. On average, we see about a 3% reduction in throughput and latency performance across batch sizes compared to running the foundation model, when using multiple unique LoRA adapters with TensorRT-LLM 0.11.</p>\n\n\n\n<h2 id=\"next_steps\"  class=\"wp-block-heading\">Next steps<a href=\"#next_steps\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With the latest updates, developers can <a href=\"https://github.com/NVIDIA/RTX-AI-Toolkit/blob/main/tutorial-llama3-finetune.md\">customize models</a> with LoRA techniques on device, and <a href=\"https://github.com/NVIDIA/RTX-AI-Toolkit/blob/main/llm-deployment/README.md\">deploy models</a> to serve multiple use cases using multi-LoRA support on RTX AI PCs and workstations.&nbsp;&nbsp;</p>\n\n\n\n<p><a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-int4-awq-llama-with-several-fp16-lora-checkpoints\">Get started</a> with multi-LoRA on TensorRT-LLM.&nbsp;</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Today\u2019s large language models (LLMs) achieve unprecedented results across many use cases. Yet, application developers often need to customize and tune these models to work specifically for their use cases, due to the general nature of foundation models.&nbsp; Full fine-tuning requires a large amount of data and compute infrastructure, resulting in model weights being updated. &hellip; <a href=\"https://developer.nvidia.com/blog/deploy-diverse-ai-apps-with-multi-lora-support-on-rtx-ai-pcs-and-workstations/\">Continued</a></p>\n", "protected": false}, "author": 1475, "featured_media": 88098, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1476588", "discourse_permalink": "https://forums.developer.nvidia.com/t/deploy-diverse-ai-apps-with-multi-lora-support-on-rtx-ai-pcs-and-workstations/304916", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110, 1903], "tags": [453, 4180, 570, 3719, 492], "coauthors": [2931, 3133], "class_list": ["post-88097", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "category-features", "tag-featured", "tag-rtx-ai", "tag-rtx", "tag-stable-diffusion", "tag-workstation"], "acf": {"post_industry": ["General"], "post_products": ["RTX GPU", "TensorRT", "TensorRT-LLM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Multi-LoRA-Support.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mUV", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88097"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1475"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=88097"}], "version-history": [{"count": 23, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88097/revisions"}], "predecessor-version": [{"id": 88267, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/88097/revisions/88267"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/88098"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=88097"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=88097"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=88097"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=88097"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87901, "date": "2024-08-27T11:30:00", "date_gmt": "2024-08-27T18:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87901"}, "modified": "2024-09-05T10:57:21", "modified_gmt": "2024-09-05T17:57:21", "slug": "simplifying-camera-calibration-to-enhance-ai-powered-multi-camera-tracking", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/simplifying-camera-calibration-to-enhance-ai-powered-multi-camera-tracking/", "title": {"rendered": "Simplifying Camera Calibration to Enhance AI-Powered Multi-Camera Tracking"}, "content": {"rendered": "\n<p><em>This post is the third in a series on building multi-camera tracking vision AI applications.</em><em> We introduce the overall end-to-end workflow and fine-tuning process to enhance system accuracy in the </em><a href=\"https://developer.nvidia.com/blog/optimize-processes-for-large-spaces-with-the-multi-camera-tracking-workflow/\"><em>first part</em></a><em> and </em><a href=\"https://developer.nvidia.com/blog/enhance-multi-camera-tracking-accuracy-by-fine-tuning-ai-models-with-synthetic-data/\"><em>second part</em></a><em>.</em></p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/metropolis\">NVIDIA Metropolis</a> is an application framework and set of developer tools that leverages AI for visual data analysis across industries. Its multi-camera tracking reference AI workflows, powered by <a href=\"https://developer.nvidia.com/metropolis-microservices\">cloud-native NVIDIA Metropolis microservices</a>, enable advanced object tracking and localization across multiple cameras. This post discusses camera calibration, how to calibrate real cameras using the <a href=\"https://docs.nvidia.com/mms/text/MDX_Camera_Calibration_User_Guide.html\">Metropolis Camera Calibration Toolkit</a>, and how to calibrate synthetic cameras using the <a href=\"https://www.nvidia.com/en-us/omniverse/\">NVIDIA Omniverse</a> extension.</p>\n\n\n\n<h2 id=\"camera_calibration&nbsp;\"  class=\"wp-block-heading\">Camera calibration&nbsp;<a href=\"#camera_calibration&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><em>Camera calibration</em> is the process of determining specific camera parameters or estimating the characteristics of a camera. Camera calibration enables translating what the camera sees in 2D into a real-world coordinate system and it is the foundation for many vision based applications. For example, camera calibration is needed to complete specific operations involving mainly coordinate conversions in creating a <a href=\"https://www.nvidia.com/en-us/use-cases/ai-powered-multi-camera-tracking/\">multi-camera tracking</a> application.&nbsp;</p>\n\n\n\n<p>Camera parameters are composed of two parts: extrinsic parameters and intrinsic parameters. Extrinsic parameters define the translation and rotation of the camera with respect to the designated world coordinate system, enabling a mapping between camera coordinates and the world coordinates. Intrinsic parameters enable mapping between camera coordinates and pixel coordinates.&nbsp;&nbsp;</p>\n\n\n\n<h2 id=\"camera_calibration_in_multi-camera_tracking\"  class=\"wp-block-heading\">Camera calibration in multi-camera tracking<a href=\"#camera_calibration_in_multi-camera_tracking\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA Metropolis reference workflows for <a href=\"https://docs.nvidia.com/mms/text/Multi_Camera_AI_toc.html\">multi-camera AI</a> primarily use cameras as sensors and extensively work with the spatial-temporal aspects of the detected moving objects. Objects are initially detected in the pixel domain within the camera view, and an essential step to enable many spatial-temporal analytics downstream is to correlate the pixel domain with a desired coordinate system.&nbsp;</p>\n\n\n\n<p>Camera positioning has a significant impact on the overall accuracy of the downstream tasks in the Metropolis AI workflow. For details, see the <a href=\"https://docs.nvidia.com/mms/text/MDX_Camera_Positioning_Guide.html\">Camera Positioning Guide</a>. With proper camera calibration, it helps to locate the detected object in a desired coordinate system, This plays an essential role in many core abilities, including but not limited to:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Location service using a camera as a sensor:</strong> Detect objects within the camera and compute their locations onto a coordinate system with real-world meaning through calibrated camera parameters. For example, a customer in a retail store seen from a camera can be located on the floor plan map.</li>\n\n\n\n<li><strong>Activity correlation across multiple cameras:</strong> When multiple cameras are calibrated against the same coordinate system, you can correlate and reason across cameras to provide integrated insights. For example, the movement history in the shared coordinate system can help determine if person_A detected by camera_1, and person_B detected by camera_2 is the same person or not. A second example is seamlessly tracking one person walking in a warehouse that has different cameras covering separate sections.&nbsp;&nbsp;</li>\n\n\n\n<li><strong>Distance-based metric computation:</strong> Due to the camera&#8217;s nature, the distance computation directly in the pixel domain is not reliable. For instance, the actual distance in meters covered by X number of pixels can vary significantly depending on the location within a frame. Calibrating a camera against a Cartesian coordinate system can make the distance computation handy.&nbsp;&nbsp;&nbsp;</li>\n</ul>\n\n\n\n<p>Figure 1 shows an example of a real-time location system where the location of each individual person is tracked on the floor plan map of a 100,000 square-foot warehouse space covered by 100 cameras. Such a system builds upon the aforementioned core abilities enabled through proper camera calibration.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"975\" height=\"425\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image6.png\" alt=\"Two side-by-side images. On the left: various camera angles; on the right: locations of each tracked object on the floor plan map.\" class=\"wp-image-87910\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image6.png 975w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image6-300x131.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image6-625x272.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image6-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image6-768x335.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image6-645x281.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image6-500x218.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image6-160x70.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image6-362x158.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image6-252x110.png 252w\" sizes=\"(max-width: 975px) 100vw, 975px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. A real-time location system tracking multiple objects across multiple cameras&nbsp;</em></em></figcaption></figure></div>\n\n\n<p>The current Metropolis reference AI workflows assume that cameras have no or very little distortion. This ensures the mapping is linear between the pixel coordinate system and the designated world coordinate system. This is calculated with reference points selected from both coordinate systems.</p>\n\n\n\n<p>In particular, a series of landmarks is required, where the pixel value (x, y) and world coordinates (X, Y, Z) are known for each of them. In many of our use cases, the designated world coordinate system is a 2D cartesian plane, or to be more concrete, an image of the floor plan map. Using a corner of the floor plan map as origin, this Cartesian can be translated as the pixel values of the plan map image.&nbsp;</p>\n\n\n\n<p>In this sense, the world coordinates (X, Y, Z) become (X, Y), the 2D-to-3D mapping problem literally becomes a 2D-to-2D perspective transformation problem, and a 3 x 3 homography matrix computed from the landmarks as the output of this calibration process can be used to perform location transformation later.</p>\n\n\n\n<p>Figure 2 provides two examples that demonstrate the process, considering the task is to find the mapping between the camera pixels on the left and the floor plan map on the right, the only thing needed is a series of landmarks {1, 2, 3, 4, \u2026} that can be located on both the left and right. Once the series of landmarks is selected, camera location (x, y) and world location (X, Y) are then given as the corresponding pixel coordinates and the homography matrix can be easily calculated with existing libraries (OpenCV, for example).&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"797\" height=\"456\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/reference-points-camera-view-floor-plan-map.png\" alt=\"On the left are selected reference points in the camera view and on the right are the same reference points on the floor plan map. Mapping between left and right can be derived with these reference points.\n\" class=\"wp-image-87913\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/reference-points-camera-view-floor-plan-map.png 797w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/reference-points-camera-view-floor-plan-map-300x172.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/reference-points-camera-view-floor-plan-map-625x358.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/reference-points-camera-view-floor-plan-map-179x102.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/reference-points-camera-view-floor-plan-map-768x439.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/reference-points-camera-view-floor-plan-map-645x369.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/reference-points-camera-view-floor-plan-map-500x286.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/reference-points-camera-view-floor-plan-map-157x90.png 157w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/reference-points-camera-view-floor-plan-map-362x207.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/reference-points-camera-view-floor-plan-map-192x110.png 192w\" sizes=\"(max-width: 797px) 100vw, 797px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. Reference points from both the camera view and the floor plan map</em></em></figcaption></figure>\n\n\n\n<p>Now, solving the pixel-to-world mapping boils down to selecting the reference points and acquiring their pixel values in both the camera frame and floor plan map. This is not an easy task, as it may seem. A properly designed tool can drastically reduce the workload and simplify the task.&nbsp;</p>\n\n\n\n<h2 id=\"simplifying_real-world_calibration_with_the_metropolis_camera_calibration_toolkit\"  class=\"wp-block-heading\">Simplifying real-world calibration with the Metropolis Camera Calibration Toolkit<a href=\"#simplifying_real-world_calibration_with_the_metropolis_camera_calibration_toolkit\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With the <a href=\"https://docs.nvidia.com/mms/text/MDX_Camera_Calibration_User_Guide.html\">Metropolis Camera Calibration Toolkit</a>, you can perform camera calibration tasks and output files in the proper format to connect with other Metropolis services seamlessly.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/-LtMmnPfpRU?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Learn how to use the Metropolis Camera Calibration Toolkit</em></figcaption></figure>\n\n\n\n<p>The Metropolis Camera Calibration Toolkit provides:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Project organization, import, and export</li>\n\n\n\n<li>Easy camera import with Metropolis media services</li>\n\n\n\n<li>GUI for reference point selection</li>\n\n\n\n<li>On-the-fly reprojection error for self-checking</li>\n\n\n\n<li>Add-ons to support more Metropolis functionalities including regions of interest (ROIs) and tripwires</li>\n\n\n\n<li>File insertion using an API</li>\n</ul>\n\n\n\n<p>Three calibrations modes are available:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Cartesian Calibration:</strong> A method for creating an extrinsic calibration without a reference building map. As a user, you can create your global coordinate system.&nbsp;</li>\n\n\n\n<li><strong>Multi-Camera Tracking:</strong> This mode is the most common way to use the Calibration Toolkit, and is the focus of this post. In this form, the user provides a building map, which each camera will calibrate to.&nbsp;</li>\n\n\n\n<li><strong>Image:</strong> Draw tripwires and ROIs on an image and get the pixel coordinates. For some downstream tasks, certain artifacts need to be added to the camera view for computing a particular metric\u2014object counting based on tripwire crossings, for example.&nbsp;</li>\n</ul>\n\n\n\n<p>The easiest way to start is to use the assets provided in the sample apps to import a project. Given the calibration.json, the imageMetadata.json, and the Images.zip, a user can import a project that is already set up.&nbsp;</p>\n\n\n\n<p>To create a Multi-Camera Tracking project from scratch, you\u2019ll need a floor plan and a <a href=\"https://docs.nvidia.com/mms/text/Media_toc.html\">media service</a>, or images from the cameras. Information from the cameras will be needed to help set the camera up for the Metropolis UI.&nbsp;</p>\n\n\n\n<p>After you import a project or create a new one, the Project page will take you through the steps to create the necessary artifacts. To start, upload the floor plan. The floorplan can be a building map representative of the space the camera sees.&nbsp;</p>\n\n\n\n<p>Next, if you\u2019re using a media service, the sensors can be imported using the URL most relevant to the deployment case. This is the easiest way to get the sensors into the toolkit, but it\u2019s not necessary. If you\u2019re not using a media service, you\u2019ll need to set up the sensors.&nbsp;</p>\n\n\n\n<p>For each sensor, first create a new sensor and add the camera details. These details are necessary to import the camera into the Metropolis workflow. After closing the modal, you\u2019ll see a sensor shown in the grid, where you can click the camera icon to upload an image. If you discovered sensors in the previous step, the images should have been pulled from the media service. Repeat this for each sensor.&nbsp;&nbsp;</p>\n\n\n\n<p>When calibrating, create a polygon for each sensor on the camera image and its corresponding polygon on the floor plan. Choose points or landmarks in the floor plan space that are visible in the camera image. These corresponding pairs enable you to create a transformation matrix to map trajectories in the camera space onto the floor plan.&nbsp;</p>\n\n\n\n<p>Ideally, select between eight and 15 points, depending on how large the space is. At least eight points are required to start. Figure 2 shows identified landmarks in the floorplan that are visible in the camera image. If the calibration seems to falter in one area, adding more points can help improve the trajectories.&nbsp;</p>\n\n\n\n<p>After creating the two polygons, add an ROI polygon, tripwires, and direction wires. See Figure 3 for an example of the calibration stage.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1250\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit.png\" alt=\"On the left is the camera view where selected reference points are displayed in red polygon. On the right is the floor plan view where the same reference points are displayed as a blue polygon, the region of interest (ROI) as a green polygon, and tripwires as red lines.\n\" class=\"wp-image-87919\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit-300x188.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit-625x391.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit-179x112.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit-768x480.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit-1536x960.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit-645x403.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit-480x300.png 480w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit-144x90.png 144w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit-362x226.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit-176x110.png 176w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/full-calibrated-camera-nvidia-metropolis-camera-calibration-toolkit-1024x640.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. An example of a fully calibrated camera after the Calibration stage in the Metropolis Camera Calibration Toolkit&nbsp;</em></em></figcaption></figure>\n\n\n\n<p>After the calibration has been set up, click Calibrate to view the reprojection error and click accept (if acceptable). Then click Validate to test the calibration. Draw trajectories or polygons to see how points in the camera domain may fall on the floor plan. If the projected points on the floorplan are acceptable, you can Validate the calibration. If not, adjust the polygons in the calibration stage until the calibration is acceptable.&nbsp;</p>\n\n\n\n<p>Next, set up the floor plan and place the camera on the floor plan map. This is necessary for the camera to appear in the UI of the Metropolis workflow. See Figure 4 for an example of how to place the sensor.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"473\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/floor-plan-sensor-metropolis-camera-calibration-toolkit-625x473.png\" alt=\"Building_K_Cam1 is displayed as a camera icon at its true location on the floor plan map.\n\" class=\"wp-image-87922\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/floor-plan-sensor-metropolis-camera-calibration-toolkit-625x473.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/floor-plan-sensor-metropolis-camera-calibration-toolkit-300x227.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/floor-plan-sensor-metropolis-camera-calibration-toolkit-152x115.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/floor-plan-sensor-metropolis-camera-calibration-toolkit-768x582.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/floor-plan-sensor-metropolis-camera-calibration-toolkit-645x488.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/floor-plan-sensor-metropolis-camera-calibration-toolkit-396x300.png 396w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/floor-plan-sensor-metropolis-camera-calibration-toolkit-119x90.png 119w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/floor-plan-sensor-metropolis-camera-calibration-toolkit-362x274.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/floor-plan-sensor-metropolis-camera-calibration-toolkit-145x110.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/floor-plan-sensor-metropolis-camera-calibration-toolkit-1024x775.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/floor-plan-sensor-metropolis-camera-calibration-toolkit.png 1302w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. A Sensor placed on the floor plan within the Metropolis Camera Calibration Toolkit&nbsp;</em></em></figcaption></figure>\n\n\n\n<p>Finally, export the artifacts <code>calibration.json</code>, <code>imageMetadata.json</code>, and <code>images.zip</code>, which can be used in the Metropolis workflow. &nbsp;</p>\n\n\n\n<p>With the Metropolis Camera Calibration Toolkit, you can easily streamline the workflow of manual camera calibration work on real cameras. It provides the formatted files for downstream Metropolis services to consume seamlessly.&nbsp;</p>\n\n\n\n<h2 id=\"auto-calibration_for_synthetic_cameras_in_nvidia_omniverse\"  class=\"wp-block-heading\">Auto-calibration for synthetic cameras in NVIDIA Omniverse<a href=\"#auto-calibration_for_synthetic_cameras_in_nvidia_omniverse\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Metropolis reference applications can work with synthetic data as well. The reference applications provide synthetic video data created within the <a href=\"https://www.nvidia.com/en-us/omniverse/\">NVIDIA Omniverse</a> platform. Like real-world cameras, synthetic cameras must be calibrated to enable mapping between pixel coordinates and floor plan maps.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/8GMn2Qn0F_0?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 2. Get started with auto-camera calibration in NVIDIA Omniverse</em></figcaption></figure>\n\n\n\n<p>With full control of the synthetic cameras in Omniverse, you don\u2019t need to manually select reference points. Instead, the auto-calibration <code>omni.replicator.agent.camera_calibration</code> extension within Omniverse can output the desired mappings of virtual cameras with a click of a button. This auto-calibration calibration tool is included in the <code>omni.replicator.agent</code> extension. To learn more, see the <a href=\"https://docs.omniverse.nvidia.com/isaacsim/latest/replicator_tutorials/ext_replicator-agent/camera_calibration.html\">Omniverse camera calibration documentation</a>.&nbsp;</p>\n\n\n\n<p>To use <code>omni.replicator.agent.camera_calibration</code>, first create a top-view camera, along with the cameras to be calibrated. The exact camera view from the top-view camera will be used as the floor-plan map. For each camera to be calibrated, the extension will auto-select points on the floor from the camera view and compute their correspondence in the top-view camera.&nbsp;</p>\n\n\n\n<p>Details for using the <code>omni.replicator.agent.camera_calibration</code> extension are provided below:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Reference points are automatically selected by casting rays randomly from the camera&#8217;s view, recording the positions where rays intersect with the floor.&nbsp;</li>\n\n\n\n<li>The camera&#8217;s extrinsic matrix is derived from the camera prim&#8217;s transform matrix, and the intrinsic matrix is computed using the properties of the camera prim.&nbsp;</li>\n\n\n\n<li>Convert the 3D positions of the reference points to 2D positions on the camera&#8217;s image plane and use this data to calculate the camera&#8217;s projection matrix.&nbsp;</li>\n\n\n\n<li>Compute the translation parameters and the scaling factor between the 3D positions of reference points and the top-view camera\u2019s image plane. This provides the correspondence between the camera view and the floor plan map.</li>\n\n\n\n<li>Determine the camera&#8217;s field of view (FOV) by uniformly projecting rays to the ground, collecting data from the hits, and then generating the FOV polygon based on the coordinates of these hits.&nbsp;</li>\n\n\n\n<li>Finally, export the camera&#8217;s intrinsic and extrinsic matrices, along with the projection matrix and the correspondence between the camera view and the floor plan map, to a JSON file and render the FOV polygon on the scene&#8217;s top view image.</li>\n</ul>\n\n\n\n<p>Creating synthetic cameras in Omniverse is relatively easy, and it is a great way to generate synthetic video data for various downstream tasks including model training and simulation. <code>omni.replicator.agent.camera_calibration</code> provides users with a handy tool to create formatted camera calibration files so that the synthetic cameras in Omniverse can be easily used in various Metropolis reference workflows or applications.&nbsp;</p>\n\n\n\n<h2 id=\"conclusion&nbsp;\"  class=\"wp-block-heading\">Conclusion&nbsp;<a href=\"#conclusion&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Camera calibration enables NVIDIA Metropolis reference applications to localize detected objects on a provided floor map and to spatially correlate object locations between multiple cameras. It is the essential step toward building large-scale, real-time location services and other meaningful services in the intelligent video analytics domain.</p>\n\n\n\n<p>To learn more, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://docs.nvidia.com/mms/text/Multi_Camera_AI_toc.html\">Developer Guide on Multi-Camera AI workflows</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/blog/real-time-vision-ai-from-digital-twins-to-cloud-native-deployment-with-nvidia-metropolis-microservices-and-nvidia-isaac-sim/\">Real-Time Vision AI From Digital Twins to Cloud-Native Deployment with NVIDIA Metropolis Microservices and NVIDIA Isaac Sim</a></li>\n\n\n\n<li><a href=\"https://developer.nvidia.com/metropolis-microservices/enterprise-gpu-get-started\">Get Started with NVIDIA Metropolis Microservices</a></li>\n</ul>\n\n\n\n<p>For technical questions, visit the <a href=\"https://forums.developer.nvidia.com/c/accelerated-computing/intelligent-video-analytics/mm-ea/500\">NVIDIA Developer forums</a>. Note that you must first <a href=\"https://developer.nvidia.com/metropolis-microservices-early-access-form\">sign up for the Developer Preview program</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>This post is the third in a series on building multi-camera tracking vision AI applications. We introduce the overall end-to-end workflow and fine-tuning process to enhance system accuracy in the first part and second part. NVIDIA Metropolis is an application framework and set of developer tools that leverages AI for visual data analysis across industries. &hellip; <a href=\"https://developer.nvidia.com/blog/simplifying-camera-calibration-to-enhance-ai-powered-multi-camera-tracking/\">Continued</a></p>\n", "protected": false}, "author": 2256, "featured_media": 87904, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1475442", "discourse_permalink": "https://forums.developer.nvidia.com/t/simplifying-camera-calibration-to-enhance-ai-powered-multi-camera-tracking/304804", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 63, 503], "tags": [453, 1950, 1472, 3327, 1718], "coauthors": [3996, 3997], "class_list": ["post-87901", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-robotics", "category-simulation-modeling-design", "tag-featured", "tag-image-recognition", "tag-metropolis", "tag-omniverse-replicator", "tag-synthetic-data"], "acf": {"post_industry": ["Manufacturing", "Retail / Consumer Packaged Goods", "Smart Cities / Spaces"], "post_products": ["Metropolis", "Omniverse Replicator"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warehouse-with-overlay.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mRL", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Computer Vision / Video Analytics", "link": "https://developer.nvidia.com/blog/category/computer-vision/", "id": 2724}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87901"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2256"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87901"}], "version-history": [{"count": 20, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87901/revisions"}], "predecessor-version": [{"id": 88061, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87901/revisions/88061"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87904"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87901"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87901"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87901"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87901"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87888, "date": "2024-08-27T09:00:00", "date_gmt": "2024-08-27T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87888"}, "modified": "2024-10-11T13:02:14", "modified_gmt": "2024-10-11T20:02:14", "slug": "optimize-large-scale-ai-workloads-with-nvidia-spectrum-x", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/optimize-large-scale-ai-workloads-with-nvidia-spectrum-x/", "title": {"rendered": "Optimize Large-Scale AI Workloads with NVIDIA Spectrum-X"}, "content": {"rendered": "\n<p>In today\u2019s rapidly evolving technological landscape, staying ahead of the curve is not just a goal\u2014it&#8217;s a necessity. The surge of innovations, particularly in AI, is driving dramatic changes across the technology stack.&nbsp;</p>\n\n\n\n<p>One area witnessing profound transformation is Ethernet networking, a cornerstone of digital communication that has been foundational to enterprise and data center environments for decades.</p>\n\n\n\n<p>Today, every data center is becoming accelerated to support modern AI workloads, increasing the demand for infrastructure that can support them. Many enterprises are already deeply familiar with Ethernet, relying on it as a trusted networking standard. However, they lack a solution to adequately support the characteristics of AI workloads using the Ethernet protocol.</p>\n\n\n\n<p>NVIDIA\u2019s desire to innovate is often driven by a deep commitment to understanding and responding to our customers\u2019 evolving needs, ensuring that our solutions not only meet but anticipate and exceed expectations.\u00a0</p>\n\n\n\n<p>Enter the era of <a href=\"https://www.nvidia.com/en-us/networking/spectrumx/\">NVIDIA Spectrum-X</a>, the world\u2019s first high-performance Ethernet fabric designed around improvements that are not just incremental. They represent a significant leap forward, ensuring that Ethernet remains a robust and future-proof technology in an era of exponential data growth.</p>\n\n\n\n<h2 id=\"from_concept_to_realized_performance\"  class=\"wp-block-heading\">From concept to realized performance<a href=\"#from_concept_to_realized_performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>As AI workloads demand ever-increasing data throughput and zero-tail latency, traditional Ethernet had to be reimagined to meet stringent requirements. Considerations for advancements with the Remote Direct Memory Access (RDMA) protocol, balancing large network flows, and a better method for congestion control must be harnessed, deployed, and proven at scale.</p>\n\n\n\n<p>While Ethernet was already being used on large-scale hyperscale clouds and data centers, practically it could only support a single server or small-scale workloads. Traditional Ethernet is inherently a lossy network, which poses significant challenges when scaling distributed computing workloads such as AI.&nbsp;</p>\n\n\n\n<p>To address these drawbacks of traditional Ethernet, we began to develop new techniques and capabilities, transforming the NVIDIA Ethernet offering into a high-performance compute fabric capable of supporting the rigorous demands of accelerated computing.</p>\n\n\n\n<p>NVIDIA Spectrum-X represents a significant advancement from traditional Ethernet, by being specifically designed as an end-to-end architecture to optimize AI workloads. It uses both <a href=\"https://www.nvidia.com/en-us/networking/products/ethernet/supernic/\">NVIDIA BlueField-3 SuperNIC</a> endpoints working in concert with <a href=\"https://www.nvidia.com/en-us/networking/ethernet-switching/\">NVIDIA Spectrum-4 switches</a>, and is particularly enhanced for GPU-to-GPU communications (also known as east-west networking traffic) within the data center environment.&nbsp;</p>\n\n\n\n<p>Here\u2019s what we did differently:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Telemetry-based congestion control</li>\n\n\n\n<li>Lossless networking</li>\n\n\n\n<li>Dynamic load balancing</li>\n</ul>\n\n\n\n<h3 id=\"telemetry-based_congestion_control\"  class=\"wp-block-heading\">Telemetry-based congestion control<a href=\"#telemetry-based_congestion_control\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>By combining high-frequency telemetry probes with flow metering, Spectrum-X congestion control ensures that workloads are protected and the fabric delivers performance isolation. This means that diverse types of AI workloads can simultaneously run on the shared infrastructure without negatively affecting performance.</p>\n\n\n\n<h3 id=\"lossless_networking\"  class=\"wp-block-heading\">Lossless networking<a href=\"#lossless_networking\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Spectrum-X configures the network to achieve lossless conditions, ensuring that no packets are dropped and tail latency is minimized. <em>Tail latency</em> refers to the delay experienced by the slowest task in a set of parallel tasks, which ultimately dictates the overall completion time of the operation.</p>\n\n\n\n<h3 id=\"dynamic_load_balancing\"  class=\"wp-block-heading\">Dynamic load balancing<a href=\"#dynamic_load_balancing\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Spectrum-X uses fine-grain adaptive routing to maximize fabric utilization and ensure the highest effective bandwidth for Ethernet. Adaptive routing avoids the pitfalls of static routing (equal-cost multipath, or ECMP) or flowlet routing found in traditional Ethernet by load balancing flows packet-by-packet across the network, without the need for deep buffers and shock absorbers.&nbsp;</p>\n\n\n\n<p>As the load balancing means that packets can arrive out-of-order at the destination, the NVIDIA BlueField-3 SuperNIC makes sure to re-order the packets and place them in the host memory, leaving the re-ordering invisible to the application.&nbsp;</p>\n\n\n\n<h2 id=\"spectrum-x_debuts_with_the_israel-1_supercomputer\"  class=\"wp-block-heading\">Spectrum-X debuts with the Israel-1 supercomputer<a href=\"#spectrum-x_debuts_with_the_israel-1_supercomputer\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA Spectrum-X debuted with the <a href=\"https://nvidianews.nvidia.com/news/nvidia-launches-accelerated-ethernet-platform-for-hyperscale-generative-ai\">Israel-1 supercomputer</a> in June 2023. Israel-1 showcases a new class of Ethernet that boosts network performance by 1.6x, demonstrating its capabilities in handling large-scale AI.&nbsp;</p>\n\n\n\n<p>Since it was built, the NVIDIA team, including some of the world\u2019s foremost experts in networking, has tested and benchmarked applications around the clock. They are continuously optimizing Spectrum-X for the absolute lowest runtimes across any scale.</p>\n\n\n\n<h2 id=\"ecosystem_gets_onboard\"  class=\"wp-block-heading\">Ecosystem gets onboard<a href=\"#ecosystem_gets_onboard\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The performance gains seen with Israel-1 raised a lot of excitement with our OEMs and solution providers. It also raised eyebrows with our large-scale cloud customers. This quickly led our <a href=\"https://nvidianews.nvidia.com/news/nvidias-new-ethernet-networking-platform-for-ai-available-soon-from-dell-technologies-hewlett-packard-enterprise-lenovo\">worldwide partners</a> to collaborate with us and integrate Spectrum-X into their data center solutions.&nbsp;</p>\n\n\n\n<p>This marked the beginning of broad adoption with our partners, who recognized the benefits of Spectrum-X&#8217;s optimized networking for AI workloads, leading to its inclusion across their product offerings.</p>\n\n\n\n<h3 id=\"customers_embrace_spectrum-x\u2019s_performance\"  class=\"wp-block-heading\">Customers embrace Spectrum-X\u2019s performance<a href=\"#customers_embrace_spectrum-x\u2019s_performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Early customers were drawn to Spectrum-X for its ability to optimize large-scale AI workloads and enhance the performance of their data centers. Working closely with our OEMs, several top-tier <a href=\"https://nvidianews.nvidia.com/news/nvidia-supercharges-ethernet-networking-for-generative-ai\">cloud service providers</a> were among the first to deploy Spectrum-X, recognizing its potential to enhance their AI infrastructure while significantly lowering their overall TCO.\u00a0</p>\n\n\n\n<p>More recent examples include the following</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Dell AI Factory with NVIDIA:</strong> Combines Dell&#8217;s compute, storage, software, and services with NVIDIA advanced AI infrastructure</li>\n\n\n\n<li><strong>NVIDIA AI Computing by HPE:</strong> Designed to accelerate the generative AI industrial revolution.</li>\n</ul>\n\n\n\n<p>NVIDIA has a proven history of deploying large-scale, integrated systems including those used for our own development and research. We publish these reference architectures to help our partners and customers adopt accelerated computing.\u00a0</p>\n\n\n\n<p>We also offer world-class infrastructure services through NVIDIA Infrastructure Services (NVIS). Boasting an installation rate of 2,560 fully tested and interconnected GPUs/day, customers using NVIS can quickly get up and running, from the acquisition of hardware to training an LLM in a matter of days.\u00a0</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The journey of Spectrum-X is just in the beginning stages. As we move forward, NVIDIA continues to innovate with Spectrum-X, playing a key role in the build-up of AI factories, generative AI clouds, and Enterprise AI data centers. The Spectrum-X platform sets the standard, offering unparalleled performance and efficiency.&nbsp;&nbsp;</p>\n\n\n\n<p>For more information about Spectrum-X, download the <a href=\"https://resources.nvidia.com/en-us-accelerated-networking-resource-library/nvidia-spectrum-x\">NVIDIA Spectrum-X Network Platform Architecture: The First Ethernet Network Designed to Accelerate AI Workloads</a> whitepaper.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In today\u2019s rapidly evolving technological landscape, staying ahead of the curve is not just a goal\u2014it&#8217;s a necessity. The surge of innovations, particularly in AI, is driving dramatic changes across the technology stack.&nbsp; One area witnessing profound transformation is Ethernet networking, a cornerstone of digital communication that has been foundational to enterprise and data center &hellip; <a href=\"https://developer.nvidia.com/blog/optimize-large-scale-ai-workloads-with-nvidia-spectrum-x/\">Continued</a></p>\n", "protected": false}, "author": 1112, "featured_media": 87892, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1475379", "discourse_permalink": "https://forums.developer.nvidia.com/t/optimize-large-scale-ai-workloads-with-nvidia-spectrum-x/304786", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 1205], "tags": [1634, 453, 4100], "coauthors": [2310], "class_list": ["post-87888", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-networking-communications", "tag-ethernet", "tag-featured", "tag-supernics"], "acf": {"post_industry": ["Hardware / Semiconductor"], "post_products": ["Spectrum Ethernet"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/spectrum-x-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mRy", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Networking / Communications", "link": "https://developer.nvidia.com/blog/category/networking-communications/", "id": 1205}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87888"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1112"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87888"}], "version-history": [{"count": 1, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87888/revisions"}], "predecessor-version": [{"id": 87893, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87888/revisions/87893"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87892"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87888"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87888"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87888"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87888"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87747, "date": "2024-08-27T09:00:00", "date_gmt": "2024-08-27T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87747"}, "modified": "2024-10-28T14:55:21", "modified_gmt": "2024-10-28T21:55:21", "slug": "enhancing-rag-applications-with-nvidia-nim", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/enhancing-rag-applications-with-nvidia-nim/", "title": {"rendered": "Enhancing RAG Applications with NVIDIA NIM"}, "content": {"rendered": "\n<p>The advent of large language models (LLMs) has significantly benefited the AI industry, offering versatile tools capable of generating human-like text and handling a wide range of tasks. However, while LLMs demonstrate impressive general knowledge, their performance in specialized fields, such as veterinary science, is limited when used out of the box. To enhance their utility in specific areas, two primary strategies are commonly adopted in the industry: <a href=\"https://developer.nvidia.com/blog/fine-tune-and-align-llms-easily-with-nvidia-nemo-customizer/\">fine-tuning</a> and <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation</a> (RAG).&nbsp;</p>\n\n\n\n<p>Fine-tuning involves training the model on a carefully curated and structured dataset, demanding substantial hardware resources, as well as the involvement of domain experts, a process that is often time-consuming and costly. Unfortunately, in many fields, it\u2019s incredibly challenging to access domain experts in a way that is compatible with business constraints.</p>\n\n\n\n<p>Conversely, RAG involves building a comprehensive corpus of knowledge literature, alongside an effective retrieval system that extracts relevant text chunks to address user queries. By adding this retrieved information to the user query, LLMs can produce better answers. Although this approach still requires subject matter experts to curate the best sources for the dataset, it is more tractable and business-compatible than fine-tuning. Also, since extensive training of the model isn\u2019t necessary, this approach is less computationally intensive and more cost-effective.&nbsp;</p>\n\n\n\n<p>In creating RAG systems, developers invest significant effort into designing an effective mechanism to retrieve the best pieces of information from the knowledge dataset. As a result, it\u2019s common to use semantic similarity measures through textual embeddings to retrieve the most pertinent text.&nbsp;</p>\n\n\n\n<h2 id=\"nvidia_nim_and_nlp_pipelines\"  class=\"wp-block-heading\">NVIDIA NIM and NLP pipelines<a href=\"#nvidia_nim_and_nlp_pipelines\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://developer.nvidia.com/blog/access-to-nvidia-nim-now-available-free-to-developer-program-members/\">NVIDIA NIM</a> streamlines the design of NLP pipelines using LLMs. These microservices simplify the deployment of generative AI models across platforms, allowing teams to self-host LLMs while offering standard APIs to build applications.&nbsp;</p>\n\n\n\n<p>NIM abstracts model inference internals like execution engines and runtime operations, ensuring optimal performance with <a href=\"https://docs.nvidia.com/tensorrt-llm/index.html\">TensorRT-LLM</a>, vLLM, and others. Key features include the following:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Scalable deployment</li>\n\n\n\n<li>Support for diverse LLM architectures with optimized engines</li>\n\n\n\n<li>Flexible integration into existing workflows</li>\n\n\n\n<li>Enterprise-grade security with safetensors and constant CVE monitoring</li>\n</ul>\n\n\n\n<p>You can run NIM microservices with Docker and perform inference using APIs. Specialized trained model weights can also be used for specific tasks, such as document parsing, by modifying container commands.</p>\n\n\n\n<p>In this post, I describe how NIM microservices are helping the AITEM team develop <a href=\"https://laika.aitemsolutions.com/home\">LAIKA</a>, an AI copilot for veterinary practitioners. Specifically, I explore how NIM supports the RAG pipeline to maximize retrieval effectiveness, showing how the models both simplify and speed up the analysis and development processes.</p>\n\n\n\n<h2 id=\"reimagining_veterinarian_care_with_ai\"  class=\"wp-block-heading\">Reimagining veterinarian care with AI<a href=\"#reimagining_veterinarian_care_with_ai\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45.png\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"550\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45-1024x550.png\" alt=\"Screenshots from LAIKA show that veterinarians can present patient cases and findings or upload laboratory analysis documents.\" class=\"wp-image-87858\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45-1024x550.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45-300x161.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45-625x335.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45-179x96.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45-768x412.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45-1536x824.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45-645x346.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45-500x268.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45-160x86.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45-362x194.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45-205x110.png 205w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Chat12_char1p45.png 1863w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 1. LAIKA example conversations</em></figcaption></figure></div>\n\n\n<p>At AITEM, which is a member of the <a href=\"https://www.nvidia.com/en-us/startups/\">NVIDIA Inception Program</a> for startups, we\u2019ve collaborated closely with NVIDIA and concentrated on the development of AI-based solutions across multiple fields, including industrial and life sciences.&nbsp;</p>\n\n\n\n<p>In the veterinary sector, we are working on <a href=\"https://laika.aitemsolutions.com/home\">LAIKA</a>, an innovative AI copilot designed to assist veterinarians by processing patient data and offering diagnostic suggestions, guidance, and clarifications. LAIKA acts as a conversational agent, capable of handling both textual queries and laboratory analysis documents. Recently launched in Italy and France, LAIKA will soon expand to other countries.</p>\n\n\n\n<p>LAIKA integrates multiple LLMs and RAG pipelines. The RAG component retrieves relevant information from a curated dataset of veterinary resources (Figure 2). During preparation, each resource is divided into chunks, with embeddings calculated and stored in the RAG database. During inference, the query is pre-processed and its embeddings are computed and compared with those in the RAG database using geometric distance metrics. The closest matches are selected as the most relevant and used to generate responses.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"154\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-architecture-625x154.png\" alt=\"The diagram shows that the user query is pre-processed. Using the preprocessed query, its embeddings are calculated and compared with the embeddings computed from a dataset of reference information. The best matches are retrieved and filtered by an MMR algorithm. The remaining chunks are forwarded to the answering stage.\" class=\"wp-image-87809\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-architecture-625x154.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-architecture-300x74.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-architecture-179x44.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-architecture-768x189.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-architecture-645x159.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-architecture-960x238.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-architecture-500x123.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-architecture-160x39.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-architecture-362x89.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-architecture-446x110.png 446w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-architecture.png 966w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. High-level architecture of RAG pipeline in LAIKA</em></figcaption></figure></div>\n\n\n<p>Due to potential redundancy in the RAG database, multiple retrieved chunks might contain the same information, limiting the diversity of concepts that are provided to the answer system. To address this, LAIKA employs the Maximal Marginal Relevance (MMR) algorithm to minimize chunk redundancy and ensure a broader range of relevant information.</p>\n\n\n\n<p>As chunk extraction from resources is automated and the volume of resources is too large for manual verification, malformed chunks can be included, such as footnotes, bad charsets, or OCR errors. Although the probability of retrieving such irrelevant chunks is low due to out-of-distribution embeddings, it is not zero and that may still affect the response quality.&nbsp;</p>\n\n\n\n<p>While distance metric-based similarity retrieval is a widely used method, it is not flawless. It can retrieve well-formed but non-useful or even misleading information. Minimizing the impact of incorrect retrievals is essential for maintaining high answer quality.</p>\n\n\n\n<h2 id=\"nvidia_nemo_retriever_reranking_nim_microservice\"  class=\"wp-block-heading\">NVIDIA NeMo Retriever Reranking NIM microservice<a href=\"#nvidia_nemo_retriever_reranking_nim_microservice\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The <a href=\"https://build.nvidia.com/explore/discover\">NVIDIA API Catalog</a> has a collection of NeMo Retriever NIM microservices that enable organizations to seamlessly connect custom models to diverse business data and deliver highly accurate responses.&nbsp;</p>\n\n\n\n<p>The collection includes <a href=\"https://build.nvidia.com/nvidia/rerank-qa-mistral-4b\">NVIDIA Retrieval QA Mistral 4B reranking NIM microservice</a>, a model designed to assess the probability that a given text passage contains relevant information for answering a user query. Integrating this model into the RAG pipeline enabled us to filter out retrievals that do not pass the reranking model\u2019s evaluation, ensuring that only the most relevant and accurate information was used. Alternatively, the output from the reranking model can be used to sort retrieved chunks, retaining only those ranked at the top.</p>\n\n\n\n<p>To assess the impact of this step on the RAG pipeline, we designed an experiment:</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Extract a dataset of ~100 anonymized questions from LAIKA users.</li>\n\n\n\n<li>Run the current RAG pipeline to retrieve chunks for each question.</li>\n\n\n\n<li>Sort the retrieved chunks based on probabilities provided by the reranking model.</li>\n\n\n\n<li>Evaluate each chunk for relevance to the query.</li>\n\n\n\n<li>Analyze the reranking model&#8217;s probability distribution in relation to the relevance determined in Step 4.</li>\n\n\n\n<li>Compare the ranking of chunks in Step 3 against their relevance from Step 4.</li>\n</ol>\n\n\n\n<p>User questions in LAIKA can vary significantly in form (Figure 3). Some queries contain detailed explanations of a situation but lack a specific question. Others contain precise inquiries regarding research, while some seek guidance or differential diagnoses based on clinical cases or analysis documents.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/question-examples-laika.png\"><img loading=\"lazy\" decoding=\"async\" width=\"941\" height=\"419\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/question-examples-laika.png\" alt=\"Questions include, \u201cHow do I diagnose EPI in cats?\u201d, \u201chello, <NAME&gt; has had polyuria and polydipsia for a few weeks now, the owner measured about 7 litres of water drunk per day.\u201d, and \u201cI would lean towards the diagnosis of psychogenic polydipsia, or diabetes insipidus: do you recommend starting therapy?\u201d\" class=\"wp-image-87808\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/question-examples-laika.png 941w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/question-examples-laika-300x134.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/question-examples-laika-625x278.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/question-examples-laika-179x80.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/question-examples-laika-768x342.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/question-examples-laika-645x287.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/question-examples-laika-500x223.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/question-examples-laika-160x71.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/question-examples-laika-362x161.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/question-examples-laika-247x110.png 247w\" sizes=\"(max-width: 941px) 100vw, 941px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 3. Question examples from LAIKA users</em></figcaption></figure></div>\n\n\n<p>Due to the large number of chunks per question, we used the <a href=\"https://build.nvidia.com/meta/llama-3_1-70b-instruct\">Llama 3.1 70B Instruct NIM microservice</a> for the evaluation, which is also available in the NVIDIA API Catalog.</p>\n\n\n\n<p>To better understand the reranking model&#8217;s performance on the dataset, we examined specific queries and model responses in detail. Table 1 highlights the top and bottom reranked chunks for the query, \u201cThe cat has been losing weight for two months with preserved appetite, without vomiting or diarrhea. What are the potential differential diagnoses?\u201d\u00a0</p>\n\n\n\n<p>The top three chunks with positive logits (relevance probability: &gt;50%), all provided useful information directly related to the question. In contrast, the bottom three chunks, which had large negative logits, were less relevant, covering limited case studies and drug complaints that were not helpful.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td><strong>Text</strong></td><td><strong>Reranking Logit</strong></td></tr><tr><td><em>Causes of weight loss that can be particularly difficult to diagnose \u2026 include gastric disease not causing vomiting, intestinal disease not causing vomiting or diarrhea, hepatic disease \u2026</em></td><td>3.3125</td></tr><tr><td><em>Differential diagnoses for nonspecific signs like anorexia, weight loss, vomiting, and diarrhea \u2026 acute pancreatitis is rare in cats, \u2026 signs are nonspecific and ill-defined (anorexia, lethargy, weight loss).</em></td><td>2.3222</td></tr><tr><td><em>Severe weight loss (with or without increased appetite) may be noted where there is cancer cachexia, maldigestion/malabsorption \u2026 Appetite may be increased in some conditions, such as hyperthyroidism in cats, \u2026 However, a normal appetite does not rule out the presence of a serious condition.</em></td><td>2.2265</td></tr><tr><td><em>Overall, weight loss was the most common presenting sign \u2026 with little difference between the groups \u2026</em></td><td>-5.0078</td></tr><tr><td><em>Other client complaints include lethargy, anorexia, weight loss, vomiting \u2026</em></td><td>-7.3672</td></tr><tr><td><em>There were 6 British Shorthair, 4 European Shorthair, and 1 Bengal cat \u2026 Reported clinical signs by owners included: reduced appetite or anorexia\u2026</em></td><td>-10.3281</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Three highest-ranked chunks and three lowest-ranked text chunks</em></figcaption></figure>\n\n\n\n<p>Figure 4 compares the reranking model probability output distribution (in logits) between relevant (good) and irrelevant (bad) chunks. The probabilities for good chunks are higher compared to bad chunks and a t-test confirmed that this difference is statistically significant, with a p-value lower than 3e-72.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"450\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/logit-distribution-good-bad-chunks-625x450.png\" alt=\"Violin plot for logit distribution of good and bad chunks.\" class=\"wp-image-87807\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/logit-distribution-good-bad-chunks-625x450.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/logit-distribution-good-bad-chunks-300x216.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/logit-distribution-good-bad-chunks-160x115.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/logit-distribution-good-bad-chunks-645x464.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/logit-distribution-good-bad-chunks-417x300.png 417w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/logit-distribution-good-bad-chunks-125x90.png 125w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/logit-distribution-good-bad-chunks-362x260.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/logit-distribution-good-bad-chunks-153x110.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/logit-distribution-good-bad-chunks.png 723w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Distribution of reranking model output in terms of logits</em></figcaption></figure></div>\n\n\n<p>Figure 5 shows the distribution difference in the reranking-induced sorting positions: good chunks are predominantly in top positions, while bad chunks are lower. The Mann-Whitney test confirmed that these differences are statistically significant, resulting in a p-value lower than 9e-31.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"458\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distribution-good-bad-chunks-model-sorting-625x458.png\" alt=\"Boxplot for ranking of good and bad chunks.\" class=\"wp-image-87806\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distribution-good-bad-chunks-model-sorting-625x458.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distribution-good-bad-chunks-model-sorting-300x220.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distribution-good-bad-chunks-model-sorting-157x115.png 157w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distribution-good-bad-chunks-model-sorting-645x473.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distribution-good-bad-chunks-model-sorting-409x300.png 409w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distribution-good-bad-chunks-model-sorting-123x90.png 123w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distribution-good-bad-chunks-model-sorting-362x266.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distribution-good-bad-chunks-model-sorting-150x110.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distribution-good-bad-chunks-model-sorting.png 709w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. Distribution of reranking model-induced sorting among the retrieved chunks</em></figcaption></figure></div>\n\n\n<p>Figure 6 shows the ranking distribution and helps define an effective cutoff point. In the top five positions, most chunks are good, while the majority of chunks in positions 11-15 are bad. Thus, retaining only the top five retrievals or another chosen number can serve as one way to effectively exclude most of the bad chunks.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"472\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/good-bad-chunk-balance-model-sorting-625x472.png\" alt=\"Stacked bar plot with good chunks on the bottom and bad chunks on top and highest values for good from left to right.\" class=\"wp-image-87805\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/good-bad-chunk-balance-model-sorting-625x472.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/good-bad-chunk-balance-model-sorting-300x227.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/good-bad-chunk-balance-model-sorting-152x115.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/good-bad-chunk-balance-model-sorting-645x488.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/good-bad-chunk-balance-model-sorting-397x300.png 397w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/good-bad-chunk-balance-model-sorting-119x90.png 119w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/good-bad-chunk-balance-model-sorting-362x274.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/good-bad-chunk-balance-model-sorting-146x110.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/good-bad-chunk-balance-model-sorting.png 688w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Balance between good and bad chunks by position in the sorting induced by the reranking model</em></figcaption></figure></div>\n\n\n<p>To optimize retrieval pipelines, and minimize ingestion costs while maximizing accuracy, a lightweight embedding model can be paired with the NVIDIA reranking NIM microservice, to boost retrieval accuracy. Execution time can be improved by 1.75x (Figure 7).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"711\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nv-rerankqa-mistral4b-v3-comparison-625x711.png\" alt=\"Bar chart with one gray bar and one green bar that compares the throughput performance of text reranking with NIM on and off. Performance improves by 1.75x with NIM on.\" class=\"wp-image-87804\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nv-rerankqa-mistral4b-v3-comparison-625x711.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nv-rerankqa-mistral4b-v3-comparison-264x300.png 264w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nv-rerankqa-mistral4b-v3-comparison-101x115.png 101w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nv-rerankqa-mistral4b-v3-comparison-768x874.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nv-rerankqa-mistral4b-v3-comparison-645x734.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nv-rerankqa-mistral4b-v3-comparison-79x90.png 79w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nv-rerankqa-mistral4b-v3-comparison-362x412.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nv-rerankqa-mistral4b-v3-comparison-97x110.png 97w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nv-rerankqa-mistral4b-v3-comparison.png 818w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. NVIDIA reranking NIM microservice comparison</em></figcaption></figure></div>\n\n\n<h2 id=\"better_answers_with_the_nvidia_reranking_nim_microservice\"  class=\"wp-block-heading\">Better answers with the NVIDIA reranking NIM microservice<a href=\"#better_answers_with_the_nvidia_reranking_nim_microservice\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The results demonstrate that adding the NVIDIA reranking NIM microservice to the LAIKA RAG pipeline positively affects the relevance of retrieved chunks. By forwarding more precise, specialized information to the downstream answering LLM, it equips the model with the knowledge that\u2019s necessary for highly specialized fields like veterinary science.\u00a0</p>\n\n\n\n<p>The NVIDIA reranking NIM microservice, available in the <a href=\"https://build.nvidia.com/explore/discover\">NVIDIA API Catalog</a>, simplifies adoption as you can easily pull and run the model and infer its evaluations through APIs. This eliminates stress related to environment settings and manual optimization, as it comes pre-quantized and optimized with NVIDIA TensorRT for almost any platform.</p>\n\n\n\n<p>For more information and the latest updates about LAIKA and other AITEM projects, see <a href=\"https://www.aitemsolutions.com/\">AITEM Solutions</a> and follow <a href=\"https://www.linkedin.com/showcase/laikaveterinary/\">LAIKA</a> and <a href=\"https://www.linkedin.com/company/aitemsolutions/\">AITEM</a> on LinkedIn.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The advent of large language models (LLMs) has significantly benefited the AI industry, offering versatile tools capable of generating human-like text and handling a wide range of tasks. However, while LLMs demonstrate impressive general knowledge, their performance in specialized fields, such as veterinary science, is limited when used out of the box. To enhance their &hellip; <a href=\"https://developer.nvidia.com/blog/enhancing-rag-applications-with-nvidia-nim/\">Continued</a></p>\n", "protected": false}, "author": 2265, "featured_media": 87749, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1475377", "discourse_permalink": "https://forums.developer.nvidia.com/t/enhancing-rag-applications-with-nvidia-nim/304784", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [453, 2932, 3613], "coauthors": [3993], "class_list": ["post-87747", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "tag-featured", "tag-large-language-models", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["Healthcare & Life Sciences"], "post_products": ["NeMo Retriever", "NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laika-rag-pipeline-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mPh", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87747"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2265"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87747"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87747/revisions"}], "predecessor-version": [{"id": 87860, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87747/revisions/87860"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87749"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87747"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87747"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87747"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87747"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87899, "date": "2024-08-27T06:00:00", "date_gmt": "2024-08-27T13:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87899"}, "modified": "2024-11-04T14:52:07", "modified_gmt": "2024-11-04T22:52:07", "slug": "nvidia-launches-nim-blueprints-for-generative-ai", "status": "publish", "type": "post", "link": "https://nvda.ws/3Z68nzr", "title": {"rendered": "NVIDIA Launches NIM Agent Blueprints for Generative AI"}, "content": {"rendered": "\n<p>Now available\u2014NIM Agent Blueprints for digital humans, multimodal PDF data extraction, and drug discovery.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Now available\u2014NIM Agent Blueprints for digital humans, multimodal PDF data extraction, and drug discovery.</p>\n", "protected": false}, "author": 840, "featured_media": 87918, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1475281", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-launches-nim-agent-blueprints-for-generative-ai/304764", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3Z68nzr", "_links_to_target": "_blank"}, "categories": [3110, 1903], "tags": [3065, 2385, 453, 4134, 3613], "coauthors": [1419], "class_list": ["post-87899", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "category-features", "tag-avatar", "tag-drug-discovery", "tag-featured", "tag-nim-agent-blueprint", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["General", "Gaming", "Healthcare & Life Sciences", "Media & Entertainment", "Restaurant / Quick-Service", "Retail / Consumer Packaged Goods"], "post_products": ["Avatar Cloud Engine (ACE)", "Audio2Face", "BioNeMo", "NIM"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Announcement", "News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/genai-press-ai-workflows-pr-1920x1080-11.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mRJ", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Top Stories", "link": "https://developer.nvidia.com/blog/category/features/", "id": 1903}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87899"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/840"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87899"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87899/revisions"}], "predecessor-version": [{"id": 87969, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87899/revisions/87969"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87918"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87899"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87899"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87899"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87899"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87952, "date": "2024-08-26T12:24:49", "date_gmt": "2024-08-26T19:24:49", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87952"}, "modified": "2024-10-09T12:45:57", "modified_gmt": "2024-10-09T19:45:57", "slug": "llm-research-rewrites-the-role-of-ai-in-safeguarding-sustainable-systems", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/llm-research-rewrites-the-role-of-ai-in-safeguarding-sustainable-systems/", "title": {"rendered": "LLM Research Rewrites the Role of AI in Safeguarding Sustainable Systems"}, "content": {"rendered": "\n<p>Large language models (LLMs) are emerging as a tool for safeguarding critical infrastructure systems, such as renewable energy, healthcare, or transportation, according to a <a href=\"https://arxiv.org/pdf/2405.14755\">new study</a> from the Massachusetts Institute of Technology (MIT).&nbsp;</p>\n\n\n\n<p>The research introduces a zero-shot LLM model that detects anomalies in complex data. Using AI-driven diagnostics for monitoring and flagging potential issues in equipment, such as wind turbines, MRI machines, and railways, the approach could reduce operational costs, boost reliability, lower downtime, and support sustainable industry operations.</p>\n\n\n\n<p>According to study senior author Kalyan Veeramachaneni, using deep learning models for detecting infrastructure issues takes time and resources for training, fine-tuning, and testing. Deploying a machine learning model involves close collaboration between the machine learning team, which trains it, and the operations team, which monitors the equipment.&nbsp;</p>\n\n\n\n<p>The teams must continuously coordinate as real-world data comes in, addressing any arising challenges and if there are changes, like adding new data signals or updating equipment, they often need to restart the entire deployment process.</p>\n\n\n\n<p>\u201cCompared to this, an LLM is plug and play. We don&#8217;t have to create an independent model for every new data stream. We can deploy the LLM directly on the data streaming in,\u201d Veeramachaneni said.&nbsp;&nbsp;&nbsp;&nbsp;</p>\n\n\n\n<p>The researchers created SigLLM, a framework that converts time-series data into text for analysis. GPT-3.5 Turbo and Mistral LLMs are then used for detecting pattern irregularities, and flagging anomalies that could signal potential operational problems in a system.&nbsp;</p>\n\n\n\n<p>The team evaluated SigLLM performance on 11 different datasets, with 492 univariate time series, and 2,349 anomalies. The diverse data was sourced from a wide range of applications, including satellites from NASA and traffic from Yahoo with various signal lengths and anomalies.&nbsp;</p>\n\n\n\n<p>Two <a href=\"https://www.nvidia.com/en-us/titan/titan-rtx/\">NVIDIA Titan RTX GPUs</a> and one <a href=\"https://www.nvidia.com/en-us/data-center/v100/\">NVIDIA V100 Tensor Core GPU</a> handled the computational demands of running GPT-3.5 Turbo and Mistral for zero-shot anomaly detection.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"2550\" height=\"852\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM.png\" alt=\"Anomaly detection SigLLM workflow.\" class=\"wp-image-87953\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM.png 2550w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM-300x100.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM-625x209.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM-179x60.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM-768x257.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM-1536x513.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM-2048x684.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM-645x216.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM-500x167.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM-160x53.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM-362x121.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM-329x110.png 329w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Screenshot-2024-08-22-at-5.08.49\u202fPM-1024x342.png 1024w\" sizes=\"(max-width: 2550px) 100vw, 2550px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The anomaly detection methods in the SigLLM framework find discrepancies between the original and forecasted signal as a sign of the presence of anomalies</em></figcaption></figure></div>\n\n\n<p><a href=\"https://arxiv.org/pdf/2405.14755\"></a>The study found that LLMs can detect anomalies, and unlike traditional detection methods, SigLLM uses the inherent ability of LLMs in pattern recognition without requiring extensive training. However, specialized deep-learning models outperformed SigLLM by about 30%.&nbsp;</p>\n\n\n\n<p>\u201cWe were surprised to find that LLM-based methods performed better than some of the deep learning transformer-based methods,\u201d&nbsp; Veeramachaneni said. \u201cStill, these methods are not as good as the current state-of-the-art models, such as <a href=\"https://arxiv.org/pdf/2212.13558\">Autoencoder with Regression (AER</a>). We have some work to do to reach that level.\u201d&nbsp;</p>\n\n\n\n<p>The research could offer a significant step in AI-driven monitoring, with the potential for efficient anomaly detection, especially with further model enhancements.&nbsp;</p>\n\n\n\n<p>A main challenge, according to Veeramachaneni, is determining how robust the method can be while maintaining the benefits LLMs offer. The team also plans to investigate how LLMs predict anomalies effectively without being fine-tuned, which will involve testing the LLM with various prompts.</p>\n\n\n\n<p>The datasets used in the study are publicly available on <a href=\"https://github.com/sintel-dev/sigllm\">GitHub</a>.</p>\n\n\n\n<p>Read the full story at <a href=\"https://news.mit.edu/2024/researchers-use-large-language-models-to-flag-problems-0814\">MIT News</a>.<br>Read the study <a href=\"https://arxiv.org/pdf/2405.14755\">Large language models can be zero-shot anomaly detectors for time series?</a></p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLMs) are emerging as a tool for safeguarding critical infrastructure systems, such as renewable energy, healthcare, or transportation, according to a new study from the Massachusetts Institute of Technology (MIT).&nbsp; The research introduces a zero-shot LLM model that detects anomalies in complex data. Using AI-driven diagnostics for monitoring and flagging potential issues &hellip; <a href=\"https://developer.nvidia.com/blog/llm-research-rewrites-the-role-of-ai-in-safeguarding-sustainable-systems/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 87954, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1474554", "discourse_permalink": "https://forums.developer.nvidia.com/t/llm-research-rewrites-the-role-of-ai-in-safeguarding-sustainable-systems/304668", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110, 1903], "tags": [3941, 1913, 453, 2932, 1877], "coauthors": [2315], "class_list": ["post-87952", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "category-features", "tag-ai-impact", "tag-climate-weather-ocean-modeling", "tag-featured", "tag-large-language-models", "tag-research"], "acf": {"post_industry": ["General"], "post_products": ["RTX GPU", "V100"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Sustainable-Infrastructure-AI.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mSA", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87952"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87952"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87952/revisions"}], "predecessor-version": [{"id": 88659, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87952/revisions/88659"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87954"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87952"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87952"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87952"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87952"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]