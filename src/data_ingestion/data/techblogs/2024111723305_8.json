[{"id": 87522, "date": "2024-08-26T09:00:00", "date_gmt": "2024-08-26T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87522"}, "modified": "2024-09-05T10:57:24", "modified_gmt": "2024-09-05T17:57:24", "slug": "nvidia-ai-workbench-simplifies-using-gpus-on-windows", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-ai-workbench-simplifies-using-gpus-on-windows/", "title": {"rendered": "NVIDIA AI Workbench Simplifies Using GPUs on Windows"}, "content": {"rendered": "\n<p><a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/overview/introduction.html\">NVIDIA AI Workbench</a> is a free, user-friendly development environment manager that streamlines data science, ML, and AI projects on your system of choice: PC, workstation, datacenter, or cloud. You can develop, test, and prototype projects locally on Windows, macOS, and Ubuntu and easily transfer development environments and computational work between systems (local and remote) to optimize cost, availability, and scale.&nbsp;</p>\n\n\n\n<p>AI Workbench is focused on streamlining the developer experience without blocking the kind of customization that power users need. That\u2019s a major reason that AI Workbench handles containers. They are the easiest way to provide and modify the environments needed for GPU-accelerated work.</p>\n\n\n\n<p>This focus also means working with ecosystem partners to improve the user experience. For example, the <a href=\"https://ubuntu.com/desktop/wsl\">collaboration with Canonical</a> facilitates using an Ubuntu WSL distribution for the AI Workbench install on Windows.&nbsp;</p>\n\n\n\n<p>More recently, NVIDIA collaborated with the <a href=\"https://www.docker.com/products/docker-desktop/\">Docker Desktop</a> team to create a feature that lets AI Workbench directly install Docker Desktop. This feature is available in the <a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/reference/version-history.html#release-2024-08-1\">latest AI Workbench release</a> and significantly streamlines the experience on Windows and macOS.</p>\n\n\n\n<p>This kind of streamlining is what makes AI Workbench the easiest way to get started on your own system, from laptops and workstations all the way to servers and VMs.</p>\n\n\n\n<h2 id=\"managed_docker_desktop_install\"  class=\"wp-block-heading\">Managed Docker Desktop install<a href=\"#managed_docker_desktop_install\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Docker Desktop is the recommended container runtime on NVIDIA AI Workbench for Windows and macOS. However, selecting Docker previously required manual steps to set up. To eliminate the manual steps, NVIDIA partnered with Docker on the AI Workbench-enabled installation of <a href=\"https://www.docker.com/products/docker-desktop/\">Docker Desktop</a> for local systems.&nbsp;</p>\n\n\n\n<p>This is the first time that Docker has enabled another application to do a managed installation for Docker Desktop. Thanks to the collaboration, installing Docker Desktop for AI Workbench is now straightforward. For more information, see <a href=\"https://www.docker.com/blog/optimizing-ai-application-development-docker-desktop-nvidia-ai-workbench/\">Optimizing AI Application Development with Docker Desktop and NVIDIA AI Workbench</a> (Docker website).</p>\n\n\n\n<p>Selecting the Docker container runtime for AI Workbench results in the following tasks being automatically completed:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Installing Docker Desktop</strong>: Previously, you had to exit the AI Workbench installer and install Docker Desktop manually if it wasn\u2019t already installed. Now you can have AI Workbench install Docker Desktop without needing to exit the AI Workbench installer.&nbsp;</li>\n\n\n\n<li><strong>Configuring Docker Desktop on Windows</strong>: AI Workbench uses its own WSL distribution, NVIDIA-Workbench. Previously, Windows users had to manually configure Docker Desktop to use this distribution. Now this happens automatically.</li>\n</ul>\n\n\n\n<h2 id=\"new_ai_workbench_projects&nbsp;\"  class=\"wp-block-heading\">New AI Workbench projects&nbsp;<a href=\"#new_ai_workbench_projects&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Included in this release is a new set of example projects for you to use and build from. An <a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/overview/projects.html\">AI Workbench project</a> is a structured Git repository that defines a containerized development environment in AI Workbench. </p>\n\n\n\n<p>These projects support IDEs like Jupyter and Visual Studio Code as well as user-configured web applications. Everything is containerized, isolated, and easily modifiable. You can clone a project from GitHub or GitLab and then AI Workbench handles everything, including connecting to GPUs.</p>\n\n\n\n<p>The best example of this so far is the <a href=\"https://github.com/NVIDIA/workbench-example-hybrid-rag\">Hybrid-RAG project</a> on GitHub. With AI Workbench, you can just clone the project and get the RAG application running in a few clicks. If you don\u2019t have a local GPU, the project lets you use either cloud endpoints or a self-hosted NIM container to run the inference for you.</p>\n\n\n\n<p>This release has some example AI Workbench projects on GitHub that continue developing the RAG theme. There are also some new Jupyter-based fine-tuning projects and a LlamaFactory project that supports the <a href=\"https://developer.nvidia.com/rtx/ai-toolkit\">NVIDIA RTX AI Toolkit</a>.</p>\n\n\n\n<h3 id=\"agentic_rag\"  class=\"wp-block-heading\">Agentic RAG<a href=\"#agentic_rag\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The <a href=\"https://github.com/NVIDIA/workbench-example-agentic-rag\">Agentic RAG</a> AI Workbench project lets you work with an AI agent to include web search tool-calling into your RAG pipeline. Instead of just working with the documents in a database, the agent also dynamically searches for new documents online as a fallback to better respond to queries.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure.png\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"259\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure-1024x259.png\" alt=\"A diagram shows the routing of user queries through either the RAG pipeline or the web search tool pipeline.\" class=\"wp-image-87531\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure-1024x259.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure-300x76.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure-625x158.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure-179x45.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure-768x194.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure-1536x389.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure-645x163.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure-500x127.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure-160x41.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure-362x92.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure-435x110.png 435w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-ai-workbench-project-structure.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 1. Structure of the agentic RAG example project.</em></figcaption></figure></div>\n\n\n<p>LLM agents are systems designed to perceive and react to an environment, typically through <a href=\"https://python.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/\">tool-calling</a>, to better take relevant actions. This project implements a LangGraph-based RAG agent with the following agentic elements to improve response generation:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://arxiv.org/abs/2403.14403\">Routing</a>: Route relevant questions to different pipelines based on the query topic.</li>\n\n\n\n<li><a href=\"https://arxiv.org/pdf/2401.15884\">Fallback</a>: Fall back to web search if retrieved docs are not relevant to the query.</li>\n\n\n\n<li><a href=\"https://arxiv.org/abs/2310.11511\">Self-reflection</a>: Fix hallucinations and answers that don\u2019t address the question.</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"800\" height=\"450\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/agentic-rag-with-gradio-chat.gif\" alt=\"A GIF shows how a user query in the agentic RAG example project is routed to the RAG pipeline or the web search tool pipeline to generate a response.\" class=\"wp-image-87532\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. Agentic RAG example project with a customizable Gradio chat UI</em></figcaption></figure></div>\n\n\n<p>This project includes a customizable Gradio Chat app that enables you to run inference using remotely running endpoints and microservices, whether on the cloud using the NVIDIA API catalog, self-hosted endpoints using NVIDIA NIM, or third-party self-hosted microservices. The mode of inference can easily be switched through the Chat app.</p>\n\n\n\n<h3 id=\"nim_anywhere\"  class=\"wp-block-heading\">NIM Anywhere<a href=\"#nim_anywhere\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><a href=\"https://github.com/NVIDIA/nim-anywhere\">NIM Anywhere</a> is an all-in-one project for building NIM-based RAG applications that include a preconfigured RAG chatbot.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"405\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-anywhere-example-project-625x405.png\" alt=\"A diagram of a monitor window facing up with building blocks labeled \u201cLibraries\u201d, \u201cNIM\u201d, \u201cIDE Config\u201d, and \u201cData Stores\u201d extending from it.\" class=\"wp-image-87533\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-anywhere-example-project-625x405.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-anywhere-example-project-300x195.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-anywhere-example-project-177x115.png 177w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-anywhere-example-project-768x498.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-anywhere-example-project-645x418.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-anywhere-example-project-463x300.png 463w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-anywhere-example-project-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-anywhere-example-project-139x90.png 139w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-anywhere-example-project-362x235.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-anywhere-example-project-170x110.png 170w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-nim-anywhere-example-project.png 859w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. NIM Anywhere example project</em></figcaption></figure></div>\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Docker automation:</strong> Run services like NIM, Milvus, and Redis as persistent containers alongside the main project.</li>\n\n\n\n<li><strong>User-configurable models:</strong> Toggle between running RAG with either a NIM microservice on the NVIDIA API Catalog or a self-hosted NIM microservice running locally.</li>\n\n\n\n<li><strong>Customizable frontend:</strong> Add views to the frontend Gradio application to extend the project and build out new use cases.&nbsp;</li>\n</ul>\n\n\n\n<p>NIM microservices are available as part of <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, but you can also join the <a href=\"https://developer.nvidia.com/developer-program\">NVIDIA Developer Program</a> to get started with NVIDIA NIM for free.</p>\n\n\n\n<h3 id=\"fine-tuning_projects\"  class=\"wp-block-heading\">Fine-tuning projects<a href=\"#fine-tuning_projects\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Finally, we introduce several fine-tuning workflows for exciting new models. Each of these projects feature models that can be quantized to fit on a single GPU:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://github.com/NVIDIA/workbench-example-mixtral-finetune\">Mixtral 8x7B</a>: First example project for AI Workbench that demonstrates fine-tuning a mixture of experts (MoE) model.&nbsp;</li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/workbench-example-llama3-finetune\">Llama 3 8B</a>: An example project demonstrating two approaches: Supervised full fine-tuning (SFT), as well as Direct Preference Optimization (DPO).</li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/workbench-example-phi3-finetune\">Phi-3 Mini</a>: A highly accessible fine-tuning example due to the small model size and quantization capability.&nbsp;</li>\n\n\n\n<li><a href=\"https://github.com/NVIDIA/workbench-llamafactory\">RTX AI Toolkit</a>: Provides an end-to-end workflow for Windows application developers. You can use popular foundation models, customize them with fine-tuning techniques using <a href=\"https://github.com/NVIDIA/RTX-AI-Toolkit/blob/main/tutorial-llama3-finetune.md\">Workbench Projects</a>, and deploy models into Windows applications for peak performance on a wide range of GPUs, from NVIDIA RTX PCs and workstations to cloud. You can get started by using AI Workbench along with the LlamaFactory GUI.&nbsp;</li>\n</ul>\n\n\n\n<h2 id=\"other_new_features\"  class=\"wp-block-heading\">Other new features<a href=\"#other_new_features\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Our development process includes direct feature requests from users. The following features are based on this user feedback:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>SSH Agent</li>\n\n\n\n<li>Ubuntu 24.04</li>\n\n\n\n<li>Logging</li>\n</ul>\n\n\n\n<h3 id=\"ssh_agent\"  class=\"wp-block-heading\">SSH Agent<a href=\"#ssh_agent\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Some enterprise users of AI Workbench need password-protected SSH keys for accessing remotes. This was addressed by adding <a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/locations/remote.html#optional-use-a-password-protected-ssh-key\">SSH agent support</a> to the 2024.07 release. You&nbsp;also have the option to use the <a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/locations/remote.html#locations-remote-connect\">earlier SSH key feature</a>.</p>\n\n\n\n<h3 id=\"ubuntu_2404\"  class=\"wp-block-heading\">Ubuntu 24.04<a href=\"#ubuntu_2404\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Previously, Ubuntu 22.04 was the only Linux distro that AI Workbench installation supported. The 2024.07 release added Ubuntu 24.04 support.</p>\n\n\n\n<h3 id=\"logging\"  class=\"wp-block-heading\">Logging<a href=\"#logging\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>AI Workbench has multiple log files that can be complicated to find and interpret. To address this, the AI Workbench CLI now has a <a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/troubleshooting/logging.html#create-a-support-bundle-for-troubleshooting\">support command</a> that lets you export metadata and logs into a zip file. This eliminates the need to find the files and includes metadata that can be sent to NVIDIA Support for faster diagnosis and remediation.</p>\n\n\n\n<h2 id=\"coming_soon\"  class=\"wp-block-heading\">Coming soon<a href=\"#coming_soon\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Here\u2019s a sneak peak at where AI Workbench is heading: app sharing and multi-container support.</p>\n\n\n\n<h3 id=\"app_sharing\"  class=\"wp-block-heading\">App sharing<a href=\"#app_sharing\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Currently, a running application in a Workbench Project is only accessible to the user running that Workbench Project. Some users have requested the ability to share running applications.&nbsp;</p>\n\n\n\n<p>In the next release, AI Workbench users will be able to securely share web apps in a Workbench Project through a link. Apps will be directly accessible to authenticated users in a web browser without requiring them to use AI Workbench.</p>\n\n\n\n<h3 id=\"multi-container_support\"  class=\"wp-block-heading\">Multi-container support<a href=\"#multi-container_support\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>The current multi-container approach with the NIM Anywhere Project is a bit of a workaround. An upcoming AI Workbench release will have streamlined support for multi-container applications.&nbsp;</p>\n\n\n\n<h2 id=\"next_steps\"  class=\"wp-block-heading\">Next steps<a href=\"#next_steps\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Get started with AI Workbench by <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/\">installing the application from the webpage</a>. Users who already have AI Workbench can follow the instructions to update to the latest version. For more information, see <a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/installation/windows.html#install-ai-workbench\">Install AI Workbench on Windows</a>.&nbsp;</p>\n\n\n\n<p>Explore a range of <a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/quickstart/example-projects.html\">example AI Workbench projects</a> from data science to RAG.Ask questions on the <a href=\"https://forums.developer.nvidia.com/c/ai-data-science/nvidia-ai-workbench/671\">NVIDIA AI Workbench developer forum</a> and learn more about how other developers are using AI Workbench.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA AI Workbench is a free, user-friendly development environment manager that streamlines data science, ML, and AI projects on your system of choice: PC, workstation, datacenter, or cloud. You can develop, test, and prototype projects locally on Windows, macOS, and Ubuntu and easily transfer development environments and computational work between systems (local and remote) to &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-ai-workbench-simplifies-using-gpus-on-windows/\">Continued</a></p>\n", "protected": false}, "author": 1840, "featured_media": 87524, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1474482", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-ai-workbench-simplifies-using-gpus-on-windows/304653", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 3110], "tags": [259, 453, 3771], "coauthors": [3466, 3685, 3227], "class_list": ["post-87522", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-data-science", "category-generative-ai", "tag-docker", "tag-featured", "tag-windows-365"], "acf": {"post_industry": ["General"], "post_products": ["AI Workbench"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ai-workbench-windows-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mLE", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87522"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1840"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87522"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87522/revisions"}], "predecessor-version": [{"id": 87838, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87522/revisions/87838"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87524"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87522"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87522"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87522"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87522"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87484, "date": "2024-08-26T09:00:00", "date_gmt": "2024-08-26T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87484"}, "modified": "2024-09-05T10:57:25", "modified_gmt": "2024-09-05T17:57:25", "slug": "cuda-q-enabled-resource-reduction-for-quantum-clustering-algorithms", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/cuda-q-enabled-resource-reduction-for-quantum-clustering-algorithms/", "title": {"rendered": "CUDA-Q Enabled Resource Reduction for Quantum Clustering Algorithms"}, "content": {"rendered": "\n<p>Quantum computers can use the quantum properties of superposition, entanglement, and interference to generalize learnings and insights from data. Such quantum machine learning (QML) techniques will eventually run on <a href=\"https://developer.nvidia.com/blog/an-introduction-to-quantum-accelerated-supercomputing/\">quantum-accelerated supercomputers</a> that combine the processing powers of CPUs, GPUs, and QPUs to solve some of the world\u2019s most complex problems.</p>\n\n\n\n<p>Many QML algorithms offer theoretical speedups by assuming that classical data can be efficiently loaded in superposition using so-called quantum random access memory (QRAM). The lack of any efficient means to implement QRAM means that early quantum computers will likely excel at compute<em>, </em>rather than data-intensive tasks.&nbsp;</p>\n\n\n\n<p>In practice, QML algorithms that are effective on near and mid-term hardware must focus on compute-intensive heuristics that can analyze data in the absence of QRAM.</p>\n\n\n\n<p>This post highlights recent research by Associate Professor Dr Petros Wallden and his team at the Quantum Software Lab, part of the School of Informatics in the University of Edinburgh. Petros is an expert in quantum informatics, with research ranging from quantum algorithms and quantum cryptography to the foundations of quantum informatics.&nbsp;&nbsp;</p>\n\n\n\n<p>Petros\u2019 team used the <a href=\"https://developer.nvidia.com/cuda-q\">NVIDIA CUDA-Q</a> (formerly CUDA Quantum) platform to develop and accelerate the simulation of new QML methods to significantly reduce the qubit count necessary to study large data sets.</p>\n\n\n\n<p>Petros\u2019 team extended <a href=\"https://arxiv.org/abs/2004.00026\">Harrow\u2019s work</a>, which uses the concept of coresets to provide a novel way of building realistic oracles for QML applications, without the need for QRAM.&nbsp;&nbsp;</p>\n\n\n\n<h2 id=\"what_are_coresets\"  class=\"wp-block-heading\">What are coresets?<a href=\"#what_are_coresets\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>A <em>coreset</em> is formed by taking a full data set and optimally mapping it to a smaller weighted data set (Figure 1). The coreset can then be analyzed to approximate traits of the full data set without the need to process the full data set directly.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"565\" height=\"455\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-representation-data-set.png\" alt=\"Scatterplot shows raw data with coresets as smaller and larger stars to represent the weights.\" class=\"wp-image-87517\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-representation-data-set.png 565w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-representation-data-set-300x242.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-representation-data-set-143x115.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-representation-data-set-373x300.png 373w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-representation-data-set-112x90.png 112w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-representation-data-set-362x292.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-representation-data-set-137x110.png 137w\" sizes=\"(max-width: 565px) 100vw, 565px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. A size 10 coreset representation of a 1000-point dataset</em></figcaption></figure></div>\n\n\n<p>Coresets are the result of a classical dimensionality reduction method that preprocesses data before a clustering application. By employing coresets, data-intensive QML tasks can be approximated with orders of magnitude fewer qubits and become more realistic applications for near-term quantum computing.</p>\n\n\n\n<p>Standard classical coreset construction techniques usually begin with a data set and a target error and then determine the optimal size of the coreset to satisfy this error requirement. Due to experimental constraints, Petros\u2019 team instead chose the coreset size based on the number of qubits that were available. They then assessed the error resulting from this choice following the quantum computations.&nbsp;</p>\n\n\n\n<h2 id=\"quantum_approaches_for_clustering_with_coresets\"  class=\"wp-block-heading\">Quantum approaches for clustering with coresets<a href=\"#quantum_approaches_for_clustering_with_coresets\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With input data reduced to the manageably sized coreset, Petros\u2019 team could explore three quantum clustering algorithms.&nbsp;</p>\n\n\n\n<p><em>Clustering</em> is an unsupervised learning technique describing a family of methods that group similar data points in a meaningful way. These groups, or clusters, can then be used for informed decision making in real-world applications, such as determining whether a tumor is malignant or benign.</p>\n\n\n\n<p>Petros\u2019s team used CUDA-Q to implement the following clustering techniques:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Divisive clustering:</strong> An approach where coreset points begin in one cluster and are successively bipartitioned until each data point is in its own cluster. In this approach, the process can be stopped at the <em>K</em>th iteration to see how data is divided into <em>K</em> clusters (Figure 2).</li>\n\n\n\n<li><strong>3-means clustering:</strong> The data points are partitioned into<em> K</em> clusters (three, in this case) based on each point\u2019s relationship to<em> K</em> evolving centers-of-mass (centroids). The process ends when the three clusters converge and no longer change with new iterations.</li>\n\n\n\n<li><strong>Gaussian mixture model (GMM) clustering:</strong> The distribution of potential coreset points locations is represented as a mixture of <em>K</em> Gaussian distributions. The data is sorted into <em>K</em> sets based on which Gaussian each coreset point most likely came from.&nbsp;</li>\n</ul>\n\n\n\n<p>Each of these clustering techniques outputs a set of coresets and a mapping from each point in the original dataset to one of these coresets. The result is an approximate clustering and dimensionality reduction of the initial, large dataset.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation.png\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"578\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation-1024x578.png\" alt=\"The divisive clustering model for an N=25 coreset results in a dendrogram that can be used to determine the coreset clusters after any number of iterations and a corresponding number of desired clusters (five, in this case.)\" class=\"wp-image-87612\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation-1024x578.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation-625x353.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation-768x433.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation-645x364.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/coreset-outcome-simulation.png 1117w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 2. Outcome of an N=25 coreset QML divisive-clustering simulation</em></figcaption></figure></div>\n\n\n<p>By using a variational quantum algorithm (VQA) framework, each technique was expressed in a way that could use a QPU. Petros and team made this possible by deriving a weighted qubit Hamiltonian (inspired by the max cut problem) encoding the respective cost functions for each clustering method described earlier. Having such a Hamiltonian enabled the iterative VQA process to repeatedly call a QPU, real or simulated, to efficiently calculate the cost minimization required in each clustering routine.</p>\n\n\n\n<h2 id=\"using_cuda-q_to_overcome_scalability_issues&nbsp;\"  class=\"wp-block-heading\">Using CUDA-Q to overcome scalability issues&nbsp;<a href=\"#using_cuda-q_to_overcome_scalability_issues&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Exploring the effectiveness of these QML-clustering approaches required the ability to run simulations of how each algorithm would perform.&nbsp;</p>\n\n\n\n<p>The NVIDIA CUDA-Q simulation toolkits enabled comprehensive simulations of each clustering method on problem sizes up to 25 qubits. CUDA-Q sped up these simulations by providing easy access to GPU hardware. It also provided out-of-the-box primitives such as hardware-efficient ansatz kernels used to parametrize the Hamiltonian-based optimization process and spin Hamiltonians that were easily adapted to the clustering algorithm cost functions.&nbsp;</p>\n\n\n\n<p>In fact, the performing simulations at the scales that Petros\u2019 team presented in their <a href=\"https://arxiv.org/abs/2402.01529\">Big data applications on small quantum computers</a> paper is only possible with the GPU-acceleration provided by CUDA-Q.&nbsp;</p>\n\n\n\n<p>Initial experiments simulated only 10 qubits on CPU hardware, but memory constraints made this impossible for the 25-qubit experiments of interest. Implementation in CUDA-Q meant that the initial 10-qubit simulation code was also instantly compatible, running without modification when Petros\u2019 team needed to switch out CPU hardware for an NVIDIA DGX H100 GPU system.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"426\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend-1024x426.png\" alt=\"Diagram shows a 320-GB state vector requirement divided between four 80-GB GPUs.\" class=\"wp-image-87515\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend-1024x426.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend-300x125.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend-625x260.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend-179x74.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend-768x319.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend-1536x639.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend-645x268.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend-500x208.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend-160x67.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend-362x151.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend-264x110.png 264w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-mqpu-backend.png 1606w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. CUDA-Q </em>mgpu<em> backend can pool the memory of multiple GPUs to perform large state vector simulations</em></figcaption></figure></div>\n\n\n<p>This kind of code scalability is a huge advantage. By pooling the memory of multiple GPUs with the <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/backends/simulators.html#multi-node-multi-gpu\">NVIDIA mgpu backend</a> (Figure 3), Petros and team have since scaled the simulation even further by changing the backend target, again without having to significantly alter their original simulation code.</p>\n\n\n\n<p>According to Boniface Yogendran, the lead developer on this research, \u201cCUDA-Q enabled us to not worry about qubit scalability limitations and be HPC-ready from day one.\u201d&nbsp;</p>\n\n\n\n<p>Yogendran\u2019s code can take the work beyond simulation, as CUDA-Q is inherently also QPU-ready, providing support for deployment on all major QPU modalities.</p>\n\n\n\n<h2 id=\"value_of_cuda-q_simulations\"  class=\"wp-block-heading\">Value of CUDA-Q simulations<a href=\"#value_of_cuda-q_simulations\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Being able to easily simulate all three clustering algorithms enabled Petros and his team to benchmark each of them against a brute force method, finding the global optimal solution, and a classical heuristic approach known as Lloyd\u2019s algorithm. The results indicated that the quantum algorithms performed best for the GMM (<em>K</em>=2) and the divisive clustering approaches were comparable to Lloyd\u2019s algorithm.&nbsp;</p>\n\n\n\n<p>Based on the success of this work, Petros\u2019 team plans to continue working with NVIDIA to continue developing and scaling new quantum-accelerated supercomputing applications with CUDA-Q.&nbsp;&nbsp;</p>\n\n\n\n<h2 id=\"explore_cuda-q\"  class=\"wp-block-heading\">Explore CUDA-Q<a href=\"#explore_cuda-q\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>CUDA-Q enabled Petros and the team to easily develop novel QML implementations and simulate them with accelerated computing. Implementation in CUDA-Q enables the code to be portable for further large-scale simulations or deployment on physical QPUs.&nbsp;</p>\n\n\n\n<p>For more information about CUDA-Q quantum or to get started right away, see the <a href=\"https://nvidia.github.io/cuda-quantum/latest/examples/python/tutorials/Divisive_clustering.html\">Divisive clustering</a> Jupyter notebook that explores the coreset-enabled divisive clustering method described in this post. The tutorial demonstrates how easy it is to scale the code and run a 34-qubit instance using GPUs.&nbsp;</p>\n\n\n\n<p>Download CUDA-Q using any of the methods provided in the <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/quick_start.html#install-cuda-q\">CUDA-Q Quick Start</a> guide and find other tutorials on the <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/tutorials.html\">CUDA-Q Tutorials</a> page.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Quantum computers can use the quantum properties of superposition, entanglement, and interference to generalize learnings and insights from data. Such quantum machine learning (QML) techniques will eventually run on quantum-accelerated supercomputers that combine the processing powers of CPUs, GPUs, and QPUs to solve some of the world\u2019s most complex problems. Many QML algorithms offer theoretical &hellip; <a href=\"https://developer.nvidia.com/blog/cuda-q-enabled-resource-reduction-for-quantum-clustering-algorithms/\">Continued</a></p>\n", "protected": false}, "author": 1950, "featured_media": 87610, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1474481", "discourse_permalink": "https://forums.developer.nvidia.com/t/cuda-q-enabled-resource-reduction-for-quantum-clustering-algorithms/304652", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [503], "tags": [453, 2735], "coauthors": [3645, 3985], "class_list": ["post-87484", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-simulation-modeling-design", "tag-featured", "tag-quantum-computing"], "acf": {"post_industry": ["HPC / Scientific Computing"], "post_products": ["CUDA-Q"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cuda-q-big-data-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mL2", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87484"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1950"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87484"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87484/revisions"}], "predecessor-version": [{"id": 87797, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87484/revisions/87797"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87610"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87484"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87484"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87484"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87484"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87847, "date": "2024-08-22T09:03:46", "date_gmt": "2024-08-22T16:03:46", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87847"}, "modified": "2024-09-05T10:57:25", "modified_gmt": "2024-09-05T17:57:25", "slug": "jamba-1-5-llms-leverage-hybrid-architecture-to-deliver-superior-reasoning-and-long-context-handling", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/jamba-1-5-llms-leverage-hybrid-architecture-to-deliver-superior-reasoning-and-long-context-handling/", "title": {"rendered": "Jamba 1.5 LLMs Leverage Hybrid Architecture to Deliver Superior Reasoning and Long Context Handling"}, "content": {"rendered": "\n<p><a href=\"https://www.ai21.com/\">AI21 Labs</a> has unveiled their latest and most advanced Jamba 1.5 model family, a cutting-edge collection of <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models (LLMs)</a> designed to excel in a wide array of generative AI tasks. These models are capable of creating content, summarizing and comparing documents, and extracting valuable insights from vast datasets.&nbsp;</p>\n\n\n\n<p>This <a href=\"https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/\">mixture of experts (MoE)</a> model takes advantage of the transformer and Mamba architectures to deliver superior efficiency, latency, and long context handling. This, coupled with the ease of deployment on any accelerated platform, enables enterprises to run their applications on secure environments close to where their data resides.</p>\n\n\n\n<p>NVIDIA recently optimized and hosted the new <a href=\"https://build.nvidia.com/search?term=ai21\">Jamba 1.5 models</a>, which are now available to experience on the NVIDIA API catalog.</p>\n\n\n\n<h2 id=\"hybrid_architecture_delivers_superior_performance\"  class=\"wp-block-heading\">Hybrid architecture delivers superior performance<a href=\"#hybrid_architecture_delivers_superior_performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The Jamba 1.5 model family is built with a unique hybrid approach that combines the strengths of the Mamba and transformer architectures, in addition to a mixture of experts (MoE) module. Specifically, the Mamba architecture excels in managing long contexts with minimal computational overhead, while the transformer layers provide unmatched accuracy and reasoning capabilities.&nbsp;</p>\n\n\n\n<p>The MoE module helps increase the model capacity referred to as the total number of available parameters without increasing the computational requirements (number of active parameters). Transformer, Mamba, and MoE layers combined into a singular decoder architecture are collectively referred to as a <em>Jamba block</em>. Each Jamba block can fit in a single <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100</a> 80 GB GPU and is configured with eight layers consisting of an attention-to-Mamba ratio of 1:7 layers. MoE is applied to every other layer with a total of 16 experts, out of which two experts are used at each token generation.</p>\n\n\n\n<p>By interweaving these architectures, these models achieve a balance between memory usage, less compute for long context, and higher model accuracy. For specific metrics regarding model accuracy, see the <a href=\"https://www.ai21.com/blog/announcing-jamba-model-family\">AI21 Labs press release</a>.</p>\n\n\n\n<p>The model also offers a substantial 256K token context window, which translates to about 800 pages of text. The extended context capability enables the model to process and generate more accurate responses by retaining more relevant information.</p>\n\n\n\n<h2 id=\"enhancing_ai_interactivity_with_function_calling_and_json_support\"  class=\"wp-block-heading\">Enhancing AI interactivity with function calling and JSON support<a href=\"#enhancing_ai_interactivity_with_function_calling_and_json_support\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>One of the standout capabilities of the Jamba 1.5 models is its robust function calling feature, with support for JSON data interchange. This functionality greatly expands what AI systems can do, enabling them to perform complex actions based on user inputs and handle sophisticated queries with structured data output.&nbsp;</p>\n\n\n\n<p>This not only improves the relevance and accuracy of responses but also enhances the overall interactivity of the applications. By extending model capabilities through external function and tool calling, the model is capable of handling a wide variety of downstream use cases for which it may not have been specifically trained.</p>\n\n\n\n<p>For example, businesses can deploy Jamba 1.5 models to handle a wide range of queries\u2014 from loan term sheet generation for a financial service to shopping assistants for retail stores\u2014all in real time and with high precision.</p>\n\n\n\n<h2 id=\"maximizing_accuracy_with_retrieval-augmented_generation\"  class=\"wp-block-heading\">Maximizing accuracy with retrieval-augmented generation<a href=\"#maximizing_accuracy_with_retrieval-augmented_generation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The Jamba 1.5 models effectively fit with <a href=\"https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/\">retrieval-augmented generation (RAG)</a>, enhancing the ability to deliver accurate and contextually relevant responses. With a 256K token context window, the models can manage large volumes of information without needing continuous chunking. This is ideal for scenarios requiring comprehensive data analysis. RAG is particularly useful in environments with extensive and scattered knowledge bases, enabling Jamba 1.5 to simplify retrieval and improve accuracy by providing more relevant information in fewer chunks.</p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Experience the <a href=\"https://build.nvidia.com/search?term=ai21\">Jamba 1.5 models</a> on the NVIDIA API catalog. They join over 100 popular AI models that are supported by <a href=\"https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/\">NVIDIA NIM</a> microservices designed to simplify the deployment of performance-optimized open and proprietary foundation models.</p>\n\n\n\n<p>NVIDIA is working with leading model builders to support their models on a fully accelerated stack, including <a href=\"https://build.nvidia.com/explore/discover#llama-3_1-405b-instruct\">Llama 3.1 405B</a>, <a href=\"https://build.nvidia.com/mistralai/mixtral-8x22b-instruct\">Mistral 8x22B</a>, <a href=\"https://build.nvidia.com/microsoft/phi-3-medium-128k-instruct\">Phi-3</a>, <a href=\"https://build.nvidia.com/explore/discover#nemotron-4-340b-reward\">Nemotron 340B Reward</a>, and many more. Visit <a href=\"http://ai.nvidia.com/\">ai.nvidia.com</a> to experience, customize, and deploy these models in enterprise applications.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>AI21 Labs has unveiled their latest and most advanced Jamba 1.5 model family, a cutting-edge collection of large language models (LLMs) designed to excel in a wide array of generative AI tasks. These models are capable of creating content, summarizing and comparing documents, and extracting valuable insights from vast datasets.&nbsp; This mixture of experts (MoE) &hellip; <a href=\"https://developer.nvidia.com/blog/jamba-1-5-llms-leverage-hybrid-architecture-to-deliver-superior-reasoning-and-long-context-handling/\">Continued</a></p>\n", "protected": false}, "author": 1837, "featured_media": 87849, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1472551", "discourse_permalink": "https://forums.developer.nvidia.com/t/jamba-1-5-llms-leverage-hybrid-architecture-to-deliver-superior-reasoning-and-long-context-handling/304267", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [453, 2932, 3613], "coauthors": [3461, 3973], "class_list": ["post-87847", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-large-language-models", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["General"], "post_products": ["H100", "NIM"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/abstract-geometric-field.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mQT", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87847"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1837"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87847"}], "version-history": [{"count": 8, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87847/revisions"}], "predecessor-version": [{"id": 87875, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87847/revisions/87875"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87849"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87847"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87847"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87847"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87847"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 86997, "date": "2024-08-21T13:30:00", "date_gmt": "2024-08-21T20:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=86997"}, "modified": "2024-09-05T10:57:26", "modified_gmt": "2024-09-05T17:57:26", "slug": "build-efficient-recommender-systems-with-co-visitation-matrices-and-rapids-cudf", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/build-efficient-recommender-systems-with-co-visitation-matrices-and-rapids-cudf/", "title": {"rendered": "Build Efficient Recommender Systems with Co-Visitation Matrices and RAPIDS cuDF"}, "content": {"rendered": "\n<p>Recommender systems play a crucial role in personalizing user experiences across various platforms. These systems are designed to predict and suggest items that users are likely to interact with, based on their past behavior and preferences. Building an effective recommender system involves understanding and leveraging huge, complex datasets that capture interactions between users and items.</p>\n\n\n\n<p>This post will show you how to build a simple yet strong recommender system based on co-visitation matrices. One of the key challenges in building co-visitation matrices is the computational complexity involved in processing large datasets. Traditional methods using libraries like <a href=\"https://www.nvidia.com/en-us/glossary/pandas-python/\">pandas</a> can be inefficient and slow, especially when dealing with millions or even billions of interactions. This is where<a href=\"https://docs.rapids.ai/api/cudf/stable/\"> RAPDIS cuDF</a> comes in. RAPIDS cuDF is a GPU DataFrame library that provides a pandas-like API for loading, filtering, and manipulating data.</p>\n\n\n\n<h2 id=\"recommender_systems_and_co-visitation_matrices\"  class=\"wp-block-heading\">Recommender systems and co-visitation matrices<a href=\"#recommender_systems_and_co-visitation_matrices\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Recommender systems are a category of machine learning algorithms aimed at delivering personalized suggestions, or recommendations, to users. These systems are used in a variety of applications, including e-commerce (Amazon, OTTO), content streaming (Netflix, Spotify), social media (Instagram, X, TikTok), and more. The role of these systems is to assist users in discovering products, services, or other content in alignment with their interests and preferences.</p>\n\n\n\n<p>Datasets used to build a recommender system typically contain the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=N&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"N\" class=\"latex\" /> items to recommend. <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=N&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"N\" class=\"latex\" /> can be huge (even millions).</li>\n\n\n\n<li>Interactions between users and items. A sequence of such interactions for a given user is called a <em>session</em>. The goal is then to infer which items the user is going to interact with next.&nbsp;&nbsp;</li>\n</ul>\n\n\n\n<p>Figure 1 shows an example session in which the user interacted with items 6543, 242, 5381, and 5391. The goal of the recommender system is to predict which items the user will interact with next. One common way to assess performance is to use the Recall on <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=k&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"k\" class=\"latex\" /> guesses made by the model (recall<em>@</em><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=k&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"k\" class=\"latex\" />). Recall counts how many ground truth items the model can retrieve normalized by the number of ground truth items.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"849\" height=\"121\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/example-session-recommender-system-1.png\" alt=\"A diagram showing an example session. Item 6543 is ordered first, then item 2424, then 5391, followed by item 5391. The next action is unknown.\n\" class=\"wp-image-87041\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/example-session-recommender-system-1.png 849w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/example-session-recommender-system-1-300x43.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/example-session-recommender-system-1-625x89.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/example-session-recommender-system-1-179x26.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/example-session-recommender-system-1-768x109.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/example-session-recommender-system-1-645x92.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/example-session-recommender-system-1-500x71.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/example-session-recommender-system-1-160x23.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/example-session-recommender-system-1-362x52.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/example-session-recommender-system-1-772x110.png 772w\" sizes=\"(max-width: 849px) 100vw, 849px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Example session used to build a recommender system</em></em></figcaption></figure>\n\n\n\n<p>During a session, a user will often interact with several items. A <em>co-visitation matrix</em> counts the items appearing together and is of size <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=N&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"N\" class=\"latex\" /> x <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=N&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"N\" class=\"latex\" />. Co-visitation matrices can be easily used to make recommendations by checking which items co-occur frequently with the items in the session. For the session shown in Figure 1, if item 2834 is frequently bought together with item 6543, then it is a good recommendation.</p>\n\n\n\n<h3 id=\"challenges_building_co-visitation_matrices\"  class=\"wp-block-heading\">Challenges building co-visitation matrices<a href=\"#challenges_building_co-visitation_matrices\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Computing co-visitation matrices requires looking at all the sessions and counting all the co-occurrences. This quickly becomes costly: for a given session of size <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=L&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"L\" class=\"latex\" />, the complexity is in <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=O%28L%5E2%29&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"O(L^2)\" class=\"latex\" />. For a real-world recommender system dataset, you can expect millions of sessions, if not billions. As a result, heavy optimization is necessary for co-visitation matrices to be usable. Sessions need to be considered simultaneously.&nbsp;</p>\n\n\n\n<p>pandas makes the computations easy to implement, but at the expense of efficiency. On one hand, sessions need to be considered in parts for the memory not to explode. On the other hand, big datasets cause significant slow-downs.</p>\n\n\n\n<p>A faster computing framework is necessary that also allows for the code clarity of pandas. This is where RAPIDS cuDF comes in. You can accelerate computations 40x with zero code changes using RAPIDS cuDF.</p>\n\n\n\n<p>This post demonstrates how to build a co-visitation matrix and accelerate the workflow using the <a href=\"https://rapids.ai/cudf-pandas/\">RAPIDS cuDF pandas accelerator mode</a>. Running the code as you read will provide you with a better understanding of how beneficial the accelerator really is. Before you begin, make sure to turn on the GPU accelerator. For more details, <a href=\"https://colab.research.google.com/drive/1CKxPGGjfdR7YUZ3BkATTMGZFV8Vjerwy?usp=sharing\">see the demo notebook</a>.&nbsp;</p>\n\n\n\n<h2 id=\"rapids_cudf_pandas&nbsp;accelerator_mode\"  class=\"wp-block-heading\">RAPIDS cuDF pandas&nbsp;accelerator mode<a href=\"#rapids_cudf_pandas&nbsp;accelerator_mode\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>RAPIDS cuDF is a Python GPU DataFrame library designed to speed up operations that can be slow when performed on CPU on big datasets (loading, joining, aggregating, and filtering, for example).</p>\n\n\n\n<p>Its API style is similar to pandas\u2019, and with the new cuDF pandas accelerator mode, you can bring accelerated computing to your pandas workflows without any code changes. The cuDF library delivers 50x to 150x faster performance for tabular data processing.</p>\n\n\n\n<p>RAPIDS cuDF pandas accelerator mode extends the capability of the cuDF library and delivers 50x to 150x faster performance for tabular data processing. It enables you to bring accelerated computing to your pandas workflows without requiring any code changes.&nbsp;</p>\n\n\n\n<h2 id=\"the_data\"  class=\"wp-block-heading\">The data<a href=\"#the_data\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The data used in this tutorial is extracted from the train set of the <a href=\"https://www.kaggle.com/competitions/otto-recommender-system\">OTTO \u2013 Multi-Objective Recommender System</a> Kaggle competition, which contains one month of sessions. The first three weeks are used for building the matrices, and the last week for evaluation. Validation sessions were truncated to build targets for the model. Recall@20 will be used to see how well the co-visitation matrices can retrieve the truncated items.</p>\n\n\n\n<p>Note that it\u2019s important to use a temporal split to avoid leaking information. The test data contains the fifth week of the dataset. The dataset contains 1.86 million items, and about 500 million interactions of users with these items. These interactions are stored in chunked parquet files for easier handling.&nbsp;</p>\n\n\n\n<p>The following information is known about the data:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><code>session</code>: The session ID; a session is equivalent to a user in this case</li>\n\n\n\n<li><code>aid</code>: The item ID</li>\n\n\n\n<li><code>ts</code>: The time at which the interaction occurred</li>\n\n\n\n<li><code>type</code>: The type of interaction; can be clicks, carts, or orders</li>\n</ul>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"337\" height=\"195\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/session-data-sample.png\" alt=\"The image shows 5 rows of session 0.  First, item 1517085 was clicked, then items 1563459, 1309446, 16246 and 1781822 were clicked.\n\" class=\"wp-image-87048\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/session-data-sample.png 337w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/session-data-sample-300x174.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/session-data-sample-179x104.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/session-data-sample-156x90.png 156w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/session-data-sample-190x110.png 190w\" sizes=\"(max-width: 337px) 100vw, 337px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2. Data sample</em></em></figcaption></figure>\n\n\n\n<h2 id=\"implementing_co-visitation_matrices\"  class=\"wp-block-heading\">Implementing co-visitation matrices<a href=\"#implementing_co-visitation_matrices\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Because of the number of items in the dataset, memory is an issue. So, split the data into two parts to prevent the co-visitation matrix from being too large. Then loop over all the parquet files in the training data.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ncovisitation_matrix = &#x5B;]\nfor part in range(PARTS):\n      print(f&quot;- Part {part + 1}/{PARTS}&quot;)\n      matrix = None\n      for file_idx, file in enumerate(tqdm(train_files)):\n</pre></div>\n\n\n<p>The first step is to load the data. Then apply some transformations to save memory: change the types of the columns to <code>int32</code>, and restrict sessions longer than 30 interactions.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfor part in range(PARTS):\n     for file_idx, file in enumerate(tqdm(train_files)):\n\t      &#x5B;...]\n          # Load sessions &amp; convert columns to save memory\n          df = pd.read_parquet(file, columns=&#x5B;&quot;session&quot;, &quot;aid&quot;, &quot;ts&quot;])\n          df&#x5B;&quot;ts&quot;] = (df&#x5B;&quot;ts&quot;] / 1000).astype(&quot;int32&quot;)\n          df&#x5B;&#x5B;&quot;session&quot;, &quot;aid&quot;]] = df&#x5B;&#x5B;&quot;session&quot;, &quot;aid&quot;]].astype(&quot;int32&quot;)\n\n          # Restrict to first 30 interactions\n          df = df.sort_values(\n               &#x5B;&quot;session&quot;, &quot;ts&quot;],\n               ascending=&#x5B;True, False],\n               ignore_index=True\n          )\n          df&#x5B;&quot;n&quot;] = df.groupby(&quot;session&quot;).cumcount()\n          df = df.loc&#x5B;df.n &lt; 30].drop(&quot;n&quot;, axis=1)\n</pre></div>\n\n\n<p>Next, you can get all the co-occurrences by aggregating the data with itself on the session column. This is already time-consuming and the resulting data frame is quite big.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n     # Compute pairs \n     df = df.merge(df, on=&quot;session&quot;)\n</pre></div>\n\n\n<p>To reduce the cost of the matrix computation, restrict the items to the ones in the part currently considered. Also only consider interactions occurring within 1 hour, on different items. Don\u2019t allow for duplicated interactions within sessions.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n     # Split in parts to reduce memory usage\n     df = df.loc&#x5B;\n          (df&#x5B;&quot;aid_x&quot;] &gt;= part * SIZE) &amp;\n          (df&#x5B;&quot;aid_x&quot;] &lt; (part + 1) * SIZE)\n     ]\n\n     # Restrict to same day and remove self-matches\n     df = df.loc&#x5B;\n          ((df&#x5B;&quot;ts_x&quot;] - df&#x5B;&quot;ts_y&quot;]).abs() &lt; 60 * 60) &amp; \n          (df.aid_x != df.aid_y)\n     ]\n\t# No duplicated interactions within sessions\n    df = df.drop_duplicates(\n          subset=&#x5B;&quot;session&quot;, &quot;aid_x&quot;, &quot;aid_y&quot;],\n          keep=&quot;first&quot;,\n     ).reset_index(drop=True)\n</pre></div>\n\n\n<p>In the next step, compute the matrix weights. Count all the cooccurrences by doing a sum aggregation on pairs.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n     # Compute weights of pairs\n     df&#x5B;&quot;wgt&quot;] = 1\n     df&#x5B;&quot;wgt&quot;] = df&#x5B;&quot;wgt&quot;].astype(&quot;float32&quot;)\n\n     df.drop(&#x5B;&quot;session&quot;, &quot;ts_x&quot;, &quot;ts_y&quot;], axis=1, inplace=True)\n     df = df.groupby(&#x5B;&quot;aid_x&quot;, &quot;aid_y&quot;]).sum()\n</pre></div>\n\n\n<p>At the end of the second loop (the one which loops over parquet files), update the coefficients by adding the newly computed weights to the previous ones. Since co-visitation matrices are big, this process is slow and consumes memory. To free some memory, delete unused variables.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n     # Update covisitation matrix with new weights\n     if matrix is None:\n          matrix = df\n     else:  # this is the bottleneck operation\n          matrix = matrix.add(df, fill_value=0)  \n\n     # Clear memory\n     del df\n     gc.collect()\n</pre></div>\n\n\n<p>After seeing all the data, reduce the size of the matrices by only keeping the <em>N </em>best candidates per item\u2014that is, the candidates with the highest weight. This is where the interesting information is located.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfor part in range(PARTS):\n     &#x5B;...]\n     # Final matrix : Sort values\n     matrix = matrix.reset_index().rename(\n          columns={&quot;aid_x&quot;: &quot;aid&quot;, &quot;aid_y&quot;: &quot;candidate&quot;}\n     )\t\n     matrix = matrix.sort_values(\n          &#x5B;&quot;aid&quot;, &quot;wgt&quot;], ascending=&#x5B;True, False], ignore_index=True\n     )\n\n     # Restrict to n candids\n     matrix&#x5B;&quot;rank&quot;] = matrix.groupby(&quot;aid&quot;).candidate.cumcount()\n     matrix = matrix&#x5B;matrix&#x5B;&quot;rank&quot;] &lt; N_CANDIDS].reset_index(drop=True)\n     covisitation_matrix.append(matrix)\n</pre></div>\n\n\n<p>The final step is to concatenate the different parts of the matrix that were computed separately. Afterwards, you can choose to save your matrix to disk if needed.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ncovisitation_matrix = pd.concat(covisitation_matrix, ignore_index=True)\n</pre></div>\n\n\n<p>There you have it. This code computes a simple co-visitation matrix using pandas. It does have one major flaw: it is slow, taking almost 10 minutes to compute the matrix! It also required restricting the data significantly for a reasonable runtime.&nbsp;</p>\n\n\n\n<h2 id=\"cudf_pandas_accelerator_mode\"  class=\"wp-block-heading\">cuDF pandas accelerator mode<a href=\"#cudf_pandas_accelerator_mode\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>And this is where the cuDF pandas accelerator mode comes in.&nbsp;Restart the kernel, and with one line of code unleash the power of your GPU:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code aligncenter\"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n%load_ext cudf.pandas\n</pre></div>\n\n\n<p>Table 1 shows the runtimes with and without cuDF acceleration. One line of code achieves a 40x speedup.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td></td><td class=\"has-text-align-center\" data-align=\"center\">pandas</td><td class=\"has-text-align-center\" data-align=\"center\">cuDF pandas</td></tr><tr><td>10% of the data</td><td class=\"has-text-align-center\" data-align=\"center\">8 min 41 s</td><td class=\"has-text-align-center\" data-align=\"center\">13 s</td></tr><tr><td>Whole dataset</td><td class=\"has-text-align-center\" data-align=\"center\">1 h 30 min+&nbsp;</td><td class=\"has-text-align-center\" data-align=\"center\">5 min 30 s</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>T<em>able 1. Performance comparison of pandas versus cuDF pandas</em></em></figcaption></figure>\n\n\n\n<h2 id=\"generating_candidates\"  class=\"wp-block-heading\">Generating candidates<a href=\"#generating_candidates\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Generating candidates to recommend is one use of co-visitation matrices. This is done by aggregating the weights of the co-visitation matrix over all the items in a session. The items with the highest weight will be recommended. The implementation is straightforward in pandas, and once again benefits from the GPU accelerator.</p>\n\n\n\n<p>Start by loading the data, then only consider the last <code>N_CANDIDS</code> seen items. Items that have already been seen in the sessions are great recommendations. They will be recommended first. Remember that self-matches were removed from the co-visitation matrices to save memory, so simply recover them here.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\ncandidates_df_list = &#x5B;]\nlast_seen_items_list = &#x5B;]\n\nfor file_idx in tqdm(range(len(val_files))):\n     # Load sessions &amp; convert columns to save memory\n     df = pd.read_parquet(\n          val_files&#x5B;file_idx], columns=&#x5B;&quot;session&quot;, &quot;aid&quot;, &quot;ts&quot;]\n     )\n     df&#x5B;&quot;ts&quot;] = (df&#x5B;&quot;ts&quot;] / 1000).astype(&quot;int32&quot;)\n     df&#x5B;&#x5B;&quot;session&quot;, &quot;aid&quot;]] = df&#x5B;&#x5B;&quot;session&quot;, &quot;aid&quot;]].astype(&quot;int32&quot;)\n\n     # Last seen items\n     df = df.sort_values(\n          &#x5B;&quot;session&quot;, &quot;ts&quot;], ascending=&#x5B;True, False], ignore_index=True\n     )\n     df&#x5B;&quot;n&quot;] = df.groupby(&quot;session&quot;).cumcount()\n\n     # Restrict to n items\n     last_seen_items_list.append(\n          df.loc&#x5B;df.n &lt; N_CANDIDS].drop(&#x5B;&quot;ts&quot;, &quot;n&quot;], axis=1)\n      )\n     df.drop(&#x5B;&quot;ts&quot;, &quot;n&quot;], axis=1, inplace=True)\n</pre></div>\n\n\n<p>Next, merge the co-visitation matrix on the items in the session. Each item in the session is then associated with <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=N&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"N\" class=\"latex\" /> (candidate, weight) pairs. To get a session-level weight for the candidates, take the sum of their weights over the session\u2019s items. By taking the <code>N_CANDIDS</code> candidates with the highest weight, you can obtain candidates for the recommender system.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nfor file_idx in tqdm(range(len(val_files))):\n     &#x5B;...]\n     # Merge covisitation matrix\n     df = df.merge(\n          covisitation_matrix.drop(&quot;rank&quot;, axis=1), how=&quot;left&quot;, on=&quot;aid&quot;\n     )\n     df = df.drop(&quot;aid&quot;, axis=1).groupby(\n          &#x5B;&quot;session&quot;, &quot;candidate&quot;]\n     ).sum().reset_index()\n\n     # Sort candidates\n     df = df.sort_values(&#x5B;&quot;session&quot;, &quot;wgt&quot;], ascending=&#x5B;True, False])\n\n     # Restrict to n items\n     df&#x5B;&quot;rank&quot;] = df.groupby(&quot;session&quot;).candidate.cumcount()\n     df = df&#x5B;df&#x5B;&quot;rank&quot;] &lt; N_CANDIDS].reset_index(drop=True)\n     candidates_df_list.append(df)\n</pre></div>\n\n\n<h2 id=\"performance_assessment\"  class=\"wp-block-heading\">Performance assessment<a href=\"#performance_assessment\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To evaluate the strength of the candidates, use the recall metric. The recall measures the proportion of items in the ground truth that were successfully found by the retriever.&nbsp;</p>\n\n\n\n<p>In this case, allow for 20 candidates. You want to see the proportion of items users purchased that the matrices successfully retrieved. The achieved recall is 0.5868, which is already a strong baseline. Out of the 20 items returned by the recommender, on average 11 of them were purchased by the user. This is already a strong baseline: during the competition, top teams reached scores close to&nbsp; 0.7. Refer to the <a href=\"https://colab.research.google.com/drive/1CKxPGGjfdR7YUZ3BkATTMGZFV8Vjerwy?usp=sharing\">demo notebook</a> for details about the implementation, which are beyond the scope of this post.&nbsp;</p>\n\n\n\n<h2 id=\"going_further\"  class=\"wp-block-heading\">Going further<a href=\"#going_further\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Accelerating co-visitation matrix computation and aggregation enables you to iterate very quickly, to improve candidate recall. The first improvement is to give more history to the matrices. With a fast implementation, this can be done without having to wait for hours for computations to end.</p>\n\n\n\n<p>Then refine the matrices. While you can try various approaches, for now, the co-visitation matrices are given a weight of one for every item co-occurring. The demo gives more weight to items that are closer in time, since such interactions seem more relevant. Another idea is to consider the type of interaction. Items purchased together, or items added to a cart together, seem more relevant to recommend than items that were simply viewed.</p>\n\n\n\n<p>Until now, all items in the session have been considered equally important when aggregating matrix weights. Items that appear later in the session have more predictive factors than the oldest ones. Therefore, it\u2019s possible to refine the aggregation by increasing the weights of the candidates of items closer to the end of the session.&nbsp;</p>\n\n\n\n<p>Using these three changes, the recall@20 improves to 0.5992. The implementations follow the same scheme as the code presented in this post, with a few added lines to bring in the improvements. For details, see the <a href=\"https://colab.research.google.com/drive/1CKxPGGjfdR7YUZ3BkATTMGZFV8Vjerwy?usp=sharing\">demo notebook</a>.</p>\n\n\n\n<p>The final step is to merge several co-visitation matrices to capture different types of candidates.</p>\n\n\n\n<h2 id=\"summary\"  class=\"wp-block-heading\">Summary<a href=\"#summary\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This post provides a comprehensive guide to building and optimizing co-visitation matrices. Although co-visitation matrices are a simple tool\u2014they count the co-occurrence of items during user sessions\u2014building them involves processing large amounts of data. Doing so efficiently is necessary to iterate quickly and improve recommender systems.</p>\n\n\n\n<p>By leveraging RAPIDS cuDF and its newly released pandas accelerator mode, co-visitation matrix computation is up to 50x faster. Building a diverse set of co-visitation matrices quickly, thanks to GPU acceleration, was a key component that enabled Kaggle Grandmasters of NVIDIA to win multiple recommender system competitions, including KDDCup 2023 and OTTO.</p>\n\n\n\n<p>While the <a href=\"https://colab.research.google.com/drive/1CKxPGGjfdR7YUZ3BkATTMGZFV8Vjerwy?usp=sharing\">demo notebook</a> only computes two types of matrices, the possibilities are limitless. Experiment with the code and try to improve the construction of the matrices to capture different candidates.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Recommender systems play a crucial role in personalizing user experiences across various platforms. These systems are designed to predict and suggest items that users are likely to interact with, based on their past behavior and preferences. Building an effective recommender system involves understanding and leveraging huge, complex datasets that capture interactions between users and items. &hellip; <a href=\"https://developer.nvidia.com/blog/build-efficient-recommender-systems-with-co-visitation-matrices-and-rapids-cudf/\">Continued</a></p>\n", "protected": false}, "author": 2245, "featured_media": 87001, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1471879", "discourse_permalink": "https://forums.developer.nvidia.com/t/build-efficient-recommender-systems-with-co-visitation-matrices-and-rapids-cudf/304142", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [1433, 453, 813, 1731, 4142], "coauthors": [3972], "class_list": ["post-86997", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "tag-cudf", "tag-featured", "tag-kaggle", "tag-pandas", "tag-recommenders-personalization"], "acf": {"post_industry": ["Retail / Consumer Packaged Goods"], "post_products": ["cuDF", "RAPIDS"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/rapids-cudf-pandas-acceleration-graphic-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mDb", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86997"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2245"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=86997"}], "version-history": [{"count": 46, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86997/revisions"}], "predecessor-version": [{"id": 87719, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86997/revisions/87719"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87001"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=86997"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=86997"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=86997"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=86997"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87666, "date": "2024-08-21T11:00:00", "date_gmt": "2024-08-21T18:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87666"}, "modified": "2024-09-05T10:57:27", "modified_gmt": "2024-09-05T17:57:27", "slug": "google-cloud-run-adds-support-for-nvidia-l4-gpus-nvidia-nim-and-serverless-ai-inference-deployments-at-scale", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/google-cloud-run-adds-support-for-nvidia-l4-gpus-nvidia-nim-and-serverless-ai-inference-deployments-at-scale/", "title": {"rendered": "Google Cloud Run Adds Support for NVIDIA L4 GPUs, NVIDIA NIM, and Serverless AI Inference Deployments at Scale"}, "content": {"rendered": "\n<p>Deploying AI-enabled applications and services presents enterprises with significant challenges:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Performance is critical as it directly shapes user experience and competitive advantage and affects deployment costs, influencing your overall return on investment.&nbsp;</li>\n\n\n\n<li>Achieving scalability is essential to meet the fluctuating demands of the deployed AI application effectively without over-provisioning compute resources. This entails scaling up resources dynamically during peak periods to ensure smooth operation and scaling down during quieter times to optimize costs.&nbsp;</li>\n\n\n\n<li>Complexity further complicates matters, involving tasks such as optimizing the performance of multiple AI models, seamlessly integrating them into existing workflows, and managing the underlying infrastructure</li>\n</ul>\n\n\n\n<p>Addressing these challenges requires a full-stack approach that can optimize performance, manage scalability effectively, and navigate the complexities of deployment, enabling organizations to maximize AI\u2019s full potential while maintaining operational efficiency and cost-effectiveness. </p>\n\n\n\n<p>Google Cloud and NVIDIA have collaborated to address these challenges and simplify AI inference deployments by combining the performance of the NVIDIA AI platform and the ease of serverless computing in the cloud.&nbsp;</p>\n\n\n\n<p>Cloud Run, Google Cloud\u2019s fully managed serverless container runtime, has <a href=\"https://cloud.google.com/blog/products/application-development/run-your-ai-inference-applications-on-cloud-run-with-nvidia-gpus\">added support for NVIDIA L4 Tensor Core GPUs</a>, available in preview. You can now run on-demand real-time AI applications accelerated at scale without worrying about infrastructure management. Combined with the power of <a href=\"http://ai.nvidia.com\">NVIDIA NIM microservices</a>, Cloud Run can significantly simplify the complexities of optimizing and serving AI models for production while maximizing application performance.</p>\n\n\n\n<h2 id=\"deploy_real-time_ai-enabled_applications\"  class=\"wp-block-heading\">Deploy real-time AI-enabled applications<a href=\"#deploy_real-time_ai-enabled_applications\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Cloud Run enables you to deploy and run containerized applications by abstracting away infrastructure management and dynamically allocating resources on demand. It automatically scales applications based on incoming traffic so you don\u2019t have to provision excess compute resources to handle peak loads. With its fast instance starts and scale to zero, you also don\u2019t have to maintain idle resources during periods of low demand.&nbsp;</p>\n\n\n\n<p>Cloud Run support for <a href=\"https://www.nvidia.com/en-us/data-center/l4/\">NVIDIA L4 Tensor Core GPUs</a> marks a significant leap from its previous CPU-only offerings.&nbsp;</p>\n\n\n\n<p>The NVIDIA L4 GPU is optimized for inference at scale for a broad range of AI applications, including recommendations, voice-based AI assistants, generative AI, visual search, and contact center automation to deliver the best personalized experiences. L4 GPUs deliver <a href=\"https://nvdam.widen.net/s/rvq98gbwsw/l4-datasheet-2595652\">up to 120x higher AI video performance</a> over CPU solutions and 2.7x more generative AI inference performance over the previous generation.</p>\n\n\n\n<p>Google Cloud was the first cloud to offer NVIDIA L4 GPUs with its <a href=\"https://cloud.google.com/compute/docs/gpus#l4-gpus\">G2 VMs</a> and they are supported across Google Cloud services including Google Compute Engine (GCE), Google Kubernetes Engine (GKE), and Vertex AI.&nbsp;</p>\n\n\n\n<p>Companies like Let\u2019s Enhance, Wombo, Writer, Descript, and AppLovin are using the power of NVIDIA L4 GPUs to bring generative AI\u2013powered applications to life and deliver delightful experiences to their customers.</p>\n\n\n\n<p>Adding support for NVIDIA L4 on Cloud Run enables you to deploy real-time inference applications with lightweight generative AI models like Gemma-2B/7B, Llama3-8B, and Mistral-8x7B. This is combined with the scalability, per-second billing, low latency, and fast cold start times of Cloud Run\u2019s serverless platform.</p>\n\n\n\n<h2 id=\"performance-optimized_serverless_ai_inference\"  class=\"wp-block-heading\">Performance-optimized serverless AI inference<a href=\"#performance-optimized_serverless_ai_inference\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Optimizing the performance of the AI model being deployed is crucial because it directly affects the resources required and influences the overall costs of deploying the AI-enabled application.</p>\n\n\n\n<p>To address this challenge, NVIDIA introduced <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a>, a set of optimized cloud-native microservices designed to simplify and accelerate the deployment of AI models. NIM provides pre-optimized, containerized models that can be easily integrated into applications, reducing development time and maximizing resource efficiency.&nbsp;</p>\n\n\n\n<p>By using NVIDIA NIM on Cloud Run, you can deploy high-performance AI applications using optimized inference engines that unlock the full potential of NVIDIA L4 GPUs and deliver the best throughput and latency, without the need for expertise in inference performance optimization.&nbsp;</p>\n\n\n\n<p>Part of <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> on <a href=\"https://console.cloud.google.com/marketplace/product/nvidia/nvidia-ai-enterprise-vmi\">Google Cloud Marketplace</a>, NIM offers flexible integration with an OpenAI API-compatible programming model and custom extensions, while prioritizing enterprise-grade security by using safetensors, continuously monitoring and patching CVEs, and conducting regular internal penetration tests. This ensures that AI applications are robust, secure, and well-supported, facilitating a smooth transition from development to production.</p>\n\n\n\n<p>In addition to Cloud Run, NVIDIA NIM can be deployed across different Google Cloud services, including Google Kubernetes Engine (GKE) or Google Compute Engine (GCE) giving you the choice of the level of abstraction you need for building and deploying AI-enabled applications.</p>\n\n\n\n<h2 id=\"deploying_a_llama3-8b-instruct_nim_microservice_on_google_cloud_run_with_nvidia_l4\"  class=\"wp-block-heading\">Deploying a Llama3-8B-Instruct NIM microservice on Google Cloud Run with NVIDIA L4<a href=\"#deploying_a_llama3-8b-instruct_nim_microservice_on_google_cloud_run_with_nvidia_l4\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Here\u2019s how you can deploy a Llama3-8B-Instruct model with Cloud Run on an NVIDIA L4 GPU using NIM. Cloud Run currently supports attaching one NVIDIA L4 GPU per Cloud Run instance. As a prerequisite, install the <a href=\"https://cloud.google.com/sdk/docs/install-sdk\">Google Cloud SDK</a> on your workstation.</p>\n\n\n\n<p>Clone the repository:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: bash; title: ; notranslate\" title=\"\">\n$ git clone https://github.com/NVIDIA/nim-deploy\n$ cd nim-deploy/cloud-service-providers/google-cloud/cloudrun\n</pre></div>\n\n\n<p>Set the environment variables needed for launch:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n$ cat env\nexport SERVICE_ACCOUNT_ID=&lt;Put your service account&gt;\nexport PROJECTID=&lt;Put your project ID&gt;\nexport PROJECTUSER=&lt;Put your user name&gt;\nexport PROJECTNUM=&lt;Put your project number&gt;\nexport REGION=&lt;Put your region&gt;\nexport GCSBUCKET=&lt;Put your GCS bucket&gt;\nexport SERVICE_NAME=llama-3-8b-instruct\n# ---- entries below created by build_nim.sh\n</pre></div>\n\n\n<p>Edit the Dockerfile with the appropriate NIM microservice name needed for deployment. Place the desired model URL from NGC in the <code>FROM</code> statement:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nFROM nvcr.io/nim/meta/llama3-8b-instruct:1.0.0\n</pre></div>\n\n\n<p>Build the container for launch:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n$ source ./env &amp;&amp; ./build_nim.sh\n</pre></div>\n\n\n<p>Deploy the container by executing the run.sh script:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\n$ source ./env &amp;&amp; ./run.sh\n</pre></div>\n\n\n<h2 id=\"ready_to_get_started\"  class=\"wp-block-heading\">Ready to get started?<a href=\"#ready_to_get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The powerful combination of the NVIDIA AI platform, including NVIDIA NIM and NVIDIA L4 GPUs, together with Google Cloud Run, addresses the critical challenges of performance, scalability, and complexity inherent in deploying AI applications. This synergy not only accelerates deployment but also boosts application performance, helping organizations make the most of AI while keeping operations efficient and costs low.</p>\n\n\n\n<p>You can experience and prototype with NVIDIA NIM microservices through the <a href=\"https://build.nvidia.com/explore/discover\">NVIDIA API catalog</a>, enabling you to test and refine your applications. You can then download the NIM containers to continue development, research, and testing on Google Cloud Run as part of the free <a href=\"https://developer.nvidia.com/developer-program\">NVIDIA Developer Program</a>. </p>\n\n\n\n<p>If you are looking for enterprise-grade security, support, and API stability, you can access NIM through a free <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/#get-started\">90-day NVIDIA AI Enterprise license</a>. You can also try a hands-on lab with NIM on <a href=\"https://www.nvidia.com/en-us/launchpad/\">NVIDIA LaunchPad</a>.&nbsp;</p>\n\n\n\n<p><a href=\"https://cloud.google.com/run/docs/configuring/services/gpu\">Cloud Run with NVIDIA L4 GPU</a> support is currently in preview and available in the us-central1 Google Cloud region. For more information about this feature and to see demos in action, see the <a href=\"https://cloudonair.withgoogle.com/events/run-ai-with-cloud-run\">launch event livestream</a> and <a href=\"https://g.co/cloudrun/gpu\">sign up</a> for access today!</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Deploying AI-enabled applications and services presents enterprises with significant challenges:&nbsp; Addressing these challenges requires a full-stack approach that can optimize performance, manage scalability effectively, and navigate the complexities of deployment, enabling organizations to maximize AI\u2019s full potential while maintaining operational efficiency and cost-effectiveness. Google Cloud and NVIDIA have collaborated to address these challenges and simplify &hellip; <a href=\"https://developer.nvidia.com/blog/google-cloud-run-adds-support-for-nvidia-l4-gpus-nvidia-nim-and-serverless-ai-inference-deployments-at-scale/\">Continued</a></p>\n", "protected": false}, "author": 1027, "featured_media": 87674, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1471833", "discourse_permalink": "https://forums.developer.nvidia.com/t/google-cloud-run-adds-support-for-nvidia-l4-gpus-nvidia-nim-and-serverless-ai-inference-deployments-at-scale/304131", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110], "tags": [296, 453, 850], "coauthors": [2093, 2764, 3987, 1302], "class_list": ["post-87666", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "tag-ai-inference-microservices", "tag-featured", "tag-google"], "acf": {"post_industry": ["General"], "post_products": ["NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/partner-press-google-io-promo-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mNY", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87666"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1027"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87666"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87666/revisions"}], "predecessor-version": [{"id": 87852, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87666/revisions/87852"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87674"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87666"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87666"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87666"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87666"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87511, "date": "2024-08-21T09:00:00", "date_gmt": "2024-08-21T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87511"}, "modified": "2024-09-05T10:57:29", "modified_gmt": "2024-09-05T17:57:29", "slug": "practical-strategies-for-optimizing-llm-inference-sizing-and-performance", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/practical-strategies-for-optimizing-llm-inference-sizing-and-performance/", "title": {"rendered": "Practical Strategies for Optimizing LLM Inference Sizing and Performance"}, "content": {"rendered": "\n<p>As the use of large language models (LLMs) grows across many applications, such as chatbots and content creation, it&#8217;s important to understand the process of scaling and optimizing inference systems to make informed decisions about hardware and resources for LLM inference.</p>\n\n\n\n<p>In the following talk, Dmitry Mironov and Sergio Perez, senior deep learning solutions architects at NVIDIA, <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s62797/\">guide you</a> through the critical aspects of LLM inference sizing. Sharing their expertise, best practices, and tips, they walk you through how to efficiently navigate the complexities of deploying and optimizing LLM Inference projects. </p>\n\n\n\n<script src=\"https://api-prod.nvidia.com/search/nvidia-search-library.js\"></script>\n \n\n<div id=\"nvidia-event-details-widget\"></div>\n<style>\n.nvidia-search-widget .cleanslate , .nvidia-search-widget .player-overlay {\ndisplay:none;\n}\n</style>\n \n\n<script>\n \n NvidiaSearchLibrary.EventSessionDetailsWidget.mount({\n          site: 'https://www.nvidia.com',\n          language: 'en-us',\n          sessionId: 'gtc24-s62797',\n          jwtToken: '',\n \u2002\u2002\u2002\u2002voltronApiUrl:  'https://api-prod.nvidia.com/services/nod/api/v1/',\n          apiUrl: 'https://api-prod.nvidia.com/search/graphql',\n           onLogin: () => { },\n          onLogout: () => { },\n       \n          onSeeAllSessions: (speakerName) => {\n            window.location.href =  'https://www.nvidia.com/en-us/on-demand/search/?q=\"' + speakerName+'\"';\n          },\n          searchApiUrl: 'https://api-prod.nvidia.com/search/graphql',\n          searchToken: '',\n          uiConfId: '50468382',\n          showSessionRating: false,\n          anonToken: '',\n        });\n \n</script>\n\n\n\n<p>Follow along with a <a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/S62797-LLM-Inference-Sizing_-Benchmarking-End-to-End-Inference-Systems.pdf\">PDF of the session</a>, while learning how to choose the right path for your AI project by understanding key metrics in LLM inference sizing. Discover how to accurately size hardware and resources, optimize performance and costs, and select the best deployment strategies, whether on-premises or in the cloud.</p>\n\n\n\n<p>You will also cover advanced tools like the NVIDIA NeMo inference sizing calculator (<a href=\"https://docs.nvidia.com/nim/benchmarking/llm/latest/index.html\">use this NIM for LLM benchmarking guide</a> to replicate it) and <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/archives/triton-inference-server-2280/user-guide/docs/user_guide/perf_analyzer.html\">NVIDIA Triton performance analyzer</a>, enabling you to measure, simulate, and improve your LLM inference systems.</p>\n\n\n\n<p>By applying their practical guidelines and improving your technical skill set, you\u2019ll be better equipped to tackle challenging AI deployment scenarios and achieve success in your AI initiatives.</p>\n\n\n\n<p>Watch the talk <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s62797/\">LLM Inference Sizing: Benchmarking End-to-End Inference Systems</a>, explore more videos on NVIDIA On-Demand, and gain valuable skills and insights from industry experts by joining the <a href=\"https://developer.nvidia.com/developer-program\">NVIDIA Developer Program</a>.</p>\n\n\n\n<p><em>This content was partially crafted with the assistance of generative AI and LLMs. It underwent careful review and was edited by the NVIDIA Technical Blog team to ensure precision, accuracy, and quality.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>As the use of large language models (LLMs) grows across many applications, such as chatbots and content creation, it&#8217;s important to understand the process of scaling and optimizing inference systems to make informed decisions about hardware and resources for LLM inference. In the following talk, Dmitry Mironov and Sergio Perez, senior deep learning solutions architects &hellip; <a href=\"https://developer.nvidia.com/blog/practical-strategies-for-optimizing-llm-inference-sizing-and-performance/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 87681, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1471767", "discourse_permalink": "https://forums.developer.nvidia.com/t/practical-strategies-for-optimizing-llm-inference-sizing-and-performance/304115", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110, 1903], "tags": [296, 1935, 453, 2932, 3545, 3986], "coauthors": [2315], "class_list": ["post-87511", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "category-features", "tag-ai-inference-microservices", "tag-education-and-training", "tag-featured", "tag-large-language-models", "tag-nlp", "tag-nvidia-on-demand"], "acf": {"post_industry": ["General"], "post_products": ["NeMo", "TensorRT", "Triton Inference Server"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Video"], "post_collections": ["GTC March 2024"]}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/llm-chatbot-purple.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mLt", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87511"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87511"}], "version-history": [{"count": 29, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87511/revisions"}], "predecessor-version": [{"id": 87821, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87511/revisions/87821"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87681"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87511"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87511"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87511"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87511"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87736, "date": "2024-08-20T13:04:23", "date_gmt": "2024-08-20T20:04:23", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87736"}, "modified": "2024-09-05T10:57:30", "modified_gmt": "2024-09-05T17:57:30", "slug": "hackathon-build-groundbreaking-generative-ai-projects-using-nvidia-ai-workbench", "status": "publish", "type": "post", "link": "https://nvda.ws/3Xeskl5", "title": {"rendered": "Hackathon: Build Groundbreaking Generative AI Projects Using NVIDIA AI Workbench"}, "content": {"rendered": "\n<p>Hosted by Dell and NVIDIA, demonstrate how AI Workbench can be used to build and deliver apps for a wide range of tasks and workflows.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Hosted by Dell and NVIDIA, demonstrate how AI Workbench can be used to build and deliver apps for a wide range of tasks and workflows.</p>\n", "protected": false}, "author": 2186, "featured_media": 79479, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1471109", "discourse_permalink": "https://forums.developer.nvidia.com/t/hackathon-build-groundbreaking-generative-ai-projects-using-nvidia-ai-workbench/304009", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/3Xeskl5", "_links_to_target": "_blank"}, "categories": [1050, 696, 3110], "tags": [453, 208, 1958, 3613], "coauthors": [3909], "class_list": ["post-87736", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-data-science", "category-generative-ai", "tag-featured", "tag-hackathon", "tag-news", "tag-retrieval-augmented-generation-rag"], "acf": {"post_industry": ["General"], "post_products": ["AI Workbench"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Announcement", "News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/03/ai-workbench.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mP6", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87736"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2186"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87736"}], "version-history": [{"count": 1, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87736/revisions"}], "predecessor-version": [{"id": 87738, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87736/revisions/87738"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/79479"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87736"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87736"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87736"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87736"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87567, "date": "2024-08-20T13:00:00", "date_gmt": "2024-08-20T20:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87567"}, "modified": "2024-08-22T11:24:50", "modified_gmt": "2024-08-22T18:24:50", "slug": "nvidia-gh200-superchip-delivers-breakthrough-energy-efficiency-and-node-consolidation-for-apache-spark", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-gh200-superchip-delivers-breakthrough-energy-efficiency-and-node-consolidation-for-apache-spark/", "title": {"rendered": "NVIDIA GH200 Superchip Delivers Breakthrough Energy Efficiency and Node Consolidation for Apache Spark"}, "content": {"rendered": "\n<p>With the rapid growth of generative AI, CIOs and IT leaders are looking for ways to reclaim data center resources to accommodate new AI use cases that promise greater return on investment without impacting current operations. This is leading IT decision makers to reassess past infrastructure decisions and explore strategies to consolidate traditional workloads into fewer, more power-efficient nodes, freeing up data center power and space.</p>\n\n\n\n<p><a href=\"https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/\">NVIDIA GH200 Grace Hopper Superchip</a> is the first memory-converged CPU-GPU superchip designed from the ground up to meet the challenges of AI, high-performance computing, and data processing. By migrating Apache Spark workloads from CPU nodes to NVIDIA GH200, data centers and enterprises can accelerate query response time by up to 35x. For large <a href=\"https://spark.apache.org/\">Apache Spark</a> clusters of 1,500+ nodes, this speedup translates to up to 22x fewer nodes and savings of up to 14 GWh in annual energy efficiency.</p>\n\n\n\n<p>This post explores the architectural innovations of NVIDIA GH200 for data processing, shares SQL benchmark results for GH200, and provides insights on seamlessly migrating Apache Spark workloads to this new platform.</p>\n\n\n\n<h2 id=\"tackling_legacy_bottlenecks_in_cpu-based_apache_spark_systems\"  class=\"wp-block-heading\">Tackling legacy bottlenecks in CPU-based Apache Spark systems<a href=\"#tackling_legacy_bottlenecks_in_cpu-based_apache_spark_systems\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Over the last decade, enterprises have grappled with the overwhelming volumes of business, consumer, and IoT data, which are increasingly pivotal for maintaining a competitive edge within industries. To address this challenge, many enterprises have turned to Apache Spark, a multi-language open-source system used for big data distributed processing.</p>\n\n\n\n<p>Apache Spark began as a research project at University of California, Berkeley with the goal of addressing the limitations of previous big data frameworks. It achieved this by caching data in CPU memory, which significantly accelerated SQL queries. Today, tens of thousands of organizations rely on Apache Spark for diverse data processing tasks spanning a wide array of industries including financial services, healthcare, manufacturing, and retail.</p>\n\n\n\n<p>Despite its ability to alleviate the bottleneck of data access from slower hard disks and cloud-based object storage through memory caching, many Apache Spark data processing workflows still encounter constraints due to hardware limitations inherent in CPU architectures.</p>\n\n\n\n<h2 id=\"pioneering_a_new_era_of_converged_cpu-gpu_superchips\"  class=\"wp-block-heading\">Pioneering a new era of converged CPU-GPU superchips<a href=\"#pioneering_a_new_era_of_converged_cpu-gpu_superchips\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Recent advancements in storage and networking bandwidth, along with the ending of Moore\u2019s law, have shifted the focus of analytics and query bottlenecks to CPUs. Meanwhile, GPUs have emerged as the preferred platform for Deep Learning workloads due to their vast number of processing cores and high-bandwidth memory which excel in highly parallelized processing. Parallelizing Apache Spark workloads and running them on GPUs delivers order of magnitude speed-ups compared to CPUs.</p>\n\n\n\n<p>Running Apache Spark workloads on GPUs formerly necessitated the transfer of data back and forth between the host CPU and GPU\u2014traditionally bound by the 128 GB/s low speed PCIe interfaces. To overcome this challenge, NVIDIA developed<a href=\"https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/\"> NVIDIA Grace Hopper</a>, a new class of superchips that bring together the Arm-based<a href=\"https://www.nvidia.com/en-us/data-center/grace-cpu/\"> NVIDIA Grace CPU</a> and<a href=\"https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/\"> NVIDIA Hopper GPU</a> architectures using<a href=\"https://www.nvidia.com/en-us/data-center/nvlink-c2c/\"> NVLink-C2C</a> interconnect technology.<a href=\"https://www.nvidia.com/en-us/data-center/nvlink-c2c/\"> NVLink-C2C</a> delivers up to 900 GB/s total bandwidth. This is 7x higher bandwidth than the standard PCIe Gen5 lanes found in traditional x86-based GPU accelerated systems.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"653\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks.png\" alt=\"Comparison of legacy PCIe architecture that has separate CPU and GPU memory and a low bandwidth PCIe connection with the Grace Hopper architecture that has a single unified virtual memory pool with fast NVLINK-C2C connection. \n\" class=\"wp-image-87577\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks-300x98.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks-625x204.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks-179x58.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks-768x251.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks-1536x502.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks-645x211.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks-500x163.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks-160x52.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks-362x118.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks-337x110.png 337w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-grace-hopper-architecture-pcle-bottlenecks-1024x335.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. NVIDIA Grace Hopper architecture overcomes PCIe bottlenecks</em></em></figcaption></figure>\n\n\n\n<p>With GH200, the CPU and GPU share a single per-process page table, enabling all CPU and GPU threads to access all system-allocated memory that can reside on physical CPU or GPU memory. When adopted, this architecture removes the need to copy memory back and forth between the CPU and GPU.</p>\n\n\n\n<h2 id=\"nvidia_gh200_sets_new_highs_in_nds_performance_benchmarks\"  class=\"wp-block-heading\">NVIDIA GH200 sets new highs in NDS performance benchmarks<a href=\"#nvidia_gh200_sets_new_highs_in_nds_performance_benchmarks\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To measure the performance and cost savings of running Apache Spark on GH200, we used the <a href=\"https://github.com/NVIDIA/spark-rapids-benchmarks/tree/dev/nds#nds-v20-automation\">NVIDIA Decision Support (NDS) benchmark</a>. NDS is derived from the widely used and adopted CPU-only data processing <a href=\"https://www.tpc.org/tpcds/\">TPC-DS</a> benchmark. NDS consists of the same SQL queries included in TPC-DS with modifications only to data generation and benchmark execution scripts. NDS is not TPC-DS and NDS results are not comparable to official, audited TPC-DS results\u2014only to other NDS results.</p>\n\n\n\n<p>Running the 100+ TPC-DS SQL queries with NDS execution scripts on a 10 TB dataset took 6 minutes using 16 GH200 superchips compared to 42 minutes on an equal number of premium x86 CPUs: a 7x end-to-end speedup.</p>\n\n\n\n<figure class=\"wp-block-image size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1630\" height=\"962\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200.png\" alt=\"Benchmark comparisons of running NDS-DS Benchmark using SF10 dataset  demonstrating how a 16x  GH200 cluster delivers 7x query acceleration vs. Premium CPU cluster with equivalent number of nodes.\n\" class=\"wp-image-87582\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200.png 1630w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200-300x177.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200-625x369.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200-179x106.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200-768x453.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200-1536x907.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200-645x381.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200-500x295.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200-152x90.png 152w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200-362x214.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200-186x110.png 186w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nds-ds-benchmark-apache-spark-nvidia-gh200-1024x604.png 1024w\" sizes=\"(max-width: 1630px) 100vw, 1630px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. <em>NDS-DS benchmark results running Apache Spark with RAPIDS Accelerator on an NVIDIA Grace Hopper16-node cluster using SF10 versus 16-node premium CPU cluster\u00a0</em></em></figcaption></figure>\n\n\n\n<p>Specifically, queries that have a high number of aggregate and join operations exhibited significantly higher acceleration of up to 36x.</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Query67, accelerated by 36x, finds top stores for different product categories based on store sales in a specific year. It involves a high number of aggregate and shuffle operations.</li>\n\n\n\n<li>Query14, accelerated by 10x, calculates the sum of the extended sales price of store transactions for each item and a specific year and month. It involves a high number of shuffle and join operations.</li>\n\n\n\n<li>Query87, accelerated by 9x, counts how many customers have ordered items on the web, the catalog and bought items in a store on the same day. It involves a high number of scan and aggregate operations.</li>\n\n\n\n<li>Query59, accelerated by 9x, reports the increase of weekly store sales from one year to the next year for each store and day of the week. It involves a high number of aggregate and join operations.</li>\n\n\n\n<li>Query38, accelerated by 8x, displays the count of customers with purchases from all three channels in a given year. It involves a high number of distinct aggregate and join operations.</li>\n</ul>\n\n\n\n<h2 id=\"reducing_power_consumption_and_cutting_energy_costs&nbsp;\"  class=\"wp-block-heading\">Reducing power consumption and cutting energy costs&nbsp;<a href=\"#reducing_power_consumption_and_cutting_energy_costs&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>As the datasets grow in size, GH200 delivers even more query acceleration and node consolidation benefits. Running the same 100+ queries on the 10x larger SF100 dataset (100 TB) required a total of 40 minutes on the 16-node GH200 cluster.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1087\" height=\"476\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/benchmark-comparisons-nvidia-gh200.png\" alt=\"Benchmark comparisons of running NDS Benchmark using different dataset sizes (3 GB, 10 GB, 30 GB, and 100 GB) demonstrating how GH200 delivers further query acceleration as dataset size increases.  \n\" class=\"wp-image-87586\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/benchmark-comparisons-nvidia-gh200.png 1087w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/benchmark-comparisons-nvidia-gh200-300x131.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/benchmark-comparisons-nvidia-gh200-625x274.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/benchmark-comparisons-nvidia-gh200-179x78.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/benchmark-comparisons-nvidia-gh200-768x336.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/benchmark-comparisons-nvidia-gh200-645x282.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/benchmark-comparisons-nvidia-gh200-500x219.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/benchmark-comparisons-nvidia-gh200-160x70.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/benchmark-comparisons-nvidia-gh200-362x159.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/benchmark-comparisons-nvidia-gh200-251x110.png 251w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/benchmark-comparisons-nvidia-gh200-1024x448.png 1024w\" sizes=\"(max-width: 1087px) 100vw, 1087px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 3. NDS benchmark results running Apache Spark 3.4.1 with RAPIDS Accelerator 24.06 on an NVIDIA Grace Hopper 16-node cluster</em></em></figcaption></figure>\n\n\n\n<p>Achieving an equivalent 40-minute response time on the 100 TB dataset using premium CPUs would have required a total of 344 CPUs. This translates to a 22x reduction in the number of nodes and 12x energy savings. For organizations running a large Apache Spark CPU cluster, which can sometimes exceed 1,500 nodes, the energy savings are significant, reaching up to 14 GWh annually.\u00a0</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"942\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy.png\" alt=\"Two side-by-side images of data center racks. The left image shows 1,500 x86 CPU nodes across multiple rows of racks. The right image shows 72 GH200 nodes in just two racks, delivering equivalent performance to the1500 x86 CPU nodes. Text appears on the right image indicating 6x Lower TCO, 22x Fewer Server Nodes and 12x More Energy Efficiency using the GH200 nodes. \n\" class=\"wp-image-87589\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy-300x141.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy-625x295.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy-768x362.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy-1536x724.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy-645x304.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy-500x236.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy-160x75.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy-362x171.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy-233x110.png 233w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/apache-spark-nvidia-gh200-comparison-nodes-energy-1024x483.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. Comparison across TCO, number of nodes, and energy savings of moving a 1,500 x86 Apache Spark cluster to GH200</em></em></figcaption></figure>\n\n\n\n<h2 id=\"exceptional_sql_acceleration_and_price_performance&nbsp;\"  class=\"wp-block-heading\">Exceptional SQL acceleration and price performance&nbsp;<a href=\"#exceptional_sql_acceleration_and_price_performance&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"http://heavy.ai/\">HEAVY.AI</a>, a leading GPU-accelerated analytics platform and database provider, benchmarked a single GH200 GPU cloud instance against an 8x NVIDIA A100 PCIe-based cloud instance running<a href=\"https://github.com/heavyai/heavydb\"> HeavyDB</a> and the<a href=\"https://github.com/NVIDIA/spark-rapids-benchmarks/tree/dev/nds-h\"> NDS-H</a> benchmark.\u00a0</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1599\" height=\"852\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware.png\" alt=\"Two side-by-side images comparing GPU hardware instances used by HEAVY.AI on Vultr cloud for benchmarking. The image on the left features 8 NVIDIA A100 GPUs with specifications: 640 GB VRAM, 2TB CPU RAM, 112 Intel 8480+ Platinum CPU cores, and a cost of $13.89 per hour. The image on the right displays a single GH200 GPU node with specifications: 96 GB VRAM, 480GB CPU RAM, 72 ARM CPU cores, and a cost of $4.32 per hour. A note appears at the bottom of the image indicating the prices quoted can vary and are for reservations of 1+ month.\n\" class=\"wp-image-87592\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware.png 1599w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware-300x160.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware-625x333.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware-179x95.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware-768x409.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware-1536x818.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware-645x344.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware-500x266.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware-160x85.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware-362x193.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware-206x110.png 206w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavyai-benchmarking-hardware-1024x546.png 1024w\" sizes=\"(max-width: 1599px) 100vw, 1599px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 5. Hardware used by HEAVY.AI during benchmarking\u00a0</em></em></figcaption></figure>\n\n\n\n<p>HEAVY.AI reported an average 5x speedup using the GH200 instance, translating to a 16x cost savings on the SF100 dataset. On the larger SF200 dataset, which does not fit on a single GH200 GPU memory and has to be offloaded to the Grace CPU memory over the low latency high bandwidth NVLink-C2C, HEAVY.AI reported a 2x speedup and 6x cost savings compared to the 8 NVIDIA A100 x86 and PCIe-based instance.</p>\n\n\n\n<p>\u201cOur customers make data-driven, time-sensitive decisions that have a high impact on their business,\u201d said Todd Mostak, CTO and co-founder of HEAVY.AI. \u201cWe\u2019re excited about the new business insights and cost savings that GH200 will unlock for our customers.\u201d</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1419\" height=\"908\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavydb-nds-h-benchmark-results-heavyai.png\" alt=\"Bar chart comparing the NVIDIA GH200 Superchip with x86-PCIe based 8xA100 GPUs using the NDS-H benchmark. GH200 demonstrates 5x speedup on the SF100 dataset and 2x speedup on the SF200 datasets. \n\" class=\"wp-image-87597\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavydb-nds-h-benchmark-results-heavyai.png 1419w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavydb-nds-h-benchmark-results-heavyai-300x192.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavydb-nds-h-benchmark-results-heavyai-625x400.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavydb-nds-h-benchmark-results-heavyai-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavydb-nds-h-benchmark-results-heavyai-768x491.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavydb-nds-h-benchmark-results-heavyai-645x413.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavydb-nds-h-benchmark-results-heavyai-469x300.png 469w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavydb-nds-h-benchmark-results-heavyai-141x90.png 141w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavydb-nds-h-benchmark-results-heavyai-362x232.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavydb-nds-h-benchmark-results-heavyai-172x110.png 172w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/heavydb-nds-h-benchmark-results-heavyai-1024x655.png 1024w\" sizes=\"(max-width: 1419px) 100vw, 1419px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 6. HeavyDB and NDS-H benchmark results for HEAVY.AI</em></em></figcaption></figure>\n\n\n\n<h2 id=\"get_started_with_your_gh200_apache_spark_migration\"  class=\"wp-block-heading\">Get started with your GH200 Apache Spark migration<a href=\"#get_started_with_your_gh200_apache_spark_migration\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Enterprises can take advantage of the <a href=\"https://docs.nvidia.com/spark-rapids/index.html\">RAPIDS Accelerator for Apache Spark</a> to seamlessly migrate Apache Spark workloads to NVIDIA GH200. RAPIDS Accelerator for Apache Spark leverages GPUs to accelerate processing by combining the power of the RAPIDS cuDF library and the scale of the Spark distributed computing framework. Enterprises can run existing Apache Spark applications on GPUs with no code change by launching Spark with the RAPIDS Accelerator for Apache Spark plug-in jar.&nbsp;</p>\n\n\n\n<p>Today, <a href=\"https://nvidianews.nvidia.com/news/nvidia-grace-hopper-ignites-new-era-of-ai-supercomputing\">GH200 powers nine supercomputers</a> around the world, is offered by a<a href=\"https://blogs.nvidia.com/blog/mgx-accelerated-systems-gtc/\"> wide array of system makers</a>, and can be accessed on demand at cloud providers such as Vultr, Lambda, and CoreWeave. You can also test GH200 through<a href=\"https://www.nvidia.com/en-us/launchpad/development-frameworks/one-grace-hopper/\"> NVIDIA LaunchPad</a>. To learn more about Apache Spark acceleration on GH200, check out the GTC 2024 session <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s62257/\">Accelerate ETL and Machine Learning in Apache Spark</a> on demand.\u00a0</p>\n", "protected": false}, "excerpt": {"rendered": "<p>With the rapid growth of generative AI, CIOs and IT leaders are looking for ways to reclaim data center resources to accommodate new AI use cases that promise greater return on investment without impacting current operations. This is leading IT decision makers to reassess past infrastructure decisions and explore strategies to consolidate traditional workloads into &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-gh200-superchip-delivers-breakthrough-energy-efficiency-and-node-consolidation-for-apache-spark/\">Continued</a></p>\n", "protected": false}, "author": 2008, "featured_media": 87580, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1471101", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-gh200-superchip-delivers-breakthrough-energy-efficiency-and-node-consolidation-for-apache-spark/304008", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852], "tags": [278, 1937, 453, 2780], "coauthors": [3708, 1327, 2682], "class_list": ["post-87567", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "tag-apache-spark", "tag-oil-and-gas", "tag-featured", "tag-hopper"], "acf": {"post_industry": ["HPC / Scientific Computing"], "post_products": ["Grace CPU", "Hopper"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-hopper-grace.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mMn", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87567"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2008"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87567"}], "version-history": [{"count": 19, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87567/revisions"}], "predecessor-version": [{"id": 87716, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87567/revisions/87716"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87580"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87567"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87567"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87567"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87567"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87302, "date": "2024-08-20T06:05:00", "date_gmt": "2024-08-20T13:05:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87302"}, "modified": "2024-08-22T11:24:51", "modified_gmt": "2024-08-22T18:24:51", "slug": "deploy-the-first-on-device-small-language-model-for-improved-game-character-roleplay", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/deploy-the-first-on-device-small-language-model-for-improved-game-character-roleplay/", "title": {"rendered": "Deploy the First On-Device Small Language Model for Improved Game Character Roleplay"}, "content": {"rendered": "\n<p>At Gamescom 2024, NVIDIA announced our first on-device small language model (SLM) for improving the conversation abilities of game characters. We also announced that the first game to showcase NVIDIA ACE and digital human technologies is Amazing Seasun Games&#8217; <a href=\"https://store.steampowered.com/app/2452280/Mecha_BREAK/\">Mecha BREAK</a>, bringing its characters to life and providing a more dynamic and immersive gameplay experience on <a href=\"https://www.nvidia.com/en-us/ai-on-rtx/\">NVIDIA GeForce RTX AI PCs</a>.</p>\n\n\n\n<h2 id=\"nvidia_ace_introduces_an_slm_purpose-built_for_roleplaying\"  class=\"wp-block-heading\">NVIDIA ACE introduces an SLM purpose-built for roleplaying<a href=\"#nvidia_ace_introduces_an_slm_purpose-built_for_roleplaying\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://developer.nvidia.com/ace\">NVIDIA ACE</a> is a suite of digital human technologies that provide speech, intelligence, and animation powered by generative AI. It now includes the first NVIDIA SLM, <a href=\"https://developer.nvidia.com/ace/get-started/\">Nemotron-4 4B Instruct</a>. This new model is available as an <a href=\"https://developer.nvidia.com/nim\">NVIDIA NIM</a> for cloud and on-device deployment by game developers. NVIDIA NIM is a set of easy-to-use microservices that speed up the deployment of foundation models on any cloud or data center.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"549\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/first-on-device-slm-game-characters-1024x549.png\" alt=\"Workflow shows use of NVIDIA ACE NIM microservices Riva ASR, Nemotron-4 4B Instruct, Riva TTS, and Audio2Face-3D. Output uses Unreal Engine 5, Omniverse RTS, and a custom renderer to produce the optimized NPC.\" class=\"wp-image-87624\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/first-on-device-slm-game-characters-1024x549.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/first-on-device-slm-game-characters-300x161.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/first-on-device-slm-game-characters-625x335.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/first-on-device-slm-game-characters-179x96.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/first-on-device-slm-game-characters-768x412.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/first-on-device-slm-game-characters-645x346.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/first-on-device-slm-game-characters-500x268.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/first-on-device-slm-game-characters-160x86.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/first-on-device-slm-game-characters-362x194.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/first-on-device-slm-game-characters-205x110.png 205w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/first-on-device-slm-game-characters.png 1132w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. NVIDIA ACE Workflow for Game Characters with Increased Roleplay Capabilities.</em></figcaption></figure></div>\n\n\n<p><mark style=\"background-color:#fcb900\" class=\"has-inline-color\"></mark>Nemotron-4 4B Instruct is <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\">optimized</a> through distillation, pruning, and INT4 quantization for speed and on-device inference. VRAM usage has been minimized to approximately 2 GB, providing a significantly faster time-to-first-token compared to large language models (LLMs).</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><em>Distillation</em> is the process of training a smaller model (called a student) to mimic the outputs of a larger model (a teacher).&nbsp;</li>\n\n\n\n<li><em>Pruning</em> is the process of removing non-critical outputs of the student model.&nbsp;</li>\n\n\n\n<li><em>Quantization</em> is the process of reducing the precision of a model\u2019s weights and activations.&nbsp;</li>\n</ul>\n\n\n\n<p>Nemotron-4 4B Instruct was distilled from the Nemotron-4 15B LLM and purpose-built with instruction tuning, a technique for fine-tuning models on instructional prompts to better perform specific tasks. This model provides better role-play, <a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">retrieval augmented generation</a> (RAG), and function-calling capabilities allowing game characters to more intuitively comprehend player instructions, respond to gamers, and perform more accurate and relevant actions.</p>\n\n\n\n<p>You can use the Nemotron-4 4B Instruct NIM in your digital human technology pipeline through the cloud or take advantage of over 100M NVIDIA GeForce RTX-powered PCs and laptops for on-device inference.\u00a0</p>\n\n\n\n<p>To make it easier for you to use Nemotron-4 4B Instruct NIM, the Unreal Engine 5 ACE sample application has been updated with on-device plugin support.</p>\n\n\n\n<h2 id=\"first_game_premiere_with_nvidia_ace_nim\"  class=\"wp-block-heading\">First game premiere with NVIDIA ACE NIM<a href=\"#first_game_premiere_with_nvidia_ace_nim\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-rich is-provider-embed-handler wp-block-embed-embed-handler wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/d5z7oIXhVqg?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. NVIDIA ACE on-device models enable the mechanic to respond to a gamer\u2019s instructions</em></figcaption></figure>\n\n\n\n<p><em>Mecha BREAK</em>, developed by Amazing Seasun Games, a Kingsoft Corporation game subsidiary, features the NVIDIA Nemotron-4 4B Instruct NIM running on the device in the first showcase of ACE-powered interactions in a game.\u00a0</p>\n\n\n\n<p>The NVIDIA Audio2Face-3D NIM and Whisper, OpenAI\u2019s automatic speech recognition model, provide facial animation and speech recognition running on-device. <a href=\"https://elevenlabs.io/\">Elevenlabs</a> provides the character\u2019s voice through the cloud.\u00a0\u00a0</p>\n\n\n\n<p>In this showcase demo, NVIDIA ACE and digital human technologies enable players to interact with a mechanic that can help them choose from a diverse range of mechs, to complement their playstyle or team needs, assist in appearance customization, and give advice on how to best prepare their colossal war machine for battle.</p>\n\n\n\n<h2 id=\"game_developers_explore_the_latest_digital_human_technologies\"  class=\"wp-block-heading\">Game developers explore the latest digital human technologies<a href=\"#game_developers_explore_the_latest_digital_human_technologies\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA ACE and digital human technologies continue to expand their footprint within the gaming industry.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-rich is-provider-embed-handler wp-block-embed-embed-handler wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/p4fvi8OPuwE?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 2. NVIDIA ACE and ChatGPT-4o provide a new level of immersion and accessibility</em></figcaption></figure>\n\n\n\n<p>Global game publisher and developer Perfect World Games is advancing its NVIDIA ACE and digital human technology demo, Legends, with new AI-powered vision capabilities. Within the demo, the character Yun Ni can see gamers and identify people and objects in the real world using the computer\u2019s camera powered by ChatGPT-4o, adding an augmented reality layer to the gameplay experience. </p>\n\n\n\n<p>These capabilities unlock a new level of immersion and accessibility for PC games. For more information, see <a href=\"https://developer.nvidia.com/ace\">NVIDIA ACE</a> and <a href=\"https://developer.nvidia.com/ace/get-started/\">download</a> the NIM to begin building game characters powered by generative AI.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>At Gamescom 2024, NVIDIA announced our first on-device small language model (SLM) for improving the conversation abilities of game characters. We also announced that the first game to showcase NVIDIA ACE and digital human technologies is Amazing Seasun Games&#8217; Mecha BREAK, bringing its characters to life and providing a more dynamic and immersive gameplay experience &hellip; <a href=\"https://developer.nvidia.com/blog/deploy-the-first-on-device-small-language-model-for-improved-game-character-roleplay/\">Continued</a></p>\n", "protected": false}, "author": 1046, "featured_media": 87327, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1470937", "discourse_permalink": "https://forums.developer.nvidia.com/t/deploy-the-first-on-device-small-language-model-for-improved-game-character-roleplay/303972", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235, 1050, 3110, 503], "tags": [296, 453], "coauthors": [2195], "class_list": ["post-87302", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-graphics", "category-conversational-ai", "category-generative-ai", "category-simulation-modeling-design", "tag-ai-inference-microservices", "tag-featured"], "acf": {"post_industry": ["Gaming"], "post_products": ["Avatar Cloud Engine (ACE)", "Audio2Face", "NIM", "RTX GPU"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mechabreak-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mI6", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87302"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1046"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87302"}], "version-history": [{"count": 9, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87302/revisions"}], "predecessor-version": [{"id": 87714, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87302/revisions/87714"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87327"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87302"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87302"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87302"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87302"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87551, "date": "2024-08-19T08:00:00", "date_gmt": "2024-08-19T15:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87551"}, "modified": "2024-08-22T11:24:51", "modified_gmt": "2024-08-22T18:24:51", "slug": "webinar-build-visual-ai-agents-with-generative-ai-and-nvidia-nim", "status": "publish", "type": "post", "link": "https://nvda.ws/4cw9ziy", "title": {"rendered": "Webinar: Build Visual AI Agents With Generative AI and NVIDIA NIM"}, "content": {"rendered": "\n<p>Learn how to build high-performance solutions with NVIDIA <a href=\"https://www.nvidia.com/en-us/use-cases/visual-ai-agents/\" id=\"\" target=\"_blank\" rel=\"noreferrer noopener\">visual AI agents</a> that help streamline operations across a range of industries.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Learn how to build high-performance solutions with NVIDIA visual AI agents that help streamline operations across a range of industries.</p>\n", "protected": false}, "author": 1106, "featured_media": 87552, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1470328", "discourse_permalink": "https://forums.developer.nvidia.com/t/webinar-build-visual-ai-agents-with-generative-ai-and-nvidia-nim/303870", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/4cw9ziy", "_links_to_target": "_blank"}, "categories": [2724, 2758, 3110], "tags": [3965, 453, 2932, 1981], "coauthors": [2302], "class_list": ["post-87551", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-edge-computing", "category-generative-ai", "tag-ai-agent", "tag-featured", "tag-large-language-models", "tag-webinar"], "acf": {"post_industry": ["General"], "post_products": ["JetPack", "Metropolis", "NIM"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Tutorial"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Siggraph24-New-Graphic-VIA-Microservices-Tech-Blog-3364326pn_-1920x1080-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mM7", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87551"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1106"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87551"}], "version-history": [{"count": 1, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87551/revisions"}], "predecessor-version": [{"id": 87554, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87551/revisions/87554"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87552"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87551"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87551"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87551"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87551"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 84322, "date": "2024-08-16T09:15:56", "date_gmt": "2024-08-16T16:15:56", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=84322"}, "modified": "2024-10-04T14:38:35", "modified_gmt": "2024-10-04T21:38:35", "slug": "leverage-our-latest-open-models-for-synthetic-data-generation-with-nvidia-nemotron-4-340b", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/leverage-our-latest-open-models-for-synthetic-data-generation-with-nvidia-nemotron-4-340b/", "title": {"rendered": "Leverage the Latest Open Models for Synthetic Data Generation with NVIDIA Nemotron-4-340B"}, "content": {"rendered": "<div class='stb-container stb-style-info stb-no-caption'><div class='stb-caption'><div class='stb-logo'><img class='stb-logo__image' src='data:image/png;base64,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' alt='img'/></div><div class='stb-caption-content'></div><div class='stb-tool'></div></div><div class='stb-content'>The <a style=\"color: #0000ff;\" href=\"https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-reward/\">Llama-3.1-Nemotron 70B-Reward model</a> helps generate high-quality training data that aligns with human preferences for finance, retail, healthcare, scientific research, telecommunications, and sovereign AI.</div></div>\n\n\n\n<p><em>This post was updated on August 16, 2024 to reflect the most recent Reward Bench results.</em></p>\n\n\n\n<p>Since the introduction and subsequent wide adoption of large language models (LLMs), data has been the lifeblood of businesses building accurate and safe AI systems. A company\u2019s data represents its cumulative knowledge and can be leveraged in various ways, from customization (supervised fine-tuning (SFT), Parameter-Efficient Fine-Tuning (PEFT), continued pretraining, and so on) to training brand-new, domain-specific small language models (SLMs). </p>\n\n\n\n<p>Data, while being one of the most critical pieces of a modern AI pipeline, has traditionally been costly and limiting during the development of innovative LLMs and SLMs, including paying human annotators and navigating the sourcing of large volumes of domain-specific data. The current process of generating high-quality data is a difficult task.</p>\n\n\n\n<p>Through a process called synthetic data generation (SDG), defined later in this post, businesses can augment existing data stores by using LLMs to create customized high-quality data in large volumes.&nbsp;</p>\n\n\n\n<p>NVIDIA is announcing a new suite of models specifically built for SDG: the Nemotron-4-340B family of models, including a state-of-the-art base model, and an Instruct model to aid in SDG. All are released under a permissive license that enables businesses and developers to use the model outputs and build incredible models.</p>\n\n\n\n<h2 id=\"nvidia_open_model_license\"  class=\"wp-block-heading\">NVIDIA Open Model License<a href=\"#nvidia_open_model_license\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>With the release of the Nemotron-4-340B family of models, which includes base, instruct, and reward models, NVIDIA introduces the <a href=\"https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA Open Model License</a>,&nbsp;a permissive license that allows the distribution, modification, and use of the Nemotron-4-340B models and their outputs for personal, research, and commercial use, without attribution requirements.&nbsp;</p>\n\n\n\n<h2 id=\"nemotron-4-340b-reward_model\"  class=\"wp-block-heading\">Nemotron-4-340B-Reward model<a href=\"#nemotron-4-340b-reward_model\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The Nemotron-4-340B-Reward model is a state-of-the-art multidimensional reward model. The model takes a text prompt as input and returns a list of floating point numbers that are associated with the five attributes in the HelpSteer2 dataset (Figure 1).</p>\n\n\n\n<p>The model has been evaluated using <a href=\"https://arxiv.org/abs/2403.13787\" target=\"_blank\" rel=\"noreferrer noopener\">Reward Bench</a> and shown to achieve benchmark-topping performance despite containing only 10K human-annotated response pairs.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"701\" height=\"329\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-1.-Reward-Bench-results-from-the-HelpSteer2-paper.png\" alt=\"A table of performance results of various models on Reward Bench, a benchmark for evaluating reward models on collections of prompt-chosen-reject trios spanning chat, reasoning and safety. \" class=\"wp-image-84328\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-1.-Reward-Bench-results-from-the-HelpSteer2-paper.png 701w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-1.-Reward-Bench-results-from-the-HelpSteer2-paper-300x141.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-1.-Reward-Bench-results-from-the-HelpSteer2-paper-625x293.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-1.-Reward-Bench-results-from-the-HelpSteer2-paper-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-1.-Reward-Bench-results-from-the-HelpSteer2-paper-645x303.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-1.-Reward-Bench-results-from-the-HelpSteer2-paper-500x235.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-1.-Reward-Bench-results-from-the-HelpSteer2-paper-160x75.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-1.-Reward-Bench-results-from-the-HelpSteer2-paper-362x170.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-1.-Reward-Bench-results-from-the-HelpSteer2-paper-234x110.png 234w\" sizes=\"(max-width: 701px) 100vw, 701px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Initial Reward Bench results from the HelpSteer2 paper</em></figcaption></figure></div>\n\n\n<p>Given a prompt, a reward model provides a score for a response according to human preference. In other words, it can align with human preferences for a given prompt and can replace a large number of human annotations. </p>\n\n\n\n<p>Nemotron-4-340B-Reward has led Reward Bench for two months, with a current overall score of 92.2. Notably, Nemotron-4-340B Reward has the most significant lead in Chat-Hard, beating the next-best alternative by more than 10 points. Chat-Hard is a subset of the test data that evaluates &#8220;a reward model\u2019s abilities to understand trick questions and subtly different instruction responses.&#8221; For more information, see <a href=\"https://arxiv.org/pdf/2403.13787\" target=\"_blank\" rel=\"noreferrer noopener\">RewardBench: Evaluating Reward Models for Language Modeling</a>.</p>\n\n\n\n<h3 id=\"helpsteer2_dataset\"  class=\"wp-block-heading\">HelpSteer2 dataset<a href=\"#helpsteer2_dataset\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>With the release of Nemotron-4-340B-Reward, we also introduced HelpSteer2. This dataset is permissively licensed (CC-BY-4.0) with 10K response pairs. Each prompt in the dataset contains two responses that are human-annotated using a Likert-5 Scale (from 0\u20134, with higher meaning better) for the following attributes:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Helpfulness:</strong> Overall helpfulness of the response to the prompt.</li>\n\n\n\n<li><strong>Correctness:</strong> Inclusion of all pertinent facts without errors.</li>\n\n\n\n<li><strong>Coherence:</strong> Consistency and clarity of expression.</li>\n\n\n\n<li><strong>Complexity:</strong> Intellectual depth required to write a response (that is, whether the response can be written by anyone with basic language competency or whether it requires deep domain expertise).</li>\n\n\n\n<li><strong>Verbosity:</strong> Amount of detail included in the response, relative to what is asked for in the prompt.</li>\n</ul>\n\n\n\n<p>The dataset is focused on conversational data, including multi-turn conversations in the English language.</p>\n\n\n\n<p>For more information, see <a href=\"https://arxiv.org/abs/2406.08673\" target=\"_blank\" rel=\"noreferrer noopener\">HelpSteer2: Open-source dataset for training top-performing reward models</a>.</p>\n\n\n\n<h3 id=\"steerlm_reward_model_training\"  class=\"wp-block-heading\">SteerLM reward model training<a href=\"#steerlm_reward_model_training\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"695\" height=\"155\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-2.-The-Nemotron-3-340B-reward-model-was-created-by-aligning-the-base-model-with-the-HelpSteer2-dataset-using-NeMo-Aligner-a-toolkit-for-efficient-model-alignment.png\" alt=\"Flow diagram starts with HelpSteer2 and moves through Nemotron-4-340B-Base, NeMo Aligner, and Nemotron-4-340B-Reward.\" class=\"wp-image-84329\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-2.-The-Nemotron-3-340B-reward-model-was-created-by-aligning-the-base-model-with-the-HelpSteer2-dataset-using-NeMo-Aligner-a-toolkit-for-efficient-model-alignment.png 695w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-2.-The-Nemotron-3-340B-reward-model-was-created-by-aligning-the-base-model-with-the-HelpSteer2-dataset-using-NeMo-Aligner-a-toolkit-for-efficient-model-alignment-300x67.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-2.-The-Nemotron-3-340B-reward-model-was-created-by-aligning-the-base-model-with-the-HelpSteer2-dataset-using-NeMo-Aligner-a-toolkit-for-efficient-model-alignment-625x139.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-2.-The-Nemotron-3-340B-reward-model-was-created-by-aligning-the-base-model-with-the-HelpSteer2-dataset-using-NeMo-Aligner-a-toolkit-for-efficient-model-alignment-179x40.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-2.-The-Nemotron-3-340B-reward-model-was-created-by-aligning-the-base-model-with-the-HelpSteer2-dataset-using-NeMo-Aligner-a-toolkit-for-efficient-model-alignment-645x144.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-2.-The-Nemotron-3-340B-reward-model-was-created-by-aligning-the-base-model-with-the-HelpSteer2-dataset-using-NeMo-Aligner-a-toolkit-for-efficient-model-alignment-500x112.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-2.-The-Nemotron-3-340B-reward-model-was-created-by-aligning-the-base-model-with-the-HelpSteer2-dataset-using-NeMo-Aligner-a-toolkit-for-efficient-model-alignment-160x36.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-2.-The-Nemotron-3-340B-reward-model-was-created-by-aligning-the-base-model-with-the-HelpSteer2-dataset-using-NeMo-Aligner-a-toolkit-for-efficient-model-alignment-362x81.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-2.-The-Nemotron-3-340B-reward-model-was-created-by-aligning-the-base-model-with-the-HelpSteer2-dataset-using-NeMo-Aligner-a-toolkit-for-efficient-model-alignment-493x110.png 493w\" sizes=\"(max-width: 695px) 100vw, 695px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Nemotron-3-340B-Reward model workflow</em></figcaption></figure></div>\n\n\n<p>The Nemotron-3-340B reward model was created by aligning the base model with the HelpSteer2 dataset using <a href=\"https://github.com/NVIDIA/NeMo-Aligner\">NeMo Aligner</a>, a toolkit for efficient model alignment.</p>\n\n\n\n<p>The Nemotron-4-340B-Reward model was trained on the Nemotron-4-340B-Base model. It has an additional linear layer that converts the final layer representation of the end-of-response token into five scalar values that correspond to a <a href=\"https://arxiv.org/abs/2311.09528\">HelpSteer</a> attribute, referred to as SteerLM reward model training. For more information about the training process, see <a href=\"https://arxiv.org/abs/2406.08673\">HelpSteer2: Open-source dataset for training top-performing reward models.</a></p>\n\n\n\n<p>Unlike binary preference-based methods, the SteerLM reward model training process enables the model to provide more expressive feedback on which responses are considered good and why. Where binary-trained reward models might sometimes conflate a long response with a good response, SteerLM reward model training explicitly teaches the model to disambiguate verbosity as a scored attribute.&nbsp;</p>\n\n\n\n<h2 id=\"synthetic_data_generation_overview\"  class=\"wp-block-heading\">Synthetic data generation overview<a href=\"#synthetic_data_generation_overview\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>SDG refers to the process of creating datasets that can be used for a variety of model customizations, from SFT, PEFT including <a href=\"https://arxiv.org/abs/2106.09685\">Low-Rank Adaptation (LoRA)</a>, and model alignment (using methods like <a href=\"https://arxiv.org/abs/2309.00267\">RLAIF</a>, <a href=\"https://arxiv.org/abs/2305.18290\">DPO</a>, and so on). </p>\n\n\n\n<p>Use cases for SDG are not limited to model alignment but can apply to a wide range of applications, from retrieval and evaluation dataset curation to recommender systems. For this post, we focus on model alignment as the primary use case for the Nemotron-4-340B family of models. </p>\n\n\n\n<p>Alignment training is a rapidly growing subdiscipline in the generative AI domain and can be implemented in several different ways. Out of all the existing methods, we discuss a specific implementation of an SDG pipeline in this post.</p>\n\n\n\n<p>Critically, robust SDG methods go beyond just generating response data. They also include verification and checks to ensure \u200cthat data quality remains high. LLM accuracy is often directly determined by the quality rather than quantity of the training data, making the step of quality filtering crucial in SDG recipes.</p>\n\n\n\n<h2 id=\"a_synthetic_data_generation_flow\"  class=\"wp-block-heading\">A synthetic data generation flow<a href=\"#a_synthetic_data_generation_flow\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"466\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-1024x466.png\" alt=\"A flow diagram shows the pipeline from the developer's domain-specific input query, the Nemotron-4-340B-Instruct model, synthetic response data, the Nemotron-4-340B-Reward model, response scores, a filter, a synthetic dataset, and NeMo Aligner.\" class=\"wp-image-89946\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-1024x466.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-300x137.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-625x285.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-179x82.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-768x350.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-1536x700.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-2048x933.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-645x294.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-500x228.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-160x73.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-362x165.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SDG-Pipeline1-242x110.png 242w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. SDG pipeline </em></figcaption></figure></div>\n\n\n<p>Figure 3 shows two steps at a high level: </p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Generating synthetic responses using the Nemotron-4-340B-Instruct model.</li>\n\n\n\n<li>Ranking the synthetic responses using the Nemotron-4-340B-Reward model and filtering the synthetic responses to retain only high-quality samples.</li>\n</ul>\n\n\n\n<h3 id=\"synthetic_response_generation\"  class=\"wp-block-heading\">Synthetic response generation<a href=\"#synthetic_response_generation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Synthetic response data can be generated by giving Nemotron-4-340B-Instruct domain-specific input queries. This enables the model to generate responses that are aligned with the input query in a format similar to those used in the <a href=\"https://arxiv.org/abs/2304.03277\" target=\"_blank\" rel=\"noreferrer noopener\">Instruction Tuning with GPT-4</a>&nbsp;paper. These responses can be generated with a zero-shot, few-shot, or chain-of-thought style prompt, depending on the desired response format. Multiple responses to each query can be generated for filtering in the next step as well, if required.</p>\n\n\n\n<p>The Nemotron-4-40B-Instruct model can also be used to generate domain-specific queries initially, alleviating the need for a dataset of preestablished queries. However, this use case is not covered in the tutorial material.&nbsp;</p>\n\n\n\n<h3 id=\"reward_model_verification\"  class=\"wp-block-heading\">Reward model verification<a href=\"#reward_model_verification\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Due to the multi-attribute nature of Nemotron-4-340B-Reward, synthetic responses can be ranked by the most desired HelpSteer2 attributes so that only the highest-performing responses are kept. This emulates the process of human evaluation of prompt quality and adds a layer of quality monitoring in SDG pipelines.</p>\n\n\n\n<h2 id=\"case_study\"  class=\"wp-block-heading\">Case study<a href=\"#case_study\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA researchers demonstrated the effectiveness of SDG in the <a href=\"https://arxiv.org/abs/2406.08673\" target=\"_blank\" rel=\"noreferrer noopener\">HelpSteer2</a> paper. A total of <a href=\"https://huggingface.co/datasets/nvidia/Daring-Anteater\" target=\"_blank\" rel=\"noreferrer noopener\">100K rows of conversational synthetic data </a>(&#8220;Daring Anteater&#8221; or &#8220;DA&#8221; in Figure 4) were created through the pipeline. </p>\n\n\n\n<p>Using this dataset,&nbsp; the NVIDIA research team aligned Llama-3-70B (base model) to match or exceed Llama-3-70B-Instruct on several standard benchmarks. This was achieved despite using only 1% of the <a href=\"https://huggingface.co/blog/llama3#whats-new-with-llama-3\" target=\"_blank\" rel=\"noreferrer noopener\">human-annotated data</a> with which the Llama-3-70B-Instruct model was trained.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"683\" height=\"346\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-3.-Results-from-HelpSteer2-Paper-for-Llama-3-70B.png\" alt=\"Results table for Llama 3 70B Synthetic Data Generation Comparison Tests\" class=\"wp-image-84331\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-3.-Results-from-HelpSteer2-Paper-for-Llama-3-70B.png 683w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-3.-Results-from-HelpSteer2-Paper-for-Llama-3-70B-300x152.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-3.-Results-from-HelpSteer2-Paper-for-Llama-3-70B-625x317.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-3.-Results-from-HelpSteer2-Paper-for-Llama-3-70B-179x91.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-3.-Results-from-HelpSteer2-Paper-for-Llama-3-70B-645x327.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-3.-Results-from-HelpSteer2-Paper-for-Llama-3-70B-500x253.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-3.-Results-from-HelpSteer2-Paper-for-Llama-3-70B-160x81.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-3.-Results-from-HelpSteer2-Paper-for-Llama-3-70B-362x183.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/Figure-3.-Results-from-HelpSteer2-Paper-for-Llama-3-70B-217x110.png 217w\" sizes=\"(max-width: 683px) 100vw, 683px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Results from HelpSteer2 paper for Llama-3-70B.&nbsp;</em></figcaption></figure></div>\n\n\n<p>The results showcase the effectiveness of SDG and how using tools like Nemotron-4-340B-Reward and Nemotron-4-340B-Instruct can be used to add value to your data pipelines today.</p>\n\n\n\n<p>There are many SDG pipelines and this is still an active topic of research. Nemotron-4-340B-Instruct was itself trained with a variation of the SDG pipeline similar to the flow in Figure 3, with 98% of its alignment training data being synthetically generated. For more information, see the <a href=\"https://research.nvidia.com/publication/2024-06_nemotron-4-340b\">Nemotron-4-340B</a> report.</p>\n\n\n\n<p>We encourage you to evaluate and develop different pipelines and share best practices, as we continue to refine our own SDG methodologies.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Data serves as the backbone of LLMs. Recognizing SDG as the next frontier of improving generative AI applications for enterprises, NVIDIA offers the Nemotron-4-340B family of models and SDG pipeline to enable developers and enterprises alike to turbocharge a wide range of synthetic data use cases, with a permissive license and several the highest-quality, openly available instruct and reward models.&nbsp;</p>\n\n\n\n<p>Instructions for how to deploy the models are available on their respective model cards, with NeMo Framework instructions available for <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-4-340b-base\" target=\"_blank\" rel=\"noreferrer noopener\">Nemotron-4-340B-Base</a> and <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-4-340b-instruct\" target=\"_blank\" rel=\"noreferrer noopener\">Nemotron-4-340B-Instruct</a>, and NeMo Aligner instructions available for <a href=\"https://huggingface.co/nvidia/Nemotron-4-340B-Reward\" target=\"_blank\" rel=\"noreferrer noopener\">Nemotron-4-340B-Reward</a>.</p>\n\n\n\n<p>A tutorial demonstrating the above SDG pipeline using the <a href=\"https://build.nvidia.com/explore/discover\" target=\"_blank\" rel=\"noreferrer noopener\">build.nvidia.com</a> API is available in the <a href=\"https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/nemotron_340B_synthetic_datagen\" target=\"_blank\" rel=\"noreferrer noopener\">/NVIDIA/NeMo-Curator</a> GitHub repo.</p>\n\n\n\n<p>In a future release, you&#8217;ll be able to use the Nemotron-4-340B <a href=\"https://www.nvidia.com/en-us/ai/\" target=\"_blank\" rel=\"noreferrer noopener\">NVIDIA NIM microservice</a> for optimized inference on NVIDIA GPUs.</p>\n\n\n\n<p>Try out Nemotron-4-340B-Instruct through the <a href=\"https://build.nvidia.com/nvidia/nemotron-4-340b-instruct\">preview inference API</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>This post was updated on August 16, 2024 to reflect the most recent Reward Bench results. Since the introduction and subsequent wide adoption of large language models (LLMs), data has been the lifeblood of businesses building accurate and safe AI systems. A company\u2019s data represents its cumulative knowledge and can be leveraged in various ways, &hellip; <a href=\"https://developer.nvidia.com/blog/leverage-our-latest-open-models-for-synthetic-data-generation-with-nvidia-nemotron-4-340b/\">Continued</a></p>\n", "protected": false}, "author": 2077, "featured_media": 84332, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1432205", "discourse_permalink": "https://forums.developer.nvidia.com/t/leverage-our-latest-open-models-for-synthetic-data-generation-with-nvidia-nemotron-4-340b/296860", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [453, 2932, 1718], "coauthors": [3791, 1693, 3611], "class_list": ["post-84322", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-large-language-models", "tag-synthetic-data"], "acf": {"post_industry": ["General"], "post_products": ["NeMo", "NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/ncg-promo-day-0-nemotron-blog-1920x1080-1.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-lW2", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/84322"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2077"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=84322"}], "version-history": [{"count": 14, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/84322/revisions"}], "predecessor-version": [{"id": 89948, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/84322/revisions/89948"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/84332"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=84322"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=84322"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=84322"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=84322"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87337, "date": "2024-08-15T10:30:00", "date_gmt": "2024-08-15T17:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87337"}, "modified": "2024-09-10T09:25:29", "modified_gmt": "2024-09-10T16:25:29", "slug": "generating-financial-market-scenarios-using-nvidia-nim", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/generating-financial-market-scenarios-using-nvidia-nim/", "title": {"rendered": "Generating Financial Market Scenarios Using NVIDIA NIM"}, "content": {"rendered": "\n<p>While <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI</a> can be used to create clever rhymes, cool images, and soothing voices, a closer look at the techniques behind these impressive content generators reveals probabilistic learners, compression tools, and sequence modelers. When applied to quantitative finance, these methods can help disentangle and learn complex associations in financial markets.&nbsp;</p>\n\n\n\n<p>Market scenarios are crucial for risk management, strategy backtesting, portfolio optimization, and regulatory compliance. These hypothetical data models represent potential future market conditions, helping financial institutions to simulate and assess outcomes and make informed investment decisions.&nbsp;</p>\n\n\n\n<p>Specific methods demonstrate proficiency in various areas, such as:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Data generation with variational autoencoders or denoising diffusion models</li>\n\n\n\n<li>Modeling sequences with intricate dependencies using transformer-based generative models</li>\n\n\n\n<li>Understanding and predicting time-series dynamics with state-space models</li>\n</ul>\n\n\n\n<p>While these methods may operate in distinct ways, they can also be combined to yield powerful results.&nbsp;</p>\n\n\n\n<p>This post explores how variational autoencoders (VAE), denoising diffusion models (DDM), and other generative tools can be integrated with <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/#:~:text=Large%20language%20models%20(LLMs)%20are,content%20using%20very%20large%20datasets.\">large language models (LLM)</a> to efficiently create market scenarios with desired properties. It showcases a scenario generation reference architecture powered by <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a>, a collection of microservices designed to accelerate the deployment of generative models.</p>\n\n\n\n<h2 id=\"one_toolset_many_applications\"  class=\"wp-block-heading\">One toolset, many applications<a href=\"#one_toolset_many_applications\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Generative AI provides a unified framework around a variety of quantitative finance problems that have previously been addressed with distinct approaches. Once a model has been trained to learn the distribution of its input data, it can be used as a foundational model for a variety of tasks.&nbsp;</p>\n\n\n\n<p>For instance, it could generate samples for the creation of simulations or risk scenarios. It could also pinpoint which samples are out of distribution, acting as an outlier detector or stress scenario generator. As market data moves at different frequencies, cross-sectional market snapshots have gaps. A generative model can supply the missing data that fits with the actual data in a plausible way, which is beneficial for nowcasting models or dealing with illiquid points. Finally, autoregressive next-token prediction and state space models can help with forecasting.&nbsp;</p>\n\n\n\n<p>A significant bottleneck for domain experts leveraging such generative models is the lack of platform support that bridges their ideas and intentions with the complex infrastructure needed to deploy these models. While LLMs have gained mainstream use across various industries including finance, they are primarily used for knowledge processing tasks, such as Q&amp;A and summarization, or coding tasks, like generating code stubs for further enhancement by human developers and integration into proprietary libraries.&nbsp;</p>\n\n\n\n<p>Integrating LLMs with complex models can bridge the communication gap between quantitative experts and generative AI models, as explained below.</p>\n\n\n\n<h2 id=\"market_scenario_generation\"  class=\"wp-block-heading\">Market scenario generation<a href=\"#market_scenario_generation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Traditionally, the generation of market scenarios has relied on techniques including expert specifications (&#8220;shift the US yield curve up by <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=50&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"50\" class=\"latex\" /> bp in parallel&#8221;), factor decompositions (&#8220;bump the EUR swap curve by <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=-10&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"-10\" class=\"latex\" /> bp along the first PCA direction&#8221;), and statistical methods such as variance-covariance or bootstrapping. While these techniques help produce new scenarios, they lack a full picture of the underlying data distribution and often require manual adjustment. Generative approaches, which learn data distributions implicitly, elegantly overcome this modeling bottleneck.</p>\n\n\n\n<p>LLMs can be combined with scenario generation models in powerful ways to enable simplified interaction while also acting as natural language user interfaces for market data exploration. For example, a trader might wish to assess her book\u2019s exposure if markets were to behave like they did during a previous event, such as the great financial crisis, the last U.S. election, the Flash Crash, or the dot-com bubble burst. An LLM trained on recorded knowledge of such events could find and extract the characteristics of interest conditional on such events or historical periods and pass them to a generative market model to create similar market conditions for use in downstream applications.&nbsp;</p>\n\n\n\n<p>Figure 1 illustrates reference architecture for market scenario generation, connecting user specifications with suitable generative tools. Sample code for an implementation powered by <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a> is shown in the Sample Implementation section.&nbsp;</p>\n\n\n\n<p>The process starts with a user instruction; for example, requesting a simulation of an interest rate environment similar to the one \u201cat the peak of the financial crisis.\u201d An agent (or collection of agents) processes this request by first routing it to an LLM-powered <em>interpreter</em> that interprets and converts the request in natural language to an intermediate format (in this case, a JSON file).&nbsp;</p>\n\n\n\n<p>The LLM then translates the \u201cpeak of financial crisis\u201d to a concrete historical period (September 15 to October 15, 2008) and maps the market objects of interest (U.S. swap curves and swaption volatility surfaces, for example) to their respective pre-trained generative models (VAE, DDM, and so on). Information about the historical period of interest can be retrieved through a <em>data</em> <em>retriever</em> component and passed on to the corresponding generative tools to generate similar market data.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"362\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim-625x362.png\" alt=\"Reference architecture for market scenario generation, connecting user specifications with suitable generative tools. Starting with a user request to generate market scenarios similar to those of a given historical period, an agent first routes it to an LLM-powered Interpreter that translates the request to a concrete historical period and maps the market objects of interest (e.g., swap curves, volatility surfaces) to corresponding pre-trained generative models. Historical data is retrieved by a data retriever component and passed on to these models which, in turn, generate novel market scenarios.\n\" class=\"wp-image-87348\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim-625x362.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim-300x174.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim-179x104.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim-768x445.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim-1536x890.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim-645x374.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim-500x290.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim-155x90.png 155w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim-362x210.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim-190x110.png 190w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim-1024x593.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/market-scenario-generator-architecture-nvidia-nim.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Market scenario generator reference architecture using NIM microservices</em></figcaption></figure>\n\n\n\n<p>The generation process entails running inference on generative models that have been pre-trained on market data. Figures 2 and 3 illustrate an example of yield curve scenarios generated using a VAE model corresponding to the start of the COVID-19 pandemic.</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"790\" height=\"402\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latent-space-us-yield-curves-covid-19.png\" alt=\"3D latent space projections shown as points that represent historical (circles) and synthetic (triangles) US yield curves at the start of the COVID-19 pandemic (Late Feb to mid April 2020).\" class=\"wp-image-87354\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latent-space-us-yield-curves-covid-19.png 790w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latent-space-us-yield-curves-covid-19-300x153.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latent-space-us-yield-curves-covid-19-625x318.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latent-space-us-yield-curves-covid-19-179x91.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latent-space-us-yield-curves-covid-19-768x391.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latent-space-us-yield-curves-covid-19-645x328.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latent-space-us-yield-curves-covid-19-500x254.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latent-space-us-yield-curves-covid-19-160x81.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latent-space-us-yield-curves-covid-19-362x184.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/latent-space-us-yield-curves-covid-19-216x110.png 216w\" sizes=\"(max-width: 790px) 100vw, 790px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Latent space projections of historical U.S. yield curves at the start of the COVID-19 pandemic, late February to mid-April 2020 (left), and scenarios added by sampling in the latent space in the neighborhoods of realized yield curves (right)</em></figcaption></figure></div>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"355\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/historical-synthetically-generated-yield-curves-covid-19-625x355.png\" alt=\"Historical and synthetically generated yield curve scenarios for the start of COVID-19. Historical curves are shown in solid lines. Newly-generated curves are shown with dashed lines. One can observe that the newly-generated curves respect the overall shape of the input data, without directly copying it.\n\" class=\"wp-image-87357\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/historical-synthetically-generated-yield-curves-covid-19-625x355.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/historical-synthetically-generated-yield-curves-covid-19-300x170.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/historical-synthetically-generated-yield-curves-covid-19-179x102.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/historical-synthetically-generated-yield-curves-covid-19-768x436.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/historical-synthetically-generated-yield-curves-covid-19-645x366.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/historical-synthetically-generated-yield-curves-covid-19-500x284.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/historical-synthetically-generated-yield-curves-covid-19-158x90.png 158w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/historical-synthetically-generated-yield-curves-covid-19-362x206.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/historical-synthetically-generated-yield-curves-covid-19-194x110.png 194w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/historical-synthetically-generated-yield-curves-covid-19.png 1000w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Historical and synthetically generated yield curve scenarios for the start of the COVID-19 pandemic</em></figcaption></figure>\n\n\n\n<p>Table 1 exemplifies additional market scenario requests along with corresponding JSON outputs. In this case, the user query indicates the model to be used for illustration purposes. In a production environment, a routing agent could automatically select the right model.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td><strong>Query</strong></td><td><strong>Answer</strong></td><td><strong>Model &amp; Comments</strong></td></tr><tr><td>&#8220;Using a VAE model, output yield curves similar to those in the second half of 2020.&#8221;</td><td>{&#8220;scenarios&#8221;: [<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;method&#8221;: &#8220;VAE&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;object_type&#8221;: &#8220;yield curve&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;period&#8221;: {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;start_date&#8221;: &#8220;2020-07-01&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;end_date&#8221;: &#8220;2020-12-31&#8221;}<br>&nbsp;&nbsp;&nbsp;&nbsp;}<br>]}</td><td>llama3-70b-instruct<br><br>Correctly finds the second half of 2020.<br></td></tr><tr><td>&#8220;Generate yield curves similar to those in the year post-COVID.&#8221;</td><td>{&#8220;scenarios&#8221;: [<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;method&#8221;: &#8220;VAE&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;object_type&#8221;: &#8220;yield curve&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;period&#8221;: {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;start_date&#8221;: &#8220;2020-03-01&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;end_date&#8221;: &#8220;2022-12-31&#8221;}<br>&nbsp;&nbsp;&nbsp;&nbsp;}<br>]}</td><td>llama3-70b-instruct<br><br>Correctly selects \u2018VAE\u2019 as the default method and identifies the post-COVID period.<br></td></tr><tr><td>&#8220;Using DDPM, generate implied vol surfaces similar to Q1 in 2024.&#8221;</td><td>{&#8220;scenarios&#8221;: [<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;method&#8221;: &#8220;DDPM&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;object_type&#8221;: &#8220;implied vol surfaces&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;period&#8221;: {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;start_date&#8221;: &#8220;2024-01-01&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;end_date&#8221;: &#8220;2024-03-31&#8221;}<br>&nbsp;&nbsp;&nbsp;&nbsp;}<br>]}</td><td>mixtral-8x22b-instruct-v0.1<br><br>Correctly dates Q1 of 2024.<br></td></tr><tr><td>\u201cGenerate yield curves similar to the two-week period surrounding the Flash Crash.\u201d</td><td>{&#8220;scenarios&#8221;: [<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;method&#8221;: &#8220;VAE&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;object_type&#8221;: &#8220;yield curve&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;period&#8221;: {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;start_date&#8221;: &#8220;2010-04-30&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;end_date&#8221;: &#8220;2010-05-13&#8221;}<br>&nbsp;&nbsp;&nbsp;&nbsp;}<br>]}</td><td>llama3-70b-instruct<br><br>Correctly dates the Flash Crash on May 6, 2010, and a 2-week period around it.<br></td></tr><tr><td>\u201cSimulate inflation curves similar to those of last year.\u201d</td><td>{&#8220;scenarios&#8221;: [<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;method&#8221;: &#8220;VAE&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;object_type&#8221;: &#8220;inflation curve&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;period&#8221;: {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;start_date&#8221;: &#8220;2022-01-01&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;end_date&#8221;: &#8220;2022-12-31&#8221;}<br>&nbsp;&nbsp;&nbsp;&nbsp;}<br>]}</td><td>llama3-70b-instruct<br><br>Caution: What is the model\u2019s knowledge cut-off date? Find out from the <a href=\"https://build.nvidia.com/meta/llama3-70b/modelcard\">model card</a> in the NVIDIA catalog. In this case, it is March 2023, so \u201clast year\u201d for this model was 2022.</td></tr><tr><td>\u201cSimulate yield curves and swaption vol surfaces at the peak of the financial crisis.\u201d</td><td>{&#8220;scenarios&#8221;: [<br>{<br>&nbsp;&nbsp;&#8220;method&#8221;: &#8220;VAE&#8221;,<br>&nbsp;&nbsp;&#8220;object_type&#8221;: &#8220;yield curve&#8221;,<br>&nbsp;&nbsp;&#8220;period&#8221;: {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;start_date&#8221;: &#8220;2008-09-15&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;end_date&#8221;: &#8220;2008-10-15&#8221;}<br>},<br>{<br>&#8220;method&#8221;: &#8220;VAE&#8221;,<br>&#8220;object_type&#8221;: &#8220;swaption volatility surface&#8221;,<br>&#8220;period&#8221;: {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;start_date&#8221;: &#8220;2008-09-15&#8221;,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#8220;end_date&#8221;: &#8220;2008-10-15&#8221;<br>}}]}</td><td>meta/llama3-70b-instruct<br><br>Correctly identifies two types of market objects, as well as the peak of the financial crisis (Lehman Brothers collapsed on September 15, 2008.)<br></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Scenario generation Q&amp;A examples obtained with NVIDIA NIM</em></figcaption></figure>\n\n\n\n<h2 id=\"market_structure_analysis_using_generative_models\"  class=\"wp-block-heading\">Market structure analysis using generative models<a href=\"#market_structure_analysis_using_generative_models\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Financial markets are inherently complex, characterized by noisy, high-dimensional data typically observed as multivariate time series. Detecting fleeting patterns that lead to financial gains at scale requires clever modeling and substantial computations.&nbsp;</p>\n\n\n\n<p>One way to reduce dimensionality is to consider the intrinsic structure present in the data: curves, surfaces, and higher-dimensional structures. These structures carry information that can be leveraged to reduce complexity. They can also be viewed as units of information to be embedded and analyzed through latent spaces of generative models. In this section we review examples of how VAEs and DDMs can be used in this context.</p>\n\n\n\n<h3 id=\"vaes_for_learning_the_distribution_of_market_curves\"  class=\"wp-block-heading\">VAEs for learning the distribution of market curves<a href=\"#vaes_for_learning_the_distribution_of_market_curves\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Bond yields, swap, inflation, and foreign exchange rates can be thought of as having one-dimensional term-structures, such as zero-coupon, spot, forward, or basis curves. Similarly, option volatilities can be seen as (hyper-)surfaces in 2D or higher dimensional spaces. A typical swap curve could have as many as 50 tenors. Instead of studying how the 50 corresponding time series relate to each other, one can consider a single time series of swap curves and use a VAE to learn the distribution of these objects, as detailed in <a href=\"https://ieeexplore.ieee.org/abstract/document/10095542\">Multiresolution Signal Processing of Financial Market Objects</a>.&nbsp;</p>\n\n\n\n<p>The strength of this approach lies in its ability to integrate previously isolated data: the behavior of market objects, traditionally modeled in isolation either by currency (USD, EUR, BRL, for example) or by task (scenario generation, nowcasting, outlier detection, for example), can now be integrated into the training of a single generative model reflecting the interconnected nature of markets.&nbsp;</p>\n\n\n\n<p>Figure 4 illustrates the training loop of a VAE on market data objects such as yield curves: an encoder compresses the input objects into a latent space with a Gaussian distribution, a decoder reconstructs curves from points in this space. The latent space is continuous, explicit, and usually lower-dimensional, making it intuitive to navigate. Curves representing different currencies or market regimes cluster in distinct areas, allowing the model to navigate within or between these clusters to generate new curves unconditional or conditional on desired market characteristics.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"213\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/vae-training-loop-625x213.png\" alt=\"Diagram showing the high-level architecture of a VAE training on market data: an encoder compresses input objects such as yield curves into a latent space with a Gaussian distribution; a decoder reconstructs curves from points in this space. The latent space of a VAE is continuous, explicit, typically of lower dimensionality, and intuitive to navigate.\n\" class=\"wp-image-87379\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/vae-training-loop-625x213.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/vae-training-loop-300x102.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/vae-training-loop-179x61.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/vae-training-loop-768x262.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/vae-training-loop-645x220.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/vae-training-loop-500x171.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/vae-training-loop-160x55.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/vae-training-loop-362x124.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/vae-training-loop-322x110.png 322w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/vae-training-loop-1024x350.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/vae-training-loop.png 1306w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. VAE training loop on market data such as yield curves</em></figcaption></figure>\n\n\n\n<p>In particular, the generation process can be conditioned on specific historical periods to generate curves that may have similar shapes to historical ones, without being exact replicas. For example, the U.S. Treasury yield curves corresponding to the start of the COVID-19 pandemic are shown in Figure 2 (left) as circles in the 3D latent space of a VAE that has been trained on yield curve data. Since VAE latent spaces are continuous, they naturally render themselves to defining neighborhoods around points of interest and to sampling from those neighborhoods to generate novel yield curve scenarios that are similar (but not the same) as historical ones, shown as triangles in Figure 2 (right).&nbsp;</p>\n\n\n\n<p>Figure 5 shows a more complete picture of the yield curves used for training the VAE model, along with clusters grouped by the level of rates, as well as an outlier corresponding to the mistaken inclusion of a data point when the Treasury markets were closed (Good Friday, 2017). One can easily imagine additional applications beyond scenario generation, including non-linear factor decompositions, outlier detection, and many more.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"480\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3d-latent-space-vae-curves-data-625x480.png\" alt=\"In the 3D latent space of a VAE trained on US Treasury curves data (1993-2019), the financial crisis forms a distinct cluster, highlighting a significant deviation from pre-crisis conditions and transitioning into a unique post-crisis low-rates environment.\n\" class=\"wp-image-87380\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3d-latent-space-vae-curves-data-625x480.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3d-latent-space-vae-curves-data-300x230.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3d-latent-space-vae-curves-data-150x115.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3d-latent-space-vae-curves-data-768x590.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3d-latent-space-vae-curves-data-645x495.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3d-latent-space-vae-curves-data-391x300.png 391w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3d-latent-space-vae-curves-data-117x90.png 117w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3d-latent-space-vae-curves-data-362x278.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3d-latent-space-vae-curves-data-143x110.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3d-latent-space-vae-curves-data-1024x786.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/3d-latent-space-vae-curves-data.png 1222w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. In the 3D latent space of a VAE trained on U.S. Treasury curves data (1993-2019), the financial crisis forms a distinct cluster, highlighting a significant deviation from pre-crisis conditions and transitioning into a unique post-crisis low-rates environment</em></figcaption></figure>\n\n\n\n<h3 id=\"ddms_for_learning_volatility_surfaces\"  class=\"wp-block-heading\">DDMs for learning volatility surfaces<a href=\"#ddms_for_learning_volatility_surfaces\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>DDMs approach the generative process through the prism of reversible diffusion. As shown in Figure 6, they operate by gradually introducing noise into the data until it becomes a standard Gaussian. Then they learn to reverse this process to generate new data samples starting from pure noise. Noise is added gradually in the forward pass, until the resulting object is indistinguishable from Gaussian noise. During the backward pass, the model learns the denoising needed to reconstruct the original surface. To learn more, see <a href=\"https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models/\">Generative AI Research Spotlight: Demystifying Diffusion-Based Models</a>.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"222\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/architecture-ddm-distribution-2d-market-objects-625x222.png\" alt=\"A diagram showing the high-level architecture of a DDM trained to learn the distribution of 2D market objects such as volatility surfaces. It consists of a forward pass of gradual noise injection and a backward pass of learning the denoising process.\n\" class=\"wp-image-87381\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/architecture-ddm-distribution-2d-market-objects-625x222.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/architecture-ddm-distribution-2d-market-objects-300x107.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/architecture-ddm-distribution-2d-market-objects-179x64.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/architecture-ddm-distribution-2d-market-objects-768x273.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/architecture-ddm-distribution-2d-market-objects-645x229.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/architecture-ddm-distribution-2d-market-objects-500x178.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/architecture-ddm-distribution-2d-market-objects-160x57.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/architecture-ddm-distribution-2d-market-objects-362x129.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/architecture-ddm-distribution-2d-market-objects-309x110.png 309w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/architecture-ddm-distribution-2d-market-objects-1024x364.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/architecture-ddm-distribution-2d-market-objects.png 1310w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. High-level architecture of a DDM trained to learn the distribution of 2D market objects such as volatility surfaces</em></figcaption></figure>\n\n\n\n<p>To learn the distribution of implied volatility surfaces, the ability of a DDM (specifically a DDPM, or denoising diffusion probabilistic model) is explored. This example uses a synthetic data set of roughly 20,000 volatility surfaces generated with the SABR (stochastic alpha-beta-rho) stochastic volatility model described in <a href=\"https://www.next-finance.net/IMG/pdf/pdf_SABR.pdf\">Managing Smile Risk</a> (subject to initial conditions <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=F_0&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"F_0\" class=\"latex\" /> and <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Calpha_0&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;alpha_0\" class=\"latex\" />):</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=dF_t+%3D+%5Calpha_t+%28F_t%29%5E%7B%5Cbeta%7D+dW_t&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"dF_t = &#92;alpha_t (F_t)^{&#92;beta} dW_t\" class=\"latex\" /><br><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=d%5Calpha_t+%3D+%5Cnu+%5Calpha_t+dZ_t&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"d&#92;alpha_t = &#92;nu &#92;alpha_t dZ_t\" class=\"latex\" /><br><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Clangle+dW_t%2C+dZ_t+%5Crangle+%3D+%5Crho+dt&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;langle dW_t, dZ_t &#92;rangle = &#92;rho dt\" class=\"latex\" /></p>\n\n\n\n<p>An example input surface is shown in Figure 7.</p>\n\n\n\n<p>The goal is to evaluate the ability of such a model to recover the input data distribution, in this case, a SABR distribution. In the case of empirical surfaces implied directly from market prices of options, the distribution would be unknown and having a tool that can capture the distribution in a non-parametric way would offer a valuable alternative to sparse parametric models that don\u2019t have sufficient degrees of freedom to represent the data. It can subsequently be used to generate volatility surface scenarios, or to fill in missing regions in plausible ways.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"477\" height=\"392\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/input-sabr-volatility-surface-model-training-2.png\" alt=\"Example of an input SABR volatility surface used for model training. The surface is shown as a 3D plot with axes: log-moneyness, time to maturity in years, and implied log-volatility.\n\" class=\"wp-image-87501\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/input-sabr-volatility-surface-model-training-2.png 477w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/input-sabr-volatility-surface-model-training-2-300x247.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/input-sabr-volatility-surface-model-training-2-140x115.png 140w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/input-sabr-volatility-surface-model-training-2-365x300.png 365w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/input-sabr-volatility-surface-model-training-2-110x90.png 110w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/input-sabr-volatility-surface-model-training-2-362x297.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/input-sabr-volatility-surface-model-training-2-134x110.png 134w\" sizes=\"(max-width: 477px) 100vw, 477px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. Example of an input SABR surface used for model training</em></figcaption></figure>\n\n\n\n<p>A <a href=\"https://github.com/NVlabs/edm\">simplified version of this architecture</a> was adapted for this work. The inputs are <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=16+%5Ctimes+16&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"16 &#92;times 16\" class=\"latex\" /> grids of implied volatilities corresponding to various option moneyness and time to maturity pairs. Other types of inputs, such as volatility cubes (moneyness-maturity-underlying) or other variations could be tackled in similar fashion.</p>\n\n\n\n<p>Figure 8 illustrates a few steps in the training of the DDPM model on volatility surface inputs.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"261\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ddpm-model-training-steps-625x261.png\" alt=\"Screenshot showing progressions of 16x16 images during the first few epochs of training a DDPM model. Beginning with pure noise, the model progressively learns the noise process needed to denoise the images and generate SABR-like surfaces.\n\" class=\"wp-image-87429\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ddpm-model-training-steps-625x261.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ddpm-model-training-steps-300x125.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ddpm-model-training-steps-179x75.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ddpm-model-training-steps-768x320.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ddpm-model-training-steps-645x269.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ddpm-model-training-steps-500x208.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ddpm-model-training-steps-160x67.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ddpm-model-training-steps-362x151.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ddpm-model-training-steps-264x110.png 264w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ddpm-model-training-steps-1024x427.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ddpm-model-training-steps.png 1214w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 8. A few training steps: beginning with pure noise, the model progressively learns the noise process</em></figcaption></figure>\n\n\n\n<p>Figure 9 shows two synthetically-generated surfaces which have subsequently been fitted with SABR models to verify that their shapes are SABR-like. Green points represent SABR surfaces fitted to the generated ones, verifying that the DDPM model has learned the SABR-like shapes.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"726\" height=\"457\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/synthetically-generated-surfaces-sabr-models.png\" alt=\"Two synthetically-generated surfaces that have subsequently been fitted with SABR models to verify that their shapes are SABR-like. Green points represent SABR surfaces fitted to the generated ones, verifying that the DDPM model has learned the SABR-like shapes.\n\" class=\"wp-image-87433\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/synthetically-generated-surfaces-sabr-models.png 726w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/synthetically-generated-surfaces-sabr-models-300x189.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/synthetically-generated-surfaces-sabr-models-625x393.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/synthetically-generated-surfaces-sabr-models-179x113.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/synthetically-generated-surfaces-sabr-models-645x406.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/synthetically-generated-surfaces-sabr-models-477x300.png 477w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/synthetically-generated-surfaces-sabr-models-143x90.png 143w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/synthetically-generated-surfaces-sabr-models-362x228.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/synthetically-generated-surfaces-sabr-models-175x110.png 175w\" sizes=\"(max-width: 726px) 100vw, 726px\" /><figcaption class=\"wp-element-caption\"><em>Figure 9. Transition from noise to a generated sample, showing an intermediate noisy surface (top) and newly generated volatility surfaces (in purple, bottom)</em></figcaption></figure>\n\n\n\n<h2 id=\"sample_implementation\"  class=\"wp-block-heading\">Sample implementation<a href=\"#sample_implementation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This section presents an example of using NVIDIA-hosted NIM endpoints including <a href=\"https://build.nvidia.com/meta/llama-3_1-70b-instruct\">Llama 3.1 70B Instruct</a> LLM to build the <code>LLMQueryInterpreter</code> component of the reference architecture in Figure 1. Note that many other NVIDIA as well as open-source LLMs are available with NIM, including Nemotron, Mixtral, Gemma, and many more. Accessing these through NIM guarantees that they are optimized for inference on NVIDIA-accelerated infrastructure and offers a quick and easy way to compare responses from multiple models.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nimport os\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\n# NVIDIA API configuration\nNVIDIA_API_KEY = os.environ.get(&quot;NVIDIA_API_KEY&quot;) # click \u201cGet API Key\u201d at https://www.nvidia.com/en-us/ai/\nif not NVIDIA_API_KEY:\n    raise ValueError(&quot;NVIDIA_API_KEY environment variable is not set&quot;)\n\nclass LLMQueryInterpreter:\n    &quot;&quot;&quot; NVIDIA NIM-powered class that processes scenario requests from user.&quot;&quot;&quot;\n    \n    def __init__(self, llm_name=&quot;meta/llama-3.1-70b-instruct&quot;, temperature=0):\n        self._llm = ChatNVIDIA(model=llm_name, temperature=temperature) \n    \n        # define output JSON format\n        self._scenario_template = &quot;&quot;&quot; \n        {&quot;scenarios&quot;: &#x5B; \n            { \n                &quot;method&quot;: &lt;generative method name, e.g., &quot;VAE&quot;&gt;,\n                &quot;object_type&quot;: &lt;object type, e.g., &quot;yield curve&quot;&gt;,\n                &quot;period&quot;: {\n                    &quot;start_date&quot;: &lt;start date of event&gt;,\n                    &quot;end_date&quot;  : &lt;end date of event&gt;\n                }\n            }, \n            { \n                &quot;method&quot;: &lt;generative method name, e.g., &quot;DDPM&quot;&gt;,\n                &quot;object_type&quot;: &lt;object type, e.g., &quot;volatility surface&quot;&gt;,\n                &quot;period&quot;: {\n                    &quot;start_date&quot;: &lt;start date of event&gt;,\n                    &quot;end_date&quot;  : &lt;end date of event&gt;\n                }\n            }, \n\n            // Additional scenarios, as needed \n        ] } \n        &quot;&quot;&quot;\n\n        # instructions for the LLM\n        self._prompt_template = &quot;&quot;&quot;&quot; \n        Your task is to generate a single, high-quality response in JSON format. \\\n        Format the output according to the following template: {scenario_template}. \\\n        Offer a refined, accurate, and comprehensive reply to the instruction. \\\n\n        Every query has a structure of this form: \\\n        &quot;Use &lt;method&gt; to generate &lt;market_object_type&gt; similar to &lt;period&gt;&quot;. \\\n        Valid methods are &quot;VAE&quot; or &quot;DDPM&quot;; use &quot;VAE&quot; by default, if no method is specified.\\\n        If a different method is requested, return &quot;invalid&quot; as the method type. \\ \n\n        Answer the following query without any additional explanations, return only a JSON output:\n        {query}  \n\n        &quot;&quot;&quot;\n\n    def process_query(self, query):\n        # query = &quot;Using a VAE model, output yield curves similar to those in the second half of 2020&quot;\n\n        llm_request = self._prompt_template.format(**{ &quot;query&quot;: query, &quot;scenario_template&quot;: self._scenario_template }) \n        llm_answer = self._llm.invoke(llm_request) \n        \n        return llm_answer.content\n\n# Example usage\nquery = &quot;Using a VAE model, output yield curves similar to the Flash Crash&quot;\nllm = LLMQueryInterpreter()\nprint(llm.process_query(query))\n\n# Output\n{\n&quot;scenarios&quot;: &#x5B;\n    {\n        &quot;method&quot;: &quot;VAE&quot;,\n        &quot;object_type&quot;: &quot;yield curve&quot;,\n        &quot;period&quot;: {\n            &quot;start_date&quot;: &quot;2010-05-06&quot;,\n            &quot;end_date&quot;: &quot;2010-05-06&quot;\n        }\n    }\n]\n}\n</pre></div>\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>It\u2019s exciting to imagine a future where quants, traders, and investment professionals increasingly collaborate with AI tools to model and explore financial markets. The integration of these advanced tools enhances financial modeling and market exploration, promising to drive forward the capabilities and insights of market participants. They can be combined in innovative ways and served with ease using <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>While generative AI can be used to create clever rhymes, cool images, and soothing voices, a closer look at the techniques behind these impressive content generators reveals probabilistic learners, compression tools, and sequence modelers. When applied to quantitative finance, these methods can help disentangle and learn complex associations in financial markets.&nbsp; Market scenarios are crucial &hellip; <a href=\"https://developer.nvidia.com/blog/generating-financial-market-scenarios-using-nvidia-nim/\">Continued</a></p>\n", "protected": false}, "author": 2015, "featured_media": 87343, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1468676", "discourse_permalink": "https://forums.developer.nvidia.com/t/generating-financial-market-scenarios-using-nvidia-nim/303520", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [453, 2932, 3739], "coauthors": [3717], "class_list": ["post-87337", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-featured", "tag-large-language-models", "tag-nim"], "acf": {"post_industry": ["Financial Services"], "post_products": ["NIM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/dubai-cityscape.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mIF", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87337"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2015"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87337"}], "version-history": [{"count": 79, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87337/revisions"}], "predecessor-version": [{"id": 88915, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87337/revisions/88915"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87343"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87337"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87337"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87337"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87337"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87227, "date": "2024-08-15T10:11:37", "date_gmt": "2024-08-15T17:11:37", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87227"}, "modified": "2024-08-22T11:24:54", "modified_gmt": "2024-08-22T18:24:54", "slug": "nvidia-tensorrt-model-optimizer-v0-15-boosts-inference-performance-and-expands-model-support", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-tensorrt-model-optimizer-v0-15-boosts-inference-performance-and-expands-model-support/", "title": {"rendered": "NVIDIA TensorRT Model Optimizer v0.15 Boosts Inference Performance and Expands Model Support"}, "content": {"rendered": "\n<p>NVIDIA has announced the latest v0.15 release of <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer\">NVIDIA TensorRT Model Optimizer</a>, a state-of-the-art quantization toolkit of model optimization techniques including quantization, sparsity, and pruning. These techniques reduce model complexity and enable downstream inference frameworks like <a href=\"https://github.com/NVIDIA/TensorRT-LLM/tree/rel\">NVIDIA TensorRT-LLM</a> and <a href=\"https://github.com/NVIDIA/TensorRT\">NVIDIA TensorRT</a> to more efficiently optimize the inference speed of generative AI models.</p>\n\n\n\n<p>This post outlines some of the key features and upgrades of recent TensorRT Model Optimizer releases, including cache diffusion, the new quantization-aware training workflow using <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo</a>, and QLoRA support.</p>\n\n\n\n<h2 id=\"cache_diffusion\"  class=\"wp-block-heading\">Cache diffusion<a href=\"#cache_diffusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Previously, TensorRT Model Optimizer (referred to as Model Optimizer) supercharged NVIDIA TensorRT to <a href=\"https://developer.nvidia.com/blog/accelerate-generative-ai-inference-performance-with-nvidia-tensorrt-model-optimizer-now-publicly-available/\">set the bar for Stable Diffusion XL performance</a> with its 8-bit post-training quantization (PTQ) technique. To further democratize fast inference for diffusion models, Model Optimizer v0.15 adds support for cache diffusion, which can be used with FP8 or INT8 PTQ to further accelerate diffusion models at inference time.</p>\n\n\n\n<p>Cache diffusion methods, such as <a href=\"https://arxiv.org/abs/2312.00858\">DeepCache</a> and <a href=\"https://arxiv.org/abs/2312.03209\">block caching</a>, optimize inference speed without the need for additional training by reusing cached outputs from previous denoising steps. The caching mechanism leverages the intrinsic characteristics of the reverse denoising process of diffusion models where high-level features between consecutive steps have a significant temporal consistency and can be cached and reused. Cache diffusion is compatible with a variety of backbone models like <a href=\"https://arxiv.org/abs/2212.09748\">DiT</a> and UNet, enabling considerable inference acceleration without compromising quality or training cost.</p>\n\n\n\n<p>To enable cache diffusion, developers only need to use a single &#8216;cachify&#8217; instance in Model Optimizer with the diffusion pipeline. For a detailed example, see the <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer/blob/main/diffusers/cache_diffusion/example.ipynb\">cache diffusion tutorial notebook</a>. For an FP16 <a href=\"https://huggingface.co/papers/2307.01952\">Stable Diffusion XL</a> (SDXL) on an NVIDIA H100 Tensor Core GPU, enabling cache diffusion in Model Optimizer delivers a 1.67x speedup in images per second (Figure 1). This speedup increases when FP8 is also enabled. Additionally, Model Optimizer enables users to customize the cache configuration for even faster inference. More diffusion models will be supported in the cache diffusion pipeline using TensorRT runtime in the near future.</p>\n\n\n\n<center><figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"636\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-625x636.png\" alt=\"The impact of image per second (higher is better) of a Stable Diffusion XL when cache diffusion in TensorRT Model Optimizer is enabled on H100 80GB HBM3. Step size is 30, batch size is 16; TensorRT v10.2.0; TensorRT Model Optimizer v0.15.\" class=\"wp-image-87839\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-625x636.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-295x300.png 295w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-113x115.png 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-768x782.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-645x657.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-88x90.png 88w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-362x369.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-108x110.png 108w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26-1024x1043.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-26.png 1043w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. The impact of images per second (higher is better) of a Stable Diffusion XL when cache diffusion in TensorRT Model Optimizer is enabled</em></em></figcaption></figure></center>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\"><em>The FP16 without caching baseline is benchmarked using the Model Optimizer cache diffusion pipeline with caching disabled, rather than using the demoDiffusion pipeline in TensorRT that has batch size limitation, to provide a fairer comparison. NVIDIA H100 80 GB HBM3 GPU; step size 30; batch size 16; TensorRT v10.2.0; TensorRT Model Optimizer v0.15</em></p>\n\n\n\n<h2 id=\"quantization-aware_training_with_nvidia_nemo\"  class=\"wp-block-heading\">Quantization-aware training with NVIDIA NeMo<a href=\"#quantization-aware_training_with_nvidia_nemo\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Quantization-aware training (QAT) is a technique to train neural networks while simulating the effects of quantization, aiming to recover model accuracy post-quantization. This process involves computing scaling factors during training and incorporating simulated quantization loss into the fine-tuning process, making the neural network more resilient to quantization. In Model Optimizer, QAT uses custom CUDA kernels for simulated quantization, achieving lower precision model weights and activations for efficient hardware deployment.</p>\n\n\n\n<p>A model quantized using the Model Optimizer <a href=\"https://nvidia.github.io/TensorRT-Model-Optimizer/guides/_pytorch_quantization.html#quantization-aware-training-qat\"><code>mtq.quantize()</code> API</a> can be directly fine-tuned with the original training pipeline. During QAT, the scaling factors inside quantizers are frozen and the model weights are fine-tuned. The QAT process needs a shorter fine-tuning duration and it is recommended to use small learning rates.&nbsp;</p>\n\n\n\n<p>Model Optimizer v0.15 expands QAT integration support from Hugging Face Trainer and <a href=\"https://github.com/NVIDIA/Megatron-LM\">Megatron</a> to <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo</a>, an enterprise-grade platform for developing custom generative AI models. Model Optimizer now has first-class support for NeMo models. To learn how to perform QAT with your existing NeMo training pipeline, see the new <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/quantization.html#quantization-aware-training-qat\">QAT example</a> in the NeMo GitHub repo. <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/llm_qat#quantization-aware-training-qat\">Learn more about QAT</a>.</p>\n\n\n\n<h2 id=\"qlora_workflow\"  class=\"wp-block-heading\">QLoRA workflow<a href=\"#qlora_workflow\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://arxiv.org/abs/2305.14314\">Quantized Low-Rank Adaptation (QLoRA)</a> is an efficient fine-tuning technique to reduce memory usage and computational complexity during model training. By combining quantization with Low-Rank Adaptation (LoRA), QLoRA makes LLM fine-tuning more accessible for developers with limited hardware resources.</p>\n\n\n\n<p>Model Optimizer has added support for the QLoRA workflow with <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nemo/\">NVIDIA NeMo</a> using the NF4 data type. For details about the workflow, refer to the <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/sft_peft/qlora.html\">NeMo documentation</a>. For a Llama 13B model on the Alpaca dataset, QLoRA can reduce the peak memory usage by 29-51%, depending on the batch size, while maintaining the same model accuracy (Figure 2). Note that QLoRA comes with the trade-offs of longer training step time compared to LoRA (Table 1).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"612\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-1024x612.png\" alt=\"Memory usage in GB for fine-tuning a Llama 13B Alpaca with QLoRA versus LoRA on all layers on H100. Sequence length is 512 and global batch size is 256.\" class=\"wp-image-87526\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-1024x612.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-300x179.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-625x374.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-179x107.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-768x459.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-1536x919.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-2048x1225.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-645x386.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-500x300.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-150x90.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-362x216.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/image-61-184x110.png 184w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 2.<em> Memory usage in GB (lower is better) for fine-tuning Llama 13B on the Alpaca dataset with QLoRA versus LoRA on all layers</em>&nbsp;</em></em></figcaption></figure></div>\n\n\n<p class=\"has-text-align-center has-small-font-size\"><em>NVIDIA H100 GPU; sequence length 512; global batch size 256; NeMo 24.07; TensorRT Model Optimizer v0.13</em></p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\"><br><strong>Batch size</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"3\"><strong>Time per global batch (s)</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>LoRA</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>QLoRA</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>% Increase</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">2</td><td class=\"has-text-align-center\" data-align=\"center\">2.7</td><td class=\"has-text-align-center\" data-align=\"center\">6.7</td><td class=\"has-text-align-center\" data-align=\"center\">148%</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">4</td><td class=\"has-text-align-center\" data-align=\"center\">2.3</td><td class=\"has-text-align-center\" data-align=\"center\">4.4</td><td class=\"has-text-align-center\" data-align=\"center\">91%</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">8</td><td class=\"has-text-align-center\" data-align=\"center\">2.2</td><td class=\"has-text-align-center\" data-align=\"center\">3.2</td><td class=\"has-text-align-center\" data-align=\"center\">46%</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. Training performance in time per global batch (smaller is better) for a Llama 2 13B using QLoRA versus LoRA on all layers </em></figcaption></figure>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\"><em>NVIDIA H100 GPU; sequence length 512; global batch size 256; NeMo 24.07; TensorRT Model Optimizer v0.13</em></p>\n\n\n\n<h2 id=\"expanded_support_for_ai_models\"  class=\"wp-block-heading\">Expanded support for AI models<a href=\"#expanded_support_for_ai_models\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>TensorRT Model Optimizer has expanded support for a wider suite of popular AI models, including Stability.ai <a href=\"https://huggingface.co/stabilityai/stable-diffusion-3-medium\">Stable Diffusion 3</a>,&nbsp;Google <a href=\"https://github.com/google-deepmind/recurrentgemma\">RecurrentGemma</a>, Microsoft Phi-3, Snowflake Arctic 2, and Databricks <a href=\"https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm\">DBRX</a>. See the <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#examples\">example scripts</a> for tutorials and <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer/blob/main/llm_ptq/README.md#model-support-list\">support matrix</a> for more details.</p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA TensorRT Model Optimizer offers seamless integration with NVIDIA TensorRT-LLM and TensorRT for deployment. It is available for installation on PyPI as <a href=\"https://pypi.nvidia.com/nvidia-modelopt/\">nvidia-modelopt</a>. Visit <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer\">NVIDIA/TensorRT-Model-Optimizer</a> on GitHub for example scripts and recipes for inference optimization. For more details, see the <a href=\"https://nvidia.github.io/TensorRT-Model-Optimizer\">Model Optimizer documentation</a>.</p>\n\n\n\n<p>We value your feedback on TensorRT Model Optimizer. If you have suggestions, issues, or feature requests, <a href=\"https://github.com/NVIDIA/TensorRT-Model-Optimizer/issues\">open a new NVIDIA/TensorRT-Model-Optimizer issue</a> on GitHub. Your input helps us iterate our quantization toolkit to better meet your needs.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA has announced the latest v0.15 release of NVIDIA TensorRT Model Optimizer, a state-of-the-art quantization toolkit of model optimization techniques including quantization, sparsity, and pruning. These techniques reduce model complexity and enable downstream inference frameworks like NVIDIA TensorRT-LLM and NVIDIA TensorRT to more efficiently optimize the inference speed of generative AI models. This post outlines &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-model-optimizer-v0-15-boosts-inference-performance-and-expands-model-support/\">Continued</a></p>\n", "protected": false}, "author": 2027, "featured_media": 87230, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1468670", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-tensorrt-model-optimizer-v0-15-boosts-inference-performance-and-expands-model-support/303519", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 3110], "tags": [296, 453, 2932, 1133, 367, 3596], "coauthors": [3727, 3978, 3726, 3979, 3725, 3980], "class_list": ["post-87227", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "tag-ai-inference-microservices", "tag-featured", "tag-large-language-models", "tag-nemo", "tag-tensorrt", "tag-training-ai-models"], "acf": {"post_industry": ["Cloud Services"], "post_products": ["NeMo", "TensorRT"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvidia-tensorrt-model-optimizer-graphic.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mGT", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87227"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2027"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87227"}], "version-history": [{"count": 34, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87227/revisions"}], "predecessor-version": [{"id": 87841, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87227/revisions/87841"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87230"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87227"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87227"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87227"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87227"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87131, "date": "2024-08-15T09:00:00", "date_gmt": "2024-08-15T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87131"}, "modified": "2024-10-03T14:17:06", "modified_gmt": "2024-10-03T21:17:06", "slug": "bringing-confidentiality-to-vector-search-with-cyborg-and-nvidia-cuvs", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/bringing-confidentiality-to-vector-search-with-cyborg-and-nvidia-cuvs/", "title": {"rendered": "Bringing Confidentiality to Vector Search with Cyborg and NVIDIA cuVS"}, "content": {"rendered": "\n<p>In the era of generative AI, vector databases have become indispensable for storing and querying high-dimensional data efficiently. However, like all databases, vector databases are vulnerable to a range of attacks, including cyber threats, phishing attempts, and unauthorized access. This vulnerability is particularly concerning considering that these databases often contain sensitive and confidential information.</p>\n\n\n\n<p>To address this critical issue, Cyborg has teamed up with NVIDIA to enhance the security of vector databases using the <a href=\"https://rapids.ai/cuvs/\">NVIDIA cuVS</a> library, an open-source toolkit that accelerates vector search with state-of-the-art algorithms. This collaboration aims to bring GPU acceleration to Cyborg&#8217;s encrypted vector search engine, ensuring robust security without compromising performance.</p>\n\n\n\n<h2 id=\"vector_database_vulnerabilities\"  class=\"wp-block-heading\">Vector database vulnerabilities<a href=\"#vector_database_vulnerabilities\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Vector databases are a cornerstone of modern data-intensive applications, powering everything from retrieval-augmented generation (RAG) pipelines to recommendation systems.&nbsp;</p>\n\n\n\n<p>The high-performance index-building and search capabilities of these databases make them essential for such applications, but the value of the data they store makes them attractive targets for malicious attacks and breaches. This risk of exposure is of particular concern for sectors where confidentiality is a business requirement:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Regulated industries</strong>: For example, healthcare, financial services, and the public sector, where stringent privacy and security requirements can outright preclude the use of vector search and its downstream applications.</li>\n\n\n\n<li><strong>IP-driven sectors</strong>: For example, pharmaceutical, manufacturing, and defense, where intellectual property forms a considerable value-driver and competitive advantage.</li>\n</ul>\n\n\n\n<p>These concerns can be ignored when prototyping AI-driven workloads but are likely to become roadblocks when it comes to production.&nbsp;</p>\n\n\n\n<h2 id=\"solution_confidential_vector_search\"  class=\"wp-block-heading\">Solution: confidential vector search<a href=\"#solution_confidential_vector_search\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p><a href=\"https://www.cyborg.co\">Cyborg</a>, a NY-based startup, has developed an end-to-end encrypted vector search engine to solve this problem. By using forward privacy and cryptographic hashing, Cyborg Vector Search enables the secure indexing and retrieval of confidential data. End-to-end encryption means that no unencrypted vectors are ever stored in a database, considerably reducing the attack surface and addressing the confidentiality concerns mentioned earlier.</p>\n\n\n\n<p>Cyborg Vector Search was designed to balance the following key performance characteristics:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>End-to-end encryption</strong>: Guarantee the highest level of security and confidentiality through cryptographically secure architecture for stringent privacy requirements.</li>\n\n\n\n<li><strong>High performance</strong>: Minimize the incremental cost of end-to-end encryption, keeping the cryptographic overhead of encrypted indexing and retrieval at &lt;5% and &lt;30%, respectively.</li>\n\n\n\n<li><strong>Compatibility</strong>: Maintain compatibility with existing vector search pipelines and workloads to provide a simple transition from prototype to production.</li>\n</ul>\n\n\n\n<h2 id=\"nvidia_hardware\"  class=\"wp-block-heading\">NVIDIA hardware<a href=\"#nvidia_hardware\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To make encrypted indexing possible on GPUs, the solution uses <a href=\"https://www.nvidia.com/en-us/data-center/solutions/confidential-computing/\">NVIDIA Confidential Computing</a>. Confidential Computing ensures that the data remains secured both cryptographically and through strong access controls, using trusted execution environments (TEEs) to provide a secure enclave for sensitive operations. This technology is crucial for maintaining the confidentiality of data during GPU-accelerated computations.</p>\n\n\n\n<p>The hardware at the core of this solution is the NVIDIA H100 Tensor Core GPU (80 GB) with Confidential Computing enabled. Confidential Computing is publicly available on all NVIDIA Hopper Tensor Core GPUs&nbsp;today and will continue to be supported in the next generation of NVIDIA Blackwell Tensor Core GPUs.&nbsp;</p>\n\n\n\n<p>NVIDIA GPUs configured in CC mode have hardware-based cryptographic engines, firewalls, and remote attestation flows activated to ensure the integrity of the TEE such that end users can ensure and validate that their confidential workloads are protected while in use on the GPU.&nbsp;</p>\n\n\n\n<p>NVIDIA Hopper Confidential Computing encrypts and signs all user data on the PCIe bus with AES-GCM256 and blocks infrastructure and out-of-band access with firewalls configured by signed and attestable firmware. NVIDIA also provides a public remote attestation service such that end users or relying parties can receive up-to-date confidence that their drivers and firmware have not been revoked due to bugs or exploits.</p>\n\n\n\n<p>Cyborg quickly accessed and developed their design using <a href=\"https://www.nvidia.com/en-us/launchpad/\">NVIDIA LaunchPad</a>. LaunchPad provides NVIDIA customers, partners, and ISVs with hands-on access to prebuilt labs in a browser-based sandbox environment. The design was preconfigured with <a href=\"https://docs.nvidia.com/nvtrust/index.html#guides\">all the necessary steps</a> to ensure the system was built and configured correctly for confidential workloads with the <a href=\"https://www.nvidia.com/en-us/launchpad/ai/develop-confidential-vm-applications/\">Develop Confidential VM Applications</a> lab. Cyborg spent no time worrying about infrastructure and focused instead on developing their solution.</p>\n\n\n\n<h2 id=\"accelerating_confidential_vector_search\"  class=\"wp-block-heading\">Accelerating confidential vector search<a href=\"#accelerating_confidential_vector_search\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Confidential vector search, much like conventional vector search, is a computationally expensive process that can prove difficult to scale. This makes it a perfect candidate for GPU acceleration. NVIDIA cuVS contains highly optimized primitives to do just that.</p>\n\n\n\n<p>To evaluate the effectiveness of this integration, Cyborg and NVIDIA conducted a joint proof-of-concept (POC). This involved integrating cuVS with Cyborg Vector Search to bring GPU-accelerated encrypted vector search to reality.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1.png\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"352\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1-1024x352.png\" alt=\"Diagram shows an encrypted indexing pipeline and encrypted retrieval pipeline with GPU-accelerated sections highlighted.\" class=\"wp-image-87404\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1-1024x352.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1-300x103.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1-625x215.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1-179x62.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1-768x264.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1-1536x528.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1-645x222.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1-500x172.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1-160x55.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1-362x124.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1-320x110.png 320w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/encrypted-indexing-retrieval-pipelines-1.png 1999w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 1. Encrypted indexing and retrieval pipelines from the joint Cyborg-NVIDIA PoC&nbsp;</em></figcaption></figure></div>\n\n\n<p>This PoC compared encrypted indexing and retrieval performance on CPUs and GPUs. Specifically, we replaced <code>scikit-learn</code> KMeans and <code>hashlib</code> on a CPU with cuVS and a custom SHA-3 CUDA kernel on a GPU, respectively. The results speak for themselves:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Index build time was sped up by an average of 47x, reducing the time required to index vector embeddings from hours to minutes. The steps accelerated with cuVS saw an even better improvement of 52.2x for clustering model training and inference.</li>\n\n\n\n<li>Retrieval also saw significant improvements: the cuVS-accelerated portion of the pipeline yielded a 9.8x performance boost with minimal code changes.</li>\n\n\n\n<li>Enabling the NVIDIA Hopper Confidential Computing modes for end-to-end encryption on indexing and retrieval came at a marginal cost of 1-2% and 15-25%, respectively, compared to their unencrypted counterparts. This was a small overhead more than offset by GPU acceleration.</li>\n</ul>\n\n\n\n<p>Starting with the index build process, Figure 2 shows the overall build time on CPU compared to GPU.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"625\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-625x625.png\" alt=\"Bar chart comparing the overall index build time on CPU and GPU. The GPU significantly accelerates the process, reducing the time from several hours to minutes.\" class=\"wp-image-87408\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-625x625.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-300x300.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-115x115.png 115w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-768x768.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-1536x1536.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-645x645.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-90x90.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-150x150.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-362x362.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-110x110.png 110w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu-1024x1024.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-cpu-vs-gpu.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Overall index build time on CPU and GPU</em></figcaption></figure></div>\n\n\n<p>Clustering model training typically dominates index build time. If you exclude training and focus solely on quantization and encrypted indexing, the GPU still provides significant acceleration (Figure 3).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"625\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-625x625.png\" alt=\"Bar chart comparing index build time on CPU and GPU without clustering model training. The GPU markedly reduces the time required compared to CPU.\" class=\"wp-image-87411\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-625x625.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-300x300.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-115x115.png 115w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-768x768.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-1536x1536.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-645x645.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-90x90.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-150x150.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-362x362.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-110x110.png 110w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training-1024x1024.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/index-build-no-training.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Index build time without clustering model training on CPU and GPU</em></figcaption></figure></div>\n\n\n<p>Finally, the shift to GPU yields a significant improvement across the entire retrieval pipeline (Figure 4).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"625\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-625x625.png\" alt=\"Bar chart comparing retrieval time on CPU and GPU. The GPU substantially speeds up retrieval compared to CPU.\" class=\"wp-image-87413\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-625x625.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-300x300.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-115x115.png 115w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-768x768.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-1536x1536.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-645x645.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-90x90.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-96x96.png 96w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-128x128.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-150x150.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-362x362.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-110x110.png 110w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu-1024x1024.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/retrieval-time-cpu-vs-gpu.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Retrieval time on CPU and GPU</em></figcaption></figure></div>\n\n\n<p class=\"has-small-font-size\">All times are from the same index configuration (recall level &gt; 0.95).</p>\n\n\n\n<p>The IVFPQ index type was employed for its optimal combination of efficiency and accuracy.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In a world where data breaches are increasingly common, security is not just a luxury but a necessity for many organizations. The integration of Cyborg Vector Search with NVIDIA cuVS and NVIDIA Confidential Computing offers a strong approach to enhancing the security of vector databases, aiming to protect sensitive data while maintaining\u00a0performance.\u00a0</p>\n\n\n\n<p>NVIDIA is the first and currently only GPU vendor that provides publicly available, general-access hardware-based Confidential Computing solutions with the NVIDIA Hopper family. The NVIDIA Blackwell generation continues to improve upon the technology, partnering with other industry leaders to increase performance, security, and ease of use.</p>\n\n\n\n<p>Cyborg Vector Search is currently in closed testing with early commercial partners. As development continues, Cyborg would love to <a href=\"https://www.cyborg.co/contact\">hear from you</a> if data security is important to your AI workloads.</p>\n\n\n\n<p>To try NVIDIA Hopper Confidential Computing today, register for the <a href=\"https://www.nvidia.com/en-us/launchpad/ai/develop-confidential-vm-applications/\">LaunchPad lab</a>. You can also see what other <a href=\"https://www.nvidia.com/en-us/launchpad/#choose-a-lab\">LaunchPad solutions are available for trial</a> from their extensive list of labs.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>In the era of generative AI, vector databases have become indispensable for storing and querying high-dimensional data efficiently. However, like all databases, vector databases are vulnerable to a range of attacks, including cyber threats, phishing attempts, and unauthorized access. This vulnerability is particularly concerning considering that these databases often contain sensitive and confidential information. To &hellip; <a href=\"https://developer.nvidia.com/blog/bringing-confidentiality-to-vector-search-with-cyborg-and-nvidia-cuvs/\">Continued</a></p>\n", "protected": false}, "author": 2250, "featured_media": 87566, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1468645", "discourse_permalink": "https://forums.developer.nvidia.com/t/bringing-confidentiality-to-vector-search-with-cyborg-and-rapids-cuvs/303515", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [696], "tags": [453, 3496], "coauthors": [3977, 2497, 3067, 3263], "class_list": ["post-87131", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-science", "tag-featured", "tag-vector-search"], "acf": {"post_industry": ["General"], "post_products": ["cuVS", "Hopper", "LaunchPad", "RAPIDS"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Bringing-Confidentiality-to-Vector-Search-with-Cyborg-and-RAPIDS-cuVS.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mFl", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87131"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2250"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87131"}], "version-history": [{"count": 6, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87131/revisions"}], "predecessor-version": [{"id": 89856, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87131/revisions/89856"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87566"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87131"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87131"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87131"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87131"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87091, "date": "2024-08-14T12:30:00", "date_gmt": "2024-08-14T19:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87091"}, "modified": "2024-08-22T11:24:55", "modified_gmt": "2024-08-22T18:24:55", "slug": "optimizing-inference-efficiency-for-llms-at-scale-with-nvidia-nim-microservices", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/optimizing-inference-efficiency-for-llms-at-scale-with-nvidia-nim-microservices/", "title": {"rendered": "Optimizing Inference Efficiency for LLMs at Scale with NVIDIA NIM Microservices"}, "content": {"rendered": "\n<p>As <a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">large language models (LLMs)</a> continue to evolve at an unprecedented pace, enterprises are looking to build <a href=\"https://www.nvidia.com/en-us/glossary/generative-ai/\">generative AI-powered applications</a> that maximize throughput to lower operational costs and minimize latency to deliver superior user experiences.</p>\n\n\n\n<p>This post discusses the critical performance metrics of <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\">throughput and latency for LLMs</a>, exploring their importance and trade-offs between the two. It also looks at how throughput and latency impact the efficiency and user experience of AI applications, and how they can be optimized with <a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a> microservices.</p>\n\n\n\n<h2 id=\"key_metrics_for_measuring_cost_efficiency\"  class=\"wp-block-heading\">Key metrics for measuring cost efficiency<a href=\"#key_metrics_for_measuring_cost_efficiency\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>When a user sends a request to an LLM, the system processes this request and begins generating a response by outputting a series of tokens. There are often multiple requests sent to the system, which the system tries to process simultaneously to minimize wait time for each request.&nbsp;</p>\n\n\n\n<p><em>Throughput</em> measures the number of successful operations per unit of time. Throughput is an important measurement for enterprises to determine how well they can handle user requests simultaneously. For LLMs, throughput is measured by tokens per second. Since tokens are the new currency, higher throughput can lower costs and bring in revenue for enterprises.&nbsp;</p>\n\n\n\n<p>Furthermore, an improved throughput provides a competitive edge in delivering high-performance applications that can scale with software like Kubernetes, leading to lower server costs and the ability to handle more users.&nbsp;</p>\n\n\n\n<p><em>Latency, </em>the delay before or between data transfers, is measured by time to first token (TTFT) and inter-token latency (ITL). Latency is crucial to ensure a smooth user experience while maximizing overall system efficiency.</p>\n\n\n\n<p>Figure 1 shows a model receiving several concurrent requests (L1 &#8211; Ln) across a period of time (T_start &#8211; T_end), with each line representing the latency for each request. Having more lines in each row that are individually shorter equates to higher throughput and lower latency overall.\u00a0</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"761\" height=\"276\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/request-processing-timeline-throughput-latency.png\" alt=\"Diagram showing the timeline of request processing from T_start to T_end, with each shorter line segment L1-Ln\u200b representing the latency of individual requests. Higher throughput is depicted by more line segments, indicating more tokens produced.\n\" class=\"wp-image-87099\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/request-processing-timeline-throughput-latency.png 761w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/request-processing-timeline-throughput-latency-300x109.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/request-processing-timeline-throughput-latency-625x227.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/request-processing-timeline-throughput-latency-179x65.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/request-processing-timeline-throughput-latency-645x234.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/request-processing-timeline-throughput-latency-500x181.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/request-processing-timeline-throughput-latency-160x58.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/request-processing-timeline-throughput-latency-362x131.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/request-processing-timeline-throughput-latency-303x110.png 303w\" sizes=\"(max-width: 761px) 100vw, 761px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Timeline of request processing on a server from T_start to T_end, with each shorter line segment L1-Ln\u200b representing the latency of individual requests</em></figcaption></figure>\n\n\n\n<p>TTFT measures the time it takes for the model to generate the first token after receiving a request, indicating how long an end user needs to wait before seeing the first token. This is essential for quick initial responses, from customer support to e-commerce bots. Most often, TTFT should be under a few seconds; the shorter, the better, though this imposes constraints on the overall system throughput (more on that in the next section).</p>\n\n\n\n<p>ITL refers to the time interval between generating consecutive tokens, which is vital for applications that require smooth and continuous text generation. ITL should be less than human reading speed, to ensure a smooth reading experience.&nbsp;</p>\n\n\n\n<p>Figure 2 shows the combination of these latency benchmarks with the user and inference service interaction. The time from the query to the first generated token is the TTFT and the time between each token is the ITL.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"725\" height=\"284\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/inference-service-token-generation-process-ttft-itl.png\" alt=\"The image illustrates the token generation process in an inference service, highlighting TTFT and ITL. A user's query is sent to the inference service, which then generates and delivers tokens sequentially. TTFT represents the time from query submission to the delivery of the first token, while ITL indicates the time intervals between subsequent tokens. The entire process, from the first token to the last token, is referred to as the &quot;generation time.&quot;\n\" class=\"wp-image-87104\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/inference-service-token-generation-process-ttft-itl.png 725w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/inference-service-token-generation-process-ttft-itl-300x118.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/inference-service-token-generation-process-ttft-itl-625x245.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/inference-service-token-generation-process-ttft-itl-179x70.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/inference-service-token-generation-process-ttft-itl-645x253.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/inference-service-token-generation-process-ttft-itl-500x196.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/inference-service-token-generation-process-ttft-itl-160x63.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/inference-service-token-generation-process-ttft-itl-362x142.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/inference-service-token-generation-process-ttft-itl-281x110.png 281w\" sizes=\"(max-width: 725px) 100vw, 725px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. The token generation process in an inference service, highlighting the role of TTFT and ITL</em></figcaption></figure>\n\n\n\n<p>The goal for enterprises is to reduce ITL and TTFT to the extent possible, lowering latency while keeping throughput high. This ensures the overall system is efficient and the individual user\u2019s experience is smooth.</p>\n\n\n\n<h2 id=\"balancing_throughput_and_latency\"  class=\"wp-block-heading\">Balancing throughput and latency<a href=\"#balancing_throughput_and_latency\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The trade-off between throughput and latency is driven by the number of concurrent requests and the latency budget, both determined by the application\u2019s use case.&nbsp; By handling a large number of user requests concurrently, enterprises can increase throughput; however, this often results in higher latency for each individual request.&nbsp;</p>\n\n\n\n<p>On the other hand, under a set latency budget, which is the acceptable amount of latency the end user will tolerate, one can maximize throughput by increasing the number of concurrent requests. Latency budget can pose a constraint for either the TTFT or the end-to-end latency.</p>\n\n\n\n<p>Figure 3 illustrates the trade-off between throughput and latency. The y-axis is throughput, and x-axis is latency (TTFT in this case), and the corresponding concurrency is labeled over each marker on the curve. This can be used to identify the point that maximizes throughput within a specified latency budget, tailored to a specific use case.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"763\" height=\"535\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/relationship-between-ttft-throughput-per-second.png\" alt=\"Graph showing throughput versus TTFT with concurrency labeled on the markers. The x-axis represents &quot;Single User: time to first token(s)&quot; on a logarithmic scale, ranging from 0.1 to 100 seconds. The y-axis represents &quot;Total System: tokens/s,&quot; ranging from 0 to over 1000 tokens per second. The graph shows a series of blue markers connected by a dotted line, illustrating the relationship between throughput and time to first token (TTFT) at different concurrency levels, with concurrencies printed over each marker. As TTFT increases, throughput rises and then plateaus, indicating a saturation point. \n\" class=\"wp-image-87107\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/relationship-between-ttft-throughput-per-second.png 763w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/relationship-between-ttft-throughput-per-second-300x210.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/relationship-between-ttft-throughput-per-second-625x438.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/relationship-between-ttft-throughput-per-second-164x115.png 164w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/relationship-between-ttft-throughput-per-second-645x452.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/relationship-between-ttft-throughput-per-second-428x300.png 428w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/relationship-between-ttft-throughput-per-second-128x90.png 128w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/relationship-between-ttft-throughput-per-second-362x254.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/relationship-between-ttft-throughput-per-second-157x110.png 157w\" sizes=\"(max-width: 763px) 100vw, 763px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. The relationship between TTFT and throughput tokens per second</em></figcaption></figure>\n\n\n\n<p>As the number of concurrent requests increases, more GPUs can be added by standing up multiple instances of the model service. This will sustain the needed level of throughput and user experience. For example, a chatbot handling shopping requests on Black Friday would need to use several GPUs to maintain throughput and latency under such peak concurrency.&nbsp;</p>\n\n\n\n<p>By focusing on how throughput and latency vary with the number of concurrent users. enterprises can make informed decisions on enhancing the efficiency of their AI solutions based on their use cases. This translates to a perfect balance between throughput and latency to avoid wasting resources and minimize server costs.&nbsp;</p>\n\n\n\n<h2 id=\"how_nvidia_nim_optimizes_throughput_and_latency\"  class=\"wp-block-heading\"><strong>How NVIDIA NIM optimizes throughput and latency</strong><a href=\"#how_nvidia_nim_optimizes_throughput_and_latency\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA offers enterprises an optimized solution to maintain high throughput and low latency\u2014<a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM</a>. NIM is a set of microservices for optimizing performance while offering security, ease of use, and the flexibility to deploy the models anywhere. NIM lowers TCO by delivering low latency and high throughput AI inference that scales efficiently with infrastructure resources.&nbsp;</p>\n\n\n\n<p>With NIM, enterprises can get optimized model performance through key techniques including runtime refinement, intelligent model representation, and tailored throughput and latency profiles. <a href=\"https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/\">NVIDIA TensorRT-LLM</a> optimizes model performance by leveraging parameters such as GPU count and batch size. With NIM, enterprises can have these parameters automatically tuned to best suit their use cases to reach optimal latency and throughput.</p>\n\n\n\n<p>As part of the <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a> suite of software, NIM goes through exhaustive tuning to ensure the high-performance configuration for each model. Additionally, techniques like Tensor Parallelism (Figure 4) and <a href=\"https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/\">in-flight batching</a> (IFB) further boost throughput and reduce latency by processing multiple requests in parallel and maximizing GPU utilization.&nbsp;</p>\n\n\n\n<p>These powerful optimization techniques are widely available to increase performance in AI applications. Furthermore, NIM performance will increase over time as NVIDIA continues to refine each NIM with each new release.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"440\" height=\"292\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/llm-tensor-parallelism.png\" alt=\"Diagram illustrating tensor parallelism (TP) in LLMs. The diagram shows a neural network graph, with the top and bottom half split into two different colors, or sections, demonstrating how Tensor Parallelism optimizes inference across multiple GPUs.\n\" class=\"wp-image-87109\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/llm-tensor-parallelism.png 440w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/llm-tensor-parallelism-300x199.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/llm-tensor-parallelism-173x115.png 173w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/llm-tensor-parallelism-136x90.png 136w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/llm-tensor-parallelism-362x240.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/llm-tensor-parallelism-166x110.png 166w\" sizes=\"(max-width: 440px) 100vw, 440px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 4. Tensor parallelism shows how models can be sharded to utilize parallel computing across multiple GPUs, increasing throughput and minimizing latency by processing requests concurrently</em></em></figcaption></figure>\n\n\n\n<h2 id=\"nvidia_nim_performance\"  class=\"wp-block-heading\"><strong>NVIDIA NIM performance</strong><a href=\"#nvidia_nim_performance\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Using NIM, throughput and latency improve significantly. Specifically, the NVIDIA Llama 3.1 8B Instruct NIM has achieved 2.5x improvement in throughput, 4x faster TTFT, and 2.2x faster ITL compared to the best open-source alternatives (Figure 5).\u00a0</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1126\" height=\"601\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/acceleration-throughput-latency-nvidia-nim-llama-instruct.png\" alt=\"This image has three charts that each show the improvement in performance metrics with NIM. With NIM, Llama 3.1 8B Instruct has a throughput of 6372 tokens/sec, TTFT of 1s, and an ITL of 30ms. With NIM Off, Llama 3.1 8B Instruct has a throughput of 2679 tokens/sec, TTFT of 4s, and an ITL of 65ms. The configuration is Llama 3.1 8B Instruct, input token length: 1,000  output token length: 1,000. Concurrent client requests: 200, on  1x NVIDIA H100 SXM.\" class=\"wp-image-87115\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/acceleration-throughput-latency-nvidia-nim-llama-instruct.png 1126w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/acceleration-throughput-latency-nvidia-nim-llama-instruct-300x160.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/acceleration-throughput-latency-nvidia-nim-llama-instruct-625x334.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/acceleration-throughput-latency-nvidia-nim-llama-instruct-179x96.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/acceleration-throughput-latency-nvidia-nim-llama-instruct-768x410.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/acceleration-throughput-latency-nvidia-nim-llama-instruct-645x344.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/acceleration-throughput-latency-nvidia-nim-llama-instruct-500x267.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/acceleration-throughput-latency-nvidia-nim-llama-instruct-160x85.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/acceleration-throughput-latency-nvidia-nim-llama-instruct-362x193.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/acceleration-throughput-latency-nvidia-nim-llama-instruct-206x110.png 206w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/acceleration-throughput-latency-nvidia-nim-llama-instruct-1024x547.png 1024w\" sizes=\"(max-width: 1126px) 100vw, 1126px\" /><figcaption class=\"wp-element-caption\"><em>Figure 5. The acceleration improvement for throughput and latency using Llama 3.1 8B Instruct</em></figcaption></figure></div>\n\n\n<p>Figure 6 is a live demo of NIM On versus NIM Off that shows real-time chatbot generation. NIM On (right) produces an output 2.4x faster than NIM Off (left). This speedup with NIM On is provided by the optimized Tensort-RT LLM and techniques previously mentioned, such as in-flight batching and tensor parallelism.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"600\" height=\"324\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/mixtral-8x7b-nim-on-nim-off.gif\" alt=\"This demo shows two chatbots generating responses when queried. It shows the performance difference between the model running with and without NIM. With NIM, the model runs 2.4x faster in terms of inter-token latency. \n\" class=\"wp-image-87118\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 6. Demo of Mixtral 8x7B running with and without NIM for a 2.4x ITL gain with NIM On</em></em></figcaption></figure>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\"><strong>Get started</strong><a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVIDIA NIM is setting a new standard in the world of enterprise AI by delivering unmatched performance, ease of use, and cost efficiency. Whether you&#8217;re looking to enhance customer service, streamline operations, or innovate in your industry, NIM provides the robust, scalable, and secure solution you need.</p>\n\n\n\n<p>Experience the high throughput and low latency of the <a href=\"https://build.nvidia.com/meta/llama3-70b\">Llama 3 70B NIM</a>.&nbsp;</p>\n\n\n\n<p>To learn more about benchmarking NIM on your machines, check out the <a href=\"https://docs.nvidia.com/nim/benchmarking/llm/latest/index.html\">NIM LLM Benchmarking Guide</a> and <a href=\"https://docs.nvidia.com/nim/index.html\">NIM documentation</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>As large language models (LLMs) continue to evolve at an unprecedented pace, enterprises are looking to build generative AI-powered applications that maximize throughput to lower operational costs and minimize latency to deliver superior user experiences. This post discusses the critical performance metrics of throughput and latency for LLMs, exploring their importance and trade-offs between the &hellip; <a href=\"https://developer.nvidia.com/blog/optimizing-inference-efficiency-for-llms-at-scale-with-nvidia-nim-microservices/\">Continued</a></p>\n", "protected": false}, "author": 2246, "featured_media": 87097, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1468149", "discourse_permalink": "https://forums.developer.nvidia.com/t/optimizing-inference-efficiency-for-llms-at-scale-with-nvidia-nim-microservices/303423", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [3110], "tags": [296, 453, 2932, 3739, 3673], "coauthors": [3973, 3612], "class_list": ["post-87091", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-generative-ai", "tag-ai-inference-microservices", "tag-featured", "tag-large-language-models", "tag-nim", "tag-tensorrtllm"], "acf": {"post_industry": ["General"], "post_products": ["AI Enterprise", "NIM", "TensorRT-LLM"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Best practice", "Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/llm-model-icons.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mEH", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87091"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2246"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87091"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87091/revisions"}], "predecessor-version": [{"id": 87119, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87091/revisions/87119"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87097"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87091"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87091"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87091"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87091"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87234, "date": "2024-08-14T10:35:11", "date_gmt": "2024-08-14T17:35:11", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87234"}, "modified": "2024-11-04T14:50:16", "modified_gmt": "2024-11-04T22:50:16", "slug": "video-build-live-media-applications-for-ai-enabled-infrastructure-with-nvidia-holoscan-for-media", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/video-build-live-media-applications-for-ai-enabled-infrastructure-with-nvidia-holoscan-for-media/", "title": {"rendered": "Video: Build Live Media Applications for AI-Enabled Infrastructure with NVIDIA Holoscan for Media"}, "content": {"rendered": "\n<p><a href=\"https://developer.nvidia.com/holoscan-for-media\">NVIDIA Holoscan for Media</a> is a software-defined, AI-enabled platform that enables live video pipelines to run on the same infrastructure as AI.&nbsp;</p>\n\n\n\n<p>This video explains how developers in live media can use NVIDIA Holoscan for Media to build and deploy applications as software on repurposable, NVIDIA-accelerated, commercial off-the-shelf hardware. The video features Guillaume Polaillon, product line manager for Live Media Solutions at NVIDIA.</p>\n\n\n\n<p>Holoscan for Media revolutionizes application development for this rapidly evolving market by providing an IP-based, cloud-native architecture that isn&#8217;t constrained by dedicated hardware, environments, or locations. Instead, it integrates open-source and ubiquitous technologies, enabling you to simplify the development process, streamline delivery to customers, and integrate emerging technologies like generative AI into your solutions, all while optimizing R&amp;D spend.\u00a0</p>\n\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/4uVTT6kno-w?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div></figure>\n\n\n\n<p><a href=\"https://developer.nvidia.com/holoscan-for-media\">Get developer access to NVIDIA Holoscan for Media</a>.</p>\n\n\n\n<p>Learn more about the platform and other <a href=\"https://www.nvidia.com/en-us/industries/media-and-entertainment/live-media/\">NVIDIA solutions for the live media industry</a>. And join <a href=\"https://www.nvidia.com/en-us/events/ibc/\">NVIDIA at IBC2024</a>, September 13-16.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA Holoscan for Media is a software-defined, AI-enabled platform that enables live video pipelines to run on the same infrastructure as AI.&nbsp; This video explains how developers in live media can use NVIDIA Holoscan for Media to build and deploy applications as software on repurposable, NVIDIA-accelerated, commercial off-the-shelf hardware. The video features Guillaume Polaillon, product &hellip; <a href=\"https://developer.nvidia.com/blog/video-build-live-media-applications-for-ai-enabled-infrastructure-with-nvidia-holoscan-for-media/\">Continued</a></p>\n", "protected": false}, "author": 1873, "featured_media": 87240, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1468113", "discourse_permalink": "https://forums.developer.nvidia.com/t/video-build-live-media-applications-for-ai-enabled-infrastructure-with-nvidia-holoscan-for-media/303406", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1050, 852, 503], "tags": [453, 2903, 3592, 3547], "coauthors": [3501], "class_list": ["post-87234", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-data-center-cloud", "category-simulation-modeling-design", "tag-featured", "tag-holoscan", "tag-media-streaming", "tag-virtualization"], "acf": {"post_industry": ["Media & Entertainment"], "post_products": ["Holoscan"], "post_learning_levels": ["General Interest"], "post_content_types": ["Video"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/holoscan-media-video.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mH0", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87234"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1873"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87234"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87234/revisions"}], "predecessor-version": [{"id": 87245, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87234/revisions/87245"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87240"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87234"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87234"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87234"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87234"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87199, "date": "2024-08-14T09:00:00", "date_gmt": "2024-08-14T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87199"}, "modified": "2024-08-22T11:24:57", "modified_gmt": "2024-08-22T18:24:57", "slug": "just-released-doca-2-8-software-framework", "status": "publish", "type": "post", "link": "https://nvda.ws/4crxaRs", "title": {"rendered": "Just Released: DOCA 2.8 Software Framework"}, "content": {"rendered": "\n<p>The new release includes support for Spectrum-X 1.1 RA and new features for AI Cloud Data Centers.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The new release includes support for Spectrum-X 1.1 RA and new features for AI Cloud Data Centers.</p>\n", "protected": false}, "author": 1294, "featured_media": 84027, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1468075", "discourse_permalink": "https://forums.developer.nvidia.com/t/just-released-doca-2-8-software-framework/303398", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/4crxaRs", "_links_to_target": "_blank"}, "categories": [852, 2758, 1205], "tags": [3563, 453, 3017], "coauthors": [2646], "class_list": ["post-87199", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-edge-computing", "category-networking-communications", "tag-cloud-networking", "tag-featured", "tag-spectrum"], "acf": {"post_industry": ["Cloud Services"], "post_products": ["BlueField DPU", "DOCA"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Announcement", "News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/06/doca.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mGr", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Center / Cloud", "link": "https://developer.nvidia.com/blog/category/data-center-cloud/", "id": 852}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87199"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1294"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87199"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87199/revisions"}], "predecessor-version": [{"id": 87307, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87199/revisions/87307"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/84027"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87199"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87199"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87199"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87199"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87164, "date": "2024-08-14T08:50:05", "date_gmt": "2024-08-14T15:50:05", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87164"}, "modified": "2024-08-22T11:24:58", "modified_gmt": "2024-08-22T18:24:58", "slug": "how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/", "title": {"rendered": "How to Prune and Distill Llama-3.1 8B to an NVIDIA Llama-3.1-Minitron 4B Model"}, "content": {"rendered": "\n<p><a href=\"https://www.nvidia.com/en-au/glossary/large-language-models/\">Large language models</a> (LLM) are now a dominant force in natural language processing and understanding, thanks to their effectiveness and versatility. LLMs such as <a href=\"https://ai.meta.com/blog/meta-llama-3-1/\">Llama 3.1 405B</a> and <a href=\"https://research.nvidia.com/publication/2024-06_nemotron-4-340b\">NVIDIA Nemotron-4 340B</a> excel in many challenging tasks, including coding, reasoning, and math. They are, however, resource-intensive to deploy. As such, there is another trend in the industry to develop small language models (SLMs), which are sufficiently proficient in many language tasks but much cheaper to deploy to the masses.</p>\n\n\n\n<p>Recently, NVIDIA researchers showed that <a href=\"https://arxiv.org/pdf/2407.14679\">structured weight pruning combined with knowledge distillation</a> forms an effective and efficient strategy for obtaining progressively smaller language models from an initial larger sibling. <a href=\"https://huggingface.co/nvidia/Minitron-8B-Base\">NVIDIA Minitron 8B</a> and <a href=\"https://huggingface.co/nvidia/Minitron-4B-Base\">4B</a> are such small models, obtained by pruning and distilling their larger <a href=\"https://arxiv.org/abs/2402.16819\">15B</a> sibling in the <a href=\"https://developer.nvidia.com/blog/leverage-our-latest-open-models-for-synthetic-data-generation-with-nvidia-nemotron-4-340b/\">NVIDIA Nemotron family</a>.&nbsp;</p>\n\n\n\n<p>Pruning and distillation lead to several benefits:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Improvement in MMLU scores by 16% compared to training from scratch.</li>\n\n\n\n<li>Fewer training tokens are required for each additional model, ~100B tokens with an up to 40x reduction.</li>\n\n\n\n<li>Compute cost saving to train a family of models, up to 1.8x compared to training all models from scratch.</li>\n\n\n\n<li>Performance is comparable to Mistral 7B, Gemma 7B, and Llama-3 8B trained on many more tokens, up to 15T.</li>\n</ul>\n\n\n\n<p>The paper also presents a set of practical and effective structured compression best practices for LLMs that combine depth, width, attention, and MLP pruning with knowledge distillation-based retraining.</p>\n\n\n\n<p>In this post, we first discuss these best practices and then show their effectiveness when applied to the Llama 3.1 8B model to obtain a Llama-3.1-Minitron 4B model. Llama-3.1-Minitron 4B performs favorably against state-of-the-art open-source models of similar size, including Minitron 4B, Phi-2 2.7B, Gemma2 2.6B, and Qwen2-1.5B. Llama-3.1-Minitron 4B will be released to the <a href=\"https://huggingface.co/collections/nvidia/minitron-669ac727dc9c86e6ab7f0f3e\">NVIDIA HuggingFace collection</a> soon, pending approvals.</p>\n\n\n\n<h2 id=\"pruning_and_distillation\"  class=\"wp-block-heading\">Pruning and distillation<a href=\"#pruning_and_distillation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Pruning is the process of making the model smaller and leaner, either by dropping layers (<em>depth pruning</em>) or dropping neurons and attention heads and embedding channels (<em>width pruning</em>). Pruning is often accompanied by some amount of retraining for accuracy recovery.</p>\n\n\n\n<p><em>Model distillation</em> is a technique used to transfer knowledge from a large, complex model, often called the teacher model, to a smaller, simpler student model. The goal is to create a more efficient model that retains much of the predictive power of the original, larger model while being faster and less resource-intensive to run.</p>\n\n\n\n<h3 id=\"classical_knowledge_distillation_vs_sdg_finetuning\"  class=\"wp-block-heading\">Classical knowledge distillation vs. SDG finetuning<a href=\"#classical_knowledge_distillation_vs_sdg_finetuning\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>There are two main styles of <a href=\"https://arxiv.org/abs/1503.02531\">distillation</a>:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>SDG </strong><strong>finetuning:</strong> The synthetic data generated from a larger teacher model is used to further fine-tune a smaller, pretrained student model. Here, the student mimics only the final token predicted by the teacher. This is exemplified by the Llama 3.1 Azure <a href=\"https://learn.microsoft.com/en-us/azure/ai-studio/concepts/concept-model-distillation\">Distillation in Azure AI Studio</a> and AWS <a href=\"https://aws.amazon.com/blogs/machine-learning/use-llama-3-1-405b-to-generate-synthetic-data-for-fine-tuning-tasks/\">Use Llama 3.1 405B for synthetic data generation and distillation to fine-tune smaller models</a> tutorials.</li>\n\n\n\n<li><strong>Classical knowledge distillation</strong><strong>:</strong> The student mimics the logits and other intermediate states of the teacher on the training dataset rather than just learning the token that has to be predicted. This can be viewed as providing better labels (a distribution compared to a one-shot label). Even with the same data, the gradient contains richer feedback, improving the training accuracy and efficiency. However, there must be training framework support for this style of distillation as the logits are too large to store.</li>\n</ul>\n\n\n\n<p>These two styles of distillation are complementary to one another, rather than mutually exclusive. This post primarily focuses on the classical knowledge distillation approach.&nbsp;</p>\n\n\n\n<h2 id=\"pruning_and_distillation_procedure\"  class=\"wp-block-heading\">Pruning and distillation procedure<a href=\"#pruning_and_distillation_procedure\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>We proposed combining pruning with classical knowledge distillation as a resource-efficient retraining technique (Figure 1).</p>\n\n\n\n<ol class=\"wp-block-list\">\n<li>We started from a 15B model. We estimated the importance of each component (layer, neuron, head, and embedding channel) and then ranked and trimmed the model to the target size: an 8B model.&nbsp;</li>\n\n\n\n<li>We performed a light retraining procedure using model distillation with the original model as the teacher and the pruned model as the student.</li>\n\n\n\n<li>After training, the small model (8B) served as a starting point to trim and distill to a smaller 4B model.&nbsp;&nbsp;</li>\n</ol>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/iterative-model-pruning-distillation-2.png\"><img loading=\"lazy\" decoding=\"async\" width=\"914\" height=\"637\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/iterative-model-pruning-distillation-2.png\" alt=\"The diagram shows progressively pruning and distilling models of smaller sizes, from 15B to 8B and from 8B to 4B.\u00a0\" class=\"wp-image-87318\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/iterative-model-pruning-distillation-2.png 914w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/iterative-model-pruning-distillation-2-300x209.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/iterative-model-pruning-distillation-2-625x436.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/iterative-model-pruning-distillation-2-165x115.png 165w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/iterative-model-pruning-distillation-2-768x535.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/iterative-model-pruning-distillation-2-645x450.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/iterative-model-pruning-distillation-2-430x300.png 430w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/iterative-model-pruning-distillation-2-129x90.png 129w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/iterative-model-pruning-distillation-2-362x252.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/iterative-model-pruning-distillation-2-158x110.png 158w\" sizes=\"(max-width: 914px) 100vw, 914px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 1. Iterative model pruning and distillation procedure</em></figcaption></figure></div>\n\n\n<p>Figure 1 shows the pruning and distillation process of a single model (top) and the chain of model pruning and distillation (bottom). In the latter, the output model of a previous stage serves as the input model for the next stage.</p>\n\n\n\n<h3 id=\"importance_analysis\"  class=\"wp-block-heading\">Importance analysis<a href=\"#importance_analysis\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To prune a model, it is critical to understand which parts of the model are important. We propose using a purely activation-based importance estimation strategy that simultaneously computes sensitivity information for all the axes considered (depth, neuron, head, and embedding channel) using a small (1024 samples) calibration dataset and only forward propagation passes. This strategy is more straightforward and cost-effective to implement compared to strategies that rely on gradient information and require a backward propagation pass.</p>\n\n\n\n<p>While pruning, you can iteratively alternate between pruning and importance estimation for a given axis or combination of axes. However, our empirical work shows that it is sufficient to use single-shot importance estimation and iterative estimation provides no benefit.</p>\n\n\n\n<h3 id=\"retraining_with_classical_knowledge_distillation\"  class=\"wp-block-heading\">Retraining with classical knowledge distillation<a href=\"#retraining_with_classical_knowledge_distillation\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Figure 2 shows the distillation process with a student model (pruned model) with <em>N</em> layers distilled from a teacher model (original unpruned model) with <em>M</em> layers. The student learns by minimizing a combination of embedding output loss, logit loss, and transformer encoder-specific losses mapped across student block <em>S</em> and teacher block <em>T</em>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses.png\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"313\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses-1024x313.png\" alt=\"The workflow diagram shows classical knowledge distillation from teacher to student, with loss function from several layers of the transformer architecture.\u00a0\" class=\"wp-image-87188\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses-1024x313.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses-300x92.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses-625x191.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses-179x55.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses-768x234.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses-645x197.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses-500x153.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses-160x49.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses-362x111.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses-360x110.png 360w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/distillation-training-losses.png 1081w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 2. Distillation training losses</em></figcaption></figure></div>\n\n\n<h2 id=\"pruning_and_distillation_best_practices\"  class=\"wp-block-heading\">Pruning and distillation best practices<a href=\"#pruning_and_distillation_best_practices\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Based on the extensive ablation studies carried out in <a href=\"https://arxiv.org/pdf/2407.14679\">Compact Language Models via Pruning and Knowledge Distillation</a>, we summarized our learnings into several structured compression best practices:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Sizing</strong>:\n<ul class=\"wp-block-list\">\n<li>To train a family of LLMs, first train the largest one, then prune and distill iteratively to obtain smaller LLMs.</li>\n\n\n\n<li>If the largest model is trained using a multi-phase training strategy, it is best to prune and retrain the model obtained from the final stage of training.</li>\n\n\n\n<li>Prune an available source model closest to the target size.</li>\n</ul>\n</li>\n\n\n\n<li><strong>Pruning</strong>:\n<ul class=\"wp-block-list\">\n<li>Prefer width over depth pruning. This worked well for the model scales considered (\u2264 15B).</li>\n\n\n\n<li>Use single-shot importance estimation. Iterative importance estimation provided no benefit.</li>\n</ul>\n</li>\n\n\n\n<li><strong>Retraining:</strong>\n<ul class=\"wp-block-list\">\n<li>Retrain exclusively with distillation loss instead of conventional training.</li>\n\n\n\n<li>Use logit plus intermediate state plus embedding distillation when the depth is reduced significantly.</li>\n\n\n\n<li>Use logit-only distillation when depth isn\u2019t reduced significantly.</li>\n</ul>\n</li>\n</ul>\n\n\n\n<h2 id=\"llama-31-minitron_putting_best_practices_to_work\"  class=\"wp-block-heading\">Llama-3.1-Minitron: putting best practices to work<a href=\"#llama-31-minitron_putting_best_practices_to_work\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Meta recently introduced the powerful <a href=\"https://ai.meta.com/blog/meta-llama-3-1/\">Llama 3.1</a> model family, a first wave of open-source models that are comparable with closed-source models across many benchmarks. Llama 3.1 ranges from the gigantic 405B model to the 70B and 8B.&nbsp;</p>\n\n\n\n<p>Equipped with experience of Nemotron distillation, we set out to distill the Llama 3.1 8B model to a smaller and more efficient 4B sibling:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Teacher fine-tuning</li>\n\n\n\n<li>Depth-only pruning</li>\n\n\n\n<li>Width-only pruning</li>\n\n\n\n<li>Accuracy benchmarks</li>\n\n\n\n<li>Performance benchmarks</li>\n</ul>\n\n\n\n<h3 id=\"teacher_fine-tuning\"  class=\"wp-block-heading\">Teacher fine-tuning<a href=\"#teacher_fine-tuning\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To correct for the distribution shift across the original dataset the model was trained on, we first fine-tuned the unpruned 8B model on our dataset (94B tokens). Experiments showed that, without correcting for the distribution shift, the teacher provides suboptimal guidance on the dataset when being distilled.</p>\n\n\n\n<h3 id=\"depth-only_pruning\"  class=\"wp-block-heading\">Depth-only pruning<a href=\"#depth-only_pruning\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>To go from an 8B to a 4B, we pruned 16 layers (50%). We first evaluated the importance of each layer or continuous subgroup of layers by dropping them from the model and observing the increase in LM loss or accuracy reduction on a downstream task.&nbsp;</p>\n\n\n\n<p>Figure 5 shows the LM loss value on the validation set after removing 1, 2, 8, or 16 layers. For example, the red plot at layer 16 indicates the LM loss if we dropped the first 16 layers. Layer 17&nbsp; indicates the LM loss if we leave the first layer and drop layers 2 to 17. We observed that the layers at the beginning and end are the most important. </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers.png\"><img loading=\"lazy\" decoding=\"async\" width=\"842\" height=\"547\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers.png\" alt=\"Line chart showing multiple sets of layer importance in depth-only pruning as measured by lm_loss. Layers at the beginning and the end are most important.\" class=\"wp-image-87185\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers.png 842w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers-300x195.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers-625x406.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers-177x115.png 177w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers-768x499.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers-645x419.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers-462x300.png 462w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers-139x90.png 139w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers-362x235.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-multiple-sets-pruned-layers-169x110.png 169w\" sizes=\"(max-width: 842px) 100vw, 842px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 5. Layer importance in depth-only pruning</em></figcaption></figure></div>\n\n\n<p>However, we observed that this LM loss is not necessarily directly correlated with downstream performance.&nbsp;</p>\n\n\n\n<p>Figure 6 shows the Winogrande accuracy for each pruned model. It indicates that it is best to remove layers 16 to 31, with 31 being the second-to-last layer, where the pruned model 5-shot accuracy is significantly greater than random (0.5). We adopted this insight and removed layers 16 to 31.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-pruned-layers-16-31.png\"><img loading=\"lazy\" decoding=\"async\" width=\"855\" height=\"547\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-pruned-layers-16-31.png\" alt=\"Line chart shows the best accuracy on layer 32 out of layers 16-32.\" class=\"wp-image-87183\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-pruned-layers-16-31.png 855w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-pruned-layers-16-31-300x192.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-pruned-layers-16-31-625x400.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-pruned-layers-16-31-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-pruned-layers-16-31-768x491.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-pruned-layers-16-31-645x413.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-pruned-layers-16-31-469x300.png 469w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-pruned-layers-16-31-141x90.png 141w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-pruned-layers-16-31-362x232.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/accuracy-winogrande-pruned-layers-16-31-172x110.png 172w\" sizes=\"(max-width: 855px) 100vw, 855px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 6. Accuracy on the Winogrande task when removing 16 layers</em></figcaption></figure></div>\n\n\n<h3 id=\"width-only_pruning\"  class=\"wp-block-heading\">Width-only pruning<a href=\"#width-only_pruning\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>We pruned both the embedding (hidden) and MLP intermediate dimensions along the width axis to compress Llama 3.1 8B. Specifically, we computed importance scores for each attention head, embedding channel, and MLP hidden dimension using the activation-based strategy described earlier. Following importance estimation, we:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Pruned (trim) the MLP intermediate dimension from 14336 to 9216.</li>\n\n\n\n<li>Pruned the hidden size from 4096 to 3072.</li>\n\n\n\n<li>Retrained the attention headcount and number of layers.</li>\n</ul>\n\n\n\n<p>It is worth mentioning that immediately after one-shot pruning, the LM loss of width pruning is higher than that of depth pruning. However, after a short retraining, the trend reverses.</p>\n\n\n\n<h3 id=\"accuracy_benchmarks\"  class=\"wp-block-heading\">Accuracy benchmarks<a href=\"#accuracy_benchmarks\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>We distilled the model with the following parameters:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Peak learning rate=1e-4</li>\n\n\n\n<li>Minimum learning rate=1e-5</li>\n\n\n\n<li>Linear warm-up of 40 steps</li>\n\n\n\n<li>Cosine decay schedule</li>\n\n\n\n<li>Global batch size=1152</li>\n</ul>\n\n\n\n<p>Table 1 shows the comparative performance of Llama-3.1-Minitron 4B model variants (width-pruned and depth-pruned) when compared with the original Llama 3.1 8B models and other models of similar size on benchmarks spanning several domains.&nbsp;</p>\n\n\n\n<p>Overall, we reconfirmed the effectiveness of a width-pruning strategy compared to depth pruning, which follows the best practices.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td rowspan=\"2\"><strong>Benchmark</strong></td><td rowspan=\"2\"><strong>No. of shots</strong></td><td rowspan=\"2\"><strong>Metric</strong></td><td rowspan=\"2\"><strong><em>Llama-3.1 8B</em></strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Minitron 4B</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\"><strong><em>Llama-3.1-Minitron 4B</em></strong></td><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\"><strong>Phi-2 2.7B</strong></td><td rowspan=\"2\"><strong>Gemma2 2.6B</strong><strong><sup>\u2020</sup></strong></td><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\"><strong>Qwen2-1.5B</strong><strong><sup>\u2020</sup></strong></td></tr><tr><td><strong>Width-pruned</strong></td><td><strong>Depth-pruned</strong></td><td><strong>Width-pruned</strong></td></tr><tr><td>winogrande</td><td>5</td><td>acc</td><td>0.7727</td><td class=\"has-text-align-center\" data-align=\"center\">0.7403*</td><td class=\"has-text-align-center\" data-align=\"center\">0.7214</td><td class=\"has-text-align-center\" data-align=\"center\">0.7348</td><td>0.7400**</td><td class=\"has-text-align-center\" data-align=\"center\">0.709</td><td class=\"has-text-align-center\" data-align=\"center\">0.662</td></tr><tr><td>arc_challenge</td><td>25</td><td>acc_norm</td><td>0.5794</td><td class=\"has-text-align-center\" data-align=\"center\">0.5085</td><td class=\"has-text-align-center\" data-align=\"center\">0.5256</td><td class=\"has-text-align-center\" data-align=\"center\">0.5555**</td><td>0.6100*</td><td class=\"has-text-align-center\" data-align=\"center\">0.554</td><td class=\"has-text-align-center\" data-align=\"center\">0.439</td></tr><tr><td>MMLU</td><td>5</td><td>acc</td><td>0.6528</td><td class=\"has-text-align-center\" data-align=\"center\">0.5860**</td><td class=\"has-text-align-center\" data-align=\"center\">0.5871</td><td class=\"has-text-align-center\" data-align=\"center\">0.6053*</td><td>0.5749</td><td class=\"has-text-align-center\" data-align=\"center\">0.513</td><td class=\"has-text-align-center\" data-align=\"center\">0.565</td></tr><tr><td>hellaswag</td><td>10</td><td>acc_norm</td><td>0.8180</td><td class=\"has-text-align-center\" data-align=\"center\">0.7496</td><td class=\"has-text-align-center\" data-align=\"center\">0.7321</td><td class=\"has-text-align-center\" data-align=\"center\">0.7606*</td><td>0.7524**</td><td class=\"has-text-align-center\" data-align=\"center\">0.73</td><td class=\"has-text-align-center\" data-align=\"center\">0.666</td></tr><tr><td>gsm8k</td><td>5</td><td>acc</td><td>0.4860</td><td class=\"has-text-align-center\" data-align=\"center\">0.2411</td><td class=\"has-text-align-center\" data-align=\"center\">0.1676</td><td class=\"has-text-align-center\" data-align=\"center\">0.4124</td><td>0.5500**</td><td class=\"has-text-align-center\" data-align=\"center\">0.239</td><td class=\"has-text-align-center\" data-align=\"center\">0.585*</td></tr><tr><td>truthfulqa</td><td>0</td><td>mc2</td><td>0.4506</td><td class=\"has-text-align-center\" data-align=\"center\">0.4288</td><td class=\"has-text-align-center\" data-align=\"center\">0.3817</td><td class=\"has-text-align-center\" data-align=\"center\">0.4289</td><td>0.4400**</td><td class=\"has-text-align-center\" data-align=\"center\">\u2013</td><td class=\"has-text-align-center\" data-align=\"center\">0.459*</td></tr><tr><td>XLSum en (20%)</td><td>3</td><td>rougeL</td><td>0.3005</td><td class=\"has-text-align-center\" data-align=\"center\">0.2954*</td><td class=\"has-text-align-center\" data-align=\"center\">0.2722</td><td class=\"has-text-align-center\" data-align=\"center\">0.2867**</td><td>0.0100</td><td class=\"has-text-align-center\" data-align=\"center\">\u2013</td><td class=\"has-text-align-center\" data-align=\"center\">\u2013</td></tr><tr><td>MBPP</td><td>0</td><td>pass@1</td><td>0.4227</td><td class=\"has-text-align-center\" data-align=\"center\">0.2817</td><td class=\"has-text-align-center\" data-align=\"center\">0.3067</td><td class=\"has-text-align-center\" data-align=\"center\">0.324</td><td>0.4700*</td><td class=\"has-text-align-center\" data-align=\"center\">0.29</td><td class=\"has-text-align-center\" data-align=\"center\">0.374**</td></tr><tr><td colspan=\"3\"><strong>Training Tokens</strong></td><td>15T</td><td colspan=\"3\">94B</td><td>1.4T</td><td class=\"has-text-align-center\" data-align=\"center\">3T</td><td class=\"has-text-align-center\" data-align=\"center\">7T</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. <em>Accuracy of Minitron 4B base models compared to similarly sized base community models</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">* Best model<br>** Second-best model<br>\u2013 Unavailable results<br><strong>\u2020</strong> Results as reported in the model report by the model publisher.</p>\n\n\n\n<p>To verify that the distilled models can be strong instruct models, we fine-tuned the Llama-3.1-Minitron 4B models using <a href=\"https://github.com/NVIDIA/NeMo-Aligner\">NeMo-Aligner</a>. We used training data used for <a href=\"https://arxiv.org/abs/2406.11704\">Nemotron-4 340B</a> and evaluated the models on <a href=\"https://arxiv.org/abs/2311.07911\">IFEval</a>, <a href=\"https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/README.md#mt-bench\">MT-Bench</a>, <a href=\"https://huggingface.co/datasets/nvidia/ChatRAG-Bench\">ChatRAG-Bench</a>, and <a href=\"https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard\">Berkeley Function Calling Leaderboard (BFCL)</a> to test instruction-following, roleplay, RAG, and function-calling capabilities. We confirmed that Llama-3.1-Minitron 4B models can be solid instruct models, which outperform other baseline SLMs (Table 2).</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td></td><td><strong>Minitron 4B</strong></td><td colspan=\"2\"><strong><em>Llama-3.1-Minitron 4B</em></strong></td><td rowspan=\"2\"><strong>Gemma 2B</strong></td><td rowspan=\"2\"><strong>Phi-2 2.7B</strong></td><td rowspan=\"2\"><strong>Gemma2 2.6B</strong></td><td rowspan=\"2\"><strong>Qwen2-1.5B</strong></td></tr><tr><td><strong>Benchmark</strong></td><td>Width-pruned</td><td>Depth-pruned</td><td>Width-pruned</td></tr><tr><td><strong>IFEval</strong></td><td>0.4484</td><td>0.4257</td><td>0.5239**</td><td>0.4050</td><td>0.4400</td><td>0.6451*</td><td>0.3981</td></tr><tr><td><strong>MT-Bench</strong></td><td>5.61</td><td>5.64</td><td>6.34**</td><td>5.19</td><td>4.29</td><td>7.73*</td><td>5.22</td></tr><tr><td><strong>ChatRAG</strong><strong><sup>\u2020</sup></strong></td><td>0.4111**</td><td>0.4013</td><td>0.4399*</td><td>0.3331</td><td>0.3760</td><td>0.3745</td><td>0.2908</td></tr><tr><td><strong>BFCL</strong></td><td>0.6423</td><td>0.6680*</td><td>0.6493**</td><td>0.4700</td><td>0.2305</td><td>0.3562</td><td>0.3275</td></tr><tr><td><strong>Training Tokens</strong></td><td colspan=\"3\">94B</td><td>3T</td><td>1.4T</td><td>2T</td><td>7T</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2. <em>Accuracy of aligned Minitron 4B base models compared to similarly sized aligned community models</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\">* Best model<br>** Second-best model<br><strong>\u2020</strong> Based on a representative subset of ChatRAG, not the whole benchmark.</p>\n\n\n\n<h3 id=\"performance_benchmarks\"  class=\"wp-block-heading\">Performance benchmarks<a href=\"#performance_benchmarks\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>We optimized the Llama 3.1 8B and Llama-3.1-Minitron 4B models with NVIDIA<br><a href=\"https://github.com/NVIDIA/TensorRT-LLM\">TensorRT-LLM</a>, an open-source toolkit for optimized LLM inference.&nbsp;</p>\n\n\n\n<p>Figures 7 and 8 show the throughput requests per second of different models in FP8 and FP16 precision on different use cases, represented as input sequence length/output sequence length (ISL/OSL) combinations at batch size 32 for the 8B model and batch size 64 for the 4B models, thanks to the smaller weights allowing for larger batches, on one NVIDIA H100 80GB GPU.&nbsp;</p>\n\n\n\n<p>The Llama-3.1-Minitron-4B-Depth-Base variant is the fastest, at an average of ~2.7x throughput of Llama 3.1 8B, while the Llama-3.1-Minitron-4B-Width-Base variant is at an average of ~1.8x throughput of Llama 3.1 8B. Deployment in FP8 also delivers a performance boost of ~1.3x across all three models compared to BF16.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1200\" height=\"742\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bf16-throughput-llama-3-1-minitron-4b.png\" alt=\"Bar chart shows the Llama-Minitron-3.1-4B-Depth-Base model being the fastest, followed by Llama-3.1-Minitron 4B-Width-Base and LLama 3.1 8B.\" class=\"wp-image-87388\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bf16-throughput-llama-3-1-minitron-4b.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bf16-throughput-llama-3-1-minitron-4b-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bf16-throughput-llama-3-1-minitron-4b-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bf16-throughput-llama-3-1-minitron-4b-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bf16-throughput-llama-3-1-minitron-4b-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bf16-throughput-llama-3-1-minitron-4b-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bf16-throughput-llama-3-1-minitron-4b-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bf16-throughput-llama-3-1-minitron-4b-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bf16-throughput-llama-3-1-minitron-4b-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bf16-throughput-llama-3-1-minitron-4b-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/bf16-throughput-llama-3-1-minitron-4b-1024x633.png 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. Performance benchmarks for request BF16 throughput at different input/output length combinations</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b.png\"><img loading=\"lazy\" decoding=\"async\" width=\"1200\" height=\"742\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b.png\" alt=\"Bar chart shows the Llama-3.1-Minitron-4B-Depth-Base model being fastest, followed by Llama-3.1-Minitron-4B-Width-Base and LLama 3.1 8B.\" class=\"wp-image-87390\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/fp8-throughput-llama-3-1-minitron-4b-1024x633.png 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 8. Performance benchmarks for request FP8 throughput at different input/output length combinations</em></figcaption></figure></div>\n\n\n<p class=\"has-small-font-size\">Combinations: BS=32 for Llama 3.1 8B and BS=64 for Llama-3.1-Minitron 4B models. 1x H100 80GB GPU.</p>\n\n\n\n<h2 id=\"conclusion\"  class=\"wp-block-heading\">Conclusion<a href=\"#conclusion\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Pruning and classical knowledge distillation is a highly cost-effective method to progressively obtain LLMs of smaller size, achieving superior accuracy compared to training from scratch across all domains. It serves as a more effective and data-efficient approach compared to either synthetic-data-style finetuning or pretraining from scratch.&nbsp;</p>\n\n\n\n<p>Llama-3.1-Minitron 4B is our first work with the state-of-the-art open-source Llama 3.1 family. To use SDG finetuning of Llama-3.1 in NVIDIA NeMo, see the <a href=\"https://github.com/NVIDIA/NeMo/tree/main/tutorials/llm/llama-3/sdg-law-title-generation\">/sdg-law-title-generation</a> notebook on GitHub.</p>\n\n\n\n<p>For more information, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://arxiv.org/abs/2407.14679\">Compact Language Models via Pruning and Knowledge Distillation</a></li>\n\n\n\n<li><a href=\"https://github.com/NVlabs/Minitron\">/NVlabs/Minitron</a> GitHub repo</li>\n\n\n\n<li>Llama-3.1-Minitron models on Hugging Face: \n<ul class=\"wp-block-list\">\n<li><a href=\"https://huggingface.co/nvidia/Llama-3.1-Minitron-4B-Width-Base\">Llama-3.1-Minitron-4B-Width-Base</a></li>\n\n\n\n<li><a href=\"https://huggingface.co/nvidia/Llama-3.1-Minitron-4B-Depth-Base\">Llama-3.1-Minitron-4B-Depth-Base</a></li>\n</ul>\n</li>\n</ul>\n\n\n\n<h3 id=\"acknowledgments\"  class=\"wp-block-heading\">Acknowledgments<a href=\"#acknowledgments\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p><em>This work would not have been possible without contributions from many people at NVIDIA. To mention a few of them:</em> <strong><em>Core Team:</em></strong><em> Sharath Turuvekere Sreenivas, Saurav Muralidharan, Marcin Chochowski, Raviraj Joshi</em>; <strong><em>Advisors:</em></strong><em> Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, Pavlo Molchanov</em>; <strong><em>Instruction-tuning: </em></strong><em>Ameya Sunil Mahabaleshwarkar, Hayley Ross, Brandon Rowlett, Oluwatobi Olabiyi, Shizhe Diao, Yoshi Suhara</em>; <strong><em>Datasets:</em></strong><em> Sanjeev Satheesh, Shengyang Sun, Jiaqi Zeng, Zhilin Wang, Yi Dong, Zihan Liu, Rajarshi Roy, Wei Ping, Makesh Narsimhan Sreedhar, Oleksii Kuchaiev</em>; <strong><em>TRT-LLM: </em></strong><em>Bobby Chen, James Shen</em>; <strong><em>HF support: </em></strong><em>Ao Tang, Greg Heinrich</em>; <strong><em>Model optimization:</em></strong><em> Chenhan Yu</em>; <strong><em>Discussion and feedback:</em></strong><em> Daniel Korzekwa; </em><strong><em>Blog post preparation:</em></strong><em> Vinh Nguyen, Sharath Turuvekere Sreenivas</em>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLM) are now a dominant force in natural language processing and understanding, thanks to their effectiveness and versatility. LLMs such as Llama 3.1 405B and NVIDIA Nemotron-4 340B excel in many challenging tasks, including coding, reasoning, and math. They are, however, resource-intensive to deploy. As such, there is another trend in the &hellip; <a href=\"https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/\">Continued</a></p>\n", "protected": false}, "author": 657, "featured_media": 87330, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1468069", "discourse_permalink": "https://forums.developer.nvidia.com/t/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/303396", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110], "tags": [453, 3650], "coauthors": [1014, 1170, 3982, 3981, 3983], "class_list": ["post-87164", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "tag-featured", "tag-llm-techniques"], "acf": {"post_industry": ["General"], "post_products": ["NeMo"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/ngc-genai-llama-visual-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mFS", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87164"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/657"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87164"}], "version-history": [{"count": 11, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87164/revisions"}], "predecessor-version": [{"id": 87391, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87164/revisions/87391"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87330"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87164"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87164"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87164"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87164"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87308, "date": "2024-08-13T13:37:24", "date_gmt": "2024-08-13T20:37:24", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87308"}, "modified": "2024-08-22T11:24:59", "modified_gmt": "2024-08-22T18:24:59", "slug": "new-nim-available-mistral-large-2-instruct-llm", "status": "publish", "type": "post", "link": "https://nvda.ws/4cE63Tr", "title": {"rendered": "New NIM Available: Mistral Large 2 Instruct LLM"}, "content": {"rendered": "\n<p>The new model by Mistral excels at a variety of complex tasks including text summarization, multilingual translation and reasoning, programming, question and answering, and conversational AI.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>The new model by Mistral excels at a variety of complex tasks including text summarization, multilingual translation and reasoning, programming, question and answering, and conversational AI.</p>\n", "protected": false}, "author": 492, "featured_media": 87309, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1466939", "discourse_permalink": "https://forums.developer.nvidia.com/t/new-nim-available-mistral-large-2-instruct-llm/303298", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "https://nvda.ws/4cE63Tr", "_links_to_target": "_blank"}, "categories": [1050, 3110], "tags": [3266, 453, 3270, 3267], "coauthors": [610, 3872], "class_list": ["post-87308", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-conversational-ai", "category-generative-ai", "tag-chatbot", "tag-featured", "tag-generative-ai-text", "tag-translation"], "acf": {"post_industry": ["Consumer Internet", "Retail / Consumer Packaged Goods"], "post_products": ["AI Foundation Models", "NIM"], "post_learning_levels": ["Beginner Technical"], "post_content_types": ["Announcement", "News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/Mistral-Large-2407.jpeg", "jetpack_shortlink": "https://wp.me/pcCQAL-mIc", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87308"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/492"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87308"}], "version-history": [{"count": 2, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87308/revisions"}], "predecessor-version": [{"id": 87322, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87308/revisions/87322"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87309"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87308"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87308"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87308"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87308"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87143, "date": "2024-08-12T14:53:11", "date_gmt": "2024-08-12T21:53:11", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87143"}, "modified": "2024-08-22T11:25:00", "modified_gmt": "2024-08-22T18:25:00", "slug": "elevating-video-communication-with-the-nvidia-maxine-ai-developer-platform-and-videorequest", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/elevating-video-communication-with-the-nvidia-maxine-ai-developer-platform-and-videorequest/", "title": {"rendered": "Elevating Video Communication with the NVIDIA Maxine AI Developer Platform and VideoRequest"}, "content": {"rendered": "\n<p>Effective video communication is important for everyone who communicates online. For businesses, educators, and content creators, it is vital.</p>\n\n\n\n<p><a href=\"https://developer.nvidia.com/maxine\">NVIDIA Maxine</a> is a suite of NVIDIA-accelerated SDKs, cloud-native containerized NVIDIA NIM microservices for deploying AI features that enhance real-time audio and video for video conferencing, digital humans, virtual presence, and content creation. Maxine continues to empower you to push the boundaries of what&#8217;s possible in intelligent real-time video enhancement.&nbsp;</p>\n\n\n\n<p>In this post, we showcase how <a href=\"https://www.nvidia.com/en-us/startups/\">NVIDIA Inception program</a> partner <a href=\"https://videorequest.io/\">VideoRequest</a> uses the Maxine AI developer platform to bring sophisticated, real-time, video-editing tools to their platform, enabling end users to focus on the message they want to send and not the video settings or processing tools.</p>\n\n\n\n<h2 id=\"transforming_video_creation_with_ai\"  class=\"wp-block-heading\">Transforming video creation with AI<a href=\"#transforming_video_creation_with_ai\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>VideoRequest is an AI-enhanced video service where users can easily create, edit, and share videos for testimonials, advocacy, marketing, training, or events. The company\u2019s built-in video editor and integrations to social media provide users with asynchronous video communication in a single easy-to-use service. VideoRequest\u2019s customers include associations, nonprofits, and higher education groups that rely on authentic communication and engagement.&nbsp;</p>\n\n\n\n<h2 id=\"simplifying_video_creation_and_editing\"  class=\"wp-block-heading\">Simplifying video creation and editing<a href=\"#simplifying_video_creation_and_editing\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Maintaining eye contact is a critical aspect of increasing authenticity, to create more engagement and deliver a polished result. Presenters looking at a script or focusing away from the camera appear less connected with their audience. Distracting backgrounds are just that\u2014distracting!&nbsp;</p>\n\n\n\n<p>Maxine enabled VideoRequest to solve these problems for users with state-of-the-art AI models and features:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Maxine Eye Contact</li>\n\n\n\n<li>Maxine Virtual Background</li>\n</ul>\n\n\n\n<h3 id=\"maxine_eye_contact\"  class=\"wp-block-heading\">Maxine Eye Contact<a href=\"#maxine_eye_contact\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>This feature uses AI to redirect the user&#8217;s gaze so that they appear to be looking directly at the camera in a natural way. Direct eye contact creates a more engaging and personal connection with viewers.</p>\n\n\n\n<p>Eye contact is an optional feature within VideoRequest, but since integration, it\u2019s become VideoRequest\u2019s most popular feature.</p>\n\n\n\n<p>\u201cIntegration with NVIDIA Maxine enables cutting-edge AI enhancements with an intuitive user experience. Most of our users look at themselves on the screen, or a script, rather than the camera. Eye Contact instantly warms up their video submissions and makes them more personal.&nbsp;This is especially important for impact statements, testimonials, sales pitches, and appeals videos,\u201d said Terry Nawrot, CEO of VideoRequest.</p>\n\n\n\n<h3 id=\"maxine_virtual_background\"  class=\"wp-block-heading\">Maxine Virtual Background<a href=\"#maxine_virtual_background\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h3>\n\n\n\n<p>Users can easily replace their original background with a virtual one, providing a professional look regardless of their actual physical environment.</p>\n\n\n\n<p>\u201cVideoRequest applies the background replacement filter to every video that our users create. This way it makes it quick for users to swap out backgrounds whenever they choose,\u201d Terry Nawrot explained.</p>\n\n\n\n<p>According to Nawrot, their previous system would take 9x the video duration for processing time to do background replacement with other software. A 1-minute video would take 9 minutes of processing time. With the NVIDIA real-time solution, the same task takes about 0.8x the duration, so a 1-minute video takes under 50 seconds of processing time using the NVIDIA Triton release on Linux.</p>\n\n\n\n<p>Through the 10-month partnership with NVIDIA, the VideoRequest team has seen a massive speed improvement, as well as quality enhancements in other features adopted from NVIDIA.</p>\n\n\n\n<h2 id=\"real-world_impact\"  class=\"wp-block-heading\">Real-world impact<a href=\"#real-world_impact\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The integration of NVIDIA Maxine into VideoRequest&#8217;s platform has already shown significant benefits for users across various industries, including online communities and marketing.</p>\n\n\n\n<p>Communities require strong, lasting relationships with their members. However, in the absence of frequent in-person events and interactions, they rely on video communication to engage, attract, and retain their members.&nbsp;</p>\n\n\n\n<p>VideoRequest\u2019s partners see a lot of value in using Eye Contact, which elevates engagement and authenticity in the videos, and therefore increases retention. It elevates the quality of their testimonial videos to a level they can use in marketing, despite having been created by regular people who normally look away from the camera to read a script.</p>\n\n\n\n<p>Beth Arritt, president and CEO of The Arritt Group, found that using Maxine&#8217;s Eye Contact in VideoRequest significantly enhanced customer testimonials. &#8220;With eye contact, the video now exudes confidence and authenticity.\u201d</p>\n\n\n\n<p>In the following video, you can see a before-and-after version of a testimonial.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/cj9vk-UsNH8?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Experience NVIDIA Maxine Eye Contact</em></figcaption></figure>\n\n\n\n<p>The final video output has both Eye Contact and Virtual Background for a polished result.&nbsp;</p>\n\n\n\n<h2 id=\"looking_ahead\"  class=\"wp-block-heading\">Looking ahead<a href=\"#looking_ahead\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>From enhancing day-to-day video conferencing to integrating AI technology, <a href=\"https://developer.nvidia.com/maxine\">NVIDIA Maxine</a> offers high-quality video communications for all professionals.</p>\n\n\n\n<p>The latest Maxine production release is included exclusively with <a href=\"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/\">NVIDIA AI Enterprise</a>, which enables you to tap into production-ready features such as NVIDIA Triton Inference Server, enterprise support, and more.</p>\n\n\n\n<p>As NVIDIA continues to develop and refine Maxine\u2019s real-time video and audio AI features, we&#8217;re excited to see startup partners like VideoRequest simplify high-quality video creation, making it accessible to businesses and individuals alike.</p>\n\n\n\n<p>With the growth in \u200ccontent creation and the need for video in marketing, communication, education, and business, the partnership between NVIDIA and VideoRequest is poised to drive innovation and improve video communication experiences for users, especially as the VideoRequest team is working on integrating more Maxine features soon.</p>\n\n\n\n<p>For more information about Maxine and how it can enhance your video applications, see the <a href=\"https://developer.nvidia.com/maxine\">NVIDIA Maxine</a> developer page.&nbsp;If you\u2019re interested in early access, with non-production access to production and soon-to-be-released features, see the <a href=\"https://developer.nvidia.com/maxine-microservice-early-access\">Maxine Early Access program</a> (requires login).</p>\n\n\n\n<p>For more information about the VideoRequest AI-enhanced video creation platform, see <a href=\"https://videorequest.io/\">VideoRequest</a>. For more information about Maxine features, see <a href=\"https://developer.nvidia.com/blog/advancing-telepresence-and-next-generation-digital-humans-with-nvidia-maxine/\">Advancing Telepresence and Next-Generation Digital Humans with NVIDIA Maxine</a> and get a preview of the Eye Contact NIM microservice in the <a href=\"https://build.nvidia.com/nvidia/eyecontact\">NVIDIA API Catalog</a>.</p>\n\n\n\n<p>To help improve features in upcoming releases, provide feedback with the <a href=\"https://crowdsource.nvidia.com/en-us/maxine/\">NVIDIA Maxine and NVIDIA Broadcast App</a> survey.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Effective video communication is important for everyone who communicates online. For businesses, educators, and content creators, it is vital. NVIDIA Maxine is a suite of NVIDIA-accelerated SDKs, cloud-native containerized NVIDIA NIM microservices for deploying AI features that enhance real-time audio and video for video conferencing, digital humans, virtual presence, and content creation. Maxine continues to &hellip; <a href=\"https://developer.nvidia.com/blog/elevating-video-communication-with-the-nvidia-maxine-ai-developer-platform-and-videorequest/\">Continued</a></p>\n", "protected": false}, "author": 2232, "featured_media": 87152, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1466030", "discourse_permalink": "https://forums.developer.nvidia.com/t/elevating-video-communication-with-the-nvidia-maxine-ai-developer-platform-and-videorequest/303185", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [1235], "tags": [453, 1950, 2057], "coauthors": [3961, 3811], "class_list": ["post-87143", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-graphics", "tag-featured", "tag-image-recognition", "tag-maxine"], "acf": {"post_industry": ["Media & Entertainment"], "post_products": ["Maxine"], "post_learning_levels": ["General Interest"], "post_content_types": ["Spotlight"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/maxine-videorequest-featured.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mFx", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Content Creation / Rendering", "link": "https://developer.nvidia.com/blog/category/graphics/", "id": 1235}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87143"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2232"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87143"}], "version-history": [{"count": 4, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87143/revisions"}], "predecessor-version": [{"id": 87510, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87143/revisions/87510"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87152"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87143"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87143"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87143"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87143"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 86936, "date": "2024-08-12T08:54:26", "date_gmt": "2024-08-12T15:54:26", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=86936"}, "modified": "2024-10-21T09:26:33", "modified_gmt": "2024-10-21T16:26:33", "slug": "interactive-ai-tool-delivers-immersive-video-content-to-blind-and-low-vision-viewers", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/interactive-ai-tool-delivers-immersive-video-content-to-blind-and-low-vision-viewers/", "title": {"rendered": "Interactive AI Tool Delivers Immersive Video Content to Blind and Low-Vision Viewers"}, "content": {"rendered": "\n<p>New research aims to revolutionize video accessibility for blind or low-vision (BLV) viewers with an AI-powered system that gives users the ability to explore content interactively. The innovative system, detailed in a recent <a href=\"https://doi.org/10.1145/3613904.3642632\">paper</a>, addresses significant gaps in conventional audio descriptions (AD), offering an enriched and immersive video viewing experience.</p>\n\n\n\n<p>\u201cAlthough videos have become an important medium to access information and entertain, BLV people often find them less accessible,\u201d said lead author Zheng Ning, a PhD in Computer Science and Engineering at the University of Notre Dame.  &#8220;With AI, we can build an interactive system to extract layered information from videos and enable users to take an active role in consuming video content through their limited vision, auditory perception, and tactility.\u201d </p>\n\n\n\n<p>ADs provide spoken narration of visual elements in videos and are crucial for accessibility. However, conventional static descriptions often leave out details and focus primarily on providing information that helps users understand the content, rather than experience it. Plus, simultaneously consuming and processing the original sound and the audio from ADs can be mentally taxing, reducing user engagement.</p>\n\n\n\n<p>Researchers from the University of Notre Dame, University of California San Diego, University of Texas at Dallas, and University of Wisconsin-Madison developed a new AI-powered system addressing these challenges.&nbsp;</p>\n\n\n\n<p>Called the System for Providing Interactive Content for Accessibility (SPICA), the tool enables users to interactively explore video content through layered ADs and spatial sound effects.&nbsp;</p>\n\n\n\n<p>The machine learning pipeline begins with scene analysis to identify key frames, followed by object detection and segmentation to pinpoint significant objects within each frame. These objects are then described in detail using a refined image captioning model and GPT-4 for consistency and comprehensiveness.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-video aligncenter\"><video controls src=\"https://developer.download.nvidia.com/video/devblog/SPICA-Demo-video.mp4\"></video><figcaption class=\"wp-element-caption\"><em>Video 1. A demo of SPICA with interactivity for BLV users to explore the video </em>by scrolling over objects</figcaption></figure>\n\n\n\n<p>The pipeline also retrieves spatial sound effects for each object, using their 3D positions to enhance spatial awareness. Depth estimation further refines the 3D positioning of objects, and the frontend interface enables users to explore these frames and objects interactively, using touch or keyboard inputs, with high-contrast overlays aiding those with residual vision.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"5211\" height=\"2571\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline.png\" alt=\"The SPICA workflow.\" class=\"wp-image-86959\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline.png 5211w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline-300x148.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline-625x308.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline-179x88.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline-768x379.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline-1536x758.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline-2048x1010.png 2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline-645x318.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline-500x247.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline-160x79.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline-362x179.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline-223x110.png 223w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-Machine-Learning-Pipeline-1024x505.png 1024w\" sizes=\"(max-width: 5211px) 100vw, 5211px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. The machine learning pipeline consists of several modules for producing layered frame-level descriptions, object-level descriptions, high-contrast color masks, and spatial sound effects</em></figcaption></figure></div>\n\n\n<p>SPICA runs on an <a href=\"https://www.nvidia.com/en-us/design-visualization/rtx-a6000/\">NVIDIA RTX A6000 GPU</a>, which the team was awarded as a recipient of the <a href=\"https://developer.nvidia.com/higher-education-and-research\">NVIDIA Academic Hardware Grant Program</a>. </p>\n\n\n\n<p>\u201cNVIDIA technology is a crucial component behind the system, offering a stable and efficient platform for running these computational models, significantly reducing the time and effort to implement the system,\u201d said Ning.&nbsp;</p>\n\n\n\n<p>This advanced integration of computer vision and natural language processing techniques enables BLV users to engage with video content in a more detailed, flexible, and immersive way. Rather than being given predefined ADs per frame, users actively explore individual objects within the frame through a touch interface or a screen reader.</p>\n\n\n\n<p>SPICA also augments existing ADs with interactive elements, spatial sound effects, and detailed object descriptions, all generated through an audio-visual machine-learning pipeline.</p>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/iopuy6LPq3I?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em>Video 2. SPICA is an AI-powered system that enables BLV users to interactively explore video content</em></figcaption></figure>\n\n\n\n<p>During the development of SPICA, the researchers used BLV video consumption studies to align the system with user needs and preferences. The team conducted a user study with 14 BLV participants to evaluate usability and usefulness. The participants found the system easy to use and effective in providing additional information that improved their understanding and immersion in video content.</p>\n\n\n\n<p>According to the researchers, the insights gained from the user study highlight the potential for further research, including improving AI models for accurate and contextually rich generated descriptions. Additionally, there&#8217;s potential for exploring using haptic feedback and other sensory channels to augment video consumption for BLV users.&nbsp;</p>\n\n\n\n<p>The team plans to pursue future research using AI to help BLV individuals with physical tasks in their daily lives, seeing potential with recent breakthroughs in large generative models.</p>\n\n\n\n<p>Learn more about <a href=\"https://sites.google.com/nd.edu/spica\">SPICA</a>.<br>Read the <a href=\"https://doi.org/10.1145/3613904.3642632\">research</a> paper.</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\"/>\n\n\n\n<p class=\"has-small-font-size\"><em>This content was partially crafted with the assistance of generative AI and LLMs. It underwent careful review by the researchers and was edited by the NVIDIA Technical Blog team to ensure precision, accuracy, and quality. Quotes are original.</em></p>\n", "protected": false}, "excerpt": {"rendered": "<p>New research aims to revolutionize video accessibility for blind or low-vision (BLV) viewers with an AI-powered system that gives users the ability to explore content interactively. The innovative system, detailed in a recent paper, addresses significant gaps in conventional audio descriptions (AD), offering an enriched and immersive video viewing experience. \u201cAlthough videos have become an &hellip; <a href=\"https://developer.nvidia.com/blog/interactive-ai-tool-delivers-immersive-video-content-to-blind-and-low-vision-viewers/\">Continued</a></p>\n", "protected": false}, "author": 1115, "featured_media": 86963, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1465916", "discourse_permalink": "https://forums.developer.nvidia.com/t/interactive-ai-tool-delivers-immersive-video-content-to-blind-and-low-vision-viewers/303163", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [2724, 3110, 1903], "tags": [3941, 453, 2932, 4125], "coauthors": [2315], "class_list": ["post-86936", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-computer-vision", "category-generative-ai", "category-features", "tag-ai-impact", "tag-featured", "tag-large-language-models", "tag-nvidia-academic-grant-program"], "acf": {"post_industry": ["General"], "post_products": ["General"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/SPICA-touchscreen-demo.gif", "jetpack_shortlink": "https://wp.me/pcCQAL-mCc", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86936"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1115"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=86936"}], "version-history": [{"count": 25, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86936/revisions"}], "predecessor-version": [{"id": 88661, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86936/revisions/88661"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/86963"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=86936"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=86936"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=86936"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=86936"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87063, "date": "2024-08-12T07:00:00", "date_gmt": "2024-08-12T14:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87063"}, "modified": "2024-08-22T11:25:32", "modified_gmt": "2024-08-22T18:25:32", "slug": "nvidia-nvlink-and-nvidia-nvswitch-supercharge-large-language-model-inference", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/nvidia-nvlink-and-nvidia-nvswitch-supercharge-large-language-model-inference/", "title": {"rendered": "NVIDIA NVLink and NVIDIA NVSwitch Supercharge Large Language Model Inference"}, "content": {"rendered": "\n<p><a href=\"https://www.nvidia.com/en-us/glossary/large-language-models/\">Large language models</a> (LLM) are getting larger, increasing the amount of compute required to process inference requests. To meet real-time latency requirements for serving today\u2019s LLMs and do so for as many users as possible, multi-GPU compute is a must. Low latency improves the user experience.&nbsp; High throughput reduces the cost of service.&nbsp; Both are simultaneously important.</p>\n\n\n\n<p>Even if a large model can fit in the memory of a single state-of-the-art GPU, the rate at which that GPU can generate tokens depends on the total compute available to process requests. By combining the compute capabilities of multiple cutting-edge GPUs, real-time user experiences on the latest models are possible.&nbsp;</p>\n\n\n\n<p>To understand the need for high tokens per second, the following GIFs show two scenarios:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>5 tokens/second:</strong> Below typical human reading speed and not real-time.</li>\n\n\n\n<li><strong>50 tokens/second:</strong> An excellent user experience.&nbsp;</li>\n</ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/5-tokens-second-macbeth.gif\"><img loading=\"lazy\" decoding=\"async\" width=\"558\" height=\"642\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/5-tokens-second-macbeth.gif\" alt=\"GIF displays three lines of a Shakespeare quote from Macbeth with words appearing one at a time.\" class=\"wp-image-87070\"/></a><figcaption class=\"wp-element-caption\"><em>Figure 1. 5 tokens/second output example</em></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"706\" height=\"814\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/50-tokens-second-macbeth.gif\" alt=\"GIF displays 20 lines of a Shakespeare quote from Macbeth with entire lines appearing quickly.\" class=\"wp-image-87071\"/><figcaption class=\"wp-element-caption\"><em>Figure 2. 50 tokens/second output example</em></figcaption></figure></div>\n\n\n<p>By using the combined compute performance of multiple GPUs with techniques such as tensor parallelism (TP) to run large models, inference requests can be processed quickly enough to enable real-time responses. By carefully selecting the number of GPUs used to run a model, cloud inference services can also simultaneously optimize both user experience and cost.&nbsp;</p>\n\n\n\n<p>For more information about parallelism techniques to balance user experience, see <a href=\"https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/\">Demystifying AI Inference Deployments for Trillion Parameter Large Language Models</a>.&nbsp;</p>\n\n\n\n<h2 id=\"multi-gpu_inference_is_communication-intensive\"  class=\"wp-block-heading\">Multi-GPU inference is communication-intensive<a href=\"#multi-gpu_inference_is_communication-intensive\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Multi-GPU TP inference works by splitting the calculation of each model layer across two, four, or even eight GPUs in a server. In theory, two GPUs could run a model 2x faster, four GPUs 4x faster, and eight GPUs 8x faster.&nbsp;</p>\n\n\n\n<p>However, each GPU cannot complete their work independently. After each GPU completes the execution of its portion of the model layer, every GPU must send the results of the calculations to every other GPU, performing an all-to-all reduction. Only then can inference execution proceed to the next model layer.</p>\n\n\n\n<p>Minimizing the time spent communicating results between GPUs is critical, as during this communication, Tensor Cores often remain idle, waiting for data to continue processing.&nbsp;</p>\n\n\n\n<p>During this communication step, a large amount of data must be transferred. A single query to Llama 3.1 70B (8K input tokens and 256 output tokens) requires that up to 20 GB of TP synchronization data be transferred from each GPU. As multiple queries are processed in parallel through batching to improve inference throughput, the amount of data transferred increases by multiples.&nbsp;</p>\n\n\n\n<p>This is why a high-bandwidth GPU-to-GPU interconnect is essential for multi-GPU inference.&nbsp;</p>\n\n\n\n<h2 id=\"nvswitch_is_critical_for_fast_multi-gpu_llm_inference\"  class=\"wp-block-heading\">NVSwitch is critical for fast multi-GPU LLM inference<a href=\"#nvswitch_is_critical_for_fast_multi-gpu_llm_inference\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>For good multi-GPU scaling, an AI server first requires GPUs with excellent per-GPU interconnect bandwidth. It must also provide fast connectivity to enable all GPUs to exchange data with all other GPUs as quickly as possible.</p>\n\n\n\n<p>The <a href=\"https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/\">NVIDIA Hopper Architecture</a> GPU can communicate at 900 GB/s with fourth-generation NVLink. With the NVSwitch, every NVIDIA Hopper GPU in a server can communicate at 900 GB/s with any other NVIDIA Hopper GPU simultaneously. </p>\n\n\n\n<p>The peak rate does not depend on the number of GPUs that are communicating. That is, the NVSwitch is non-blocking. Every NVIDIA HGX H100 and NVIDIA HGX H200 system with eight GPUs features four third-generation NVSwitch chips.&nbsp;The total bidirectional bandwidth of each NVSwitch chip is a staggering 25.6 terabits per second.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"281\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVSwitch-image-fixed-1-625x281.png\" alt=\"Picture of the NVIDIA Hopper Architecture GPU with a callout showing the four NVSwitch chips.\" class=\"wp-image-87426\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVSwitch-image-fixed-1-625x281.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVSwitch-image-fixed-1-300x135.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVSwitch-image-fixed-1-179x80.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVSwitch-image-fixed-1-768x345.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVSwitch-image-fixed-1-645x290.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVSwitch-image-fixed-1-500x225.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVSwitch-image-fixed-1-160x72.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVSwitch-image-fixed-1-362x163.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVSwitch-image-fixed-1-245x110.png 245w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/NVSwitch-image-fixed-1.png 993w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. HGX H200 8-GPU with four NVIDIA NVSwitch devices</em></figcaption></figure></div>\n\n\n<p>For comparison, consider a hypothetical server with eight H200 GPUs without NVSwitch that instead uses point-to-point connections on the server motherboard (Figure 4).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"467\" height=\"599\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/gpu-to-gpu-bandwidth-nvswitch-comparison-b.png\" alt=\"Diagram shows 8 GPUs on the top, each with links going to every other GPU. On the bottom, 8 GPUs are connected to each other with a centralized NVSwitch.\" class=\"wp-image-87158\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/gpu-to-gpu-bandwidth-nvswitch-comparison-b.png 467w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/gpu-to-gpu-bandwidth-nvswitch-comparison-b-234x300.png 234w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/gpu-to-gpu-bandwidth-nvswitch-comparison-b-90x115.png 90w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/gpu-to-gpu-bandwidth-nvswitch-comparison-b-70x90.png 70w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/gpu-to-gpu-bandwidth-nvswitch-comparison-b-362x464.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/gpu-to-gpu-bandwidth-nvswitch-comparison-b-86x110.png 86w\" sizes=\"(max-width: 467px) 100vw, 467px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. G<em>PU-to-GPU bandwidth with and without NVSwitch all-to-all switch topology</em></em></figcaption></figure></div>\n\n\n<p>In the point-to-point design, though it is a lower system cost without four high-speed switches, each GPU must split the same 900 GB/s connectivity into seven dedicated 128 GB/s point-to-point connections, each connecting to one of the other GPUs in the system. This means that the speed at which GPUs can communicate depends on the number of GPUs that are communicating.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>GPU Count</strong></td><td><strong>Point-to-Point Bandwidth</strong></td><td><strong>NVSwitch Bandwidth</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">2&nbsp;</td><td>128 GB/s</td><td>900 GB/s</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">4&nbsp;</td><td>3 x 128 GB/s</td><td>900 GB/s</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">8&nbsp;</td><td>7 x 128 GB/s</td><td>900 GB/s</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1. GPU-to-GPU bandwidth comparison</em></figcaption></figure>\n\n\n\n<p>Table 1 shows a GPU-to-GPU bandwidth comparison between GPUs connected through a point-to-point interconnect and GPUs connected with NVSwitch. </p>\n\n\n\n<p>For models that only require two GPUs for the best balance of user experience and cost, such as Llama 3.1 70B, a point-to-point architecture only provides 128 GB/s of bandwidth. 20 GB of data would consume 150 ms to perform just one of the many all-to-all reductions.&nbsp;With high communication overhead, Amdahl\u2019s Law limits the speed-up possible with each additional GPU.</p>\n\n\n\n<p>Meanwhile, the system using NVSwitch would provide the full 900 GB/s of bandwidth, taking only 22 ms to transfer 20 GB, dramatically reducing the time spent during GPU-to-GPU communication. This has a significant impact on overall inference throughput and user experience.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison.png\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"611\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison-625x611.png\" alt=\"On the top of the diagram are two GPUs connected with a small green line, with an indicator that communication makes up a large portion of the execution time. On the bottom, two GPUs are connected via NVSwitch, with communication making up a small portion of the execution time.\u00a0\" class=\"wp-image-87068\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison-625x611.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison-300x293.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison-118x115.png 118w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison-645x631.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison-307x300.png 307w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison-92x90.png 92w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison-32x32.png 32w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison-50x50.png 50w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison-64x64.png 64w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison-362x354.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison-113x110.png 113w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/multi-gpu-communication-nvswitch-comparison.png 763w\" sizes=\"(max-width: 625px) 100vw, 625px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 5. Multi-GPU communication with and without NVSwitch</em></figcaption></figure></div>\n\n\n<p>Cloud services often set fixed response time budgets for model serving, to provide good end-user experiences. This typically means being able to generate tokens faster than human reading speed. To maximize throughput and decrease serving costs, requests are batched as high as possible while maintaining the response time. </p>\n\n\n\n<p>Table 2 shows the measured Llama 3.1 70B throughput at various real-time response time budgets from 30-50 tokens/s/user.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\"><br><strong>Real-time Response Budget</strong> <strong>tok/s/user</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"3\"><strong>Throughput </strong><strong>tok/s/GPU (batch size)</strong></td><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\"><strong>NVSwitch&nbsp;</strong> <strong>Benefit</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Single GPU</strong> <strong>TP=1</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Point-to-Point</strong> <strong>TP=2</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>NVSwitch</strong> <strong>TP=2</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">30</td><td class=\"has-text-align-center\" data-align=\"center\">67 (2)</td><td class=\"has-text-align-center\" data-align=\"center\">80 (6)</td><td class=\"has-text-align-center\" data-align=\"center\">115 (9)</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.4x</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">35</td><td class=\"has-text-align-center\" data-align=\"center\">Does Not Meet</td><td class=\"has-text-align-center\" data-align=\"center\">74 (5)</td><td class=\"has-text-align-center\" data-align=\"center\">104 (7)</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.4x</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">40</td><td class=\"has-text-align-center\" data-align=\"center\">Does Not Meet</td><td class=\"has-text-align-center\" data-align=\"center\">67 (4)</td><td class=\"has-text-align-center\" data-align=\"center\">87 (5)</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.3x</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">45</td><td class=\"has-text-align-center\" data-align=\"center\">Does Not Meet</td><td class=\"has-text-align-center\" data-align=\"center\">56 (3)</td><td class=\"has-text-align-center\" data-align=\"center\">76 (4)</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.4x</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">50</td><td class=\"has-text-align-center\" data-align=\"center\">Does Not Meet</td><td class=\"has-text-align-center\" data-align=\"center\">43 (2)</td><td class=\"has-text-align-center\" data-align=\"center\">63 (3)</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.5x</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em><em>Table 2. Throughput and NVSwitch benefit for Llama 3.1 70B inference at various real-time user experience targets with batch sizes</em></em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\"><em>Throughput modeled using internal measurements. H200 GPU, ISL/OSL = 8k/256.&nbsp;</em></p>\n\n\n\n<p>As Table 2 shows, a single GPU configuration (TP=1) is challenged to achieve real-time performance. Splitting the model using tensor parallel across two GPUs combines the compute resources of both GPUs to achieve high throughput across a wide range of real-time experience budgets. Real-time inference throughput on NVIDIA H200 GPUs with TP=2 and NVSwitch is up to 1.5x greater than a comparable GPU without NVSwitch.</p>\n\n\n\n<p>To show how NVSwitch benefits scenarios with greater GPU-to-GPU communication traffic, Table 3 shows overall server throughput at fixed batch sizes. Larger batch sizes mean that requests from an increasing number of users can be processed at one time, improving overall server utilization and reducing cost per inference.&nbsp;</p>\n\n\n\n<figure class=\"wp-block-table aligncenter is-style-stripes\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\"><br><strong>Batch Size</strong></td><td class=\"has-text-align-center\" data-align=\"center\" colspan=\"2\"><strong>Throughput</strong> <strong>tok/s/GPU</strong></td><td class=\"has-text-align-center\" data-align=\"center\" rowspan=\"2\"><br><strong>NVSwitch&nbsp;</strong><strong>Benefit</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Point-to-Point</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>NVSwitch</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">1</td><td class=\"has-text-align-center\" data-align=\"center\">25</td><td class=\"has-text-align-center\" data-align=\"center\">26</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.0x</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">2</td><td class=\"has-text-align-center\" data-align=\"center\">44</td><td class=\"has-text-align-center\" data-align=\"center\">47</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.1x</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">4</td><td class=\"has-text-align-center\" data-align=\"center\">66</td><td class=\"has-text-align-center\" data-align=\"center\">76</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.2x</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">8</td><td class=\"has-text-align-center\" data-align=\"center\">87</td><td class=\"has-text-align-center\" data-align=\"center\">110</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.3x</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">16</td><td class=\"has-text-align-center\" data-align=\"center\">103</td><td class=\"has-text-align-center\" data-align=\"center\">142</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.4x</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">32</td><td class=\"has-text-align-center\" data-align=\"center\">112</td><td class=\"has-text-align-center\" data-align=\"center\">168</td><td class=\"has-text-align-center\" data-align=\"center\"><strong>1.5x</strong></td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 3. Throughput and NVSwitch benefit for Llama 3.1 70B inference at various fixed-batch sizes</em></figcaption></figure>\n\n\n\n<p class=\"has-small-font-size\"><em>Throughput modeled using internal measurements. H200 GPU, TP=2, ISL/OSL = 8K/256.&nbsp;</em></p>\n\n\n\n<p>As batch size increases, GPU-to-GPU traffic increases, as does the benefit provided by NVSwitch compared to a point-to-point topology. However, even at relatively modest batch sizes, the gains can be significant.&nbsp;&nbsp;</p>\n\n\n\n<h2 id=\"continued_nvlink_innovation_for_trillion-parameter_model_inference\"  class=\"wp-block-heading\">Continued NVLink innovation for trillion-parameter model inference<a href=\"#continued_nvlink_innovation_for_trillion-parameter_model_inference\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>NVLink and NVSwitch provide high bandwidth communication between GPUs based on the NVIDIA Hopper architecture and provide significant benefits for real-time, cost-effective large model inference today.&nbsp;</p>\n\n\n\n<p>As model sizes continue to grow, NVIDIA continues to innovate with both NVLink and NVSwitch to push the boundaries of real-time inference performance for even larger NVLink domains. </p>\n\n\n\n<p>The NVIDIA Blackwell architecture features fifth-generation NVLink, which doubles per-GPU NVLink speeds to 1,800 GB/s. For Blackwell, a new NVSwitch chip and NVLink switch trays have also been introduced to enable even larger NVLink domain sizes.</p>\n\n\n\n<p>The NVIDIA GB200 NVL72 system connects 36 NVIDIA Grace CPUs and 72 NVIDIA Blackwell GPUs in a rack-scale design, and with the fifth-generation NVLink, enables all 72 GPUs to act as a single GPU, enabling 30x faster real-time trillion-parameter inference compared to the prior generation.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>Large language models (LLM) are getting larger, increasing the amount of compute required to process inference requests. To meet real-time latency requirements for serving today\u2019s LLMs and do so for as many users as possible, multi-GPU compute is a must. Low latency improves the user experience.&nbsp; High throughput reduces the cost of service.&nbsp; Both are &hellip; <a href=\"https://developer.nvidia.com/blog/nvidia-nvlink-and-nvidia-nvswitch-supercharge-large-language-model-inference/\">Continued</a></p>\n", "protected": false}, "author": 2128, "featured_media": 87065, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1465859", "discourse_permalink": "https://forums.developer.nvidia.com/t/nvidia-nvlink-and-nvidia-nvswitch-supercharge-large-language-model-inference/303149", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 3110], "tags": [296, 453, 2932, 49], "coauthors": [3849, 2940, 2732, 3020, 3848], "class_list": ["post-87063", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-generative-ai", "tag-ai-inference-microservices", "tag-featured", "tag-large-language-models", "tag-multi-gpu"], "acf": {"post_industry": ["Hardware / Semiconductor"], "post_products": ["NVLink", "NVSwitch"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Benchmark"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/nvlink-nvswitch-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mEf", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Generative AI", "link": "https://developer.nvidia.com/blog/category/generative-ai/", "id": 3110}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87063"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/2128"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87063"}], "version-history": [{"count": 16, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87063/revisions"}], "predecessor-version": [{"id": 87461, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87063/revisions/87461"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87065"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87063"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87063"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87063"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87063"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 87019, "date": "2024-08-09T09:00:00", "date_gmt": "2024-08-09T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=87019"}, "modified": "2024-08-22T11:25:33", "modified_gmt": "2024-08-22T18:25:33", "slug": "rapids-cudf-unified-memory-accelerates-pandas-up-to-30x-on-large-datasets", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/rapids-cudf-unified-memory-accelerates-pandas-up-to-30x-on-large-datasets/", "title": {"rendered": "RAPIDS cuDF Unified Memory Accelerates pandas up to 30x on Large Datasets"}, "content": {"rendered": "\n<p>NVIDIA has released RAPIDS cuDF unified memory and text data processing features that help data scientists continue to use pandas when working with larger and text-heavy datasets in demanding workloads. Data scientists can now accelerate these workloads by up to 30x.</p>\n\n\n\n<p><a href=\"https://rapids.ai/\">RAPIDS</a> is a collection of open-source GPU-accelerated data science and AI libraries. <a href=\"https://docs.rapids.ai/api/cudf/stable/user_guide/\">cuDF</a> is a Python GPU DataFrame library for loading, joining, aggregating, and filtering data.</p>\n\n\n\n<p>A top data processing library for data scientists, <a href=\"https://www.nvidia.com/en-us/glossary/pandas-python/\">pandas</a> is a flexible and powerful data analysis and manipulation library for Python. It was downloaded more than 200 million times in the last month. However, as dataset sizes grow, pandas struggles with processing speed and efficiency in CPU-only systems. This forces data scientists to choose between slow execution times and switching costs associated with using other tools.&nbsp;</p>\n\n\n\n<p>NVIDIA announced at GTC 2024 that <a href=\"https://developer.nvidia.com/blog/rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes/\">RAPIDS cuDF accelerates pandas nearly 150x with zero code changes</a>. Google then announced that <a href=\"https://developer.nvidia.com/blog/rapids-cudf-instantly-accelerates-pandas-up-to-50x-on-google-colab/\">RAPIDS cuDF is available by default on Colab at Google I/O</a>, making pandas code acceleration more accessible than ever before.</p>\n\n\n\n<p>While this momentum is exciting, user feedback highlighted limitations in the size and types of workloads that can use RAPIDS cuDF pandas acceleration. More specifically:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>To maximize acceleration, datasets need to fit within GPU memory, with ample space for group-by and joins operations that spike memory requirements for workloads. This requirement has significantly limited the amount of data and operational complexity of the pandas code that users can apply acceleration to, especially on lower-memory GPUs.&nbsp;</li>\n\n\n\n<li>Users also quickly ran into processing constraints with text-heavy datasets in the original release of cuDF, which supported up to 2.1 billion characters in a column. This limit can be easily reached with standard customer logs, product reviews, and location-dense datasets. This created limitations when preprocessing large strings for LLMs and similar generative AI use cases.</li>\n</ul>\n\n\n\n<p>To address this feedback, the latest release of RAPIDS cuDF pandas accelerator mode includes:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>A built-in optimized <a href=\"https://developer.nvidia.com/blog/unified-memory-cuda-beginners/\">CUDA unified memory</a> feature that optimizes memory utilization of the CPU+GPU system to enable up to 30x speedups of larger datasets and more complex workloads.&nbsp;</li>\n\n\n\n<li>Expanded string support from 2.1 billion characters in a column to 2.1 billion rows of tabular text data to support data preprocessing text-heavy data used in LLMs and other demanding use cases.</li>\n</ul>\n\n\n\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/AgFVwqDcXCs?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\n</div><figcaption class=\"wp-element-caption\"><em><em>Video 1. Watch RAPIDS cuDF accelerate pandas code processing tabular text data&nbsp;</em></em></figcaption></figure>\n\n\n\n<p>With this release, you can efficiently accelerate larger datasets and text-heavy use cases on the GPU, making cuDF a better fit for diverse and demanding workflows.</p>\n\n\n\n<h2 id=\"accelerated_data_processing_with_unified_memory&nbsp;\"  class=\"wp-block-heading\">Accelerated data processing with unified memory&nbsp;<a href=\"#accelerated_data_processing_with_unified_memory&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>To enable zero code change acceleration and a seamless experience, <a href=\"https://docs.rapids.ai/api/cudf/stable/cudf_pandas/how-it-works/\">cuDF relies on CPU fallback</a>. When the memory requirements of a workflow exceed GPU memory, cuDF transfers the data into CPU memory and uses pandas to process the data.</p>\n\n\n\n<p>Because common operations like joins and group-by aggregations spike memory higher than the size of the input data, avoiding CPU fallback when processing large datasets requires input data to leave significant space in GPU memory.&nbsp;</p>\n\n\n\n<p>To address this, cuDF now uses CUDA unified memory to enable scaling pandas workloads beyond GPU memory. Unified memory provides a single address space spanning the CPUs and GPUs in your system, enables virtual memory allocations larger than available GPU memory (oversubscription), and migrates data in and out of GPU memory as needed (paging).&nbsp;</p>\n\n\n\n<p>However, using unified memory can result in repeated page faults when data is needed but isn\u2019t on the GPU, which can slow down performance. To maximize performance, cuDF includes optimizations to help ensure data is resident on the GPU before CUDA kernels need it. This enables you to take advantage of complete system memory to scale beyond GPU memory and continue to leverage the benefits of accelerated computing. To achieve peak acceleration, datasets should still be sized to fit entirely in GPU memory.&nbsp;</p>\n\n\n\n<p>As shown in the benchmark for accelerated pandas outside of GPU memory (Figure 1), running data processing workloads with a 10 GB dataset using cuDF achieved up to 30x speedups for data joins on a 16 GB memory GPU compared to CPU-only pandas. Previously, processing datasets larger than 4 GB would have slowed performance because of GPU memory constraints. Reminder: the maximum GPU memory utilized by a workload during its execution might exceed the input dataset size.</p>\n\n\n\n<div><figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"778\" height=\"248\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cudf-pandas-duckdb-benchmark-performance-comparison-1.png\" alt=\"Bar chart shows 30x speed increase for a join function and a 5x speed increase for advanced group-by operations using pandas with RAPIDS cuDF on NVIDIA T4 GPUs.\" class=\"wp-image-87034\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cudf-pandas-duckdb-benchmark-performance-comparison-1.png 778w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cudf-pandas-duckdb-benchmark-performance-comparison-1-300x96.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cudf-pandas-duckdb-benchmark-performance-comparison-1-625x199.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cudf-pandas-duckdb-benchmark-performance-comparison-1-179x57.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cudf-pandas-duckdb-benchmark-performance-comparison-1-768x245.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cudf-pandas-duckdb-benchmark-performance-comparison-1-645x206.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cudf-pandas-duckdb-benchmark-performance-comparison-1-500x159.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cudf-pandas-duckdb-benchmark-performance-comparison-1-160x51.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cudf-pandas-duckdb-benchmark-performance-comparison-1-362x115.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/cudf-pandas-duckdb-benchmark-performance-comparison-1-345x110.png 345w\" sizes=\"(max-width: 778px) 100vw, 778px\" /><figcaption class=\"wp-element-caption\"><em><em>Figure 1. Modified DuckDB Data Benchmark (10 GB) performance comparison between cuDF pandas and traditional pandas v2.2</em></em></figcaption></figure></div>\n\n\n\n<p class=\"has-text-align-center has-small-font-size\"><em>HW: NVIDIA T4, CPU: Intel Xeon Gold 6130 CPU; SW: pandas v2.2.2, RAPIDS cuDF 24.08</em></p>\n\n\n\n<p>For more information about these benchmark results and how to reproduce them, see the <a href=\"https://docs.rapids.ai/api/cudf/stable/cudf_pandas/benchmarks/\">cuDF benchmarks guide</a><a href=\"https://github.com/rapidsai/db-benchmark/\">.</a> Note that our previous DuckDB benchmarks were run on 0.5 GB and 5 GB, so this would require changing the data generation scripts and data processing scripts to enable the creation of 10 GB benchmarks.</p>\n\n\n\n<h2 id=\"processing_tabular_text_data_at_scale_with_rapids_cudf\"  class=\"wp-block-heading\">Processing tabular text data at scale with RAPIDS cuDF<a href=\"#processing_tabular_text_data_at_scale_with_rapids_cudf\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>In the original release, cuDF supported up to 2.1 billion total characters in a column of text data, which is easily outstripped in common large datasets. Processing product review data, customer service chat logs, or large datasets with substantial locations or user ID data are common examples of how that limitation can be easily met.&nbsp;</p>\n\n\n\n<p>Strings are notoriously slow to execute using CPU-only pandas, making it infeasible to use pandas code to process tabular text data at scale. User feedback indicates that large strings in text-heavy datasets is a use case that could benefit from acceleration. This was originally limited by the design of cuDF.</p>\n\n\n\n<p>cuDF can now process up to 2.1 billion rows of tabular text data using pandas code on the GPU by significantly increasing the number of characters supported in strings. This makes pandas a viable tool for data preparation in generative AI pipelines.</p>\n\n\n\n<h2 id=\"get_started\"  class=\"wp-block-heading\">Get started<a href=\"#get_started\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>All these features are available with RAPIDS 24.08, which you can download from the <a href=\"https://docs.rapids.ai/install\">RAPIDS Installation Guide</a>. Note that the unified memory feature only works on Linux-based systems.</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>To experience unified memory, try the <a href=\"https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/accelerated_data_processing_examples/cudf_pandas_single_gpu_large_dataset.ipynb\">Unified Memory demo notebook</a>, which explains more of how the expanded memory capability works.&nbsp;</li>\n\n\n\n<li>To test a large strings preprocessing workflow, check out the <a href=\"https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/accelerated_data_processing_examples/cudf_pandas_large_string.ipynb\">Job Postings demo notebook</a>.</li>\n</ul>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA has released RAPIDS cuDF unified memory and text data processing features that help data scientists continue to use pandas when working with larger and text-heavy datasets in demanding workloads. Data scientists can now accelerate these workloads by up to 30x. RAPIDS is a collection of open-source GPU-accelerated data science and AI libraries. cuDF is &hellip; <a href=\"https://developer.nvidia.com/blog/rapids-cudf-unified-memory-accelerates-pandas-up-to-30x-on-large-datasets/\">Continued</a></p>\n", "protected": false}, "author": 1425, "featured_media": 87024, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1464992", "discourse_permalink": "https://forums.developer.nvidia.com/t/rapids-cudf-unified-memory-accelerates-pandas-up-to-30x-on-large-datasets/302936", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696], "tags": [3273, 453, 1731], "coauthors": [2878], "class_list": ["post-87019", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-data-science", "tag-accelerated-data-analytics", "tag-featured", "tag-pandas"], "acf": {"post_industry": ["Financial Services"], "post_products": ["cuDF", "RAPIDS"], "post_learning_levels": ["General Interest"], "post_content_types": ["News"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/laptop-data-science.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mDx", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Data Science", "link": "https://developer.nvidia.com/blog/category/data-science/", "id": 696}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87019"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1425"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=87019"}], "version-history": [{"count": 12, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87019/revisions"}], "predecessor-version": [{"id": 87052, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/87019/revisions/87052"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/87024"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=87019"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=87019"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=87019"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=87019"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 86868, "date": "2024-08-08T09:30:00", "date_gmt": "2024-08-08T16:30:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=86868"}, "modified": "2024-08-28T10:14:55", "modified_gmt": "2024-08-28T17:14:55", "slug": "improving-gpu-performance-by-reducing-instruction-cache-misses-2", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/improving-gpu-performance-by-reducing-instruction-cache-misses-2/", "title": {"rendered": "Improving GPU Performance by Reducing Instruction Cache Misses"}, "content": {"rendered": "\n<p>GPUs are specially designed to crunch through massive amounts of data at high speed. They have a large amount of compute resources, called streaming multiprocessors (SMs), and an array of facilities to keep them fed with data: high bandwidth to memory, sizable data caches, and the capability to switch to other teams of workers (warps) without any overhead if an active team has run out of data.\u202f&nbsp;</p>\n\n\n\n<p>Yet data starvation may still occur, and much of code optimization focuses on that issue. In some cases, \u200cSMs are starved not for data, but for instructions. This post presents an investigation of a GPU workload that experiences a slowdown due to instruction cache misses. It describes how to identify this bottleneck, as well as techniques for removing it to improve performance.&nbsp;</p>\n\n\n\n<h2 id=\"recognizing_the_problem&nbsp;\"  class=\"wp-block-heading\">Recognizing the problem&nbsp;<a href=\"#recognizing_the_problem&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The origin of this investigation is an application from the domain of genomics in which many small, independent problems related to aligning small sections of a DNA sample with a reference genome must be solved. The background is the well-known\u202f<a href=\"https://cs.stanford.edu/people/eroberts/courses/soco/projects/computers-and-the-hgp/smith_waterman.html\">Smith-Waterman algorithm</a>\u202f(but that by itself is not important for the discussion).\u202f&nbsp;</p>\n\n\n\n<p>Running the program on a medium-sized dataset on a powerful\u202f<a href=\"https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/\">NVIDIA H100 Hopper</a>\u202fGPU, with 114 SMs, showed good promise. The\u202f<a href=\"https://developer.nvidia.com/nsight-compute\">NVIDIA Nsight Compute</a>\u202ftool, which analyzes a program\u2019s execution on the GPU, confirmed that the SMs were quite busy with useful computations, but there was a snag.&nbsp;</p>\n\n\n\n<p>So many of the small problems composing the overall workload\u2014each handled by its own thread\u2014could be run on the GPU simultaneously that not all compute resources were fully used all the time. This is expressed as a small and non-integral number of waves.&nbsp;</p>\n\n\n\n<p>Work for the GPU is divided into chunks called <em>thread blocks</em>, and one or more can reside on an SM. If some SMs receive fewer thread blocks than others, they will run out of work and must idle while the other SMs continue working.&nbsp;</p>\n\n\n\n<p>Filling up all SMs completely with thread blocks constitutes one wave. NVIDIA Nsight Compute dutifully reports the number of waves per SM. If that number happens to be 100.5, it means that not all SMs have the same amount of work to do and that some are forced to idle. However, the impact of the uneven distribution is not substantial.&nbsp;</p>\n\n\n\n<p>Most of the time, the load on the SMs is balanced. That situation changes if the number of waves is just 0.5, for example. For a much larger percentage of the time, SMs experience an uneven work distribution, which is called the <em>tail effect</em>.&nbsp;</p>\n\n\n\n<h2 id=\"addressing_the_tail_effect&nbsp;&nbsp;\"  class=\"wp-block-heading\">Addressing the tail effect&nbsp;&nbsp;<a href=\"#addressing_the_tail_effect&nbsp;&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>This phenomenon is exactly what materialized with the genomics workload. The number of waves was just 1.6. The obvious solution is to give the GPU more work to do (more threads, leading to more warps of 32 threads each), which is usually not a problem.&nbsp;</p>\n\n\n\n<p>The original workload was relatively modest, and in a practical environment, larger problems must be completed. However, increasing the original workload by doubling, tripling, and quadrupling the number of subproblems saw performance deteriorate rather than improve. What could cause this outcome?&nbsp;</p>\n\n\n\n<p>The combined NVIDIA Nsight Compute report of those four workload sizes sheds light on the situation. In the section called <strong>Warp State</strong>, which lists the reasons threads cannot make progress, the value for <strong>No Instruction</strong> stands out for increasing significantly with workload size (Figure 1).\u202f&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"871\" height=\"438\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-four-workloads.png\" alt=\"Screenshot of of bar chart, where the value for \u201cNo Instruction\u201d is causing the most stalls.\u00a0\" class=\"wp-image-86930\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-four-workloads.png 871w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-four-workloads-300x151.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-four-workloads-625x314.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-four-workloads-179x90.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-four-workloads-768x386.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-four-workloads-645x324.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-four-workloads-500x251.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-four-workloads-160x80.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-four-workloads-362x182.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-four-workloads-219x110.png 219w\" sizes=\"(max-width: 871px) 100vw, 871px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Warp stall reasons for four workload sizes from the combined NVIDIA Nsight Compute report</em></figcaption></figure></div>\n\n\n<p><strong>No Instruction</strong> means the SMs could not be fed instructions fast enough from memory. <strong>Long Scoreboard</strong> indicates that the SMs could not be fed data fast enough from memory. Fetching instructions on time is so critical that the GPU provides a number of stations where instructions can be placed after they have been fetched to keep them close to the SMs. Those stations are called <em>instruction caches</em>, and there are even more levels of them than data caches.&nbsp;</p>\n\n\n\n<p>The fact that instruction cache misses apparently increase so quickly, as evidenced by the quick growth of warp stalls due to <strong>No Instruction</strong>, implies the following:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Not all instructions in the busiest part of the code fit in that cache.</li>\n\n\n\n<li>The need for more different instructions increases as the workload size increases.&nbsp;</li>\n</ul>\n\n\n\n<p>The reason for the latter is somewhat subtle. Multiple thread blocks, composed of warps, reside on the SM simultaneously, but not all warps execute simultaneously. The SM is internally divided into four partitions, each of which can generally execute one warp instruction per clock cycle.&nbsp;</p>\n\n\n\n<p>When a warp is stalled for any reason, another warp that also resides on the SM can take over. Each warp can execute its own instruction stream independently from other warps.&nbsp;</p>\n\n\n\n<p>At the start of the main kernel in this program, warps running on each SM are mostly in sync. They start with the first instruction and keep chugging along.\u202fHowever, they are not explicitly synchronizing.&nbsp;</p>\n\n\n\n<p>As time goes on and warps take turns idling and executing, they drift further and further apart in terms of the instructions that they execute. This means that a growing set of different instructions must be active as the execution progresses, and this, in turn, means that the instruction cache overflows more frequently. Instruction cache pressure builds, and more misses occur.&nbsp;</p>\n\n\n\n<h2 id=\"solving_the_problem&nbsp;\"  class=\"wp-block-heading\">Solving the problem&nbsp;<a href=\"#solving_the_problem&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The gradual drifting apart of the warp instruction streams cannot be controlled, except by synchronizing the streams. But synchronization typically reduces performance, because it requires warps to wait for each other when there is no fundamental need.&nbsp;</p>\n\n\n\n<p>However, you can attempt to decrease the overall instruction footprint so that spilling from the instruction cache occurs less frequently, and perhaps not at all.\u00a0</p>\n\n\n\n<p>The code in question contains a collection of nested loops, and most of the loops are unrolled. Unrolling improves performance by enabling the compiler to do the following:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Reorder (independent) instructions for better scheduling.</li>\n\n\n\n<li>Eliminate some instructions that can be shared by successive iterations of the loop.</li>\n\n\n\n<li>Reduce branching.&nbsp;</li>\n\n\n\n<li>Allocate different registers to the same variable referenced in different iterations of the loop, to avoid having to wait for specific registers to become available.</li>\n</ul>\n\n\n\n<p>Unrolling loops brings many benefits, but it does increase the number of instructions. It also tends to increase the number of registers used, which may depress performance, because fewer warps may reside on the SMs simultaneously. This reduced warp occupancy comes with less latency hiding.&nbsp;</p>\n\n\n\n<p>The two outermost loops of the kernel are the focus. Practical unrolling is best left to the compiler, which has myriad heuristics to generate good code. That is, the user expresses that there is an expected benefit of unrolling by using hints, called <em>pragmas</em> in C/C++, before the top of the loop.&nbsp;</p>\n\n\n\n<p>These take the following form:&nbsp;</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\n#pragma unroll X\u00a0\n</pre></div>\n\n\n<p>Where\u202f<code>X</code>\u202fcan be blank (canonical unroll), the compiler is only told that unrolling may be beneficial but is not given any suggestion of how many iterations to unroll.</p>\n\n\n\n<p>For convenience, we\u2019ve adopted the following notation for unroll factors:&nbsp;</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>0 = No unroll pragma at all.</li>\n\n\n\n<li>1 = An unroll pragma without any number (canonical).&nbsp;</li>\n\n\n\n<li><code>n</code>\u202flarger than 1 = A positive number that suggests unrolling in groups of\u202f<code>n</code>\u202fiterations.&nbsp;</li>\n</ul>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\n#pragma unroll (n)\u00a0\n</pre></div>\n\n\n<p>The next experiment comprises a suite of runs in which the\u202funroll factor varies between 0 and 4 for both levels of the two outermost loops in the code, producing a performance figure for each of the four workload sizes. Unrolling by more is not needed, because experiments show that the compiler does not generate different code for higher unroll factors for this particular program. Figure 2 shows the outcome of the suite.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"706\" height=\"415\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/smith-waterman-code-performance.png\" alt=\"A graph plotting code performance for each of the workload sizes. For each instance of unroll factors, the size of the executable is shown\u00a0 in units of 500 KB.\" class=\"wp-image-86932\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/smith-waterman-code-performance.png 706w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/smith-waterman-code-performance-300x176.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/smith-waterman-code-performance-625x367.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/smith-waterman-code-performance-179x105.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/smith-waterman-code-performance-645x379.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/smith-waterman-code-performance-500x294.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/smith-waterman-code-performance-153x90.png 153w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/smith-waterman-code-performance-362x213.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/smith-waterman-code-performance-187x110.png 187w\" sizes=\"(max-width: 706px) 100vw, 706px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2.\u202fPerformance of Smith-Waterman code for different workload sizes and different loop unroll factors</em></figcaption></figure></div>\n\n\n<p>The top horizontal axis shows unroll factors for the outermost loop (top-level). The bottom horizontal axis shows unroll factors for the second-level loop. Each point on any of the four performance curves (higher is better) corresponds to two unroll factors, one for each of the outermost loops as indicated on the horizontal axes.\u202f&nbsp;</p>\n\n\n\n<p>Figure 2 also shows, for each instance of unroll factors, the size of the executable in units of 500 KB. While the expectation might be to see increasing executable sizes with each higher level of unrolling, that is not consistently the case. Unroll pragmas are hints that may be ignored by the compiler if they are not deemed beneficial.&nbsp;</p>\n\n\n\n<p>Measurements corresponding to the initial version of the code (indicated by the ellipse labeled A) are for the canonical unrolling of the top-level loop, and no unrolling of the second-level loop. The anomalous behavior of the code is apparent, where larger workload sizes lead to poorer performance, due to increasing instruction cache misses.\u202f\u00a0</p>\n\n\n\n<p>In the next isolated experiment (indicated by the ellipse labeled B), attempted before the full suite of runs, neither of the outermost loops is unrolled. Now the anomalous behavior is gone and larger workload sizes lead to the expected better performance. </p>\n\n\n\n<p>However, absolute performance is reduced, especially for the original workload size.\u00a0Two phenomena, revealed by NVIDIA Nsight Compute, help explain this result. Due to the smaller instruction memory footprint, instruction cache misses have reduced for all sizes of the workload, which can be deduced from the fact that the <strong>No Instruction</strong> warp stalls (not depicted) have dropped to virtually negligible values. However, the compiler assigned a relatively large number of registers to each thread, such that the number of warps that can reside on the SM is not optimal.\u202f\u00a0</p>\n\n\n\n<p>Doing the full sweep of unroll factors suggests that the experiment in the ellipse labeled C is the proverbial sweet spot. It corresponds to no unrolling of the top-level loop, and unrolling by a factor of 2 of the second-level loop. NVIDIA Nsight Compute still shows negligible values for <strong>No Instruction</strong> warp stalls (Figure 3) and a reduced number of registers per thread, such that more warps can fit on the SM than in experiment B, leading to more latency hiding.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"871\" height=\"522\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-optimal-unroll-factors.png\" alt=\"Bar chart of the Warp State graph, with negligible values for No Instruction warp stalls.\u00a0\" class=\"wp-image-86933\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-optimal-unroll-factors.png 871w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-optimal-unroll-factors-300x180.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-optimal-unroll-factors-625x375.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-optimal-unroll-factors-179x107.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-optimal-unroll-factors-768x460.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-optimal-unroll-factors-645x387.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-optimal-unroll-factors-500x300.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-optimal-unroll-factors-150x90.png 150w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-optimal-unroll-factors-362x217.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/warp-stall-reasons-optimal-unroll-factors-184x110.png 184w\" sizes=\"(max-width: 871px) 100vw, 871px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Warp stall reasons for four workload sizes from the combined NVIDIA Nsight Compute report for optimal unroll factors</em></figcaption></figure></div>\n\n\n<p>While the absolute performance of the smallest workload still lags behind that of experiment A, the difference is not much, and larger workloads fare increasingly better, leading to the best average performance across all workload sizes.&nbsp;</p>\n\n\n\n<p>Further inspection of the NVIDIA Nsight Compute reports for the three different unrolling scenarios (A, B, and C) elucidates the performance results.&nbsp;</p>\n\n\n\n<p>Total instruction memory footprint sizes, as shown by the dashed line in Figure 2, are not accurate measures of instruction cache pressure, because they may include code sections that are only executed a small number of times. It is better to study the aggregate sizes of the \u201chottest\u201d parts of the code, which can be identified by looking for the maximum values of the <strong>Instructions Executed</strong> metric in the source view of NVIDIA Nsight Compute. </p>\n\n\n\n<p>For scenarios A, B, and C these sizes are 39360, 15680, and 16912, respectively.&nbsp;Clearly, scenarios B and C have much reduced hot instruction memory footprints compared to scenario A, leading to less instruction cache pressure.&nbsp;</p>\n\n\n\n<h2 id=\"conclusion&nbsp;\"  class=\"wp-block-heading\">Conclusion&nbsp;<a href=\"#conclusion&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Instruction cache misses can cause performance degradation for kernels that have a large instruction footprint, which is often caused by substantial loop unrolling. When the compiler is put in charge of unrolling through pragmas, the heuristics it applies to the code to determine the best actual level of unrolling are necessarily complicated and are not always predictable by the programmer.&nbsp;</p>\n\n\n\n<p>It may pay to experiment with different compiler hints regarding loop unrolling to arrive at the optimal code with good warp occupancy and reduced instruction cache misses.&nbsp;</p>\n\n\n\n<p>Get started with <a href=\"http://developer.nvidia.com/nsight-compute\">Nsight Compute</a> today. For more information and tutorials, see <a href=\"http://developer.nvidia.com/tools-tutorials\">Nsight Developer Tools Tutorials</a>.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>GPUs are specially designed to crunch through massive amounts of data at high speed. They have a large amount of compute resources, called streaming multiprocessors (SMs), and an array of facilities to keep them fed with data: high bandwidth to memory, sizable data caches, and the capability to switch to other teams of workers (warps) &hellip; <a href=\"https://developer.nvidia.com/blog/improving-gpu-performance-by-reducing-instruction-cache-misses-2/\">Continued</a></p>\n", "protected": false}, "author": 1202, "featured_media": 75549, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1464254", "discourse_permalink": "https://forums.developer.nvidia.com/t/improving-gpu-performance-by-reducing-instruction-cache-misses/302753", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 696, 503, 1903], "tags": [23, 453, 529, 126, 26, 527], "coauthors": [2498, 2319], "class_list": ["post-86868", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-data-science", "category-simulation-modeling-design", "category-features", "tag-cuda-c", "tag-featured", "tag-nsight-tools-compute", "tag-optimization", "tag-debugging", "tag-programming-languages-and-compilers"], "acf": {"post_industry": ["HPC / Scientific Computing"], "post_products": ["CUDA", "Nsight Tools - Compute"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hpc-featured.jpg", "jetpack_shortlink": "https://wp.me/pcCQAL-mB6", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86868"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1202"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=86868"}], "version-history": [{"count": 5, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86868/revisions"}], "predecessor-version": [{"id": 88155, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86868/revisions/88155"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75549"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=86868"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=86868"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=86868"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=86868"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}, {"id": 86390, "date": "2024-08-08T09:00:00", "date_gmt": "2024-08-08T16:00:00", "guid": {"rendered": "https://developer.nvidia.com/blog/?p=86390"}, "modified": "2024-08-22T11:25:34", "modified_gmt": "2024-08-22T18:25:34", "slug": "performant-quantum-programming-even-easier-with-nvidia-cuda-q-v0-8", "status": "publish", "type": "post", "link": "https://developer.nvidia.com/blog/performant-quantum-programming-even-easier-with-nvidia-cuda-q-v0-8/", "title": {"rendered": "Performant Quantum Programming Even Easier with NVIDIA CUDA-Q v0.8"}, "content": {"rendered": "\n<p><a href=\"https://developer.nvidia.com/cuda-q\">NVIDIA CUDA-Q</a> (formerly NVIDIA CUDA Quantum) is an open-source programming model for building hybrid-quantum classical applications that take full advantage of CPU, GPU, and <a href=\"https://blogs.nvidia.com/blog/what-is-a-qpu/\">QPU</a> compute abilities. Developing these applications today is challenging and requires a flexible, easy-to-use coding environment coupled with powerful quantum simulation capabilities to efficiently evaluate and improve the performance of new algorithms.&nbsp;</p>\n\n\n\n<p>CUDA-Q is a platform built specifically with this in mind. Its ability to effortlessly switch between simulation and a broad range of actual QPU hardware backends makes it a long-term solution to quantum application development. The recent v0.8 release further improves the CUDA-Q simulation performance, developer experience, and flexibility.&nbsp;</p>\n\n\n\n<p>In this post, we discuss some of the top highlights from <a href=\"https://github.com/NVIDIA/cuda-quantum/releases/tag/0.8.0\" target=\"_blank\" rel=\"noreferrer noopener\">CUDA-Q v0.8</a>:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>State handling</li>\n\n\n\n<li>Pauli words</li>\n\n\n\n<li>Custom unitary operations</li>\n\n\n\n<li>Visualization tools</li>\n\n\n\n<li>NVIDIA Grace Hopper integration</li>\n</ul>\n\n\n\n<h2 id=\"state_handling&nbsp;\"  class=\"wp-block-heading\">State handling&nbsp;<a href=\"#state_handling&nbsp;\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Quantum state preparation is a core component of useful quantum algorithms, but it is often one of the most complex and expensive elements to simulate. Keeping the same state in memory and reusing it can optimize simulations when the state is used multiple times with different parameters or following classical pre\u2013 and post-processing steps.</p>\n\n\n\n<p>You can now construct CUDA-Q kernels based on a provided state vector. The following code example shows this by specifying a 4-entry state vector corresponding to a 2-qubit state.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nc = &#x5B;.707, 0, 0, .707]\n\n\n@cudaq.kernel\ndef initial_state():\n    q = cudaq.qvector(c)\n</pre></div>\n\n\n<p>Following the execution of a quantum circuit, the state can now be retained on GPU memory and easily passed to future kernels using optimized memory transfer tools.</p>\n\n\n\n<p>The state-handling tools can be especially helpful for quantum algorithms that are recursive or iterative in nature.&nbsp;</p>\n\n\n\n<p>Figure 1 represents an example case where subsequent quantum operations <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=U_%7Bn%2B1%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"U_{n+1}\" class=\"latex\" /> (green boxes) are determined classically based on measurement results from the previous state (<img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Cpsi_n&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;psi_n\" class=\"latex\" />).&nbsp;</p>\n\n\n\n<p>Without state handling, the <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=n%2B1&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"n+1\" class=\"latex\" /> step would require the simulation of all previous operations (gray boxes) to produce <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Cpsi_n&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;psi_n\" class=\"latex\" /><em><sub> </sub></em>&nbsp;before <img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=U_%7Bn%2B1%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"U_{n+1}\" class=\"latex\" /> is applied and the process continues.&nbsp;</p>\n\n\n\n<p>Retaining the previous state in GPU memory eliminates the need to perform all previous operations, significantly boosting performance.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"546\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm-1024x546.png\" alt=\"Diagram of a four-step algorithm shows that state handling can significantly boost the performance of quantum simulations by making the simulation of previous operations unnecessary.\" class=\"wp-image-86394\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm-1024x546.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm-300x160.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm-625x333.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm-179x95.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm-768x410.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm-1536x819.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm-645x344.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm-500x267.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm-160x85.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm-362x193.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm-206x110.png 206w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/recursive-quantum-algorithm.png 1564w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Schematic of recursive quantum algorithm</em></figcaption></figure></div>\n\n\n<p>Simulating Trotter dynamics is a great quantitative example of a state-handling performance boost. In a <a href=\"https://github.com/NVIDIA/cuda-quantum/blob/main/docs/sphinx/examples/python/trotter_kernel_mode.py\">25-qubit benchmark of a Heisenberg Hamiltonian simulation</a> (Figure 2), saving the previous state means each step takes approximately the same amount of time. Without state handling, the steps become increasingly onerous. For a simulation with 100 steps, state handling results in an impressive 24x faster total simulation time.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"510\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/trotter-simulation-1024x510.png\" alt=\"Trotter simulation steps generally become more onerous, but state handling enables each step to restart from the previous step and provides a 24x speedup.\" class=\"wp-image-86395\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/trotter-simulation-1024x510.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/trotter-simulation-300x149.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/trotter-simulation-625x311.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/trotter-simulation-179x89.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/trotter-simulation-768x382.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/trotter-simulation-645x321.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/trotter-simulation-500x249.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/trotter-simulation-160x80.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/trotter-simulation-362x180.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/trotter-simulation-221x110.png 221w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/trotter-simulation.png 1474w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Trotter simulation of a Heisenberg Hamiltonian simulation</em></figcaption></figure></div>\n\n\n<h2 id=\"pauli_words\"  class=\"wp-block-heading\">Pauli words<a href=\"#pauli_words\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Pauli words are tensor products of the single-qubit X, Y, Z, I Pauli operators (P). Exponentiating these operators (<img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=e%5E%7B-i%5Ctheta+P%7D&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"e^{-i&#92;theta P}\" class=\"latex\" />) results in single qubit rotation gates: <em>R</em><sub>x</sub>(<img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;theta\" class=\"latex\" />), <em>R</em><sub>y</sub>(<img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;theta\" class=\"latex\" />), <em>R</em><sub>z</sub>(<img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=transparent&#038;fg=000&#038;s=0&#038;c=20201002\" alt=\"&#92;theta\" class=\"latex\" />).</p>\n\n\n\n<p>Certain algorithms require more complex operations derived from the exponentiation of whole Pauli words.</p>\n\n\n\n<p>CUDA-Q v0.8 now includes a new <code>pauli_word</code> type that can be input to a quantum kernel and converted into a quantum circuit operation with <code>exp_pauli</code>.&nbsp; The following code example shows how a list of Pauli words and their associated coefficients can be used to apply the following operation:&nbsp;</p>\n\n\n\n<p class=\"has-text-align-center\"><img decoding=\"async\" src=\"https://s0.wp.com/latex.php?latex=e%5E%7Bi%280.432XYZ+%2B+0.324IXX%29%7D&#038;bg=transparent&#038;fg=000&#038;s=2&#038;c=20201002\" alt=\"e^{i(0.432XYZ + 0.324IXX)}\" class=\"latex\" /></p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nwords = &#x5B;&#039;XYZ&#039;, &#039;IXX&#039;]\ncoefficients = &#x5B;0.432, 0.324]\n\n\n@cudaq.kernel\ndef kernel(coefficients: list&#x5B;float], words: list&#x5B;cudaq.pauli_word]):\n    q = cudaq.qvector(3)\n\n    for i in range(len(coefficients)):\n        exp_pauli(coefficients&#x5B;i], q, words&#x5B;i])\n</pre></div>\n\n\n<p>The Pauli words and their respective coefficients are input as lists to the kernel and then transformed into a quantum circuit operation with <code>exp_pauli</code>. The Trotter simulation example discussed in the previous section makes use of this feature for Hamiltonian simulation. For more information, see the <a href=\"https://github.com/NVIDIA/cuda-quantum/blob/main/docs/sphinx/examples/python/trotter_kernel_mode.py\">trotter_kernel_mode.py</a> script on GitHub.</p>\n\n\n\n<h2 id=\"custom_unitary_operations\"  class=\"wp-block-heading\">Custom unitary operations<a href=\"#custom_unitary_operations\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Using a unitary operation is sometimes preferred to gates when designing quantum algorithms that are more abstract, have oracles, or have no exactly known gate set.&nbsp;</p>\n\n\n\n<p>You can now execute <a href=\"https://nvidia.github.io/cuda-quantum/latest/api/default_ops.html#user-defined-custom-operations\" target=\"_blank\" rel=\"noreferrer noopener\">custom unitary operations</a> within CUDA-Q kernels. The following code example shows how to specify a custom unitary operation as a NumPy array, name it, and then use it in a kernel. A custom standard X gate is specified as a 2&#215;2 unitary matrix with rows <code>[0,1]</code> and <code>[1,0]</code>. The example also demonstrates how custom unitaries can be applied using a controlled operation of one or more qubits.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nimport numpy as np\n\ncudaq.register_operation(&quot;custom_x&quot;, np.array(&#x5B;0, 1, 1, 0]))\n\n\n@cudaq.kernel\ndef kernel():\n    qubits = cudaq.qvector(2)\n    h(qubits&#x5B;0])\n    custom_x(qubits&#x5B;0])\n    custom_x.ctrl(qubits&#x5B;0], qubits&#x5B;1])\n\n\ncounts = cudaq.sample(kernel)\ncounts.dump()\n</pre></div>\n\n\n<h2 id=\"visualization_tools\"  class=\"wp-block-heading\">Visualization tools<a href=\"#visualization_tools\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>Visualization tools are invaluable for learning quantum computing concepts, designing algorithms, and collaborating on research. Participants in the 2024 Unitary Hack event contributed enhanced quantum-circuit and Bloch sphere visualization to CUDA-Q.</p>\n\n\n\n<p>Any kernel can be visualized using the <code>print(cudaq.draw(kernel))</code> command. By default, an ASCII representation prints in the terminal (Figure 4).</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"413\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/quantum-circuits-print-to-terminal-625x413.png\" alt=\"CUDA-Q ASCII representation of a quantum circuit.\" class=\"wp-image-86396\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/quantum-circuits-print-to-terminal-625x413.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/quantum-circuits-print-to-terminal-300x198.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/quantum-circuits-print-to-terminal-174x115.png 174w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/quantum-circuits-print-to-terminal-768x508.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/quantum-circuits-print-to-terminal-645x426.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/quantum-circuits-print-to-terminal-454x300.png 454w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/quantum-circuits-print-to-terminal-136x90.png 136w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/quantum-circuits-print-to-terminal-362x239.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/quantum-circuits-print-to-terminal-166x110.png 166w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/quantum-circuits-print-to-terminal.png 980w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. CUDA-Q can print quantum circuits directly to the terminal</em></figcaption></figure></div>\n\n\n<p><a href=\"https://en.wikipedia.org/wiki/LaTeX\">LaTeX</a> is a text preparation and typesetting tool often used to prepare scientific publications. The following command prints a LaTeX string:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\nprint(cudaq.draw(\u2018latex\u2019, kernel))\n</pre></div>\n\n\n<p>To prepare a typeset quantum circuit, copy the string into any LaTeX editor (such as <a href=\"https://www.overleaf.com\">Overleaf</a>) and then export it to PDF.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full-page-width\"><a href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit.png\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"804\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit-1024x804.png\" alt=\"Diagram shows a LaTeX expression using the quantkz package and the resulting visual diagram of quantum circuits.\" class=\"wp-image-86397\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit-1024x804.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit-300x236.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit-625x491.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit-146x115.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit-768x603.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit-645x506.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit-382x300.png 382w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit-115x90.png 115w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit-362x284.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit-140x110.png 140w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/cuda-q-latex-quantum-circuit.png 1164w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\"><em>Figure 5. CUDA-Q quantum circuit in LaTeX editor</em></figcaption></figure></div>\n\n\n<p>CUDA-Q v0.8 now also uses QuTip, an open-source Python package for dynamics simulations, to visualize Bloch spheres corresponding to single-qubit states. Figure 6 shows a detailed example of visualizing one or more Bloch spheres. For more information, see <a href=\"https://nvidia.github.io/cuda-quantum/latest/examples/python/tutorials/visualization.html\" target=\"_blank\" rel=\"noreferrer noopener\">Qubit Visualization</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"625\" height=\"281\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q-625x281.png\" alt=\"Image shows two Bloch spheres.\" class=\"wp-image-86398\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q-625x281.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q-300x135.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q-179x81.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q-768x346.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q-1536x691.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q-645x290.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q-500x225.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q-160x72.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q-362x163.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q-244x110.png 244w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q-1024x461.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/bloch-spheres-cuda-q.png 1618w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. CUDA-Q visualizes Bloch spheres</em></figcaption></figure></div>\n\n\n<h2 id=\"nvidia_grace_hopper_integration\"  class=\"wp-block-heading\">NVIDIA Grace Hopper integration<a href=\"#nvidia_grace_hopper_integration\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>You can now use CUDA-Q to leverage the full performance of the <a href=\"https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/?ncid=so-link-950064-vt04#cid=hpc012_so-link_en-us\">NVIDIA GH200 Superchip</a>, pushing the boundaries of quantum simulation even further. The large memory bandwidth enables significant speedup in simulating quantum systems, with simulations on the GH200 Superchip needing only a quarter of the nodes previously required. This is particularly important for quantum simulations, which are usually bottlenecked by memory.</p>\n\n\n\n<h2 id=\"getting_started_with_cuda-q\"  class=\"wp-block-heading\">Getting started with CUDA-Q<a href=\"#getting_started_with_cuda-q\" class=\"heading-anchor-link\"><i class=\"fas fa-link\"></i></a></h2>\n\n\n\n<p>The continued improvement of CUDA-Q provides you with a more performant platform to build quantum-accelerated supercomputing applications. It\u2019s a more performant simulation platform and applications constructed on CUDA-Q are also positioned to deploy in the future hybrid CPU, GPU, and QPU environments necessary for practical quantum computing.</p>\n\n\n\n<p>For more information, see the following resources:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><a href=\"https://nvidia.github.io/cuda-quantum/latest/using/quick_start.html\">CUDA-Q Quick Start guide</a>: Quickly set up your environment.&nbsp;</li>\n\n\n\n<li><a href=\"https://nvidia.github.io/cuda-quantum/latest/using/basics/basics.html\">CUDA-Q Basics</a>: Write your first CUDA-Q application.</li>\n\n\n\n<li><a href=\"https://nvidia.github.io/cuda-quantum/latest/using/examples/examples.html\">CUDA-Q By Example</a> and <a href=\"https://nvidia.github.io/cuda-quantum/latest/using/tutorials.html\">CUDA-Q Tutorials</a>: Get inspiration for your own quantum application development.&nbsp;</li>\n</ul>\n\n\n\n<p>To provide feedback and suggestions, see the <a href=\"https://github.com/NVIDIA/cuda-quantum\">/NVIDIA/cuda-quantum</a> GitHub repo.</p>\n", "protected": false}, "excerpt": {"rendered": "<p>NVIDIA CUDA-Q (formerly NVIDIA CUDA Quantum) is an open-source programming model for building hybrid-quantum classical applications that take full advantage of CPU, GPU, and QPU compute abilities. Developing these applications today is challenging and requires a flexible, easy-to-use coding environment coupled with powerful quantum simulation capabilities to efficiently evaluate and improve the performance of new &hellip; <a href=\"https://developer.nvidia.com/blog/performant-quantum-programming-even-easier-with-nvidia-cuda-q-v0-8/\">Continued</a></p>\n", "protected": false}, "author": 1477, "featured_media": 86551, "comment_status": "closed", "ping_status": "closed", "sticky": false, "template": "", "format": "standard", "meta": {"_acf_changed": false, "publish_to_discourse": "", "publish_post_category": "318", "wpdc_auto_publish_overridden": "1", "wpdc_topic_tags": "", "wpdc_pin_topic": "", "wpdc_pin_until": "", "discourse_post_id": "1464231", "discourse_permalink": "https://forums.developer.nvidia.com/t/performant-quantum-programming-even-easier-with-nvidia-cuda-q-v0-8/302749", "wpdc_publishing_response": "success", "wpdc_publishing_error": "", "footnotes": "", "_links_to": "", "_links_to_target": ""}, "categories": [852, 503], "tags": [453, 2735], "coauthors": [2935, 3632, 3629, 3645], "class_list": ["post-86390", "post", "type-post", "status-publish", "format-standard", "has-post-thumbnail", "hentry", "category-data-center-cloud", "category-simulation-modeling-design", "tag-featured", "tag-quantum-computing"], "acf": {"post_industry": ["HPC / Scientific Computing"], "post_products": ["CUDA-Q"], "post_learning_levels": ["Intermediate Technical"], "post_content_types": ["Deep dive"], "post_collections": ""}, "jetpack_featured_media_url": "https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/quantumcircuit-1.png", "jetpack_shortlink": "https://wp.me/pcCQAL-mto", "jetpack_likes_enabled": true, "jetpack_sharing_enabled": true, "primary_category": {"category": "Simulation / Modeling / Design", "link": "https://developer.nvidia.com/blog/category/simulation-modeling-design/", "id": 503}, "_links": {"self": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86390"}], "collection": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts"}], "about": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post"}], "author": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1477"}], "replies": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=86390"}], "version-history": [{"count": 21, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86390/revisions"}], "predecessor-version": [{"id": 87062, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/86390/revisions/87062"}], "wp:featuredmedia": [{"embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media/86551"}], "wp:attachment": [{"href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=86390"}], "wp:term": [{"taxonomy": "category", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=86390"}, {"taxonomy": "post_tag", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=86390"}, {"taxonomy": "author", "embeddable": true, "href": "https://developer-blogs.nvidia.com/wp-json/wp/v2/coauthors?post=86390"}], "curies": [{"name": "wp", "href": "https://api.w.org/{rel}", "templated": true}]}}]